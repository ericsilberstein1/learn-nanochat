{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661e825c-20ff-4b37-ae93-3fcbfea939c1",
   "metadata": {},
   "source": [
    "I'll use lambda cloud. Assuming I can create a machine in us-south-2, my storage is already there so I won't need to download data files, scp the tokenizer and model, etc. Here are the instructions from `challenge-25-pretrain-d20/trying-lambda-cloud.ipynb` without the stuff I won't need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bcd79-9da4-4e13-a33a-0698b4a70c61",
   "metadata": {},
   "source": [
    "```\n",
    "ssh ssh ubuntu@[ip]\n",
    "\n",
    "# ssh key for git\n",
    "ssh-keygen -t ed25519 -C \"lambda-cloud\"\n",
    "cat ~/.ssh/id_ed25519.pub\n",
    "copy into github UI (https://github.com/settings/keys)\n",
    "\n",
    "git config --global user.email \"ericsilberstein@gmail.com\"\n",
    "git config --global user.name \"Eric Silberstein\"\n",
    "\n",
    "# clone this repo\n",
    "git clone git@github.com:ericsilberstein1/nanogpt-learning.git\n",
    "\n",
    "# UV\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "echo '. \"$HOME/.cargo/env\"' >> .bashrc\n",
    "\n",
    "echo 'export NANOCHAT_BASE_DIR=\"/home/ubuntu/mynanochat\"' >> .bashrc\n",
    "\n",
    "# in .bashrc add\n",
    "# export WANDB_API_KEY=\"XXX\"\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "cd nanogpt-learning\n",
    "\n",
    "uv sync\n",
    "source .venv/bin/activate\n",
    "\n",
    "# for now until organize this better\n",
    "uv tool install maturin\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "cd -\n",
    "\n",
    "# looks like lambda automatically runs jupyter but for now at least let me run it\n",
    "# in the way I understand\n",
    "uv run jupyter lab --port=7001\n",
    "jupyter server list\n",
    "\n",
    "# ON MY LAPTOP make a tunnel to jupyter\n",
    "ssh -N -L 7001:localhost:7001 ubuntu@[ip]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8858295c-ba9d-402b-8e8c-27e225e97094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 12:55:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ce58-3f42-42c4-a9d0-0c666380abd7",
   "metadata": {},
   "source": [
    "If my calculation in challenge 26 is right, training will be around 10 minutes, so I'll just run everything from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f89b98-57a3-4598-a459-6c338d6a8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417d92a-f58d-44fd-bc9f-3728707219f6",
   "metadata": {},
   "source": [
    "First do a CORE evaluation. This is a sanity check. It should match the final eval from the training in challenge 24.\n",
    "\n",
    "```\n",
    "Step 21400: CORE metric: 0.2084\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca40da2-791a-4935-b059-6778e6e98e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] \n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] *****************************************\n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4425 | centered: 0.2567 | time: 13.38s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1186 | centered: 0.1186 | time: 3.10s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5366 | centered: 0.5366 | time: 27.08s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6469 | centered: 0.5292 | time: 4.13s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3515 | centered: 0.1354 | time: 1.97s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.19s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2301 | centered: 0.0377 | time: 2.13s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6795 | centered: 0.3591 | time: 3.18s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.74s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3745 | centered: 0.3745 | time: 6.83s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4471 | centered: 0.2628 | time: 25.97s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.38s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5280 | centered: 0.0560 | time: 1.60s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1020 | centered: 0.1020 | time: 1.74s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.59s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3583 | centered: 0.3583 | time: 2.10s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.36s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2345 | centered: 0.2345 | time: 21.72s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1813 | centered: 0.1813 | time: 11.87s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5070 | centered: -0.2973 | time: 8.13s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2530 | centered: 0.1782 | time: 41.23s\n",
      "CORE metric: 0.2012\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.25672173500061035,\n",
      "    \"jeopardy\": 0.11856400221586227,\n",
      "    \"bigbench_qa_wikidata\": 0.5365877747535706,\n",
      "    \"arc_easy\": 0.5291806856791178,\n",
      "    \"arc_challenge\": 0.13538110256195068,\n",
      "    \"copa\": 0.3799999952316284,\n",
      "    \"commonsense_qa\": 0.03767403215169905,\n",
      "    \"piqa\": 0.35908591747283936,\n",
      "    \"openbook_qa\": 0.14666668574015299,\n",
      "    \"lambada_openai\": 0.3745391070842743,\n",
      "    \"hellaswag\": 0.26282942295074463,\n",
      "    \"winograd\": 0.28205132484436035,\n",
      "    \"winogrande\": 0.05603790283203125,\n",
      "    \"bigbench_dyck_languages\": 0.10200000554323196,\n",
      "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
      "    \"bigbench_cs_algorithms\": 0.3583333194255829,\n",
      "    \"bigbench_operators\": 0.1666666716337204,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.2345316857099533,\n",
      "    \"coqa\": 0.18126018345355988,\n",
      "    \"boolq\": -0.2972798786665264,\n",
      "    \"bigbench_language_identification\": 0.1782178120775716\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd7c26-88e5-4fed-9522-632bdfdf9803",
   "metadata": {},
   "source": [
    "^ Does not match. That's not good. Will move on and then go back and investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391e2b2-8afb-4c84-9383-c4eed97f29a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be01f3-4a53-415d-b23d-4ede3aae8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53782fff-1b58-4c46-b38b-9473890eda9b",
   "metadata": {},
   "source": [
    "Do the midtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff5d9fb-db15-4459-99d9-80b38495e99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] \n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] *****************************************\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-28-1\n",
      "user_config: {'run': 'challenge-28-1', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 6srdpeb7 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/nanogpt-learning/challenge-28-midtrain-d20/wandb/run-20251119_130245-6srdpeb7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-28-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/6srdpeb7\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
      "Tokens / micro-batch: 524,288\n",
      "Total batch size 524,288 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "README.md: 2.24kB [00:00, 8.45MB/s]\n",
      "data/train-00000-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 117MB/s]\n",
      "data/train-00001-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 142MB/s]\n",
      "data/train-00002-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231M/231M [00:01<00:00, 183MB/s]\n",
      "data/train-00003-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232M/232M [00:01<00:00, 230MB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 48.2M/48.2M [00:00<00:00, 77.6MB/s]\n",
      "Generating train split: 100%|â–ˆ| 460341/460341 [00:03<00:00, 146948.21 examples/s\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 24229/24229 [00:00<00:00, 153715.52 examples/s]\n",
      "README.md: 53.2kB [00:00, 129MB/s]\n",
      "dataset_infos.json: 138kB [00:00, 268MB/s]\n",
      "auxiliary_train/train-00000-of-00001.par(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 7\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 99842/99842 [00:00<00:00, 448670.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 21134.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20971.48 examples/s]\n",
      "README.md: 7.94kB [00:00, 28.9MB/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20933.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20277.28 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19832.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19622.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19552.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19050.59 examples/s]\n",
      "main/train-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆ| 2.31M/2.31M [00:00<00:00, 8.14MB/s]\n",
      "main/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419k/419k [00:00<00:00, 3.07MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 660235.79 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 402260.38 examples/s]\n",
      "[rank5]: Traceback (most recent call last):\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank5]:     return _run_code(code, main_globals, None,\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank5]:     exec(code, run_globals)\n",
      "[rank5]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank5]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank5]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank5]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank5]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank3]:     return _run_code(code, main_globals, None,\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank3]:     exec(code, run_globals)\n",
      "[rank3]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank3]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank3]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank3]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank3]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank1]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank1]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank1]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank7]: Traceback (most recent call last):\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank7]:     return _run_code(code, main_globals, None,\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank7]:     exec(code, run_globals)\n",
      "[rank7]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank7]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank7]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank7]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank7]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank6]: Traceback (most recent call last):\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank6]:     return _run_code(code, main_globals, None,\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank6]:     exec(code, run_globals)\n",
      "[rank6]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank6]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank6]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank6]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank6]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank2]:     return _run_code(code, main_globals, None,\n",
      "[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank2]:     exec(code, run_globals)\n",
      "[rank2]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank2]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank2]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank2]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank2]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "    MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "  File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "    with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank0]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank0]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank0]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank4]: Traceback (most recent call last):\n",
      "[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank4]:     return _run_code(code, main_globals, None,\n",
      "[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank4]:     exec(code, run_globals)\n",
      "[rank4]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank4]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank4]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank4]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank4]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-28-1\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251119_130245-6srdpeb7/logs\u001b[0m\n",
      "[rank3]:[W1119 13:03:08.967626658 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1119 13:03:08.104334164 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank5]:[W1119 13:03:09.522907408 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank1]:[W1119 13:03:09.554252948 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank7]:[W1119 13:03:09.554535389 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank2]:[W1119 13:03:09.614750866 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank4]:[W1119 13:03:09.622136607 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank6]:[W1119 13:03:09.635130744 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1119 13:03:09.341390890 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1119 13:03:10.388000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240428 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240429 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240430 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240432 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240433 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240434 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240435 closing signal SIGTERM\n",
      "E1119 13:03:11.306000 240329 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 3 (pid: 240431) of binary: /home/ubuntu/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_mid_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-19_13:03:10\n",
      "  host      : 192-222-53-251\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 240431)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bff3e2f-c79e-4e16-9908-3a052f8d1d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2235k  100 2235k    0     0  3982k      0 --:--:-- --:--:-- --:--:-- 3977k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o $NANOCHAT_BASE_DIR/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866b1c4d-c1f6-480e-9587-abeac67bc6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] \n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-28-2\n",
      "user_config: {'run': 'challenge-28-2', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/nanogpt-learning/challenge-28-midtrain-d20/wandb/run-20251119_130651-hiwg9hf8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-28-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hiwg9hf8\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
      "Tokens / micro-batch: 524,288\n",
      "Total batch size 524,288 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "downloading https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt...\n",
      "downloaded to /home/ubuntu/mynanochat/words_alpha.txt\n",
      "all/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50M/3.50M [00:00<00:00, 8.02MB/s]\n",
      "all/validation-00000-of-00001.parquet: 100%|â–ˆ| 408k/408k [00:00<00:00, 2.48MB/s]\n",
      "all/dev-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76.5k/76.5k [00:00<00:00, 528kB/s]\n",
      "all/auxiliary_train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 6\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 14042/14042 [00:00<00:00, 656169.00 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 1531/1531 [00:00<00:00, 416573.43 examples/\n",
      "Generating dev split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 163705.37 examples/s]\n",
      "Generating auxiliary_train split: 100%|â–ˆ| 99842/99842 [00:00<00:00, 473652.68 ex\n",
      "step 00000 | Validation bpb: 0.6856\n",
      "step 00001 (0.23%) | loss: 1.400719 | lrm: 1.00 | dt: 51538.59ms | tok/sec: 10,172 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002 (0.37%) | loss: 1.747523 | lrm: 1.00 | dt: 647.26ms | tok/sec: 810,012 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003 (0.51%) | loss: 1.902251 | lrm: 1.00 | dt: 461.36ms | tok/sec: 1,136,399 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004 (0.63%) | loss: 1.963819 | lrm: 1.00 | dt: 528.19ms | tok/sec: 992,610 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005 (0.75%) | loss: 1.975641 | lrm: 1.00 | dt: 462.28ms | tok/sec: 1,134,126 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006 (0.86%) | loss: 1.975919 | lrm: 1.00 | dt: 468.65ms | tok/sec: 1,118,722 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007 (0.99%) | loss: 1.967225 | lrm: 1.00 | dt: 532.39ms | tok/sec: 984,785 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008 (1.12%) | loss: 1.970013 | lrm: 1.00 | dt: 525.83ms | tok/sec: 997,069 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009 (1.24%) | loss: 1.955470 | lrm: 1.00 | dt: 463.13ms | tok/sec: 1,132,050 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 (1.36%) | loss: 1.926862 | lrm: 1.00 | dt: 470.36ms | tok/sec: 1,114,650 | mfu: -1.00 | total time: 0.00m\n",
      "step 00011 (1.48%) | loss: 1.921186 | lrm: 1.00 | dt: 521.07ms | tok/sec: 1,006,172 | mfu: -1.00 | total time: 0.01m\n",
      "step 00012 (1.60%) | loss: 1.905569 | lrm: 1.00 | dt: 464.75ms | tok/sec: 1,128,096 | mfu: -1.00 | total time: 0.02m\n",
      "step 00013 (1.72%) | loss: 1.892437 | lrm: 1.00 | dt: 525.61ms | tok/sec: 997,483 | mfu: -1.00 | total time: 0.03m\n",
      "step 00014 (1.85%) | loss: 1.875717 | lrm: 1.00 | dt: 467.57ms | tok/sec: 1,121,300 | mfu: -1.00 | total time: 0.03m\n",
      "step 00015 (1.99%) | loss: 1.863180 | lrm: 1.00 | dt: 471.09ms | tok/sec: 1,112,926 | mfu: -1.00 | total time: 0.04m\n",
      "step 00016 (2.12%) | loss: 1.848839 | lrm: 1.00 | dt: 539.35ms | tok/sec: 972,078 | mfu: -1.00 | total time: 0.05m\n",
      "step 00017 (2.25%) | loss: 1.820703 | lrm: 1.00 | dt: 528.39ms | tok/sec: 992,242 | mfu: -1.00 | total time: 0.06m\n",
      "step 00018 (2.37%) | loss: 1.803278 | lrm: 1.00 | dt: 465.03ms | tok/sec: 1,127,426 | mfu: -1.00 | total time: 0.07m\n",
      "step 00019 (2.50%) | loss: 1.787878 | lrm: 1.00 | dt: 531.31ms | tok/sec: 986,782 | mfu: -1.00 | total time: 0.08m\n",
      "step 00020 (2.62%) | loss: 1.774139 | lrm: 1.00 | dt: 463.80ms | tok/sec: 1,130,409 | mfu: -1.00 | total time: 0.08m\n",
      "step 00021 (2.75%) | loss: 1.758026 | lrm: 1.00 | dt: 585.14ms | tok/sec: 896,011 | mfu: -1.00 | total time: 0.09m\n",
      "step 00022 (2.88%) | loss: 1.735044 | lrm: 1.00 | dt: 527.05ms | tok/sec: 994,759 | mfu: -1.00 | total time: 0.10m\n",
      "step 00023 (2.98%) | loss: 1.713869 | lrm: 1.00 | dt: 464.04ms | tok/sec: 1,129,835 | mfu: -1.00 | total time: 0.11m\n",
      "step 00024 (3.09%) | loss: 1.705340 | lrm: 1.00 | dt: 471.60ms | tok/sec: 1,111,714 | mfu: -1.00 | total time: 0.12m\n",
      "step 00025 (3.21%) | loss: 1.720073 | lrm: 1.00 | dt: 537.52ms | tok/sec: 975,375 | mfu: -1.00 | total time: 0.13m\n",
      "step 00026 (3.34%) | loss: 1.720567 | lrm: 1.00 | dt: 465.41ms | tok/sec: 1,126,497 | mfu: -1.00 | total time: 0.13m\n",
      "step 00027 (3.47%) | loss: 1.718257 | lrm: 1.00 | dt: 472.27ms | tok/sec: 1,110,146 | mfu: -1.00 | total time: 0.14m\n",
      "step 00028 (3.58%) | loss: 1.706081 | lrm: 1.00 | dt: 529.63ms | tok/sec: 989,919 | mfu: -1.00 | total time: 0.15m\n",
      "step 00029 (3.69%) | loss: 1.698468 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,582 | mfu: -1.00 | total time: 0.16m\n",
      "step 00030 (3.83%) | loss: 1.693783 | lrm: 1.00 | dt: 526.15ms | tok/sec: 996,466 | mfu: -1.00 | total time: 0.17m\n",
      "step 00031 (3.95%) | loss: 1.690893 | lrm: 1.00 | dt: 464.81ms | tok/sec: 1,127,973 | mfu: -1.00 | total time: 0.17m\n",
      "step 00032 (4.08%) | loss: 1.677007 | lrm: 1.00 | dt: 475.18ms | tok/sec: 1,103,339 | mfu: -1.00 | total time: 0.18m\n",
      "step 00033 (4.20%) | loss: 1.689859 | lrm: 1.00 | dt: 475.51ms | tok/sec: 1,102,590 | mfu: -1.00 | total time: 0.19m\n",
      "step 00034 (4.33%) | loss: 1.681781 | lrm: 1.00 | dt: 472.77ms | tok/sec: 1,108,970 | mfu: -1.00 | total time: 0.20m\n",
      "step 00035 (4.43%) | loss: 1.673353 | lrm: 1.00 | dt: 473.36ms | tok/sec: 1,107,577 | mfu: -1.00 | total time: 0.21m\n",
      "step 00036 (4.56%) | loss: 1.666692 | lrm: 1.00 | dt: 471.92ms | tok/sec: 1,110,958 | mfu: -1.00 | total time: 0.21m\n",
      "step 00037 (4.70%) | loss: 1.664981 | lrm: 1.00 | dt: 475.16ms | tok/sec: 1,103,393 | mfu: -1.00 | total time: 0.22m\n",
      "step 00038 (4.82%) | loss: 1.658996 | lrm: 1.00 | dt: 615.74ms | tok/sec: 851,472 | mfu: -1.00 | total time: 0.23m\n",
      "step 00039 (4.94%) | loss: 1.655778 | lrm: 1.00 | dt: 523.17ms | tok/sec: 1,002,136 | mfu: -1.00 | total time: 0.24m\n",
      "step 00040 (5.07%) | loss: 1.653775 | lrm: 1.00 | dt: 465.44ms | tok/sec: 1,126,442 | mfu: -1.00 | total time: 0.25m\n",
      "step 00041 (5.16%) | loss: 1.657090 | lrm: 1.00 | dt: 473.71ms | tok/sec: 1,106,772 | mfu: -1.00 | total time: 0.26m\n",
      "step 00042 (5.31%) | loss: 1.640446 | lrm: 1.00 | dt: 474.59ms | tok/sec: 1,104,718 | mfu: -1.00 | total time: 0.26m\n",
      "step 00043 (5.43%) | loss: 1.636663 | lrm: 1.00 | dt: 469.55ms | tok/sec: 1,116,575 | mfu: -1.00 | total time: 0.27m\n",
      "step 00044 (5.59%) | loss: 1.636674 | lrm: 1.00 | dt: 475.86ms | tok/sec: 1,101,770 | mfu: -1.00 | total time: 0.28m\n",
      "step 00045 (5.70%) | loss: 1.626831 | lrm: 1.00 | dt: 473.61ms | tok/sec: 1,107,003 | mfu: -1.00 | total time: 0.29m\n",
      "step 00046 (5.83%) | loss: 1.621240 | lrm: 1.00 | dt: 534.64ms | tok/sec: 980,641 | mfu: -1.00 | total time: 0.30m\n",
      "step 00047 (5.95%) | loss: 1.620626 | lrm: 1.00 | dt: 528.21ms | tok/sec: 992,579 | mfu: -1.00 | total time: 0.31m\n",
      "step 00048 (6.07%) | loss: 1.615212 | lrm: 1.00 | dt: 466.15ms | tok/sec: 1,124,727 | mfu: -1.00 | total time: 0.31m\n",
      "step 00049 (6.19%) | loss: 1.610728 | lrm: 1.00 | dt: 536.32ms | tok/sec: 977,560 | mfu: -1.00 | total time: 0.32m\n",
      "step 00050 (6.32%) | loss: 1.603117 | lrm: 1.00 | dt: 527.73ms | tok/sec: 993,486 | mfu: -1.00 | total time: 0.33m\n",
      "step 00051 (6.44%) | loss: 1.601942 | lrm: 1.00 | dt: 526.95ms | tok/sec: 994,944 | mfu: -1.00 | total time: 0.34m\n",
      "step 00052 (6.59%) | loss: 1.602345 | lrm: 1.00 | dt: 587.84ms | tok/sec: 891,895 | mfu: -1.00 | total time: 0.35m\n",
      "step 00053 (6.71%) | loss: 1.593852 | lrm: 1.00 | dt: 465.42ms | tok/sec: 1,126,491 | mfu: -1.00 | total time: 0.36m\n",
      "step 00054 (6.85%) | loss: 1.587035 | lrm: 1.00 | dt: 475.10ms | tok/sec: 1,103,527 | mfu: -1.00 | total time: 0.37m\n",
      "step 00055 (6.98%) | loss: 1.577263 | lrm: 1.00 | dt: 474.47ms | tok/sec: 1,105,008 | mfu: -1.00 | total time: 0.37m\n",
      "step 00056 (7.09%) | loss: 1.573517 | lrm: 1.00 | dt: 472.76ms | tok/sec: 1,109,001 | mfu: -1.00 | total time: 0.38m\n",
      "step 00057 (7.23%) | loss: 1.559000 | lrm: 1.00 | dt: 474.20ms | tok/sec: 1,105,626 | mfu: -1.00 | total time: 0.39m\n",
      "step 00058 (7.34%) | loss: 1.561112 | lrm: 1.00 | dt: 471.89ms | tok/sec: 1,111,027 | mfu: -1.00 | total time: 0.40m\n",
      "step 00059 (7.45%) | loss: 1.568527 | lrm: 1.00 | dt: 528.80ms | tok/sec: 991,458 | mfu: -1.00 | total time: 0.41m\n",
      "step 00060 (7.57%) | loss: 1.574243 | lrm: 1.00 | dt: 468.06ms | tok/sec: 1,120,125 | mfu: -1.00 | total time: 0.41m\n",
      "step 00061 (7.67%) | loss: 1.586518 | lrm: 1.00 | dt: 541.06ms | tok/sec: 968,992 | mfu: -1.00 | total time: 0.42m\n",
      "step 00062 (7.81%) | loss: 1.582291 | lrm: 1.00 | dt: 466.88ms | tok/sec: 1,122,965 | mfu: -1.00 | total time: 0.43m\n",
      "step 00063 (7.93%) | loss: 1.579162 | lrm: 1.00 | dt: 474.89ms | tok/sec: 1,104,021 | mfu: -1.00 | total time: 0.44m\n",
      "step 00064 (8.05%) | loss: 1.562610 | lrm: 1.00 | dt: 481.62ms | tok/sec: 1,088,596 | mfu: -1.00 | total time: 0.45m\n",
      "step 00065 (8.18%) | loss: 1.562059 | lrm: 1.00 | dt: 469.76ms | tok/sec: 1,116,069 | mfu: -1.00 | total time: 0.45m\n",
      "step 00066 (8.30%) | loss: 1.569454 | lrm: 1.00 | dt: 478.50ms | tok/sec: 1,095,682 | mfu: -1.00 | total time: 0.46m\n",
      "step 00067 (8.42%) | loss: 1.578914 | lrm: 1.00 | dt: 470.85ms | tok/sec: 1,113,493 | mfu: -1.00 | total time: 0.47m\n",
      "step 00068 (8.55%) | loss: 1.569824 | lrm: 1.00 | dt: 535.87ms | tok/sec: 978,379 | mfu: -1.00 | total time: 0.48m\n",
      "step 00069 (8.67%) | loss: 1.565320 | lrm: 1.00 | dt: 527.89ms | tok/sec: 993,178 | mfu: -1.00 | total time: 0.49m\n",
      "step 00070 (8.81%) | loss: 1.564662 | lrm: 1.00 | dt: 466.58ms | tok/sec: 1,123,672 | mfu: -1.00 | total time: 0.50m\n",
      "step 00071 (8.93%) | loss: 1.564871 | lrm: 1.00 | dt: 473.76ms | tok/sec: 1,106,648 | mfu: -1.00 | total time: 0.50m\n",
      "step 00072 (9.04%) | loss: 1.567935 | lrm: 1.00 | dt: 477.22ms | tok/sec: 1,098,632 | mfu: -1.00 | total time: 0.51m\n",
      "step 00073 (9.19%) | loss: 1.557514 | lrm: 1.00 | dt: 594.12ms | tok/sec: 882,468 | mfu: -1.00 | total time: 0.52m\n",
      "step 00074 (9.32%) | loss: 1.554494 | lrm: 1.00 | dt: 518.33ms | tok/sec: 1,011,503 | mfu: -1.00 | total time: 0.53m\n",
      "step 00075 (9.44%) | loss: 1.551050 | lrm: 1.00 | dt: 541.10ms | tok/sec: 968,924 | mfu: -1.00 | total time: 0.54m\n",
      "step 00076 (9.56%) | loss: 1.543270 | lrm: 1.00 | dt: 582.41ms | tok/sec: 900,201 | mfu: -1.00 | total time: 0.55m\n",
      "step 00077 (9.68%) | loss: 1.527433 | lrm: 1.00 | dt: 467.28ms | tok/sec: 1,121,993 | mfu: -1.00 | total time: 0.56m\n",
      "step 00078 (9.82%) | loss: 1.529635 | lrm: 1.00 | dt: 537.10ms | tok/sec: 976,144 | mfu: -1.00 | total time: 0.57m\n",
      "step 00079 (9.93%) | loss: 1.531968 | lrm: 1.00 | dt: 525.66ms | tok/sec: 997,393 | mfu: -1.00 | total time: 0.57m\n",
      "step 00080 (10.06%) | loss: 1.529968 | lrm: 1.00 | dt: 521.31ms | tok/sec: 1,005,719 | mfu: -1.00 | total time: 0.58m\n",
      "step 00081 (10.19%) | loss: 1.523328 | lrm: 1.00 | dt: 464.85ms | tok/sec: 1,127,865 | mfu: -1.00 | total time: 0.59m\n",
      "step 00082 (10.31%) | loss: 1.525329 | lrm: 1.00 | dt: 473.17ms | tok/sec: 1,108,028 | mfu: -1.00 | total time: 0.60m\n",
      "step 00083 (10.46%) | loss: 1.513258 | lrm: 1.00 | dt: 474.78ms | tok/sec: 1,104,275 | mfu: -1.00 | total time: 0.61m\n",
      "step 00084 (10.58%) | loss: 1.518929 | lrm: 1.00 | dt: 470.30ms | tok/sec: 1,114,784 | mfu: -1.00 | total time: 0.61m\n",
      "step 00085 (10.71%) | loss: 1.508393 | lrm: 1.00 | dt: 477.50ms | tok/sec: 1,097,985 | mfu: -1.00 | total time: 0.62m\n",
      "step 00086 (10.82%) | loss: 1.492033 | lrm: 1.00 | dt: 472.28ms | tok/sec: 1,110,118 | mfu: -1.00 | total time: 0.63m\n",
      "step 00087 (10.95%) | loss: 1.496561 | lrm: 1.00 | dt: 472.07ms | tok/sec: 1,110,625 | mfu: -1.00 | total time: 0.64m\n",
      "step 00088 (11.06%) | loss: 1.492397 | lrm: 1.00 | dt: 475.40ms | tok/sec: 1,102,833 | mfu: -1.00 | total time: 0.65m\n",
      "step 00089 (11.18%) | loss: 1.492160 | lrm: 1.00 | dt: 471.77ms | tok/sec: 1,111,319 | mfu: -1.00 | total time: 0.65m\n",
      "step 00090 (11.29%) | loss: 1.477997 | lrm: 1.00 | dt: 596.73ms | tok/sec: 878,601 | mfu: -1.00 | total time: 0.66m\n",
      "step 00091 (11.41%) | loss: 1.482413 | lrm: 1.00 | dt: 468.47ms | tok/sec: 1,119,145 | mfu: -1.00 | total time: 0.67m\n",
      "step 00092 (11.53%) | loss: 1.475598 | lrm: 1.00 | dt: 593.37ms | tok/sec: 883,570 | mfu: -1.00 | total time: 0.68m\n",
      "step 00093 (11.65%) | loss: 1.472640 | lrm: 1.00 | dt: 578.10ms | tok/sec: 906,911 | mfu: -1.00 | total time: 0.69m\n",
      "step 00094 (11.78%) | loss: 1.474808 | lrm: 1.00 | dt: 523.88ms | tok/sec: 1,000,775 | mfu: -1.00 | total time: 0.70m\n",
      "step 00095 (11.91%) | loss: 1.479519 | lrm: 1.00 | dt: 615.94ms | tok/sec: 851,204 | mfu: -1.00 | total time: 0.71m\n",
      "step 00096 (12.03%) | loss: 1.474898 | lrm: 1.00 | dt: 525.38ms | tok/sec: 997,919 | mfu: -1.00 | total time: 0.72m\n",
      "step 00097 (12.14%) | loss: 1.484096 | lrm: 1.00 | dt: 579.94ms | tok/sec: 904,045 | mfu: -1.00 | total time: 0.73m\n",
      "step 00098 (12.26%) | loss: 1.486963 | lrm: 1.00 | dt: 525.67ms | tok/sec: 997,378 | mfu: -1.00 | total time: 0.74m\n",
      "step 00099 (12.38%) | loss: 1.483838 | lrm: 1.00 | dt: 465.17ms | tok/sec: 1,127,095 | mfu: -1.00 | total time: 0.75m\n",
      "step 00100 (12.49%) | loss: 1.482958 | lrm: 1.00 | dt: 473.50ms | tok/sec: 1,107,255 | mfu: -1.00 | total time: 0.75m\n",
      "step 00101 (12.62%) | loss: 1.482401 | lrm: 1.00 | dt: 482.99ms | tok/sec: 1,085,504 | mfu: -1.00 | total time: 0.76m\n",
      "step 00102 (12.75%) | loss: 1.478752 | lrm: 1.00 | dt: 472.42ms | tok/sec: 1,109,780 | mfu: -1.00 | total time: 0.77m\n",
      "step 00103 (12.87%) | loss: 1.481096 | lrm: 1.00 | dt: 536.30ms | tok/sec: 977,593 | mfu: -1.00 | total time: 0.78m\n",
      "step 00104 (13.00%) | loss: 1.475181 | lrm: 1.00 | dt: 526.41ms | tok/sec: 995,964 | mfu: -1.00 | total time: 0.79m\n",
      "step 00105 (13.10%) | loss: 1.470203 | lrm: 1.00 | dt: 613.67ms | tok/sec: 854,347 | mfu: -1.00 | total time: 0.80m\n",
      "step 00106 (13.21%) | loss: 1.467948 | lrm: 1.00 | dt: 467.60ms | tok/sec: 1,121,242 | mfu: -1.00 | total time: 0.80m\n",
      "step 00107 (13.34%) | loss: 1.468807 | lrm: 1.00 | dt: 477.68ms | tok/sec: 1,097,566 | mfu: -1.00 | total time: 0.81m\n",
      "step 00108 (13.45%) | loss: 1.468944 | lrm: 1.00 | dt: 700.47ms | tok/sec: 748,484 | mfu: -1.00 | total time: 0.82m\n",
      "step 00109 (13.57%) | loss: 1.464914 | lrm: 1.00 | dt: 581.26ms | tok/sec: 901,983 | mfu: -1.00 | total time: 0.83m\n",
      "step 00110 (13.70%) | loss: 1.463361 | lrm: 1.00 | dt: 467.42ms | tok/sec: 1,121,662 | mfu: -1.00 | total time: 0.84m\n",
      "step 00111 (13.81%) | loss: 1.456152 | lrm: 1.00 | dt: 523.38ms | tok/sec: 1,001,739 | mfu: -1.00 | total time: 0.85m\n",
      "step 00112 (13.93%) | loss: 1.451192 | lrm: 1.00 | dt: 520.97ms | tok/sec: 1,006,372 | mfu: -1.00 | total time: 0.86m\n",
      "step 00113 (14.07%) | loss: 1.456900 | lrm: 1.00 | dt: 465.27ms | tok/sec: 1,126,846 | mfu: -1.00 | total time: 0.87m\n",
      "step 00114 (14.19%) | loss: 1.472748 | lrm: 1.00 | dt: 519.20ms | tok/sec: 1,009,792 | mfu: -1.00 | total time: 0.88m\n",
      "step 00115 (14.32%) | loss: 1.464826 | lrm: 1.00 | dt: 524.62ms | tok/sec: 999,359 | mfu: -1.00 | total time: 0.88m\n",
      "step 00116 (14.47%) | loss: 1.469439 | lrm: 1.00 | dt: 465.42ms | tok/sec: 1,126,489 | mfu: -1.00 | total time: 0.89m\n",
      "step 00117 (14.59%) | loss: 1.471613 | lrm: 1.00 | dt: 472.58ms | tok/sec: 1,109,409 | mfu: -1.00 | total time: 0.90m\n",
      "step 00118 (14.71%) | loss: 1.464132 | lrm: 1.00 | dt: 532.73ms | tok/sec: 984,151 | mfu: -1.00 | total time: 0.91m\n",
      "step 00119 (14.83%) | loss: 1.462122 | lrm: 1.00 | dt: 608.92ms | tok/sec: 861,013 | mfu: -1.00 | total time: 0.92m\n",
      "step 00120 (14.96%) | loss: 1.459069 | lrm: 1.00 | dt: 684.09ms | tok/sec: 766,396 | mfu: -1.00 | total time: 0.93m\n",
      "step 00121 (15.07%) | loss: 1.456990 | lrm: 1.00 | dt: 602.64ms | tok/sec: 869,988 | mfu: -1.00 | total time: 0.94m\n",
      "step 00122 (15.21%) | loss: 1.451830 | lrm: 1.00 | dt: 528.62ms | tok/sec: 991,801 | mfu: -1.00 | total time: 0.95m\n",
      "step 00123 (15.35%) | loss: 1.450719 | lrm: 1.00 | dt: 466.08ms | tok/sec: 1,124,896 | mfu: -1.00 | total time: 0.96m\n",
      "step 00124 (15.47%) | loss: 1.454913 | lrm: 1.00 | dt: 538.05ms | tok/sec: 974,417 | mfu: -1.00 | total time: 0.97m\n",
      "step 00125 (15.58%) | loss: 1.445208 | lrm: 1.00 | dt: 528.15ms | tok/sec: 992,680 | mfu: -1.00 | total time: 0.97m\n",
      "step 00126 (15.69%) | loss: 1.444891 | lrm: 1.00 | dt: 573.44ms | tok/sec: 914,279 | mfu: -1.00 | total time: 0.98m\n",
      "step 00127 (15.82%) | loss: 1.440246 | lrm: 1.00 | dt: 521.83ms | tok/sec: 1,004,701 | mfu: -1.00 | total time: 0.99m\n",
      "step 00128 (15.96%) | loss: 1.435527 | lrm: 1.00 | dt: 464.45ms | tok/sec: 1,128,835 | mfu: -1.00 | total time: 1.00m\n",
      "step 00129 (16.09%) | loss: 1.440259 | lrm: 1.00 | dt: 596.48ms | tok/sec: 878,968 | mfu: -1.00 | total time: 1.01m\n",
      "step 00130 (16.22%) | loss: 1.439946 | lrm: 1.00 | dt: 673.21ms | tok/sec: 778,792 | mfu: -1.00 | total time: 1.02m\n",
      "step 00131 (16.34%) | loss: 1.453542 | lrm: 1.00 | dt: 519.75ms | tok/sec: 1,008,724 | mfu: -1.00 | total time: 1.03m\n",
      "step 00132 (16.46%) | loss: 1.447675 | lrm: 1.00 | dt: 531.56ms | tok/sec: 986,325 | mfu: -1.00 | total time: 1.04m\n",
      "step 00133 (16.58%) | loss: 1.445934 | lrm: 1.00 | dt: 532.61ms | tok/sec: 984,369 | mfu: -1.00 | total time: 1.05m\n",
      "step 00134 (16.69%) | loss: 1.451379 | lrm: 1.00 | dt: 466.39ms | tok/sec: 1,124,139 | mfu: -1.00 | total time: 1.06m\n",
      "step 00135 (16.81%) | loss: 1.437797 | lrm: 1.00 | dt: 476.17ms | tok/sec: 1,101,049 | mfu: -1.00 | total time: 1.06m\n",
      "step 00136 (16.94%) | loss: 1.445775 | lrm: 1.00 | dt: 618.81ms | tok/sec: 847,245 | mfu: -1.00 | total time: 1.07m\n",
      "step 00137 (17.09%) | loss: 1.443333 | lrm: 1.00 | dt: 528.64ms | tok/sec: 991,764 | mfu: -1.00 | total time: 1.08m\n",
      "step 00138 (17.20%) | loss: 1.439898 | lrm: 1.00 | dt: 528.34ms | tok/sec: 992,334 | mfu: -1.00 | total time: 1.09m\n",
      "step 00139 (17.33%) | loss: 1.448909 | lrm: 1.00 | dt: 678.72ms | tok/sec: 772,466 | mfu: -1.00 | total time: 1.10m\n",
      "step 00140 (17.45%) | loss: 1.441460 | lrm: 1.00 | dt: 597.31ms | tok/sec: 877,748 | mfu: -1.00 | total time: 1.11m\n",
      "step 00141 (17.56%) | loss: 1.436493 | lrm: 1.00 | dt: 657.03ms | tok/sec: 797,969 | mfu: -1.00 | total time: 1.12m\n",
      "step 00142 (17.69%) | loss: 1.429735 | lrm: 1.00 | dt: 684.51ms | tok/sec: 765,937 | mfu: -1.00 | total time: 1.14m\n",
      "step 00143 (17.79%) | loss: 1.426973 | lrm: 1.00 | dt: 526.08ms | tok/sec: 996,592 | mfu: -1.00 | total time: 1.14m\n",
      "step 00144 (17.92%) | loss: 1.431134 | lrm: 1.00 | dt: 464.02ms | tok/sec: 1,129,888 | mfu: -1.00 | total time: 1.15m\n",
      "step 00145 (18.04%) | loss: 1.431802 | lrm: 1.00 | dt: 540.15ms | tok/sec: 970,626 | mfu: -1.00 | total time: 1.16m\n",
      "step 00146 (18.17%) | loss: 1.434683 | lrm: 1.00 | dt: 523.95ms | tok/sec: 1,000,638 | mfu: -1.00 | total time: 1.17m\n",
      "step 00147 (18.30%) | loss: 1.442457 | lrm: 1.00 | dt: 597.18ms | tok/sec: 877,946 | mfu: -1.00 | total time: 1.18m\n",
      "step 00148 (18.40%) | loss: 1.444358 | lrm: 1.00 | dt: 466.89ms | tok/sec: 1,122,939 | mfu: -1.00 | total time: 1.19m\n",
      "step 00149 (18.52%) | loss: 1.443610 | lrm: 1.00 | dt: 535.25ms | tok/sec: 979,518 | mfu: -1.00 | total time: 1.20m\n",
      "step 00150 (18.64%) | loss: 1.441184 | lrm: 1.00 | dt: 581.48ms | tok/sec: 901,649 | mfu: -1.00 | total time: 1.21m\n",
      "step 00150 | Validation bpb: 0.4493\n",
      "step 00151 (18.76%) | loss: 1.433998 | lrm: 1.00 | dt: 550.58ms | tok/sec: 952,244 | mfu: -1.00 | total time: 1.22m\n",
      "step 00152 (18.89%) | loss: 1.432243 | lrm: 1.00 | dt: 467.52ms | tok/sec: 1,121,422 | mfu: -1.00 | total time: 1.22m\n",
      "step 00153 (19.01%) | loss: 1.439116 | lrm: 1.00 | dt: 533.71ms | tok/sec: 982,347 | mfu: -1.00 | total time: 1.23m\n",
      "step 00154 (19.13%) | loss: 1.434504 | lrm: 1.00 | dt: 585.77ms | tok/sec: 895,047 | mfu: -1.00 | total time: 1.24m\n",
      "step 00155 (19.25%) | loss: 1.429834 | lrm: 1.00 | dt: 541.76ms | tok/sec: 967,757 | mfu: -1.00 | total time: 1.25m\n",
      "step 00156 (19.40%) | loss: 1.428446 | lrm: 1.00 | dt: 465.01ms | tok/sec: 1,127,466 | mfu: -1.00 | total time: 1.26m\n",
      "step 00157 (19.52%) | loss: 1.419102 | lrm: 1.00 | dt: 607.20ms | tok/sec: 863,455 | mfu: -1.00 | total time: 1.27m\n",
      "step 00158 (19.66%) | loss: 1.412782 | lrm: 1.00 | dt: 588.04ms | tok/sec: 891,584 | mfu: -1.00 | total time: 1.28m\n",
      "step 00159 (19.79%) | loss: 1.410320 | lrm: 1.00 | dt: 533.28ms | tok/sec: 983,142 | mfu: -1.00 | total time: 1.29m\n",
      "step 00160 (19.92%) | loss: 1.417150 | lrm: 1.00 | dt: 581.74ms | tok/sec: 901,243 | mfu: -1.00 | total time: 1.30m\n",
      "step 00161 (20.06%) | loss: 1.411545 | lrm: 1.00 | dt: 589.52ms | tok/sec: 889,348 | mfu: -1.00 | total time: 1.31m\n",
      "step 00162 (20.19%) | loss: 1.400109 | lrm: 1.00 | dt: 466.19ms | tok/sec: 1,124,618 | mfu: -1.00 | total time: 1.31m\n",
      "step 00163 (20.31%) | loss: 1.397685 | lrm: 1.00 | dt: 529.35ms | tok/sec: 990,432 | mfu: -1.00 | total time: 1.32m\n",
      "step 00164 (20.43%) | loss: 1.394038 | lrm: 1.00 | dt: 527.52ms | tok/sec: 993,877 | mfu: -1.00 | total time: 1.33m\n",
      "step 00165 (20.58%) | loss: 1.387320 | lrm: 1.00 | dt: 522.30ms | tok/sec: 1,003,808 | mfu: -1.00 | total time: 1.34m\n",
      "step 00166 (20.69%) | loss: 1.390896 | lrm: 1.00 | dt: 466.37ms | tok/sec: 1,124,200 | mfu: -1.00 | total time: 1.35m\n",
      "step 00167 (20.80%) | loss: 1.388544 | lrm: 1.00 | dt: 533.22ms | tok/sec: 983,246 | mfu: -1.00 | total time: 1.36m\n",
      "step 00168 (20.94%) | loss: 1.399336 | lrm: 1.00 | dt: 527.70ms | tok/sec: 993,541 | mfu: -1.00 | total time: 1.37m\n",
      "step 00169 (21.07%) | loss: 1.395493 | lrm: 1.00 | dt: 612.86ms | tok/sec: 855,471 | mfu: -1.00 | total time: 1.38m\n",
      "step 00170 (21.18%) | loss: 1.402471 | lrm: 1.00 | dt: 466.50ms | tok/sec: 1,123,870 | mfu: -1.00 | total time: 1.38m\n",
      "step 00171 (21.30%) | loss: 1.398608 | lrm: 1.00 | dt: 533.44ms | tok/sec: 982,844 | mfu: -1.00 | total time: 1.39m\n",
      "step 00172 (21.42%) | loss: 1.398628 | lrm: 1.00 | dt: 531.02ms | tok/sec: 987,319 | mfu: -1.00 | total time: 1.40m\n",
      "step 00173 (21.54%) | loss: 1.396857 | lrm: 1.00 | dt: 525.58ms | tok/sec: 997,539 | mfu: -1.00 | total time: 1.41m\n",
      "step 00174 (21.67%) | loss: 1.389847 | lrm: 1.00 | dt: 604.18ms | tok/sec: 867,761 | mfu: -1.00 | total time: 1.42m\n",
      "step 00175 (21.79%) | loss: 1.393507 | lrm: 1.00 | dt: 526.03ms | tok/sec: 996,697 | mfu: -1.00 | total time: 1.43m\n",
      "step 00176 (21.91%) | loss: 1.386854 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,577 | mfu: -1.00 | total time: 1.44m\n",
      "step 00177 (22.05%) | loss: 1.393102 | lrm: 1.00 | dt: 583.88ms | tok/sec: 897,934 | mfu: -1.00 | total time: 1.45m\n",
      "step 00178 (22.17%) | loss: 1.392526 | lrm: 1.00 | dt: 529.22ms | tok/sec: 990,673 | mfu: -1.00 | total time: 1.46m\n",
      "step 00179 (22.28%) | loss: 1.388190 | lrm: 1.00 | dt: 537.37ms | tok/sec: 975,653 | mfu: -1.00 | total time: 1.47m\n",
      "step 00180 (22.42%) | loss: 1.386485 | lrm: 1.00 | dt: 526.19ms | tok/sec: 996,388 | mfu: -1.00 | total time: 1.47m\n",
      "step 00181 (22.55%) | loss: 1.387389 | lrm: 1.00 | dt: 584.40ms | tok/sec: 897,133 | mfu: -1.00 | total time: 1.48m\n",
      "step 00182 (22.68%) | loss: 1.385751 | lrm: 1.00 | dt: 684.59ms | tok/sec: 765,839 | mfu: -1.00 | total time: 1.50m\n",
      "step 00183 (22.81%) | loss: 1.388674 | lrm: 1.00 | dt: 598.45ms | tok/sec: 876,071 | mfu: -1.00 | total time: 1.51m\n",
      "step 00184 (22.93%) | loss: 1.390654 | lrm: 1.00 | dt: 529.45ms | tok/sec: 990,245 | mfu: -1.00 | total time: 1.51m\n",
      "step 00185 (23.07%) | loss: 1.384720 | lrm: 1.00 | dt: 596.74ms | tok/sec: 878,586 | mfu: -1.00 | total time: 1.52m\n",
      "step 00186 (23.20%) | loss: 1.373532 | lrm: 1.00 | dt: 469.09ms | tok/sec: 1,117,673 | mfu: -1.00 | total time: 1.53m\n",
      "step 00187 (23.31%) | loss: 1.365575 | lrm: 1.00 | dt: 621.75ms | tok/sec: 843,240 | mfu: -1.00 | total time: 1.54m\n",
      "step 00188 (23.46%) | loss: 1.372157 | lrm: 1.00 | dt: 609.49ms | tok/sec: 860,204 | mfu: -1.00 | total time: 1.55m\n",
      "step 00189 (23.57%) | loss: 1.379779 | lrm: 1.00 | dt: 529.59ms | tok/sec: 989,982 | mfu: -1.00 | total time: 1.56m\n",
      "step 00190 (23.71%) | loss: 1.380857 | lrm: 1.00 | dt: 536.32ms | tok/sec: 977,570 | mfu: -1.00 | total time: 1.57m\n",
      "step 00191 (23.84%) | loss: 1.383707 | lrm: 1.00 | dt: 624.86ms | tok/sec: 839,044 | mfu: -1.00 | total time: 1.58m\n",
      "step 00192 (23.96%) | loss: 1.384188 | lrm: 1.00 | dt: 526.41ms | tok/sec: 995,966 | mfu: -1.00 | total time: 1.59m\n",
      "step 00193 (24.07%) | loss: 1.393610 | lrm: 1.00 | dt: 467.31ms | tok/sec: 1,121,923 | mfu: -1.00 | total time: 1.60m\n",
      "step 00194 (24.20%) | loss: 1.396424 | lrm: 1.00 | dt: 526.86ms | tok/sec: 995,121 | mfu: -1.00 | total time: 1.61m\n",
      "step 00195 (24.34%) | loss: 1.397247 | lrm: 1.00 | dt: 701.19ms | tok/sec: 747,707 | mfu: -1.00 | total time: 1.62m\n",
      "step 00196 (24.45%) | loss: 1.402262 | lrm: 1.00 | dt: 587.04ms | tok/sec: 893,111 | mfu: -1.00 | total time: 1.63m\n",
      "step 00197 (24.59%) | loss: 1.401998 | lrm: 1.00 | dt: 643.16ms | tok/sec: 815,176 | mfu: -1.00 | total time: 1.64m\n",
      "step 00198 (24.70%) | loss: 1.399762 | lrm: 1.00 | dt: 534.06ms | tok/sec: 981,702 | mfu: -1.00 | total time: 1.65m\n",
      "step 00199 (24.82%) | loss: 1.406143 | lrm: 1.00 | dt: 527.99ms | tok/sec: 992,983 | mfu: -1.00 | total time: 1.66m\n",
      "step 00200 (24.96%) | loss: 1.400293 | lrm: 1.00 | dt: 468.45ms | tok/sec: 1,119,192 | mfu: -1.00 | total time: 1.66m\n",
      "step 00201 (25.10%) | loss: 1.410797 | lrm: 1.00 | dt: 527.54ms | tok/sec: 993,842 | mfu: -1.00 | total time: 1.67m\n",
      "step 00202 (25.23%) | loss: 1.398757 | lrm: 1.00 | dt: 600.26ms | tok/sec: 873,434 | mfu: -1.00 | total time: 1.68m\n",
      "step 00203 (25.35%) | loss: 1.392462 | lrm: 1.00 | dt: 467.08ms | tok/sec: 1,122,471 | mfu: -1.00 | total time: 1.69m\n",
      "step 00204 (25.47%) | loss: 1.385098 | lrm: 1.00 | dt: 700.29ms | tok/sec: 748,677 | mfu: -1.00 | total time: 1.70m\n",
      "step 00205 (25.60%) | loss: 1.383242 | lrm: 1.00 | dt: 528.28ms | tok/sec: 992,443 | mfu: -1.00 | total time: 1.71m\n",
      "step 00206 (25.72%) | loss: 1.373990 | lrm: 1.00 | dt: 468.04ms | tok/sec: 1,120,185 | mfu: -1.00 | total time: 1.72m\n",
      "step 00207 (25.83%) | loss: 1.381428 | lrm: 1.00 | dt: 626.11ms | tok/sec: 837,379 | mfu: -1.00 | total time: 1.73m\n",
      "step 00208 (25.94%) | loss: 1.382342 | lrm: 1.00 | dt: 467.08ms | tok/sec: 1,122,480 | mfu: -1.00 | total time: 1.74m\n",
      "step 00209 (26.06%) | loss: 1.385231 | lrm: 1.00 | dt: 588.40ms | tok/sec: 891,040 | mfu: -1.00 | total time: 1.75m\n",
      "step 00210 (26.18%) | loss: 1.388406 | lrm: 1.00 | dt: 587.15ms | tok/sec: 892,941 | mfu: -1.00 | total time: 1.76m\n",
      "step 00211 (26.30%) | loss: 1.389073 | lrm: 1.00 | dt: 471.66ms | tok/sec: 1,111,590 | mfu: -1.00 | total time: 1.76m\n",
      "step 00212 (26.45%) | loss: 1.389556 | lrm: 1.00 | dt: 533.93ms | tok/sec: 981,939 | mfu: -1.00 | total time: 1.77m\n",
      "step 00213 (26.57%) | loss: 1.385741 | lrm: 1.00 | dt: 539.88ms | tok/sec: 971,125 | mfu: -1.00 | total time: 1.78m\n",
      "step 00214 (26.69%) | loss: 1.381480 | lrm: 1.00 | dt: 464.42ms | tok/sec: 1,128,911 | mfu: -1.00 | total time: 1.79m\n",
      "step 00215 (26.82%) | loss: 1.377537 | lrm: 1.00 | dt: 534.39ms | tok/sec: 981,094 | mfu: -1.00 | total time: 1.80m\n",
      "step 00216 (26.93%) | loss: 1.374844 | lrm: 1.00 | dt: 534.10ms | tok/sec: 981,634 | mfu: -1.00 | total time: 1.81m\n",
      "step 00217 (27.07%) | loss: 1.371703 | lrm: 1.00 | dt: 464.75ms | tok/sec: 1,128,099 | mfu: -1.00 | total time: 1.82m\n",
      "step 00218 (27.21%) | loss: 1.374722 | lrm: 1.00 | dt: 477.51ms | tok/sec: 1,097,964 | mfu: -1.00 | total time: 1.82m\n",
      "step 00219 (27.33%) | loss: 1.372468 | lrm: 1.00 | dt: 547.04ms | tok/sec: 958,413 | mfu: -1.00 | total time: 1.83m\n",
      "step 00220 (27.45%) | loss: 1.390257 | lrm: 1.00 | dt: 466.70ms | tok/sec: 1,123,394 | mfu: -1.00 | total time: 1.84m\n",
      "step 00221 (27.57%) | loss: 1.390712 | lrm: 1.00 | dt: 474.37ms | tok/sec: 1,105,232 | mfu: -1.00 | total time: 1.85m\n",
      "step 00222 (27.70%) | loss: 1.386724 | lrm: 1.00 | dt: 530.42ms | tok/sec: 988,441 | mfu: -1.00 | total time: 1.86m\n",
      "step 00223 (27.82%) | loss: 1.383378 | lrm: 1.00 | dt: 527.42ms | tok/sec: 994,071 | mfu: -1.00 | total time: 1.87m\n",
      "step 00224 (27.94%) | loss: 1.390184 | lrm: 1.00 | dt: 523.67ms | tok/sec: 1,001,180 | mfu: -1.00 | total time: 1.87m\n",
      "step 00225 (28.07%) | loss: 1.390018 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,577 | mfu: -1.00 | total time: 1.88m\n",
      "step 00226 (28.20%) | loss: 1.395824 | lrm: 1.00 | dt: 476.69ms | tok/sec: 1,099,851 | mfu: -1.00 | total time: 1.89m\n",
      "step 00227 (28.33%) | loss: 1.388964 | lrm: 1.00 | dt: 474.77ms | tok/sec: 1,104,293 | mfu: -1.00 | total time: 1.90m\n",
      "step 00228 (28.47%) | loss: 1.390400 | lrm: 1.00 | dt: 534.05ms | tok/sec: 981,724 | mfu: -1.00 | total time: 1.91m\n",
      "step 00229 (28.60%) | loss: 1.394151 | lrm: 1.00 | dt: 521.38ms | tok/sec: 1,005,578 | mfu: -1.00 | total time: 1.92m\n",
      "step 00230 (28.72%) | loss: 1.391165 | lrm: 1.00 | dt: 526.09ms | tok/sec: 996,568 | mfu: -1.00 | total time: 1.92m\n",
      "step 00231 (28.84%) | loss: 1.384216 | lrm: 1.00 | dt: 464.72ms | tok/sec: 1,128,185 | mfu: -1.00 | total time: 1.93m\n",
      "step 00232 (28.97%) | loss: 1.397894 | lrm: 1.00 | dt: 480.39ms | tok/sec: 1,091,375 | mfu: -1.00 | total time: 1.94m\n",
      "step 00233 (29.08%) | loss: 1.391716 | lrm: 1.00 | dt: 472.89ms | tok/sec: 1,108,679 | mfu: -1.00 | total time: 1.95m\n",
      "step 00234 (29.21%) | loss: 1.388776 | lrm: 1.00 | dt: 533.33ms | tok/sec: 983,043 | mfu: -1.00 | total time: 1.96m\n",
      "step 00235 (29.34%) | loss: 1.396827 | lrm: 1.00 | dt: 523.55ms | tok/sec: 1,001,403 | mfu: -1.00 | total time: 1.97m\n",
      "step 00236 (29.48%) | loss: 1.390509 | lrm: 1.00 | dt: 465.45ms | tok/sec: 1,126,418 | mfu: -1.00 | total time: 1.97m\n",
      "step 00237 (29.58%) | loss: 1.393083 | lrm: 1.00 | dt: 478.28ms | tok/sec: 1,096,200 | mfu: -1.00 | total time: 1.98m\n",
      "step 00238 (29.69%) | loss: 1.401981 | lrm: 1.00 | dt: 471.80ms | tok/sec: 1,111,253 | mfu: -1.00 | total time: 1.99m\n",
      "step 00239 (29.81%) | loss: 1.397152 | lrm: 1.00 | dt: 473.30ms | tok/sec: 1,107,732 | mfu: -1.00 | total time: 2.00m\n",
      "step 00240 (29.93%) | loss: 1.396746 | lrm: 1.00 | dt: 474.07ms | tok/sec: 1,105,918 | mfu: -1.00 | total time: 2.00m\n",
      "step 00241 (30.05%) | loss: 1.402181 | lrm: 1.00 | dt: 534.98ms | tok/sec: 980,020 | mfu: -1.00 | total time: 2.01m\n",
      "step 00242 (30.19%) | loss: 1.409441 | lrm: 1.00 | dt: 525.64ms | tok/sec: 997,422 | mfu: -1.00 | total time: 2.02m\n",
      "step 00243 (30.29%) | loss: 1.415891 | lrm: 1.00 | dt: 526.32ms | tok/sec: 996,132 | mfu: -1.00 | total time: 2.03m\n",
      "step 00244 (30.42%) | loss: 1.410788 | lrm: 1.00 | dt: 518.69ms | tok/sec: 1,010,797 | mfu: -1.00 | total time: 2.04m\n",
      "step 00245 (30.53%) | loss: 1.415683 | lrm: 1.00 | dt: 526.12ms | tok/sec: 996,527 | mfu: -1.00 | total time: 2.05m\n",
      "step 00246 (30.66%) | loss: 1.421179 | lrm: 1.00 | dt: 464.34ms | tok/sec: 1,129,103 | mfu: -1.00 | total time: 2.06m\n",
      "step 00247 (30.76%) | loss: 1.418796 | lrm: 1.00 | dt: 535.77ms | tok/sec: 978,576 | mfu: -1.00 | total time: 2.07m\n",
      "step 00248 (30.90%) | loss: 1.414320 | lrm: 1.00 | dt: 527.71ms | tok/sec: 993,512 | mfu: -1.00 | total time: 2.07m\n",
      "step 00249 (31.02%) | loss: 1.405868 | lrm: 1.00 | dt: 465.05ms | tok/sec: 1,127,390 | mfu: -1.00 | total time: 2.08m\n",
      "step 00250 (31.15%) | loss: 1.398676 | lrm: 1.00 | dt: 474.85ms | tok/sec: 1,104,111 | mfu: -1.00 | total time: 2.09m\n",
      "step 00251 (31.27%) | loss: 1.390683 | lrm: 1.00 | dt: 473.13ms | tok/sec: 1,108,135 | mfu: -1.00 | total time: 2.10m\n",
      "step 00252 (31.39%) | loss: 1.385379 | lrm: 1.00 | dt: 476.02ms | tok/sec: 1,101,406 | mfu: -1.00 | total time: 2.11m\n",
      "step 00253 (31.51%) | loss: 1.392520 | lrm: 1.00 | dt: 474.01ms | tok/sec: 1,106,062 | mfu: -1.00 | total time: 2.11m\n",
      "step 00254 (31.63%) | loss: 1.384228 | lrm: 1.00 | dt: 469.77ms | tok/sec: 1,116,053 | mfu: -1.00 | total time: 2.12m\n",
      "step 00255 (31.75%) | loss: 1.382081 | lrm: 1.00 | dt: 533.11ms | tok/sec: 983,452 | mfu: -1.00 | total time: 2.13m\n",
      "step 00256 (31.88%) | loss: 1.381671 | lrm: 1.00 | dt: 466.49ms | tok/sec: 1,123,906 | mfu: -1.00 | total time: 2.14m\n",
      "step 00257 (32.00%) | loss: 1.371145 | lrm: 1.00 | dt: 477.86ms | tok/sec: 1,097,154 | mfu: -1.00 | total time: 2.15m\n",
      "step 00258 (32.12%) | loss: 1.376814 | lrm: 1.00 | dt: 473.90ms | tok/sec: 1,106,321 | mfu: -1.00 | total time: 2.15m\n",
      "step 00259 (32.26%) | loss: 1.372808 | lrm: 1.00 | dt: 474.12ms | tok/sec: 1,105,813 | mfu: -1.00 | total time: 2.16m\n",
      "step 00260 (32.38%) | loss: 1.367628 | lrm: 1.00 | dt: 470.78ms | tok/sec: 1,113,664 | mfu: -1.00 | total time: 2.17m\n",
      "step 00261 (32.50%) | loss: 1.376178 | lrm: 1.00 | dt: 473.15ms | tok/sec: 1,108,089 | mfu: -1.00 | total time: 2.18m\n",
      "step 00262 (32.62%) | loss: 1.377464 | lrm: 1.00 | dt: 474.65ms | tok/sec: 1,104,583 | mfu: -1.00 | total time: 2.19m\n",
      "step 00263 (32.75%) | loss: 1.373821 | lrm: 1.00 | dt: 469.94ms | tok/sec: 1,115,649 | mfu: -1.00 | total time: 2.19m\n",
      "step 00264 (32.87%) | loss: 1.384896 | lrm: 1.00 | dt: 475.15ms | tok/sec: 1,103,404 | mfu: -1.00 | total time: 2.20m\n",
      "step 00265 (32.97%) | loss: 1.387125 | lrm: 1.00 | dt: 539.99ms | tok/sec: 970,929 | mfu: -1.00 | total time: 2.21m\n",
      "step 00266 (33.10%) | loss: 1.382559 | lrm: 1.00 | dt: 587.03ms | tok/sec: 893,125 | mfu: -1.00 | total time: 2.22m\n",
      "step 00267 (33.22%) | loss: 1.391694 | lrm: 1.00 | dt: 577.70ms | tok/sec: 907,543 | mfu: -1.00 | total time: 2.23m\n",
      "step 00268 (33.33%) | loss: 1.397969 | lrm: 1.00 | dt: 531.12ms | tok/sec: 987,128 | mfu: -1.00 | total time: 2.24m\n",
      "step 00269 (33.44%) | loss: 1.406253 | lrm: 1.00 | dt: 523.29ms | tok/sec: 1,001,905 | mfu: -1.00 | total time: 2.25m\n",
      "step 00270 (33.57%) | loss: 1.400023 | lrm: 1.00 | dt: 533.28ms | tok/sec: 983,139 | mfu: -1.00 | total time: 2.26m\n",
      "step 00271 (33.69%) | loss: 1.403173 | lrm: 1.00 | dt: 467.24ms | tok/sec: 1,122,098 | mfu: -1.00 | total time: 2.26m\n",
      "step 00272 (33.80%) | loss: 1.398426 | lrm: 1.00 | dt: 542.18ms | tok/sec: 967,007 | mfu: -1.00 | total time: 2.27m\n",
      "step 00273 (33.95%) | loss: 1.393507 | lrm: 1.00 | dt: 533.48ms | tok/sec: 982,766 | mfu: -1.00 | total time: 2.28m\n",
      "step 00274 (34.07%) | loss: 1.380992 | lrm: 1.00 | dt: 467.73ms | tok/sec: 1,120,913 | mfu: -1.00 | total time: 2.29m\n",
      "step 00275 (34.19%) | loss: 1.374875 | lrm: 1.00 | dt: 544.62ms | tok/sec: 962,659 | mfu: -1.00 | total time: 2.30m\n",
      "step 00276 (34.31%) | loss: 1.373719 | lrm: 1.00 | dt: 528.57ms | tok/sec: 991,901 | mfu: -1.00 | total time: 2.31m\n",
      "step 00277 (34.43%) | loss: 1.386493 | lrm: 1.00 | dt: 466.13ms | tok/sec: 1,124,768 | mfu: -1.00 | total time: 2.32m\n",
      "step 00278 (34.53%) | loss: 1.383550 | lrm: 1.00 | dt: 477.92ms | tok/sec: 1,097,010 | mfu: -1.00 | total time: 2.32m\n",
      "step 00279 (34.65%) | loss: 1.378935 | lrm: 1.00 | dt: 472.30ms | tok/sec: 1,110,082 | mfu: -1.00 | total time: 2.33m\n",
      "step 00280 (34.77%) | loss: 1.373449 | lrm: 1.00 | dt: 614.09ms | tok/sec: 853,759 | mfu: -1.00 | total time: 2.34m\n",
      "step 00281 (34.89%) | loss: 1.366996 | lrm: 1.00 | dt: 468.83ms | tok/sec: 1,118,301 | mfu: -1.00 | total time: 2.35m\n",
      "step 00282 (35.02%) | loss: 1.358073 | lrm: 1.00 | dt: 542.93ms | tok/sec: 965,660 | mfu: -1.00 | total time: 2.36m\n",
      "step 00283 (35.15%) | loss: 1.366089 | lrm: 1.00 | dt: 538.84ms | tok/sec: 972,988 | mfu: -1.00 | total time: 2.37m\n",
      "step 00284 (35.27%) | loss: 1.374306 | lrm: 1.00 | dt: 469.45ms | tok/sec: 1,116,809 | mfu: -1.00 | total time: 2.38m\n",
      "step 00285 (35.37%) | loss: 1.380248 | lrm: 1.00 | dt: 485.23ms | tok/sec: 1,080,490 | mfu: -1.00 | total time: 2.38m\n",
      "step 00286 (35.48%) | loss: 1.374378 | lrm: 1.00 | dt: 468.55ms | tok/sec: 1,118,951 | mfu: -1.00 | total time: 2.39m\n",
      "step 00287 (35.59%) | loss: 1.384219 | lrm: 1.00 | dt: 544.72ms | tok/sec: 962,490 | mfu: -1.00 | total time: 2.40m\n",
      "step 00288 (35.71%) | loss: 1.390036 | lrm: 1.00 | dt: 466.85ms | tok/sec: 1,123,037 | mfu: -1.00 | total time: 2.41m\n",
      "step 00289 (35.84%) | loss: 1.379598 | lrm: 1.00 | dt: 478.26ms | tok/sec: 1,096,234 | mfu: -1.00 | total time: 2.42m\n",
      "step 00290 (35.96%) | loss: 1.371501 | lrm: 1.00 | dt: 531.49ms | tok/sec: 986,449 | mfu: -1.00 | total time: 2.42m\n",
      "step 00291 (36.09%) | loss: 1.390901 | lrm: 1.00 | dt: 620.24ms | tok/sec: 845,305 | mfu: -1.00 | total time: 2.43m\n",
      "step 00292 (36.22%) | loss: 1.372771 | lrm: 1.00 | dt: 605.17ms | tok/sec: 866,342 | mfu: -1.00 | total time: 2.45m\n",
      "step 00293 (36.34%) | loss: 1.363294 | lrm: 1.00 | dt: 471.39ms | tok/sec: 1,112,215 | mfu: -1.00 | total time: 2.45m\n",
      "step 00294 (36.46%) | loss: 1.376494 | lrm: 1.00 | dt: 482.77ms | tok/sec: 1,085,993 | mfu: -1.00 | total time: 2.46m\n",
      "step 00295 (36.60%) | loss: 1.376800 | lrm: 1.00 | dt: 470.23ms | tok/sec: 1,114,967 | mfu: -1.00 | total time: 2.47m\n",
      "step 00296 (36.73%) | loss: 1.366835 | lrm: 1.00 | dt: 477.45ms | tok/sec: 1,098,110 | mfu: -1.00 | total time: 2.48m\n",
      "step 00297 (36.86%) | loss: 1.361940 | lrm: 1.00 | dt: 536.24ms | tok/sec: 977,719 | mfu: -1.00 | total time: 2.49m\n",
      "step 00298 (37.00%) | loss: 1.370607 | lrm: 1.00 | dt: 536.65ms | tok/sec: 976,972 | mfu: -1.00 | total time: 2.49m\n",
      "step 00299 (37.10%) | loss: 1.363296 | lrm: 1.00 | dt: 536.37ms | tok/sec: 977,482 | mfu: -1.00 | total time: 2.50m\n",
      "step 00300 (37.23%) | loss: 1.363002 | lrm: 1.00 | dt: 468.74ms | tok/sec: 1,118,502 | mfu: -1.00 | total time: 2.51m\n",
      "step 00300 | Validation bpb: 0.4284\n",
      "step 00301 (37.34%) | loss: 1.353422 | lrm: 1.00 | dt: 525.42ms | tok/sec: 997,841 | mfu: -1.00 | total time: 2.52m\n",
      "step 00302 (37.45%) | loss: 1.348556 | lrm: 1.00 | dt: 467.77ms | tok/sec: 1,120,829 | mfu: -1.00 | total time: 2.53m\n",
      "step 00303 (37.58%) | loss: 1.352444 | lrm: 1.00 | dt: 475.58ms | tok/sec: 1,102,409 | mfu: -1.00 | total time: 2.54m\n",
      "step 00304 (37.71%) | loss: 1.348781 | lrm: 1.00 | dt: 476.15ms | tok/sec: 1,101,099 | mfu: -1.00 | total time: 2.54m\n",
      "step 00305 (37.84%) | loss: 1.346935 | lrm: 1.00 | dt: 524.34ms | tok/sec: 999,895 | mfu: -1.00 | total time: 2.55m\n",
      "step 00306 (37.97%) | loss: 1.354352 | lrm: 1.00 | dt: 623.10ms | tok/sec: 841,412 | mfu: -1.00 | total time: 2.56m\n",
      "step 00307 (38.09%) | loss: 1.356755 | lrm: 1.00 | dt: 534.28ms | tok/sec: 981,302 | mfu: -1.00 | total time: 2.57m\n",
      "step 00308 (38.20%) | loss: 1.367309 | lrm: 1.00 | dt: 466.97ms | tok/sec: 1,122,740 | mfu: -1.00 | total time: 2.58m\n",
      "step 00309 (38.31%) | loss: 1.375881 | lrm: 1.00 | dt: 542.70ms | tok/sec: 966,075 | mfu: -1.00 | total time: 2.59m\n",
      "step 00310 (38.44%) | loss: 1.375789 | lrm: 1.00 | dt: 614.84ms | tok/sec: 852,720 | mfu: -1.00 | total time: 2.60m\n",
      "step 00311 (38.54%) | loss: 1.367561 | lrm: 1.00 | dt: 531.80ms | tok/sec: 985,883 | mfu: -1.00 | total time: 2.61m\n",
      "step 00312 (38.66%) | loss: 1.363997 | lrm: 1.00 | dt: 468.46ms | tok/sec: 1,119,163 | mfu: -1.00 | total time: 2.62m\n",
      "step 00313 (38.79%) | loss: 1.353372 | lrm: 1.00 | dt: 554.53ms | tok/sec: 945,469 | mfu: -1.00 | total time: 2.62m\n",
      "step 00314 (38.90%) | loss: 1.362745 | lrm: 1.00 | dt: 467.95ms | tok/sec: 1,120,381 | mfu: -1.00 | total time: 2.63m\n",
      "step 00315 (39.01%) | loss: 1.353990 | lrm: 1.00 | dt: 539.43ms | tok/sec: 971,930 | mfu: -1.00 | total time: 2.64m\n",
      "step 00316 (39.12%) | loss: 1.362448 | lrm: 1.00 | dt: 526.81ms | tok/sec: 995,215 | mfu: -1.00 | total time: 2.65m\n",
      "step 00317 (39.25%) | loss: 1.359707 | lrm: 1.00 | dt: 619.39ms | tok/sec: 846,453 | mfu: -1.00 | total time: 2.66m\n",
      "step 00318 (39.35%) | loss: 1.363977 | lrm: 1.00 | dt: 538.39ms | tok/sec: 973,811 | mfu: -1.00 | total time: 2.67m\n",
      "step 00319 (39.46%) | loss: 1.358658 | lrm: 1.00 | dt: 594.47ms | tok/sec: 881,936 | mfu: -1.00 | total time: 2.68m\n",
      "step 00320 (39.59%) | loss: 1.356551 | lrm: 1.00 | dt: 531.75ms | tok/sec: 985,972 | mfu: -1.00 | total time: 2.69m\n",
      "step 00321 (39.72%) | loss: 1.351219 | lrm: 1.00 | dt: 529.75ms | tok/sec: 989,689 | mfu: -1.00 | total time: 2.70m\n",
      "step 00322 (39.83%) | loss: 1.347753 | lrm: 1.00 | dt: 533.81ms | tok/sec: 982,169 | mfu: -1.00 | total time: 2.71m\n",
      "step 00323 (39.94%) | loss: 1.351392 | lrm: 1.00 | dt: 534.69ms | tok/sec: 980,542 | mfu: -1.00 | total time: 2.72m\n",
      "step 00324 (40.06%) | loss: 1.356004 | lrm: 1.00 | dt: 466.88ms | tok/sec: 1,122,950 | mfu: -1.00 | total time: 2.72m\n",
      "step 00325 (40.19%) | loss: 1.344115 | lrm: 1.00 | dt: 528.77ms | tok/sec: 991,519 | mfu: -1.00 | total time: 2.73m\n",
      "step 00326 (40.31%) | loss: 1.350026 | lrm: 1.00 | dt: 466.80ms | tok/sec: 1,123,143 | mfu: -1.00 | total time: 2.74m\n",
      "step 00327 (40.43%) | loss: 1.363280 | lrm: 1.00 | dt: 548.35ms | tok/sec: 956,118 | mfu: -1.00 | total time: 2.75m\n",
      "step 00328 (40.56%) | loss: 1.362974 | lrm: 1.00 | dt: 465.19ms | tok/sec: 1,127,029 | mfu: -1.00 | total time: 2.76m\n",
      "step 00329 (40.68%) | loss: 1.350794 | lrm: 1.00 | dt: 541.89ms | tok/sec: 967,512 | mfu: -1.00 | total time: 2.77m\n",
      "step 00330 (40.80%) | loss: 1.342899 | lrm: 1.00 | dt: 529.31ms | tok/sec: 990,510 | mfu: -1.00 | total time: 2.77m\n",
      "step 00331 (40.91%) | loss: 1.336548 | lrm: 1.00 | dt: 467.97ms | tok/sec: 1,120,335 | mfu: -1.00 | total time: 2.78m\n",
      "step 00332 (41.02%) | loss: 1.338346 | lrm: 1.00 | dt: 478.17ms | tok/sec: 1,096,444 | mfu: -1.00 | total time: 2.79m\n",
      "step 00333 (41.15%) | loss: 1.328829 | lrm: 1.00 | dt: 475.06ms | tok/sec: 1,103,620 | mfu: -1.00 | total time: 2.80m\n",
      "step 00334 (41.28%) | loss: 1.329947 | lrm: 1.00 | dt: 541.80ms | tok/sec: 967,676 | mfu: -1.00 | total time: 2.81m\n",
      "step 00335 (41.40%) | loss: 1.322311 | lrm: 1.00 | dt: 466.11ms | tok/sec: 1,124,821 | mfu: -1.00 | total time: 2.81m\n",
      "step 00336 (41.54%) | loss: 1.327100 | lrm: 1.00 | dt: 548.44ms | tok/sec: 955,954 | mfu: -1.00 | total time: 2.82m\n",
      "step 00337 (41.68%) | loss: 1.327425 | lrm: 1.00 | dt: 536.29ms | tok/sec: 977,617 | mfu: -1.00 | total time: 2.83m\n",
      "step 00338 (41.79%) | loss: 1.324352 | lrm: 1.00 | dt: 466.33ms | tok/sec: 1,124,278 | mfu: -1.00 | total time: 2.84m\n",
      "step 00339 (41.93%) | loss: 1.323704 | lrm: 1.00 | dt: 479.96ms | tok/sec: 1,092,365 | mfu: -1.00 | total time: 2.85m\n",
      "step 00340 (42.06%) | loss: 1.333981 | lrm: 1.00 | dt: 549.20ms | tok/sec: 954,634 | mfu: -1.00 | total time: 2.86m\n",
      "step 00341 (42.19%) | loss: 1.328509 | lrm: 1.00 | dt: 530.95ms | tok/sec: 987,449 | mfu: -1.00 | total time: 2.87m\n",
      "step 00342 (42.30%) | loss: 1.338544 | lrm: 1.00 | dt: 466.73ms | tok/sec: 1,123,310 | mfu: -1.00 | total time: 2.87m\n",
      "step 00343 (42.43%) | loss: 1.330907 | lrm: 1.00 | dt: 479.95ms | tok/sec: 1,092,383 | mfu: -1.00 | total time: 2.88m\n",
      "step 00344 (42.57%) | loss: 1.324976 | lrm: 1.00 | dt: 541.85ms | tok/sec: 967,593 | mfu: -1.00 | total time: 2.89m\n",
      "step 00345 (42.70%) | loss: 1.320919 | lrm: 1.00 | dt: 468.18ms | tok/sec: 1,119,838 | mfu: -1.00 | total time: 2.90m\n",
      "step 00346 (42.82%) | loss: 1.318818 | lrm: 1.00 | dt: 479.56ms | tok/sec: 1,093,279 | mfu: -1.00 | total time: 2.91m\n",
      "step 00347 (42.94%) | loss: 1.320625 | lrm: 1.00 | dt: 471.58ms | tok/sec: 1,111,777 | mfu: -1.00 | total time: 2.92m\n",
      "step 00348 (43.07%) | loss: 1.317773 | lrm: 1.00 | dt: 476.35ms | tok/sec: 1,100,644 | mfu: -1.00 | total time: 2.92m\n",
      "step 00349 (43.19%) | loss: 1.313510 | lrm: 1.00 | dt: 471.70ms | tok/sec: 1,111,490 | mfu: -1.00 | total time: 2.93m\n",
      "step 00350 (43.31%) | loss: 1.318201 | lrm: 1.00 | dt: 545.86ms | tok/sec: 960,489 | mfu: -1.00 | total time: 2.94m\n",
      "step 00351 (43.44%) | loss: 1.322504 | lrm: 1.00 | dt: 551.52ms | tok/sec: 950,623 | mfu: -1.00 | total time: 2.95m\n",
      "step 00352 (43.57%) | loss: 1.313623 | lrm: 1.00 | dt: 517.48ms | tok/sec: 1,013,152 | mfu: -1.00 | total time: 2.96m\n",
      "step 00353 (43.70%) | loss: 1.314878 | lrm: 1.00 | dt: 535.25ms | tok/sec: 979,515 | mfu: -1.00 | total time: 2.97m\n",
      "step 00354 (43.80%) | loss: 1.338078 | lrm: 1.00 | dt: 467.44ms | tok/sec: 1,121,622 | mfu: -1.00 | total time: 2.97m\n",
      "step 00355 (43.94%) | loss: 1.335650 | lrm: 1.00 | dt: 482.91ms | tok/sec: 1,085,689 | mfu: -1.00 | total time: 2.98m\n",
      "step 00356 (44.05%) | loss: 1.331695 | lrm: 1.00 | dt: 541.56ms | tok/sec: 968,098 | mfu: -1.00 | total time: 2.99m\n",
      "step 00357 (44.15%) | loss: 1.319215 | lrm: 1.00 | dt: 466.84ms | tok/sec: 1,123,046 | mfu: -1.00 | total time: 3.00m\n",
      "step 00358 (44.28%) | loss: 1.303504 | lrm: 1.00 | dt: 534.89ms | tok/sec: 980,177 | mfu: -1.00 | total time: 3.01m\n",
      "step 00359 (44.42%) | loss: 1.299407 | lrm: 1.00 | dt: 465.76ms | tok/sec: 1,125,664 | mfu: -1.00 | total time: 3.02m\n",
      "step 00360 (44.55%) | loss: 1.300923 | lrm: 1.00 | dt: 478.40ms | tok/sec: 1,095,928 | mfu: -1.00 | total time: 3.02m\n",
      "step 00361 (44.68%) | loss: 1.299734 | lrm: 1.00 | dt: 531.35ms | tok/sec: 986,705 | mfu: -1.00 | total time: 3.03m\n",
      "step 00362 (44.82%) | loss: 1.300182 | lrm: 1.00 | dt: 465.60ms | tok/sec: 1,126,038 | mfu: -1.00 | total time: 3.04m\n",
      "step 00363 (44.94%) | loss: 1.298434 | lrm: 1.00 | dt: 478.80ms | tok/sec: 1,095,012 | mfu: -1.00 | total time: 3.05m\n",
      "step 00364 (45.09%) | loss: 1.305278 | lrm: 1.00 | dt: 545.49ms | tok/sec: 961,136 | mfu: -1.00 | total time: 3.06m\n",
      "step 00365 (45.20%) | loss: 1.302621 | lrm: 1.00 | dt: 540.31ms | tok/sec: 970,355 | mfu: -1.00 | total time: 3.07m\n",
      "step 00366 (45.32%) | loss: 1.296157 | lrm: 1.00 | dt: 465.14ms | tok/sec: 1,127,173 | mfu: -1.00 | total time: 3.07m\n",
      "step 00367 (45.45%) | loss: 1.294816 | lrm: 1.00 | dt: 480.48ms | tok/sec: 1,091,177 | mfu: -1.00 | total time: 3.08m\n",
      "step 00368 (45.59%) | loss: 1.307780 | lrm: 1.00 | dt: 473.24ms | tok/sec: 1,107,860 | mfu: -1.00 | total time: 3.09m\n",
      "step 00369 (45.71%) | loss: 1.317780 | lrm: 1.00 | dt: 595.23ms | tok/sec: 880,822 | mfu: -1.00 | total time: 3.10m\n",
      "step 00370 (45.83%) | loss: 1.320529 | lrm: 1.00 | dt: 530.38ms | tok/sec: 988,522 | mfu: -1.00 | total time: 3.11m\n",
      "step 00371 (45.98%) | loss: 1.312213 | lrm: 1.00 | dt: 533.76ms | tok/sec: 982,258 | mfu: -1.00 | total time: 3.12m\n",
      "step 00372 (46.11%) | loss: 1.305470 | lrm: 1.00 | dt: 467.21ms | tok/sec: 1,122,179 | mfu: -1.00 | total time: 3.13m\n",
      "step 00373 (46.24%) | loss: 1.310095 | lrm: 1.00 | dt: 540.90ms | tok/sec: 969,280 | mfu: -1.00 | total time: 3.13m\n",
      "step 00374 (46.37%) | loss: 1.318748 | lrm: 1.00 | dt: 465.41ms | tok/sec: 1,126,507 | mfu: -1.00 | total time: 3.14m\n",
      "step 00375 (46.49%) | loss: 1.318384 | lrm: 1.00 | dt: 601.76ms | tok/sec: 871,251 | mfu: -1.00 | total time: 3.15m\n",
      "step 00376 (46.60%) | loss: 1.323478 | lrm: 1.00 | dt: 534.34ms | tok/sec: 981,195 | mfu: -1.00 | total time: 3.16m\n",
      "step 00377 (46.74%) | loss: 1.318159 | lrm: 1.00 | dt: 540.64ms | tok/sec: 969,754 | mfu: -1.00 | total time: 3.17m\n",
      "step 00378 (46.87%) | loss: 1.329694 | lrm: 1.00 | dt: 595.32ms | tok/sec: 880,689 | mfu: -1.00 | total time: 3.18m\n",
      "step 00379 (47.01%) | loss: 1.333061 | lrm: 1.00 | dt: 468.64ms | tok/sec: 1,118,741 | mfu: -1.00 | total time: 3.19m\n",
      "step 00380 (47.13%) | loss: 1.334996 | lrm: 1.00 | dt: 484.94ms | tok/sec: 1,081,140 | mfu: -1.00 | total time: 3.20m\n",
      "step 00381 (47.26%) | loss: 1.337796 | lrm: 1.00 | dt: 537.67ms | tok/sec: 975,118 | mfu: -1.00 | total time: 3.21m\n",
      "step 00382 (47.39%) | loss: 1.334987 | lrm: 1.00 | dt: 529.93ms | tok/sec: 989,347 | mfu: -1.00 | total time: 3.21m\n",
      "step 00383 (47.51%) | loss: 1.327960 | lrm: 1.00 | dt: 539.75ms | tok/sec: 971,355 | mfu: -1.00 | total time: 3.22m\n",
      "step 00384 (47.64%) | loss: 1.337891 | lrm: 1.00 | dt: 465.46ms | tok/sec: 1,126,397 | mfu: -1.00 | total time: 3.23m\n",
      "step 00385 (47.78%) | loss: 1.348678 | lrm: 1.00 | dt: 481.25ms | tok/sec: 1,089,438 | mfu: -1.00 | total time: 3.24m\n",
      "step 00386 (47.92%) | loss: 1.346603 | lrm: 1.00 | dt: 535.31ms | tok/sec: 979,404 | mfu: -1.00 | total time: 3.25m\n",
      "step 00387 (48.06%) | loss: 1.348768 | lrm: 1.00 | dt: 606.11ms | tok/sec: 865,002 | mfu: -1.00 | total time: 3.26m\n",
      "step 00388 (48.17%) | loss: 1.339115 | lrm: 1.00 | dt: 522.78ms | tok/sec: 1,002,892 | mfu: -1.00 | total time: 3.27m\n",
      "step 00389 (48.29%) | loss: 1.356920 | lrm: 1.00 | dt: 524.21ms | tok/sec: 1,000,140 | mfu: -1.00 | total time: 3.28m\n",
      "step 00390 (48.43%) | loss: 1.350074 | lrm: 1.00 | dt: 527.95ms | tok/sec: 993,069 | mfu: -1.00 | total time: 3.28m\n",
      "step 00391 (48.55%) | loss: 1.343846 | lrm: 1.00 | dt: 543.67ms | tok/sec: 964,355 | mfu: -1.00 | total time: 3.29m\n",
      "step 00392 (48.66%) | loss: 1.344041 | lrm: 1.00 | dt: 539.91ms | tok/sec: 971,074 | mfu: -1.00 | total time: 3.30m\n",
      "step 00393 (48.78%) | loss: 1.345600 | lrm: 1.00 | dt: 529.61ms | tok/sec: 989,956 | mfu: -1.00 | total time: 3.31m\n",
      "step 00394 (48.90%) | loss: 1.348539 | lrm: 1.00 | dt: 606.15ms | tok/sec: 864,940 | mfu: -1.00 | total time: 3.32m\n",
      "step 00395 (49.03%) | loss: 1.350052 | lrm: 1.00 | dt: 467.22ms | tok/sec: 1,122,134 | mfu: -1.00 | total time: 3.33m\n",
      "step 00396 (49.17%) | loss: 1.337807 | lrm: 1.00 | dt: 536.79ms | tok/sec: 976,706 | mfu: -1.00 | total time: 3.34m\n",
      "step 00397 (49.30%) | loss: 1.335183 | lrm: 1.00 | dt: 465.23ms | tok/sec: 1,126,932 | mfu: -1.00 | total time: 3.35m\n",
      "step 00398 (49.44%) | loss: 1.339429 | lrm: 1.00 | dt: 542.37ms | tok/sec: 966,666 | mfu: -1.00 | total time: 3.35m\n",
      "step 00399 (49.55%) | loss: 1.337937 | lrm: 1.00 | dt: 465.72ms | tok/sec: 1,125,749 | mfu: -1.00 | total time: 3.36m\n",
      "step 00400 (49.67%) | loss: 1.349107 | lrm: 1.00 | dt: 610.78ms | tok/sec: 858,394 | mfu: -1.00 | total time: 3.37m\n",
      "step 00401 (49.81%) | loss: 1.344376 | lrm: 1.00 | dt: 528.75ms | tok/sec: 991,561 | mfu: -1.00 | total time: 3.38m\n",
      "step 00402 (49.94%) | loss: 1.335010 | lrm: 1.00 | dt: 603.16ms | tok/sec: 869,231 | mfu: -1.00 | total time: 3.39m\n",
      "step 00403 (50.06%) | loss: 1.322186 | lrm: 1.00 | dt: 527.28ms | tok/sec: 994,326 | mfu: -1.00 | total time: 3.40m\n",
      "step 00404 (50.18%) | loss: 1.331711 | lrm: 1.00 | dt: 527.88ms | tok/sec: 993,204 | mfu: -1.00 | total time: 3.41m\n",
      "step 00405 (50.29%) | loss: 1.317719 | lrm: 1.00 | dt: 530.30ms | tok/sec: 988,656 | mfu: -1.00 | total time: 3.42m\n",
      "step 00406 (50.41%) | loss: 1.318672 | lrm: 1.00 | dt: 468.24ms | tok/sec: 1,119,701 | mfu: -1.00 | total time: 3.43m\n",
      "step 00407 (50.54%) | loss: 1.316224 | lrm: 1.00 | dt: 473.55ms | tok/sec: 1,107,146 | mfu: -1.00 | total time: 3.43m\n",
      "step 00408 (50.65%) | loss: 1.314686 | lrm: 1.00 | dt: 475.09ms | tok/sec: 1,103,554 | mfu: -1.00 | total time: 3.44m\n",
      "step 00409 (50.78%) | loss: 1.316400 | lrm: 1.00 | dt: 540.23ms | tok/sec: 970,498 | mfu: -1.00 | total time: 3.45m\n",
      "step 00410 (50.90%) | loss: 1.310576 | lrm: 1.00 | dt: 468.92ms | tok/sec: 1,118,076 | mfu: -1.00 | total time: 3.46m\n",
      "step 00411 (51.01%) | loss: 1.310149 | lrm: 1.00 | dt: 478.94ms | tok/sec: 1,094,673 | mfu: -1.00 | total time: 3.47m\n",
      "step 00412 (51.13%) | loss: 1.311507 | lrm: 1.00 | dt: 476.61ms | tok/sec: 1,100,035 | mfu: -1.00 | total time: 3.47m\n",
      "step 00413 (51.25%) | loss: 1.309176 | lrm: 1.00 | dt: 472.72ms | tok/sec: 1,109,080 | mfu: -1.00 | total time: 3.48m\n",
      "step 00414 (51.38%) | loss: 1.309370 | lrm: 1.00 | dt: 477.63ms | tok/sec: 1,097,676 | mfu: -1.00 | total time: 3.49m\n",
      "step 00415 (51.49%) | loss: 1.305257 | lrm: 1.00 | dt: 471.86ms | tok/sec: 1,111,120 | mfu: -1.00 | total time: 3.50m\n",
      "step 00416 (51.60%) | loss: 1.300752 | lrm: 1.00 | dt: 476.23ms | tok/sec: 1,100,916 | mfu: -1.00 | total time: 3.51m\n",
      "step 00417 (51.72%) | loss: 1.299332 | lrm: 1.00 | dt: 470.42ms | tok/sec: 1,114,499 | mfu: -1.00 | total time: 3.51m\n",
      "step 00418 (51.83%) | loss: 1.298393 | lrm: 1.00 | dt: 477.13ms | tok/sec: 1,098,832 | mfu: -1.00 | total time: 3.52m\n",
      "step 00419 (51.97%) | loss: 1.292150 | lrm: 1.00 | dt: 473.40ms | tok/sec: 1,107,503 | mfu: -1.00 | total time: 3.53m\n",
      "step 00420 (52.09%) | loss: 1.284988 | lrm: 1.00 | dt: 475.69ms | tok/sec: 1,102,152 | mfu: -1.00 | total time: 3.54m\n",
      "step 00421 (52.19%) | loss: 1.285886 | lrm: 1.00 | dt: 526.66ms | tok/sec: 995,489 | mfu: -1.00 | total time: 3.55m\n",
      "step 00422 (52.33%) | loss: 1.294541 | lrm: 1.00 | dt: 467.76ms | tok/sec: 1,120,848 | mfu: -1.00 | total time: 3.55m\n",
      "step 00423 (52.46%) | loss: 1.293664 | lrm: 1.00 | dt: 477.02ms | tok/sec: 1,099,101 | mfu: -1.00 | total time: 3.56m\n",
      "step 00424 (52.59%) | loss: 1.286093 | lrm: 1.00 | dt: 473.45ms | tok/sec: 1,107,378 | mfu: -1.00 | total time: 3.57m\n",
      "step 00425 (52.71%) | loss: 1.292211 | lrm: 1.00 | dt: 534.14ms | tok/sec: 981,553 | mfu: -1.00 | total time: 3.58m\n",
      "step 00426 (52.81%) | loss: 1.298986 | lrm: 1.00 | dt: 467.53ms | tok/sec: 1,121,406 | mfu: -1.00 | total time: 3.59m\n",
      "step 00427 (52.91%) | loss: 1.297212 | lrm: 1.00 | dt: 480.06ms | tok/sec: 1,092,125 | mfu: -1.00 | total time: 3.59m\n",
      "step 00428 (53.04%) | loss: 1.294095 | lrm: 1.00 | dt: 534.60ms | tok/sec: 980,719 | mfu: -1.00 | total time: 3.60m\n",
      "step 00429 (53.15%) | loss: 1.295230 | lrm: 1.00 | dt: 465.57ms | tok/sec: 1,126,131 | mfu: -1.00 | total time: 3.61m\n",
      "step 00430 (53.27%) | loss: 1.307514 | lrm: 1.00 | dt: 478.33ms | tok/sec: 1,096,072 | mfu: -1.00 | total time: 3.62m\n",
      "step 00431 (53.38%) | loss: 1.296462 | lrm: 1.00 | dt: 472.80ms | tok/sec: 1,108,891 | mfu: -1.00 | total time: 3.63m\n",
      "step 00432 (53.49%) | loss: 1.299061 | lrm: 1.00 | dt: 473.55ms | tok/sec: 1,107,154 | mfu: -1.00 | total time: 3.64m\n",
      "step 00433 (53.61%) | loss: 1.304932 | lrm: 1.00 | dt: 474.22ms | tok/sec: 1,105,574 | mfu: -1.00 | total time: 3.64m\n",
      "step 00434 (53.72%) | loss: 1.300936 | lrm: 1.00 | dt: 472.03ms | tok/sec: 1,110,700 | mfu: -1.00 | total time: 3.65m\n",
      "step 00435 (53.85%) | loss: 1.292463 | lrm: 1.00 | dt: 473.54ms | tok/sec: 1,107,176 | mfu: -1.00 | total time: 3.66m\n",
      "step 00436 (53.96%) | loss: 1.288114 | lrm: 1.00 | dt: 471.60ms | tok/sec: 1,111,718 | mfu: -1.00 | total time: 3.67m\n",
      "step 00437 (54.10%) | loss: 1.292919 | lrm: 1.00 | dt: 474.38ms | tok/sec: 1,105,205 | mfu: -1.00 | total time: 3.67m\n",
      "step 00438 (54.21%) | loss: 1.289550 | lrm: 1.00 | dt: 473.19ms | tok/sec: 1,107,991 | mfu: -1.00 | total time: 3.68m\n",
      "step 00439 (54.33%) | loss: 1.287134 | lrm: 1.00 | dt: 475.28ms | tok/sec: 1,103,125 | mfu: -1.00 | total time: 3.69m\n",
      "step 00440 (54.45%) | loss: 1.293736 | lrm: 1.00 | dt: 471.40ms | tok/sec: 1,112,182 | mfu: -1.00 | total time: 3.70m\n",
      "step 00441 (54.58%) | loss: 1.300806 | lrm: 1.00 | dt: 541.86ms | tok/sec: 967,566 | mfu: -1.00 | total time: 3.71m\n",
      "step 00442 (54.68%) | loss: 1.303017 | lrm: 1.00 | dt: 536.96ms | tok/sec: 976,406 | mfu: -1.00 | total time: 3.72m\n",
      "step 00443 (54.78%) | loss: 1.293997 | lrm: 1.00 | dt: 534.54ms | tok/sec: 980,818 | mfu: -1.00 | total time: 3.72m\n",
      "step 00444 (54.88%) | loss: 1.296265 | lrm: 1.00 | dt: 468.94ms | tok/sec: 1,118,024 | mfu: -1.00 | total time: 3.73m\n",
      "step 00445 (55.02%) | loss: 1.290839 | lrm: 1.00 | dt: 533.09ms | tok/sec: 983,493 | mfu: -1.00 | total time: 3.74m\n",
      "step 00446 (55.13%) | loss: 1.300366 | lrm: 1.00 | dt: 466.77ms | tok/sec: 1,123,219 | mfu: -1.00 | total time: 3.75m\n",
      "step 00447 (55.25%) | loss: 1.293611 | lrm: 1.00 | dt: 477.52ms | tok/sec: 1,097,944 | mfu: -1.00 | total time: 3.76m\n",
      "step 00448 (55.36%) | loss: 1.291630 | lrm: 1.00 | dt: 543.16ms | tok/sec: 965,253 | mfu: -1.00 | total time: 3.77m\n",
      "step 00449 (55.49%) | loss: 1.292091 | lrm: 1.00 | dt: 466.04ms | tok/sec: 1,124,987 | mfu: -1.00 | total time: 3.77m\n",
      "step 00450 (55.60%) | loss: 1.288789 | lrm: 1.00 | dt: 482.54ms | tok/sec: 1,086,521 | mfu: -1.00 | total time: 3.78m\n",
      "step 00450 | Validation bpb: 0.4169\n",
      "step 00451 (55.72%) | loss: 1.296067 | lrm: 1.00 | dt: 465.92ms | tok/sec: 1,125,277 | mfu: -1.00 | total time: 3.79m\n",
      "step 00452 (55.85%) | loss: 1.290058 | lrm: 1.00 | dt: 473.84ms | tok/sec: 1,106,456 | mfu: -1.00 | total time: 3.80m\n",
      "step 00453 (55.95%) | loss: 1.285000 | lrm: 1.00 | dt: 475.22ms | tok/sec: 1,103,253 | mfu: -1.00 | total time: 3.81m\n",
      "step 00454 (56.08%) | loss: 1.285054 | lrm: 1.00 | dt: 470.96ms | tok/sec: 1,113,222 | mfu: -1.00 | total time: 3.81m\n",
      "step 00455 (56.20%) | loss: 1.281280 | lrm: 1.00 | dt: 472.94ms | tok/sec: 1,108,583 | mfu: -1.00 | total time: 3.82m\n",
      "step 00456 (56.32%) | loss: 1.280266 | lrm: 1.00 | dt: 527.94ms | tok/sec: 993,076 | mfu: -1.00 | total time: 3.83m\n",
      "step 00457 (56.46%) | loss: 1.289124 | lrm: 1.00 | dt: 465.52ms | tok/sec: 1,126,243 | mfu: -1.00 | total time: 3.84m\n",
      "step 00458 (56.58%) | loss: 1.293783 | lrm: 1.00 | dt: 479.49ms | tok/sec: 1,093,426 | mfu: -1.00 | total time: 3.85m\n",
      "step 00459 (56.69%) | loss: 1.294803 | lrm: 1.00 | dt: 472.94ms | tok/sec: 1,108,577 | mfu: -1.00 | total time: 3.85m\n",
      "step 00460 (56.81%) | loss: 1.294508 | lrm: 1.00 | dt: 536.11ms | tok/sec: 977,943 | mfu: -1.00 | total time: 3.86m\n",
      "step 00461 (56.94%) | loss: 1.300783 | lrm: 1.00 | dt: 466.04ms | tok/sec: 1,124,973 | mfu: -1.00 | total time: 3.87m\n",
      "step 00462 (57.06%) | loss: 1.303570 | lrm: 1.00 | dt: 532.35ms | tok/sec: 984,855 | mfu: -1.00 | total time: 3.88m\n",
      "step 00463 (57.17%) | loss: 1.305265 | lrm: 1.00 | dt: 534.92ms | tok/sec: 980,128 | mfu: -1.00 | total time: 3.89m\n",
      "step 00464 (57.30%) | loss: 1.301788 | lrm: 1.00 | dt: 525.48ms | tok/sec: 997,725 | mfu: -1.00 | total time: 3.90m\n",
      "step 00465 (57.44%) | loss: 1.295794 | lrm: 1.00 | dt: 530.22ms | tok/sec: 988,817 | mfu: -1.00 | total time: 3.91m\n",
      "step 00466 (57.57%) | loss: 1.297973 | lrm: 1.00 | dt: 533.78ms | tok/sec: 982,223 | mfu: -1.00 | total time: 3.92m\n",
      "step 00467 (57.70%) | loss: 1.306268 | lrm: 1.00 | dt: 531.08ms | tok/sec: 987,215 | mfu: -1.00 | total time: 3.92m\n",
      "step 00468 (57.82%) | loss: 1.307136 | lrm: 1.00 | dt: 527.10ms | tok/sec: 994,668 | mfu: -1.00 | total time: 3.93m\n",
      "step 00469 (57.93%) | loss: 1.301865 | lrm: 1.00 | dt: 465.52ms | tok/sec: 1,126,232 | mfu: -1.00 | total time: 3.94m\n",
      "step 00470 (58.04%) | loss: 1.306690 | lrm: 1.00 | dt: 477.09ms | tok/sec: 1,098,922 | mfu: -1.00 | total time: 3.95m\n",
      "step 00471 (58.15%) | loss: 1.305735 | lrm: 1.00 | dt: 473.33ms | tok/sec: 1,107,655 | mfu: -1.00 | total time: 3.96m\n",
      "step 00472 (58.27%) | loss: 1.299781 | lrm: 1.00 | dt: 471.07ms | tok/sec: 1,112,967 | mfu: -1.00 | total time: 3.96m\n",
      "step 00473 (58.39%) | loss: 1.302735 | lrm: 1.00 | dt: 474.35ms | tok/sec: 1,105,284 | mfu: -1.00 | total time: 3.97m\n",
      "step 00474 (58.51%) | loss: 1.295049 | lrm: 1.00 | dt: 533.39ms | tok/sec: 982,944 | mfu: -1.00 | total time: 3.98m\n",
      "step 00475 (58.64%) | loss: 1.291023 | lrm: 1.00 | dt: 524.45ms | tok/sec: 999,699 | mfu: -1.00 | total time: 3.99m\n",
      "step 00476 (58.77%) | loss: 1.295283 | lrm: 1.00 | dt: 533.57ms | tok/sec: 982,595 | mfu: -1.00 | total time: 4.00m\n",
      "step 00477 (58.89%) | loss: 1.298050 | lrm: 1.00 | dt: 607.08ms | tok/sec: 863,616 | mfu: -1.00 | total time: 4.01m\n",
      "step 00478 (59.02%) | loss: 1.284580 | lrm: 1.00 | dt: 545.01ms | tok/sec: 961,972 | mfu: -1.00 | total time: 4.02m\n",
      "step 00479 (59.13%) | loss: 1.291152 | lrm: 1.00 | dt: 531.39ms | tok/sec: 986,643 | mfu: -1.00 | total time: 4.03m\n",
      "step 00480 (59.28%) | loss: 1.299416 | lrm: 1.00 | dt: 533.11ms | tok/sec: 983,458 | mfu: -1.00 | total time: 4.04m\n",
      "step 00481 (59.39%) | loss: 1.296389 | lrm: 1.00 | dt: 466.16ms | tok/sec: 1,124,706 | mfu: -1.00 | total time: 4.04m\n",
      "step 00482 (59.50%) | loss: 1.292751 | lrm: 1.00 | dt: 530.21ms | tok/sec: 988,832 | mfu: -1.00 | total time: 4.05m\n",
      "step 00483 (59.63%) | loss: 1.311493 | lrm: 1.00 | dt: 524.95ms | tok/sec: 998,743 | mfu: -1.00 | total time: 4.06m\n",
      "step 00484 (59.75%) | loss: 1.301236 | lrm: 1.00 | dt: 465.90ms | tok/sec: 1,125,331 | mfu: -1.00 | total time: 4.07m\n",
      "step 00485 (59.88%) | loss: 1.294111 | lrm: 1.00 | dt: 633.26ms | tok/sec: 827,917 | mfu: -1.00 | total time: 4.08m\n",
      "step 00486 (60.01%) | loss: 1.288808 | lrm: 1.00 | dt: 521.08ms | tok/sec: 1,006,158 | mfu: -1.00 | total time: 4.09m\n",
      "step 00487 (60.13%) | loss: 1.288528 | lrm: 1.00 | dt: 466.81ms | tok/sec: 1,123,128 | mfu: -1.00 | total time: 4.10m\n",
      "step 00488 (60.25%) | loss: 1.280160 | lrm: 1.00 | dt: 478.99ms | tok/sec: 1,094,570 | mfu: -1.00 | total time: 4.10m\n",
      "step 00489 (60.40%) | loss: 1.275157 | lrm: 1.00 | dt: 474.86ms | tok/sec: 1,104,098 | mfu: -1.00 | total time: 4.11m\n",
      "step 00490 (60.52%) | loss: 1.279172 | lrm: 1.00 | dt: 475.26ms | tok/sec: 1,103,153 | mfu: -1.00 | total time: 4.12m\n",
      "step 00491 (60.65%) | loss: 1.286684 | lrm: 1.00 | dt: 540.79ms | tok/sec: 969,483 | mfu: -1.00 | total time: 4.13m\n",
      "step 00492 (60.78%) | loss: 1.294183 | lrm: 1.00 | dt: 466.96ms | tok/sec: 1,122,774 | mfu: -1.00 | total time: 4.14m\n",
      "step 00493 (60.90%) | loss: 1.297246 | lrm: 1.00 | dt: 479.52ms | tok/sec: 1,093,358 | mfu: -1.00 | total time: 4.14m\n",
      "step 00494 (61.04%) | loss: 1.291644 | lrm: 1.00 | dt: 470.33ms | tok/sec: 1,114,725 | mfu: -1.00 | total time: 4.15m\n",
      "step 00495 (61.18%) | loss: 1.294056 | lrm: 1.00 | dt: 532.06ms | tok/sec: 985,400 | mfu: -1.00 | total time: 4.16m\n",
      "step 00496 (61.32%) | loss: 1.291370 | lrm: 1.00 | dt: 466.40ms | tok/sec: 1,124,120 | mfu: -1.00 | total time: 4.17m\n",
      "step 00497 (61.45%) | loss: 1.295624 | lrm: 1.00 | dt: 538.14ms | tok/sec: 974,253 | mfu: -1.00 | total time: 4.18m\n",
      "step 00498 (61.56%) | loss: 1.297453 | lrm: 1.00 | dt: 546.47ms | tok/sec: 959,401 | mfu: -1.00 | total time: 4.19m\n",
      "step 00499 (61.70%) | loss: 1.299693 | lrm: 1.00 | dt: 618.73ms | tok/sec: 847,357 | mfu: -1.00 | total time: 4.20m\n",
      "step 00500 (61.81%) | loss: 1.309739 | lrm: 1.00 | dt: 683.06ms | tok/sec: 767,562 | mfu: -1.00 | total time: 4.21m\n",
      "step 00501 (61.95%) | loss: 1.303043 | lrm: 1.00 | dt: 524.91ms | tok/sec: 998,816 | mfu: -1.00 | total time: 4.22m\n",
      "step 00502 (62.07%) | loss: 1.298893 | lrm: 1.00 | dt: 466.17ms | tok/sec: 1,124,679 | mfu: -1.00 | total time: 4.23m\n",
      "step 00503 (62.19%) | loss: 1.307933 | lrm: 1.00 | dt: 476.82ms | tok/sec: 1,099,553 | mfu: -1.00 | total time: 4.23m\n",
      "step 00504 (62.32%) | loss: 1.313222 | lrm: 1.00 | dt: 553.18ms | tok/sec: 947,771 | mfu: -1.00 | total time: 4.24m\n",
      "step 00505 (62.45%) | loss: 1.304463 | lrm: 1.00 | dt: 615.10ms | tok/sec: 852,360 | mfu: -1.00 | total time: 4.25m\n",
      "step 00506 (62.59%) | loss: 1.304101 | lrm: 1.00 | dt: 533.47ms | tok/sec: 982,783 | mfu: -1.00 | total time: 4.26m\n",
      "step 00507 (62.71%) | loss: 1.300708 | lrm: 1.00 | dt: 537.71ms | tok/sec: 975,036 | mfu: -1.00 | total time: 4.27m\n",
      "step 00508 (62.83%) | loss: 1.302833 | lrm: 1.00 | dt: 533.22ms | tok/sec: 983,258 | mfu: -1.00 | total time: 4.28m\n",
      "step 00509 (62.95%) | loss: 1.299581 | lrm: 1.00 | dt: 530.55ms | tok/sec: 988,194 | mfu: -1.00 | total time: 4.29m\n",
      "step 00510 (63.07%) | loss: 1.292043 | lrm: 1.00 | dt: 533.47ms | tok/sec: 982,782 | mfu: -1.00 | total time: 4.30m\n",
      "step 00511 (63.20%) | loss: 1.293192 | lrm: 1.00 | dt: 529.88ms | tok/sec: 989,447 | mfu: -1.00 | total time: 4.31m\n",
      "step 00512 (63.32%) | loss: 1.294483 | lrm: 1.00 | dt: 696.81ms | tok/sec: 752,412 | mfu: -1.00 | total time: 4.32m\n",
      "step 00513 (63.44%) | loss: 1.291364 | lrm: 1.00 | dt: 685.25ms | tok/sec: 765,100 | mfu: -1.00 | total time: 4.33m\n",
      "step 00514 (63.55%) | loss: 1.301950 | lrm: 1.00 | dt: 614.43ms | tok/sec: 853,295 | mfu: -1.00 | total time: 4.34m\n",
      "step 00515 (63.69%) | loss: 1.307491 | lrm: 1.00 | dt: 610.67ms | tok/sec: 858,538 | mfu: -1.00 | total time: 4.35m\n",
      "step 00516 (63.81%) | loss: 1.312190 | lrm: 1.00 | dt: 634.27ms | tok/sec: 826,603 | mfu: -1.00 | total time: 4.36m\n",
      "step 00517 (63.94%) | loss: 1.319146 | lrm: 1.00 | dt: 536.22ms | tok/sec: 977,748 | mfu: -1.00 | total time: 4.37m\n",
      "step 00518 (64.06%) | loss: 1.311891 | lrm: 1.00 | dt: 466.60ms | tok/sec: 1,123,646 | mfu: -1.00 | total time: 4.38m\n",
      "step 00519 (64.18%) | loss: 1.303122 | lrm: 1.00 | dt: 543.07ms | tok/sec: 965,417 | mfu: -1.00 | total time: 4.39m\n",
      "step 00520 (64.29%) | loss: 1.307459 | lrm: 1.00 | dt: 467.88ms | tok/sec: 1,120,552 | mfu: -1.00 | total time: 4.39m\n",
      "step 00521 (64.41%) | loss: 1.303520 | lrm: 1.00 | dt: 543.36ms | tok/sec: 964,907 | mfu: -1.00 | total time: 4.40m\n",
      "step 00522 (64.52%) | loss: 1.305999 | lrm: 1.00 | dt: 614.97ms | tok/sec: 852,537 | mfu: -1.00 | total time: 4.41m\n",
      "step 00523 (64.64%) | loss: 1.306121 | lrm: 1.00 | dt: 468.17ms | tok/sec: 1,119,867 | mfu: -1.00 | total time: 4.42m\n",
      "step 00524 (64.77%) | loss: 1.300823 | lrm: 1.00 | dt: 544.32ms | tok/sec: 963,197 | mfu: -1.00 | total time: 4.43m\n",
      "step 00525 (64.91%) | loss: 1.295550 | lrm: 1.00 | dt: 533.72ms | tok/sec: 982,321 | mfu: -1.00 | total time: 4.44m\n",
      "step 00526 (65.03%) | loss: 1.299356 | lrm: 1.00 | dt: 467.77ms | tok/sec: 1,120,814 | mfu: -1.00 | total time: 4.45m\n",
      "step 00527 (65.17%) | loss: 1.298715 | lrm: 1.00 | dt: 548.37ms | tok/sec: 956,092 | mfu: -1.00 | total time: 4.46m\n",
      "step 00528 (65.29%) | loss: 1.292808 | lrm: 1.00 | dt: 529.31ms | tok/sec: 990,518 | mfu: -1.00 | total time: 4.46m\n",
      "step 00529 (65.42%) | loss: 1.290660 | lrm: 1.00 | dt: 468.65ms | tok/sec: 1,118,714 | mfu: -1.00 | total time: 4.47m\n",
      "step 00530 (65.54%) | loss: 1.291835 | lrm: 1.00 | dt: 613.22ms | tok/sec: 854,982 | mfu: -1.00 | total time: 4.48m\n",
      "step 00531 (65.65%) | loss: 1.296413 | lrm: 1.00 | dt: 610.68ms | tok/sec: 858,531 | mfu: -1.00 | total time: 4.49m\n",
      "step 00532 (65.78%) | loss: 1.290801 | lrm: 1.00 | dt: 466.75ms | tok/sec: 1,123,280 | mfu: -1.00 | total time: 4.50m\n",
      "step 00533 (65.89%) | loss: 1.304938 | lrm: 1.00 | dt: 479.71ms | tok/sec: 1,092,938 | mfu: -1.00 | total time: 4.51m\n",
      "step 00534 (66.04%) | loss: 1.304742 | lrm: 1.00 | dt: 472.68ms | tok/sec: 1,109,171 | mfu: -1.00 | total time: 4.52m\n",
      "step 00535 (66.17%) | loss: 1.305344 | lrm: 1.00 | dt: 473.89ms | tok/sec: 1,106,349 | mfu: -1.00 | total time: 4.52m\n",
      "step 00536 (66.28%) | loss: 1.314246 | lrm: 1.00 | dt: 617.97ms | tok/sec: 848,406 | mfu: -1.00 | total time: 4.53m\n",
      "step 00537 (66.40%) | loss: 1.312436 | lrm: 1.00 | dt: 527.32ms | tok/sec: 994,256 | mfu: -1.00 | total time: 4.54m\n",
      "step 00538 (66.54%) | loss: 1.313647 | lrm: 1.00 | dt: 522.81ms | tok/sec: 1,002,820 | mfu: -1.00 | total time: 4.55m\n",
      "step 00539 (66.67%) | loss: 1.313590 | lrm: 1.00 | dt: 533.56ms | tok/sec: 982,615 | mfu: -1.00 | total time: 4.56m\n",
      "step 00540 (66.80%) | loss: 1.304114 | lrm: 1.00 | dt: 465.59ms | tok/sec: 1,126,076 | mfu: -1.00 | total time: 4.57m\n",
      "step 00541 (66.93%) | loss: 1.301052 | lrm: 1.00 | dt: 470.97ms | tok/sec: 1,113,205 | mfu: -1.00 | total time: 4.58m\n",
      "step 00542 (67.06%) | loss: 1.292454 | lrm: 1.00 | dt: 534.78ms | tok/sec: 980,376 | mfu: -1.00 | total time: 4.59m\n",
      "step 00543 (67.16%) | loss: 1.296889 | lrm: 1.00 | dt: 521.19ms | tok/sec: 1,005,938 | mfu: -1.00 | total time: 4.59m\n",
      "step 00544 (67.28%) | loss: 1.284395 | lrm: 1.00 | dt: 623.28ms | tok/sec: 841,177 | mfu: -1.00 | total time: 4.60m\n",
      "step 00545 (67.42%) | loss: 1.283435 | lrm: 1.00 | dt: 541.07ms | tok/sec: 968,988 | mfu: -1.00 | total time: 4.61m\n",
      "step 00546 (67.53%) | loss: 1.279291 | lrm: 1.00 | dt: 536.96ms | tok/sec: 976,393 | mfu: -1.00 | total time: 4.62m\n",
      "step 00547 (67.67%) | loss: 1.279706 | lrm: 1.00 | dt: 526.36ms | tok/sec: 996,064 | mfu: -1.00 | total time: 4.63m\n",
      "step 00548 (67.80%) | loss: 1.276585 | lrm: 1.00 | dt: 464.65ms | tok/sec: 1,128,350 | mfu: -1.00 | total time: 4.64m\n",
      "step 00549 (67.94%) | loss: 1.283906 | lrm: 1.00 | dt: 539.40ms | tok/sec: 971,975 | mfu: -1.00 | total time: 4.65m\n",
      "step 00550 (68.08%) | loss: 1.272882 | lrm: 1.00 | dt: 582.70ms | tok/sec: 899,756 | mfu: -1.00 | total time: 4.66m\n",
      "step 00551 (68.21%) | loss: 1.280104 | lrm: 1.00 | dt: 466.16ms | tok/sec: 1,124,706 | mfu: -1.00 | total time: 4.67m\n",
      "step 00552 (68.34%) | loss: 1.290099 | lrm: 1.00 | dt: 478.03ms | tok/sec: 1,096,764 | mfu: -1.00 | total time: 4.67m\n",
      "step 00553 (68.46%) | loss: 1.281157 | lrm: 1.00 | dt: 535.82ms | tok/sec: 978,480 | mfu: -1.00 | total time: 4.68m\n",
      "step 00554 (68.60%) | loss: 1.286391 | lrm: 1.00 | dt: 467.96ms | tok/sec: 1,120,377 | mfu: -1.00 | total time: 4.69m\n",
      "step 00555 (68.73%) | loss: 1.289082 | lrm: 1.00 | dt: 533.97ms | tok/sec: 981,874 | mfu: -1.00 | total time: 4.70m\n",
      "step 00556 (68.85%) | loss: 1.291650 | lrm: 1.00 | dt: 465.72ms | tok/sec: 1,125,756 | mfu: -1.00 | total time: 4.71m\n",
      "step 00557 (68.98%) | loss: 1.291661 | lrm: 1.00 | dt: 473.92ms | tok/sec: 1,106,289 | mfu: -1.00 | total time: 4.71m\n",
      "step 00558 (69.11%) | loss: 1.301819 | lrm: 1.00 | dt: 473.21ms | tok/sec: 1,107,935 | mfu: -1.00 | total time: 4.72m\n",
      "step 00559 (69.22%) | loss: 1.309024 | lrm: 1.00 | dt: 473.85ms | tok/sec: 1,106,444 | mfu: -1.00 | total time: 4.73m\n",
      "step 00560 (69.34%) | loss: 1.311867 | lrm: 1.00 | dt: 473.11ms | tok/sec: 1,108,182 | mfu: -1.00 | total time: 4.74m\n",
      "step 00561 (69.48%) | loss: 1.316777 | lrm: 1.00 | dt: 625.78ms | tok/sec: 837,812 | mfu: -1.00 | total time: 4.75m\n",
      "step 00562 (69.62%) | loss: 1.317113 | lrm: 1.00 | dt: 531.80ms | tok/sec: 985,868 | mfu: -1.00 | total time: 4.76m\n",
      "step 00563 (69.73%) | loss: 1.311579 | lrm: 1.00 | dt: 533.09ms | tok/sec: 983,494 | mfu: -1.00 | total time: 4.77m\n",
      "step 00564 (69.86%) | loss: 1.309029 | lrm: 1.00 | dt: 464.65ms | tok/sec: 1,128,347 | mfu: -1.00 | total time: 4.77m\n",
      "step 00565 (69.98%) | loss: 1.299394 | lrm: 1.00 | dt: 542.02ms | tok/sec: 967,280 | mfu: -1.00 | total time: 4.78m\n",
      "step 00566 (70.10%) | loss: 1.296377 | lrm: 1.00 | dt: 464.63ms | tok/sec: 1,128,395 | mfu: -1.00 | total time: 4.79m\n",
      "step 00567 (70.23%) | loss: 1.300217 | lrm: 1.00 | dt: 479.76ms | tok/sec: 1,092,814 | mfu: -1.00 | total time: 4.80m\n",
      "step 00568 (70.36%) | loss: 1.304150 | lrm: 1.00 | dt: 472.47ms | tok/sec: 1,109,685 | mfu: -1.00 | total time: 4.81m\n",
      "step 00569 (70.51%) | loss: 1.303430 | lrm: 1.00 | dt: 620.52ms | tok/sec: 844,918 | mfu: -1.00 | total time: 4.82m\n",
      "step 00570 (70.62%) | loss: 1.296190 | lrm: 1.00 | dt: 617.07ms | tok/sec: 849,643 | mfu: -1.00 | total time: 4.83m\n",
      "step 00571 (70.72%) | loss: 1.287355 | lrm: 1.00 | dt: 468.57ms | tok/sec: 1,118,922 | mfu: -1.00 | total time: 4.84m\n",
      "step 00572 (70.84%) | loss: 1.300121 | lrm: 1.00 | dt: 480.22ms | tok/sec: 1,091,760 | mfu: -1.00 | total time: 4.84m\n",
      "step 00573 (70.97%) | loss: 1.300471 | lrm: 1.00 | dt: 470.76ms | tok/sec: 1,113,711 | mfu: -1.00 | total time: 4.85m\n",
      "step 00574 (71.08%) | loss: 1.298063 | lrm: 1.00 | dt: 476.37ms | tok/sec: 1,100,581 | mfu: -1.00 | total time: 4.86m\n",
      "step 00575 (71.19%) | loss: 1.304670 | lrm: 1.00 | dt: 474.68ms | tok/sec: 1,104,515 | mfu: -1.00 | total time: 4.87m\n",
      "step 00576 (71.30%) | loss: 1.304389 | lrm: 1.00 | dt: 535.64ms | tok/sec: 978,807 | mfu: -1.00 | total time: 4.88m\n",
      "step 00577 (71.42%) | loss: 1.299402 | lrm: 1.00 | dt: 466.52ms | tok/sec: 1,123,829 | mfu: -1.00 | total time: 4.88m\n",
      "step 00578 (71.55%) | loss: 1.292024 | lrm: 1.00 | dt: 478.35ms | tok/sec: 1,096,023 | mfu: -1.00 | total time: 4.89m\n",
      "step 00579 (71.69%) | loss: 1.293113 | lrm: 1.00 | dt: 474.83ms | tok/sec: 1,104,155 | mfu: -1.00 | total time: 4.90m\n",
      "step 00580 (71.82%) | loss: 1.301557 | lrm: 1.00 | dt: 475.91ms | tok/sec: 1,101,648 | mfu: -1.00 | total time: 4.91m\n",
      "step 00581 (71.95%) | loss: 1.296312 | lrm: 1.00 | dt: 539.08ms | tok/sec: 972,569 | mfu: -1.00 | total time: 4.92m\n",
      "step 00582 (72.07%) | loss: 1.298416 | lrm: 1.00 | dt: 534.61ms | tok/sec: 980,694 | mfu: -1.00 | total time: 4.93m\n",
      "step 00583 (72.18%) | loss: 1.304377 | lrm: 1.00 | dt: 538.17ms | tok/sec: 974,208 | mfu: -1.00 | total time: 4.93m\n",
      "step 00584 (72.29%) | loss: 1.310128 | lrm: 1.00 | dt: 547.09ms | tok/sec: 958,315 | mfu: -1.00 | total time: 4.94m\n",
      "step 00585 (72.43%) | loss: 1.298865 | lrm: 1.00 | dt: 465.03ms | tok/sec: 1,127,435 | mfu: -1.00 | total time: 4.95m\n",
      "step 00586 (72.55%) | loss: 1.296011 | lrm: 1.00 | dt: 547.83ms | tok/sec: 957,032 | mfu: -1.00 | total time: 4.96m\n",
      "step 00587 (72.67%) | loss: 1.293252 | lrm: 1.00 | dt: 532.98ms | tok/sec: 983,690 | mfu: -1.00 | total time: 4.97m\n",
      "step 00588 (72.81%) | loss: 1.304699 | lrm: 1.00 | dt: 467.01ms | tok/sec: 1,122,658 | mfu: -1.00 | total time: 4.98m\n",
      "step 00589 (72.92%) | loss: 1.313018 | lrm: 1.00 | dt: 532.10ms | tok/sec: 985,320 | mfu: -1.00 | total time: 4.99m\n",
      "step 00590 (73.05%) | loss: 1.314462 | lrm: 1.00 | dt: 538.15ms | tok/sec: 974,243 | mfu: -1.00 | total time: 4.99m\n",
      "step 00591 (73.19%) | loss: 1.316577 | lrm: 1.00 | dt: 527.90ms | tok/sec: 993,153 | mfu: -1.00 | total time: 5.00m\n",
      "step 00592 (73.31%) | loss: 1.303981 | lrm: 1.00 | dt: 534.77ms | tok/sec: 980,394 | mfu: -1.00 | total time: 5.01m\n",
      "step 00593 (73.42%) | loss: 1.303025 | lrm: 1.00 | dt: 537.51ms | tok/sec: 975,401 | mfu: -1.00 | total time: 5.02m\n",
      "step 00594 (73.54%) | loss: 1.314762 | lrm: 1.00 | dt: 467.68ms | tok/sec: 1,121,040 | mfu: -1.00 | total time: 5.03m\n",
      "step 00595 (73.67%) | loss: 1.315223 | lrm: 1.00 | dt: 477.33ms | tok/sec: 1,098,377 | mfu: -1.00 | total time: 5.04m\n",
      "step 00596 (73.77%) | loss: 1.327560 | lrm: 1.00 | dt: 471.11ms | tok/sec: 1,112,878 | mfu: -1.00 | total time: 5.04m\n",
      "step 00597 (73.93%) | loss: 1.328968 | lrm: 1.00 | dt: 475.98ms | tok/sec: 1,101,492 | mfu: -1.00 | total time: 5.05m\n",
      "step 00598 (74.05%) | loss: 1.333021 | lrm: 1.00 | dt: 470.51ms | tok/sec: 1,114,304 | mfu: -1.00 | total time: 5.06m\n",
      "step 00599 (74.16%) | loss: 1.329064 | lrm: 1.00 | dt: 536.63ms | tok/sec: 977,008 | mfu: -1.00 | total time: 5.07m\n",
      "step 00600 (74.26%) | loss: 1.321007 | lrm: 1.00 | dt: 532.07ms | tok/sec: 985,381 | mfu: -1.00 | total time: 5.08m\n",
      "step 00600 | Validation bpb: 0.4097\n",
      "step 00601 (74.39%) | loss: 1.317131 | lrm: 1.00 | dt: 549.61ms | tok/sec: 953,927 | mfu: -1.00 | total time: 5.09m\n",
      "step 00602 (74.50%) | loss: 1.308639 | lrm: 1.00 | dt: 470.06ms | tok/sec: 1,115,352 | mfu: -1.00 | total time: 5.10m\n",
      "step 00603 (74.62%) | loss: 1.313549 | lrm: 1.00 | dt: 480.05ms | tok/sec: 1,092,163 | mfu: -1.00 | total time: 5.10m\n",
      "step 00604 (74.72%) | loss: 1.302663 | lrm: 1.00 | dt: 472.99ms | tok/sec: 1,108,463 | mfu: -1.00 | total time: 5.11m\n",
      "step 00605 (74.84%) | loss: 1.304126 | lrm: 1.00 | dt: 474.06ms | tok/sec: 1,105,955 | mfu: -1.00 | total time: 5.12m\n",
      "step 00606 (74.98%) | loss: 1.318566 | lrm: 1.00 | dt: 471.82ms | tok/sec: 1,111,207 | mfu: -1.00 | total time: 5.13m\n",
      "step 00607 (75.12%) | loss: 1.310253 | lrm: 1.00 | dt: 471.34ms | tok/sec: 1,112,341 | mfu: -1.00 | total time: 5.14m\n",
      "step 00608 (75.23%) | loss: 1.291959 | lrm: 1.00 | dt: 472.29ms | tok/sec: 1,110,101 | mfu: -1.00 | total time: 5.14m\n",
      "step 00609 (75.34%) | loss: 1.287857 | lrm: 1.00 | dt: 470.46ms | tok/sec: 1,114,405 | mfu: -1.00 | total time: 5.15m\n",
      "step 00610 (75.45%) | loss: 1.292777 | lrm: 1.00 | dt: 473.33ms | tok/sec: 1,107,664 | mfu: -1.00 | total time: 5.16m\n",
      "step 00611 (75.58%) | loss: 1.285638 | lrm: 1.00 | dt: 472.68ms | tok/sec: 1,109,176 | mfu: -1.00 | total time: 5.17m\n",
      "step 00612 (75.70%) | loss: 1.273699 | lrm: 1.00 | dt: 472.18ms | tok/sec: 1,110,349 | mfu: -1.00 | total time: 5.17m\n",
      "step 00613 (75.83%) | loss: 1.265621 | lrm: 1.00 | dt: 470.60ms | tok/sec: 1,114,081 | mfu: -1.00 | total time: 5.18m\n",
      "step 00614 (75.93%) | loss: 1.267945 | lrm: 1.00 | dt: 472.05ms | tok/sec: 1,110,654 | mfu: -1.00 | total time: 5.19m\n",
      "step 00615 (76.08%) | loss: 1.267921 | lrm: 1.00 | dt: 470.90ms | tok/sec: 1,113,377 | mfu: -1.00 | total time: 5.20m\n",
      "step 00616 (76.19%) | loss: 1.270037 | lrm: 1.00 | dt: 470.94ms | tok/sec: 1,113,286 | mfu: -1.00 | total time: 5.21m\n",
      "step 00617 (76.31%) | loss: 1.271436 | lrm: 1.00 | dt: 539.06ms | tok/sec: 972,589 | mfu: -1.00 | total time: 5.21m\n",
      "step 00618 (76.44%) | loss: 1.274245 | lrm: 1.00 | dt: 466.29ms | tok/sec: 1,124,380 | mfu: -1.00 | total time: 5.22m\n",
      "step 00619 (76.55%) | loss: 1.283437 | lrm: 1.00 | dt: 477.57ms | tok/sec: 1,097,823 | mfu: -1.00 | total time: 5.23m\n",
      "step 00620 (76.68%) | loss: 1.293633 | lrm: 1.00 | dt: 471.71ms | tok/sec: 1,111,458 | mfu: -1.00 | total time: 5.24m\n",
      "step 00621 (76.79%) | loss: 1.289469 | lrm: 1.00 | dt: 476.17ms | tok/sec: 1,101,055 | mfu: -1.00 | total time: 5.25m\n",
      "step 00622 (76.92%) | loss: 1.287642 | lrm: 1.00 | dt: 472.57ms | tok/sec: 1,109,451 | mfu: -1.00 | total time: 5.25m\n",
      "step 00623 (77.03%) | loss: 1.296010 | lrm: 1.00 | dt: 474.74ms | tok/sec: 1,104,364 | mfu: -1.00 | total time: 5.26m\n",
      "step 00624 (77.16%) | loss: 1.290249 | lrm: 1.00 | dt: 473.43ms | tok/sec: 1,107,432 | mfu: -1.00 | total time: 5.27m\n",
      "step 00625 (77.27%) | loss: 1.293415 | lrm: 1.00 | dt: 472.57ms | tok/sec: 1,109,437 | mfu: -1.00 | total time: 5.28m\n",
      "step 00626 (77.39%) | loss: 1.284668 | lrm: 1.00 | dt: 472.39ms | tok/sec: 1,109,856 | mfu: -1.00 | total time: 5.29m\n",
      "step 00627 (77.50%) | loss: 1.279524 | lrm: 1.00 | dt: 474.17ms | tok/sec: 1,105,689 | mfu: -1.00 | total time: 5.29m\n",
      "step 00628 (77.64%) | loss: 1.283179 | lrm: 1.00 | dt: 471.50ms | tok/sec: 1,111,953 | mfu: -1.00 | total time: 5.30m\n",
      "step 00629 (77.77%) | loss: 1.281612 | lrm: 1.00 | dt: 472.15ms | tok/sec: 1,110,436 | mfu: -1.00 | total time: 5.31m\n",
      "step 00630 (77.90%) | loss: 1.283117 | lrm: 1.00 | dt: 550.54ms | tok/sec: 952,312 | mfu: -1.00 | total time: 5.32m\n",
      "step 00631 (78.02%) | loss: 1.274191 | lrm: 1.00 | dt: 532.76ms | tok/sec: 984,101 | mfu: -1.00 | total time: 5.33m\n",
      "step 00632 (78.13%) | loss: 1.277218 | lrm: 1.00 | dt: 535.46ms | tok/sec: 979,134 | mfu: -1.00 | total time: 5.34m\n",
      "step 00633 (78.25%) | loss: 1.273645 | lrm: 1.00 | dt: 466.13ms | tok/sec: 1,124,765 | mfu: -1.00 | total time: 5.34m\n",
      "step 00634 (78.37%) | loss: 1.266338 | lrm: 1.00 | dt: 477.59ms | tok/sec: 1,097,773 | mfu: -1.00 | total time: 5.35m\n",
      "step 00635 (78.49%) | loss: 1.262527 | lrm: 1.00 | dt: 471.91ms | tok/sec: 1,110,987 | mfu: -1.00 | total time: 5.36m\n",
      "step 00636 (78.61%) | loss: 1.264862 | lrm: 1.00 | dt: 476.66ms | tok/sec: 1,099,924 | mfu: -1.00 | total time: 5.37m\n",
      "step 00637 (78.72%) | loss: 1.267675 | lrm: 1.00 | dt: 472.14ms | tok/sec: 1,110,456 | mfu: -1.00 | total time: 5.38m\n",
      "step 00638 (78.84%) | loss: 1.266862 | lrm: 1.00 | dt: 472.47ms | tok/sec: 1,109,679 | mfu: -1.00 | total time: 5.38m\n",
      "step 00639 (78.95%) | loss: 1.261468 | lrm: 1.00 | dt: 473.84ms | tok/sec: 1,106,476 | mfu: -1.00 | total time: 5.39m\n",
      "step 00640 (79.08%) | loss: 1.262477 | lrm: 1.00 | dt: 472.96ms | tok/sec: 1,108,536 | mfu: -1.00 | total time: 5.40m\n",
      "step 00641 (79.19%) | loss: 1.268572 | lrm: 1.00 | dt: 474.68ms | tok/sec: 1,104,502 | mfu: -1.00 | total time: 5.41m\n",
      "step 00642 (79.33%) | loss: 1.276472 | lrm: 1.00 | dt: 470.99ms | tok/sec: 1,113,151 | mfu: -1.00 | total time: 5.42m\n",
      "step 00643 (79.47%) | loss: 1.274022 | lrm: 1.00 | dt: 545.93ms | tok/sec: 960,361 | mfu: -1.00 | total time: 5.42m\n",
      "step 00644 (79.58%) | loss: 1.272048 | lrm: 1.00 | dt: 466.44ms | tok/sec: 1,124,019 | mfu: -1.00 | total time: 5.43m\n",
      "step 00645 (79.69%) | loss: 1.270205 | lrm: 1.00 | dt: 549.62ms | tok/sec: 953,902 | mfu: -1.00 | total time: 5.44m\n",
      "step 00646 (79.84%) | loss: 1.266672 | lrm: 1.00 | dt: 538.78ms | tok/sec: 973,096 | mfu: -1.00 | total time: 5.45m\n",
      "step 00647 (79.95%) | loss: 1.268244 | lrm: 1.00 | dt: 531.24ms | tok/sec: 986,918 | mfu: -1.00 | total time: 5.46m\n",
      "step 00648 (80.07%) | loss: 1.263656 | lrm: 1.00 | dt: 466.83ms | tok/sec: 1,123,086 | mfu: -1.00 | total time: 5.47m\n",
      "step 00649 (80.18%) | loss: 1.258830 | lrm: 0.99 | dt: 481.72ms | tok/sec: 1,088,372 | mfu: -1.00 | total time: 5.47m\n",
      "step 00650 (80.30%) | loss: 1.252199 | lrm: 0.99 | dt: 472.08ms | tok/sec: 1,110,586 | mfu: -1.00 | total time: 5.48m\n",
      "step 00651 (80.41%) | loss: 1.249886 | lrm: 0.98 | dt: 478.40ms | tok/sec: 1,095,928 | mfu: -1.00 | total time: 5.49m\n",
      "step 00652 (80.54%) | loss: 1.241798 | lrm: 0.97 | dt: 470.85ms | tok/sec: 1,113,498 | mfu: -1.00 | total time: 5.50m\n",
      "step 00653 (80.66%) | loss: 1.246329 | lrm: 0.97 | dt: 474.02ms | tok/sec: 1,106,045 | mfu: -1.00 | total time: 5.51m\n",
      "step 00654 (80.79%) | loss: 1.249982 | lrm: 0.96 | dt: 539.18ms | tok/sec: 972,372 | mfu: -1.00 | total time: 5.52m\n",
      "step 00655 (80.91%) | loss: 1.257687 | lrm: 0.95 | dt: 467.78ms | tok/sec: 1,120,793 | mfu: -1.00 | total time: 5.52m\n",
      "step 00656 (81.03%) | loss: 1.252724 | lrm: 0.95 | dt: 479.34ms | tok/sec: 1,093,780 | mfu: -1.00 | total time: 5.53m\n",
      "step 00657 (81.15%) | loss: 1.246997 | lrm: 0.94 | dt: 538.81ms | tok/sec: 973,054 | mfu: -1.00 | total time: 5.54m\n",
      "step 00658 (81.27%) | loss: 1.257573 | lrm: 0.94 | dt: 534.85ms | tok/sec: 980,260 | mfu: -1.00 | total time: 5.55m\n",
      "step 00659 (81.39%) | loss: 1.260873 | lrm: 0.93 | dt: 466.40ms | tok/sec: 1,124,104 | mfu: -1.00 | total time: 5.56m\n",
      "step 00660 (81.52%) | loss: 1.257482 | lrm: 0.92 | dt: 477.69ms | tok/sec: 1,097,559 | mfu: -1.00 | total time: 5.56m\n",
      "step 00661 (81.65%) | loss: 1.255800 | lrm: 0.92 | dt: 469.71ms | tok/sec: 1,116,197 | mfu: -1.00 | total time: 5.57m\n",
      "step 00662 (81.77%) | loss: 1.251302 | lrm: 0.91 | dt: 476.65ms | tok/sec: 1,099,943 | mfu: -1.00 | total time: 5.58m\n",
      "step 00663 (81.90%) | loss: 1.252930 | lrm: 0.90 | dt: 471.03ms | tok/sec: 1,113,067 | mfu: -1.00 | total time: 5.59m\n",
      "step 00664 (82.03%) | loss: 1.268847 | lrm: 0.90 | dt: 472.87ms | tok/sec: 1,108,742 | mfu: -1.00 | total time: 5.60m\n",
      "step 00665 (82.14%) | loss: 1.278134 | lrm: 0.89 | dt: 473.13ms | tok/sec: 1,108,136 | mfu: -1.00 | total time: 5.60m\n",
      "step 00666 (82.27%) | loss: 1.271844 | lrm: 0.89 | dt: 474.80ms | tok/sec: 1,104,236 | mfu: -1.00 | total time: 5.61m\n",
      "step 00667 (82.40%) | loss: 1.275257 | lrm: 0.88 | dt: 473.99ms | tok/sec: 1,106,119 | mfu: -1.00 | total time: 5.62m\n",
      "step 00668 (82.52%) | loss: 1.272896 | lrm: 0.87 | dt: 470.11ms | tok/sec: 1,115,240 | mfu: -1.00 | total time: 5.63m\n",
      "step 00669 (82.62%) | loss: 1.295430 | lrm: 0.87 | dt: 545.62ms | tok/sec: 960,894 | mfu: -1.00 | total time: 5.64m\n",
      "step 00670 (82.75%) | loss: 1.292525 | lrm: 0.86 | dt: 533.14ms | tok/sec: 983,400 | mfu: -1.00 | total time: 5.65m\n",
      "step 00671 (82.87%) | loss: 1.294756 | lrm: 0.86 | dt: 531.33ms | tok/sec: 986,743 | mfu: -1.00 | total time: 5.65m\n",
      "step 00672 (82.99%) | loss: 1.294830 | lrm: 0.85 | dt: 466.21ms | tok/sec: 1,124,566 | mfu: -1.00 | total time: 5.66m\n",
      "step 00673 (83.11%) | loss: 1.279760 | lrm: 0.84 | dt: 481.43ms | tok/sec: 1,089,025 | mfu: -1.00 | total time: 5.67m\n",
      "step 00674 (83.22%) | loss: 1.269268 | lrm: 0.84 | dt: 472.64ms | tok/sec: 1,109,275 | mfu: -1.00 | total time: 5.68m\n",
      "step 00675 (83.34%) | loss: 1.248378 | lrm: 0.83 | dt: 476.35ms | tok/sec: 1,100,646 | mfu: -1.00 | total time: 5.69m\n",
      "step 00676 (83.46%) | loss: 1.250980 | lrm: 0.83 | dt: 538.71ms | tok/sec: 973,224 | mfu: -1.00 | total time: 5.70m\n",
      "step 00677 (83.59%) | loss: 1.254368 | lrm: 0.82 | dt: 532.20ms | tok/sec: 985,131 | mfu: -1.00 | total time: 5.70m\n",
      "step 00678 (83.71%) | loss: 1.260930 | lrm: 0.81 | dt: 530.12ms | tok/sec: 989,006 | mfu: -1.00 | total time: 5.71m\n",
      "step 00679 (83.84%) | loss: 1.268166 | lrm: 0.81 | dt: 466.15ms | tok/sec: 1,124,728 | mfu: -1.00 | total time: 5.72m\n",
      "step 00680 (83.95%) | loss: 1.272261 | lrm: 0.80 | dt: 481.93ms | tok/sec: 1,087,893 | mfu: -1.00 | total time: 5.73m\n",
      "step 00681 (84.06%) | loss: 1.272173 | lrm: 0.80 | dt: 472.61ms | tok/sec: 1,109,343 | mfu: -1.00 | total time: 5.74m\n",
      "step 00682 (84.20%) | loss: 1.275695 | lrm: 0.79 | dt: 477.99ms | tok/sec: 1,096,853 | mfu: -1.00 | total time: 5.74m\n",
      "step 00683 (84.31%) | loss: 1.269367 | lrm: 0.78 | dt: 536.54ms | tok/sec: 977,166 | mfu: -1.00 | total time: 5.75m\n",
      "step 00684 (84.43%) | loss: 1.273081 | lrm: 0.78 | dt: 466.75ms | tok/sec: 1,123,281 | mfu: -1.00 | total time: 5.76m\n",
      "step 00685 (84.54%) | loss: 1.276883 | lrm: 0.77 | dt: 479.76ms | tok/sec: 1,092,814 | mfu: -1.00 | total time: 5.77m\n",
      "step 00686 (84.66%) | loss: 1.277611 | lrm: 0.77 | dt: 470.98ms | tok/sec: 1,113,177 | mfu: -1.00 | total time: 5.78m\n",
      "step 00687 (84.76%) | loss: 1.276311 | lrm: 0.76 | dt: 480.18ms | tok/sec: 1,091,862 | mfu: -1.00 | total time: 5.79m\n",
      "step 00688 (84.88%) | loss: 1.300669 | lrm: 0.76 | dt: 540.82ms | tok/sec: 969,437 | mfu: -1.00 | total time: 5.79m\n",
      "step 00689 (85.01%) | loss: 1.286982 | lrm: 0.75 | dt: 467.18ms | tok/sec: 1,122,231 | mfu: -1.00 | total time: 5.80m\n",
      "step 00690 (85.15%) | loss: 1.282462 | lrm: 0.74 | dt: 485.81ms | tok/sec: 1,079,200 | mfu: -1.00 | total time: 5.81m\n",
      "step 00691 (85.26%) | loss: 1.275988 | lrm: 0.74 | dt: 469.14ms | tok/sec: 1,117,551 | mfu: -1.00 | total time: 5.82m\n",
      "step 00692 (85.37%) | loss: 1.285287 | lrm: 0.73 | dt: 477.48ms | tok/sec: 1,098,034 | mfu: -1.00 | total time: 5.83m\n",
      "step 00693 (85.50%) | loss: 1.290032 | lrm: 0.72 | dt: 547.27ms | tok/sec: 958,011 | mfu: -1.00 | total time: 5.84m\n",
      "step 00694 (85.63%) | loss: 1.291385 | lrm: 0.72 | dt: 531.92ms | tok/sec: 985,656 | mfu: -1.00 | total time: 5.84m\n",
      "step 00695 (85.77%) | loss: 1.298252 | lrm: 0.71 | dt: 468.07ms | tok/sec: 1,120,109 | mfu: -1.00 | total time: 5.85m\n",
      "step 00696 (85.88%) | loss: 1.302863 | lrm: 0.71 | dt: 480.28ms | tok/sec: 1,091,628 | mfu: -1.00 | total time: 5.86m\n",
      "step 00697 (86.02%) | loss: 1.291195 | lrm: 0.70 | dt: 470.59ms | tok/sec: 1,114,096 | mfu: -1.00 | total time: 5.87m\n",
      "step 00698 (86.16%) | loss: 1.294104 | lrm: 0.69 | dt: 543.41ms | tok/sec: 964,819 | mfu: -1.00 | total time: 5.88m\n",
      "step 00699 (86.29%) | loss: 1.297959 | lrm: 0.69 | dt: 528.81ms | tok/sec: 991,454 | mfu: -1.00 | total time: 5.89m\n",
      "step 00700 (86.41%) | loss: 1.293498 | lrm: 0.68 | dt: 468.63ms | tok/sec: 1,118,766 | mfu: -1.00 | total time: 5.89m\n",
      "step 00701 (86.51%) | loss: 1.283041 | lrm: 0.67 | dt: 477.44ms | tok/sec: 1,098,124 | mfu: -1.00 | total time: 5.90m\n",
      "step 00702 (86.65%) | loss: 1.280007 | lrm: 0.67 | dt: 469.60ms | tok/sec: 1,116,459 | mfu: -1.00 | total time: 5.91m\n",
      "step 00703 (86.79%) | loss: 1.273675 | lrm: 0.66 | dt: 543.15ms | tok/sec: 965,273 | mfu: -1.00 | total time: 5.92m\n",
      "step 00704 (86.90%) | loss: 1.274071 | lrm: 0.65 | dt: 534.25ms | tok/sec: 981,353 | mfu: -1.00 | total time: 5.93m\n",
      "step 00705 (87.03%) | loss: 1.278181 | lrm: 0.65 | dt: 467.21ms | tok/sec: 1,122,160 | mfu: -1.00 | total time: 5.93m\n",
      "step 00706 (87.15%) | loss: 1.275883 | lrm: 0.64 | dt: 480.91ms | tok/sec: 1,090,204 | mfu: -1.00 | total time: 5.94m\n",
      "step 00707 (87.27%) | loss: 1.276480 | lrm: 0.64 | dt: 470.28ms | tok/sec: 1,114,836 | mfu: -1.00 | total time: 5.95m\n",
      "step 00708 (87.39%) | loss: 1.272023 | lrm: 0.63 | dt: 479.22ms | tok/sec: 1,094,052 | mfu: -1.00 | total time: 5.96m\n",
      "step 00709 (87.49%) | loss: 1.266653 | lrm: 0.63 | dt: 472.19ms | tok/sec: 1,110,330 | mfu: -1.00 | total time: 5.97m\n",
      "step 00710 (87.62%) | loss: 1.256642 | lrm: 0.62 | dt: 472.61ms | tok/sec: 1,109,340 | mfu: -1.00 | total time: 5.97m\n",
      "step 00711 (87.74%) | loss: 1.252811 | lrm: 0.61 | dt: 472.14ms | tok/sec: 1,110,457 | mfu: -1.00 | total time: 5.98m\n",
      "step 00712 (87.85%) | loss: 1.251188 | lrm: 0.61 | dt: 471.35ms | tok/sec: 1,112,316 | mfu: -1.00 | total time: 5.99m\n",
      "step 00713 (87.97%) | loss: 1.256026 | lrm: 0.60 | dt: 474.90ms | tok/sec: 1,104,005 | mfu: -1.00 | total time: 6.00m\n",
      "step 00714 (88.10%) | loss: 1.253407 | lrm: 0.60 | dt: 472.21ms | tok/sec: 1,110,275 | mfu: -1.00 | total time: 6.01m\n",
      "step 00715 (88.23%) | loss: 1.260450 | lrm: 0.59 | dt: 475.18ms | tok/sec: 1,103,344 | mfu: -1.00 | total time: 6.01m\n",
      "step 00716 (88.36%) | loss: 1.252923 | lrm: 0.58 | dt: 473.35ms | tok/sec: 1,107,608 | mfu: -1.00 | total time: 6.02m\n",
      "step 00717 (88.49%) | loss: 1.250301 | lrm: 0.58 | dt: 474.61ms | tok/sec: 1,104,674 | mfu: -1.00 | total time: 6.03m\n",
      "step 00718 (88.62%) | loss: 1.252243 | lrm: 0.57 | dt: 539.33ms | tok/sec: 972,108 | mfu: -1.00 | total time: 6.04m\n",
      "step 00719 (88.73%) | loss: 1.244215 | lrm: 0.56 | dt: 467.87ms | tok/sec: 1,120,585 | mfu: -1.00 | total time: 6.05m\n",
      "step 00720 (88.85%) | loss: 1.248041 | lrm: 0.56 | dt: 480.35ms | tok/sec: 1,091,464 | mfu: -1.00 | total time: 6.05m\n",
      "step 00721 (88.96%) | loss: 1.244062 | lrm: 0.55 | dt: 471.88ms | tok/sec: 1,111,057 | mfu: -1.00 | total time: 6.06m\n",
      "step 00722 (89.10%) | loss: 1.245639 | lrm: 0.55 | dt: 475.70ms | tok/sec: 1,102,145 | mfu: -1.00 | total time: 6.07m\n",
      "step 00723 (89.22%) | loss: 1.242011 | lrm: 0.54 | dt: 542.68ms | tok/sec: 966,113 | mfu: -1.00 | total time: 6.08m\n",
      "step 00724 (89.34%) | loss: 1.244352 | lrm: 0.53 | dt: 533.42ms | tok/sec: 982,875 | mfu: -1.00 | total time: 6.09m\n",
      "step 00725 (89.45%) | loss: 1.233984 | lrm: 0.53 | dt: 468.32ms | tok/sec: 1,119,506 | mfu: -1.00 | total time: 6.10m\n",
      "step 00726 (89.56%) | loss: 1.237334 | lrm: 0.52 | dt: 480.34ms | tok/sec: 1,091,500 | mfu: -1.00 | total time: 6.10m\n",
      "step 00727 (89.67%) | loss: 1.245082 | lrm: 0.52 | dt: 470.24ms | tok/sec: 1,114,929 | mfu: -1.00 | total time: 6.11m\n",
      "step 00728 (89.81%) | loss: 1.256744 | lrm: 0.51 | dt: 553.54ms | tok/sec: 947,146 | mfu: -1.00 | total time: 6.12m\n",
      "step 00729 (89.92%) | loss: 1.248342 | lrm: 0.50 | dt: 465.80ms | tok/sec: 1,125,555 | mfu: -1.00 | total time: 6.13m\n",
      "step 00730 (90.05%) | loss: 1.255661 | lrm: 0.50 | dt: 478.22ms | tok/sec: 1,096,327 | mfu: -1.00 | total time: 6.14m\n",
      "step 00731 (90.18%) | loss: 1.261060 | lrm: 0.49 | dt: 472.94ms | tok/sec: 1,108,574 | mfu: -1.00 | total time: 6.14m\n",
      "step 00732 (90.31%) | loss: 1.258410 | lrm: 0.48 | dt: 539.99ms | tok/sec: 970,926 | mfu: -1.00 | total time: 6.15m\n",
      "step 00733 (90.43%) | loss: 1.256838 | lrm: 0.48 | dt: 468.01ms | tok/sec: 1,120,254 | mfu: -1.00 | total time: 6.16m\n",
      "step 00734 (90.56%) | loss: 1.249633 | lrm: 0.47 | dt: 478.67ms | tok/sec: 1,095,304 | mfu: -1.00 | total time: 6.17m\n",
      "step 00735 (90.69%) | loss: 1.262596 | lrm: 0.47 | dt: 471.52ms | tok/sec: 1,111,919 | mfu: -1.00 | total time: 6.18m\n",
      "step 00736 (90.83%) | loss: 1.258622 | lrm: 0.46 | dt: 478.71ms | tok/sec: 1,095,202 | mfu: -1.00 | total time: 6.19m\n",
      "step 00737 (90.96%) | loss: 1.252002 | lrm: 0.45 | dt: 550.22ms | tok/sec: 952,873 | mfu: -1.00 | total time: 6.19m\n",
      "step 00738 (91.08%) | loss: 1.253393 | lrm: 0.45 | dt: 465.26ms | tok/sec: 1,126,878 | mfu: -1.00 | total time: 6.20m\n",
      "step 00739 (91.19%) | loss: 1.246624 | lrm: 0.44 | dt: 479.65ms | tok/sec: 1,093,072 | mfu: -1.00 | total time: 6.21m\n",
      "step 00740 (91.31%) | loss: 1.235466 | lrm: 0.43 | dt: 539.89ms | tok/sec: 971,098 | mfu: -1.00 | total time: 6.22m\n",
      "step 00741 (91.42%) | loss: 1.234559 | lrm: 0.43 | dt: 468.08ms | tok/sec: 1,120,073 | mfu: -1.00 | total time: 6.23m\n",
      "step 00742 (91.55%) | loss: 1.232930 | lrm: 0.42 | dt: 482.13ms | tok/sec: 1,087,443 | mfu: -1.00 | total time: 6.23m\n",
      "step 00743 (91.70%) | loss: 1.238549 | lrm: 0.42 | dt: 469.88ms | tok/sec: 1,115,781 | mfu: -1.00 | total time: 6.24m\n",
      "step 00744 (91.81%) | loss: 1.236476 | lrm: 0.41 | dt: 475.92ms | tok/sec: 1,101,625 | mfu: -1.00 | total time: 6.25m\n",
      "step 00745 (91.93%) | loss: 1.233585 | lrm: 0.40 | dt: 475.08ms | tok/sec: 1,103,584 | mfu: -1.00 | total time: 6.26m\n",
      "step 00746 (92.05%) | loss: 1.227560 | lrm: 0.40 | dt: 472.73ms | tok/sec: 1,109,065 | mfu: -1.00 | total time: 6.27m\n",
      "step 00747 (92.17%) | loss: 1.230381 | lrm: 0.39 | dt: 541.02ms | tok/sec: 969,078 | mfu: -1.00 | total time: 6.28m\n",
      "step 00748 (92.29%) | loss: 1.219646 | lrm: 0.39 | dt: 466.40ms | tok/sec: 1,124,113 | mfu: -1.00 | total time: 6.28m\n",
      "step 00749 (92.40%) | loss: 1.225389 | lrm: 0.38 | dt: 481.22ms | tok/sec: 1,089,494 | mfu: -1.00 | total time: 6.29m\n",
      "step 00750 (92.51%) | loss: 1.229165 | lrm: 0.37 | dt: 471.94ms | tok/sec: 1,110,914 | mfu: -1.00 | total time: 6.30m\n",
      "step 00750 | Validation bpb: 0.3984\n",
      "step 00751 (92.63%) | loss: 1.224991 | lrm: 0.37 | dt: 466.54ms | tok/sec: 1,123,768 | mfu: -1.00 | total time: 6.31m\n",
      "step 00752 (92.76%) | loss: 1.246129 | lrm: 0.36 | dt: 469.76ms | tok/sec: 1,116,081 | mfu: -1.00 | total time: 6.31m\n",
      "step 00753 (92.88%) | loss: 1.243243 | lrm: 0.36 | dt: 475.31ms | tok/sec: 1,103,033 | mfu: -1.00 | total time: 6.32m\n",
      "step 00754 (93.00%) | loss: 1.244942 | lrm: 0.35 | dt: 469.16ms | tok/sec: 1,117,500 | mfu: -1.00 | total time: 6.33m\n",
      "step 00755 (93.11%) | loss: 1.239824 | lrm: 0.34 | dt: 538.39ms | tok/sec: 973,805 | mfu: -1.00 | total time: 6.34m\n",
      "step 00756 (93.25%) | loss: 1.229491 | lrm: 0.34 | dt: 465.51ms | tok/sec: 1,126,261 | mfu: -1.00 | total time: 6.35m\n",
      "step 00757 (93.37%) | loss: 1.229391 | lrm: 0.33 | dt: 554.04ms | tok/sec: 946,306 | mfu: -1.00 | total time: 6.36m\n",
      "step 00758 (93.47%) | loss: 1.219080 | lrm: 0.33 | dt: 465.68ms | tok/sec: 1,125,846 | mfu: -1.00 | total time: 6.36m\n",
      "step 00759 (93.59%) | loss: 1.216588 | lrm: 0.32 | dt: 478.36ms | tok/sec: 1,096,018 | mfu: -1.00 | total time: 6.37m\n",
      "step 00760 (93.72%) | loss: 1.224571 | lrm: 0.31 | dt: 471.79ms | tok/sec: 1,111,262 | mfu: -1.00 | total time: 6.38m\n",
      "step 00761 (93.85%) | loss: 1.229895 | lrm: 0.31 | dt: 475.59ms | tok/sec: 1,102,388 | mfu: -1.00 | total time: 6.39m\n",
      "step 00762 (93.96%) | loss: 1.221160 | lrm: 0.30 | dt: 624.13ms | tok/sec: 840,025 | mfu: -1.00 | total time: 6.40m\n",
      "step 00763 (94.07%) | loss: 1.225269 | lrm: 0.30 | dt: 466.65ms | tok/sec: 1,123,505 | mfu: -1.00 | total time: 6.41m\n",
      "step 00764 (94.19%) | loss: 1.224056 | lrm: 0.29 | dt: 472.59ms | tok/sec: 1,109,388 | mfu: -1.00 | total time: 6.41m\n",
      "step 00765 (94.30%) | loss: 1.222914 | lrm: 0.29 | dt: 543.66ms | tok/sec: 964,359 | mfu: -1.00 | total time: 6.42m\n",
      "step 00766 (94.43%) | loss: 1.222611 | lrm: 0.28 | dt: 530.33ms | tok/sec: 988,603 | mfu: -1.00 | total time: 6.43m\n",
      "step 00767 (94.57%) | loss: 1.224804 | lrm: 0.27 | dt: 532.66ms | tok/sec: 984,288 | mfu: -1.00 | total time: 6.44m\n",
      "step 00768 (94.69%) | loss: 1.214873 | lrm: 0.27 | dt: 531.81ms | tok/sec: 985,859 | mfu: -1.00 | total time: 6.45m\n",
      "step 00769 (94.81%) | loss: 1.210039 | lrm: 0.26 | dt: 543.17ms | tok/sec: 965,244 | mfu: -1.00 | total time: 6.46m\n",
      "step 00770 (94.94%) | loss: 1.216682 | lrm: 0.25 | dt: 549.83ms | tok/sec: 953,544 | mfu: -1.00 | total time: 6.47m\n",
      "step 00771 (95.05%) | loss: 1.214478 | lrm: 0.25 | dt: 524.32ms | tok/sec: 999,929 | mfu: -1.00 | total time: 6.48m\n",
      "step 00772 (95.18%) | loss: 1.218483 | lrm: 0.24 | dt: 466.80ms | tok/sec: 1,123,164 | mfu: -1.00 | total time: 6.48m\n",
      "step 00773 (95.30%) | loss: 1.222725 | lrm: 0.24 | dt: 478.30ms | tok/sec: 1,096,149 | mfu: -1.00 | total time: 6.49m\n",
      "step 00774 (95.42%) | loss: 1.214922 | lrm: 0.23 | dt: 472.01ms | tok/sec: 1,110,755 | mfu: -1.00 | total time: 6.50m\n",
      "step 00775 (95.56%) | loss: 1.220031 | lrm: 0.22 | dt: 472.41ms | tok/sec: 1,109,803 | mfu: -1.00 | total time: 6.51m\n",
      "step 00776 (95.68%) | loss: 1.217860 | lrm: 0.22 | dt: 535.83ms | tok/sec: 978,459 | mfu: -1.00 | total time: 6.52m\n",
      "step 00777 (95.78%) | loss: 1.211609 | lrm: 0.21 | dt: 466.64ms | tok/sec: 1,123,548 | mfu: -1.00 | total time: 6.52m\n",
      "step 00778 (95.91%) | loss: 1.209064 | lrm: 0.20 | dt: 479.77ms | tok/sec: 1,092,795 | mfu: -1.00 | total time: 6.53m\n",
      "step 00779 (96.03%) | loss: 1.206016 | lrm: 0.20 | dt: 545.33ms | tok/sec: 961,408 | mfu: -1.00 | total time: 6.54m\n",
      "step 00780 (96.14%) | loss: 1.211814 | lrm: 0.19 | dt: 464.70ms | tok/sec: 1,128,225 | mfu: -1.00 | total time: 6.55m\n",
      "step 00781 (96.28%) | loss: 1.231561 | lrm: 0.19 | dt: 533.29ms | tok/sec: 983,113 | mfu: -1.00 | total time: 6.56m\n",
      "step 00782 (96.39%) | loss: 1.227707 | lrm: 0.18 | dt: 465.98ms | tok/sec: 1,125,134 | mfu: -1.00 | total time: 6.57m\n",
      "step 00783 (96.52%) | loss: 1.223338 | lrm: 0.17 | dt: 530.48ms | tok/sec: 988,321 | mfu: -1.00 | total time: 6.58m\n",
      "step 00784 (96.65%) | loss: 1.221186 | lrm: 0.17 | dt: 466.68ms | tok/sec: 1,123,435 | mfu: -1.00 | total time: 6.58m\n",
      "step 00785 (96.78%) | loss: 1.226021 | lrm: 0.16 | dt: 540.42ms | tok/sec: 970,144 | mfu: -1.00 | total time: 6.59m\n",
      "step 00786 (96.89%) | loss: 1.230169 | lrm: 0.16 | dt: 467.54ms | tok/sec: 1,121,374 | mfu: -1.00 | total time: 6.60m\n",
      "step 00787 (97.02%) | loss: 1.217840 | lrm: 0.15 | dt: 541.11ms | tok/sec: 968,912 | mfu: -1.00 | total time: 6.61m\n",
      "step 00788 (97.15%) | loss: 1.221070 | lrm: 0.14 | dt: 466.11ms | tok/sec: 1,124,806 | mfu: -1.00 | total time: 6.62m\n",
      "step 00789 (97.28%) | loss: 1.224236 | lrm: 0.14 | dt: 477.89ms | tok/sec: 1,097,099 | mfu: -1.00 | total time: 6.62m\n",
      "step 00790 (97.41%) | loss: 1.221091 | lrm: 0.13 | dt: 471.05ms | tok/sec: 1,113,008 | mfu: -1.00 | total time: 6.63m\n",
      "step 00791 (97.55%) | loss: 1.218586 | lrm: 0.12 | dt: 536.71ms | tok/sec: 976,856 | mfu: -1.00 | total time: 6.64m\n",
      "step 00792 (97.67%) | loss: 1.226717 | lrm: 0.12 | dt: 534.13ms | tok/sec: 981,566 | mfu: -1.00 | total time: 6.65m\n",
      "step 00793 (97.79%) | loss: 1.218754 | lrm: 0.11 | dt: 466.92ms | tok/sec: 1,122,875 | mfu: -1.00 | total time: 6.66m\n",
      "step 00794 (97.92%) | loss: 1.226166 | lrm: 0.10 | dt: 479.68ms | tok/sec: 1,092,990 | mfu: -1.00 | total time: 6.67m\n",
      "step 00795 (98.04%) | loss: 1.217390 | lrm: 0.10 | dt: 470.34ms | tok/sec: 1,114,708 | mfu: -1.00 | total time: 6.67m\n",
      "step 00796 (98.16%) | loss: 1.217533 | lrm: 0.09 | dt: 479.94ms | tok/sec: 1,092,402 | mfu: -1.00 | total time: 6.68m\n",
      "step 00797 (98.30%) | loss: 1.217931 | lrm: 0.09 | dt: 471.87ms | tok/sec: 1,111,097 | mfu: -1.00 | total time: 6.69m\n",
      "step 00798 (98.41%) | loss: 1.215258 | lrm: 0.08 | dt: 473.97ms | tok/sec: 1,106,174 | mfu: -1.00 | total time: 6.70m\n",
      "step 00799 (98.53%) | loss: 1.210837 | lrm: 0.07 | dt: 471.30ms | tok/sec: 1,112,439 | mfu: -1.00 | total time: 6.71m\n",
      "step 00800 (98.64%) | loss: 1.212448 | lrm: 0.07 | dt: 474.10ms | tok/sec: 1,105,861 | mfu: -1.00 | total time: 6.71m\n",
      "step 00801 (98.78%) | loss: 1.214125 | lrm: 0.06 | dt: 552.83ms | tok/sec: 948,363 | mfu: -1.00 | total time: 6.72m\n",
      "step 00802 (98.91%) | loss: 1.207010 | lrm: 0.05 | dt: 527.44ms | tok/sec: 994,023 | mfu: -1.00 | total time: 6.73m\n",
      "step 00803 (99.04%) | loss: 1.210230 | lrm: 0.05 | dt: 466.56ms | tok/sec: 1,123,738 | mfu: -1.00 | total time: 6.74m\n",
      "step 00804 (99.18%) | loss: 1.211243 | lrm: 0.04 | dt: 484.27ms | tok/sec: 1,082,629 | mfu: -1.00 | total time: 6.75m\n",
      "step 00805 (99.31%) | loss: 1.230007 | lrm: 0.03 | dt: 469.80ms | tok/sec: 1,115,987 | mfu: -1.00 | total time: 6.76m\n",
      "step 00806 (99.44%) | loss: 1.232932 | lrm: 0.03 | dt: 477.73ms | tok/sec: 1,097,451 | mfu: -1.00 | total time: 6.76m\n",
      "step 00807 (99.58%) | loss: 1.228554 | lrm: 0.02 | dt: 545.18ms | tok/sec: 961,681 | mfu: -1.00 | total time: 6.77m\n",
      "step 00808 (99.71%) | loss: 1.230171 | lrm: 0.01 | dt: 467.03ms | tok/sec: 1,122,607 | mfu: -1.00 | total time: 6.78m\n",
      "step 00809 (99.85%) | loss: 1.223937 | lrm: 0.01 | dt: 480.01ms | tok/sec: 1,092,246 | mfu: -1.00 | total time: 6.79m\n",
      "step 00809 | Validation bpb: 0.3947\n",
      "[W1119 13:15:42.054043091 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/mid_checkpoints/d20/model_000809.pt\n",
      "[W1119 13:15:42.253257148 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.254425247 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.255118289 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.261507596 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved metadata to /home/ubuntu/mynanochat/mid_checkpoints/d20/meta_000809.json\n",
      "saved optimizer to /home/ubuntu/mynanochat/mid_checkpoints/d20/optim_000809_rank0.pt\n",
      "Peak memory usage: 75422.02MiB\n",
      "Total training time: 6.79m\n",
      "Minimum validation bpb: 0.3947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 86-86, summary, console lines 824-830 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt â–â–ƒâ–â–â–ƒâ–â–â–ˆâ–ˆâ–…â–ƒâ–â–…â–ƒâ–â–†â–â–†â–ƒâ–„â–â–â–â–â–ƒâ–â–ˆâ–ƒâ–†â–â–â–â–„â–â–â–‚â–â–‚â–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss â–ˆâ–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–…â–„â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–â–â–„â–ˆâ–ƒâ–…â–ˆâ–‡â–…â–ƒâ–ˆâ–…â–‡â–ˆâ–‡â–‡â–…â–‡â–‡â–ƒâ–„â–‡â–…â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb â–ˆâ–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step 809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time 407.27086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt 0.4741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.21245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm 0.06793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec 1105861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb 0.39469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-28-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hiwg9hf8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251119_130651-hiwg9hf8/logs\u001b[0m\n",
      "[W1119 13:15:51.081676247 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:51.089072704 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:51.230440317 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c0eab-8684-4ca7-b624-cd61bcbf8415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9242091f-e1e3-45d8-afd3-8117dce4bc29",
   "metadata": {},
   "source": [
    "^ The starting bpb validation is way off of the final from the training, this seems bad.\n",
    "\n",
    "End of base training: step 21400 | Validation bpb: 0.8135\n",
    "\n",
    "Start of this: step 00000 | Validation bpb: 0.6856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48fd01-9466-4970-a9fb-fbaf6b12b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a8a1ff-a287-49f9-a6dc-68b82be03bc9",
   "metadata": {},
   "source": [
    "Chat eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42db5a9e-9c6d-4fab-a872-6f0689a48345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] \n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 9.00kB [00:00, 31.0MB/s]\n",
      "ARC-Easy/train-00000-of-00001.parquet: 100%|â–ˆ| 331k/331k [00:00<00:00, 1.21MB/s]\n",
      "ARC-Easy/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆ| 346k/346k [00:00<00:00, 1.82MB/s]\n",
      "ARC-Easy/validation-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 86.1k/86.1k [00:00<00:00, 5\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2251/2251 [00:00<00:00, 407606.02 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 649482.94 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 570/570 [00:00<00:00, 301634.28 examples/s]\n",
      "final: 1017/2376 (42.80%)\n",
      "ARC-Easy accuracy: 42.80%\n",
      "ARC-Challenge/train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 190k/190k [00:00<00:00, 812\n",
      "ARC-Challenge/test-00000-of-00001.parque(â€¦): 100%|â–ˆ| 204k/204k [00:00<00:00, 838\n",
      "ARC-Challenge/validation-00000-of-00001.(â€¦): 100%|â–ˆ| 55.7k/55.7k [00:00<00:00, 4\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1119/1119 [00:00<00:00, 241655.14 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 385184.48 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 299/299 [00:00<00:00, 169289.54 examples/s]\n",
      "final: 367/1172 (31.31%)\n",
      "ARC-Challenge accuracy: 31.31%\n",
      "final: 4568/14042 (32.53%)\n",
      "MMLU accuracy: 32.53%\n",
      "\u001b[KRank 6 | 3/165 (1.82%)]\n",
      "\u001b[KRank 4 | 3/165 (1.82%)]\n",
      "\u001b[KRank 5 | 2/165 (1.21%)]\n",
      "\u001b[KRank 2 | 6/165 (3.64%)]\n",
      "\u001b[KRank 1 | 6/165 (3.64%)]\n",
      "\u001b[KRank 0 | 7/165 (4.24%)]\n",
      "\u001b[KRank 7 | 2/164 (1.22%)]\n",
      "\u001b[KRank 3 | 8/165 (4.85%)]\n",
      "==================================================\n",
      "final: 37/1319 (2.81%)\n",
      "GSM8K accuracy: 2.81%\n",
      "README.md: 6.52kB [00:00, 21.4MB/s]\n",
      "openai_humaneval/test-00000-of-00001.par(â€¦): 100%|â–ˆ| 83.9k/83.9k [00:00<00:00, 3\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 47718.76 examples/s]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 4 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 2 | 3/21 (14.29%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 7 | 3/20 (15.00%)]\n",
      "\u001b[KRank 0 | 2/21 (9.52%)]]\n",
      "==================================================\n",
      "final: 16/164 (9.76%)\n",
      "HumanEval accuracy: 9.76%\n",
      "\u001b[KRank 5 | 29/32 (90.62%)]]\n",
      "\u001b[KRank 4 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 3 | 32/32 (100.00%)]\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e76c3-76a5-4954-96f6-98defc1c59d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a4a760-2805-48a9-a5dd-3bb4dd95d7a0",
   "metadata": {},
   "source": [
    "Also run limited chat evals on base and mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e42e91-a08d-4302-972f-c0c50c9b67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] \n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 25/100 (25.00%)\n",
      "ARC-Easy accuracy: 25.00%\n",
      "final: 30/100 (30.00%)\n",
      "ARC-Challenge accuracy: 30.00%\n",
      "final: 28/100 (28.00%)\n",
      "MMLU accuracy: 28.00%\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "GSM8K accuracy: 0.00%\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "HumanEval accuracy: 0.00%\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "SpellingBee accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=base --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8825f-50f9-4be2-a844-fe4de903da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f541b64d-b44e-432e-83e8-55da2230e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] \n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 39/100 (39.00%)\n",
      "ARC-Easy accuracy: 39.00%\n",
      "final: 33/100 (33.00%)\n",
      "ARC-Challenge accuracy: 33.00%\n",
      "final: 31/100 (31.00%)\n",
      "MMLU accuracy: 31.00%\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 2 | 1/13 (7.69%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 1/100 (1.00%)\n",
      "GSM8K accuracy: 1.00%\n",
      "\u001b[KRank 4 | 1/12 (8.33%)]]\n",
      "\u001b[KRank 7 | 1/12 (8.33%)]]\n",
      "\u001b[KRank 0 | 1/13 (7.69%)]\n",
      "\u001b[KRank 6 | 1/12 (8.33%)]\n",
      "\u001b[KRank 5 | 1/12 (8.33%)]\n",
      "\u001b[KRank 2 | 2/13 (15.38%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 7/100 (7.00%)\n",
      "HumanEval accuracy: 7.00%\n",
      "\u001b[KRank 7 | 12/12 (100.00%)]\n",
      "\u001b[KRank 4 | 11/12 (91.67%)]\n",
      "\u001b[KRank 2 | 13/13 (100.00%)]\n",
      "\u001b[KRank 3 | 13/13 (100.00%)]\n",
      "\u001b[KRank 0 | 13/13 (100.00%)]\n",
      "\u001b[KRank 5 | 10/12 (83.33%)]]\n",
      "\u001b[KRank 6 | 12/12 (100.00%)]\n",
      "\u001b[KRank 1 | 13/13 (100.00%)]\n",
      "==================================================\n",
      "final: 97/100 (97.00%)\n",
      "SpellingBee accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876f88c-107f-46de-800a-e957ec3c1955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee620c0-35eb-47a4-afaf-c48dfeb78ae4",
   "metadata": {},
   "source": [
    "Do the base eval on the mid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db03321-2435-41ef-9625-eefb4fa5a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] \n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4374 | centered: 0.2498 | time: 13.45s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0671 | centered: 0.0671 | time: 3.09s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.4806 | centered: 0.4806 | time: 26.67s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6090 | centered: 0.4787 | time: 4.08s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3157 | centered: 0.0876 | time: 1.95s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6300 | centered: 0.2600 | time: 0.18s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2351 | centered: 0.0438 | time: 2.14s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6616 | centered: 0.3232 | time: 3.22s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 0.72s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3497 | centered: 0.3497 | time: 6.57s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4303 | centered: 0.2404 | time: 25.88s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6154 | centered: 0.2308 | time: 0.37s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5470 | centered: 0.0939 | time: 1.52s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1330 | centered: 0.1330 | time: 1.66s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2435 | centered: 0.0543 | time: 0.57s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.4295 | centered: 0.4295 | time: 1.99s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1429 | centered: 0.1429 | time: 0.35s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2487 | centered: 0.2487 | time: 21.71s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1919 | centered: 0.1919 | time: 11.34s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5713 | centered: -0.1283 | time: 8.15s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2501 | centered: 0.1750 | time: 41.31s\n",
      "CORE metric: 0.1918\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.24981741110483804,\n",
      "    \"jeopardy\": 0.06707604974508286,\n",
      "    \"bigbench_qa_wikidata\": 0.48063579201698303,\n",
      "    \"arc_easy\": 0.47867560386657715,\n",
      "    \"arc_challenge\": 0.08759951591491699,\n",
      "    \"copa\": 0.25999999046325684,\n",
      "    \"commonsense_qa\": 0.04381655156612395,\n",
      "    \"piqa\": 0.3231773376464844,\n",
      "    \"openbook_qa\": 0.0666666825612386,\n",
      "    \"lambada_openai\": 0.3496991991996765,\n",
      "    \"hellaswag\": 0.24039034048716226,\n",
      "    \"winograd\": 0.23076927661895752,\n",
      "    \"winogrande\": 0.09392261505126953,\n",
      "    \"bigbench_dyck_languages\": 0.13300000131130219,\n",
      "    \"agi_eval_lsat_ar\": 0.05434781685471533,\n",
      "    \"bigbench_cs_algorithms\": 0.42954543232917786,\n",
      "    \"bigbench_operators\": 0.1428571492433548,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.24872279167175293,\n",
      "    \"coqa\": 0.19190780818462372,\n",
      "    \"boolq\": -0.1282793785396375,\n",
      "    \"bigbench_language_identification\": 0.17502748821959374\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b62c8b-8d38-48c8-9150-33e7331987cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac83f8e5-6de9-4ee7-9f7c-5a1024c8043b",
   "metadata": {},
   "source": [
    "Repeat the base eval on the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c5b55a-1ce7-4e41-9013-87d0aff4c14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] \n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4425 | centered: 0.2567 | time: 13.65s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1186 | centered: 0.1186 | time: 3.22s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5366 | centered: 0.5366 | time: 26.73s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6469 | centered: 0.5292 | time: 4.13s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3515 | centered: 0.1354 | time: 1.95s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.18s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2301 | centered: 0.0377 | time: 2.13s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6795 | centered: 0.3591 | time: 3.17s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.71s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3745 | centered: 0.3745 | time: 6.63s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4471 | centered: 0.2628 | time: 25.85s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.35s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5280 | centered: 0.0560 | time: 1.55s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1020 | centered: 0.1020 | time: 1.68s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.58s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3583 | centered: 0.3583 | time: 2.00s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.35s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2345 | centered: 0.2345 | time: 21.66s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1813 | centered: 0.1813 | time: 11.45s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5070 | centered: -0.2973 | time: 8.18s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2530 | centered: 0.1782 | time: 41.42s\n",
      "CORE metric: 0.2012\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.25672173500061035,\n",
      "    \"jeopardy\": 0.11856400221586227,\n",
      "    \"bigbench_qa_wikidata\": 0.5365877747535706,\n",
      "    \"arc_easy\": 0.5291806856791178,\n",
      "    \"arc_challenge\": 0.13538110256195068,\n",
      "    \"copa\": 0.3799999952316284,\n",
      "    \"commonsense_qa\": 0.03767403215169905,\n",
      "    \"piqa\": 0.35908591747283936,\n",
      "    \"openbook_qa\": 0.14666668574015299,\n",
      "    \"lambada_openai\": 0.3745391070842743,\n",
      "    \"hellaswag\": 0.26282942295074463,\n",
      "    \"winograd\": 0.28205132484436035,\n",
      "    \"winogrande\": 0.05603790283203125,\n",
      "    \"bigbench_dyck_languages\": 0.10200000554323196,\n",
      "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
      "    \"bigbench_cs_algorithms\": 0.3583333194255829,\n",
      "    \"bigbench_operators\": 0.1666666716337204,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.2345316857099533,\n",
      "    \"coqa\": 0.18126018345355988,\n",
      "    \"boolq\": -0.2972798786665264,\n",
      "    \"bigbench_language_identification\": 0.1782178120775716\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1ad9b-3ff3-4ed5-9ba2-f3c00e4579d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c95a0-d1b9-4f6d-adac-eeaeda4e954e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f5b1a-759c-403a-a235-a776c5757ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
