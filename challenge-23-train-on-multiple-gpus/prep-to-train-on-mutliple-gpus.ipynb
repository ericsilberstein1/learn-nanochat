{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329edd73-aaa3-4dcd-b215-c55a889f2fb2",
   "metadata": {},
   "source": [
    "I'll be going to a new machine and so I'll first need to download data. As before, do that with:\n",
    "\n",
    "`python -m nanochat.dataset -n 20` for example\n",
    "\n",
    "I'll then want to train the tokenizer but I never put that in a script.\n",
    "\n",
    "In `challenge-14-baby-pretrain-on-gpu` I did it in a notebook `train-tokenizer.ipynb`\n",
    "\n",
    "And then in `challenge-18-add-evaluate-bpb` I wrote/ran the code to cache the mapping from token to number of bytes.\n",
    "\n",
    "It's time to put all that into `my_tok_train.py` to keep things organized.\n",
    "\n",
    "An errow below reminded me I'll also need to do this on the new machine:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cafb85b-f0f0-4f8f-960f-e49363e43eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b024aeb8-5c07-47af-adb6-fdf6eeafc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe1a6-4788-48d7-ae0c-1c339f8a409c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87637ffa-5d04-454a-a2f7-c7890eb8436d",
   "metadata": {},
   "source": [
    "So far I've been running scripts with python. From looking at his [speedrun.sh](https://github.com/karpathy/nanochat/blob/master/speedrun.sh), it looks we use `torchrun` to use the torch distributed stuff. Let me first see if that can run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671a2ac-1545-48fd-b5ab-a3c852155ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:42:49.242000 96239 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE]\n",
      "                [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]\n",
      "                [--max-restarts MAX_RESTARTS]\n",
      "                [--monitor-interval MONITOR_INTERVAL]\n",
      "                [--start-method {spawn,fork,forkserver}]\n",
      "                [--event-log-handler EVENT_LOG_HANDLER] [--role ROLE] [-m]\n",
      "                [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--local-ranks-filter LOCAL_RANKS_FILTER]\n",
      "                [--node-rank NODE_RANK] [--master-addr MASTER_ADDR]\n",
      "                [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR]\n",
      "                [--logs-specs LOGS_SPECS]\n",
      "                [--numa-binding {node,socket,exclusive,core-complex}]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "!torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f628c7-c034-4a8d-b574-530227043d43",
   "metadata": {},
   "source": [
    "He calls it like this:\n",
    "\n",
    "`torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_train -- --depth=20 --run=$WANDB_RUN`\n",
    "\n",
    "What does the --standalone flag do?\n",
    "\n",
    "ChatGPT seems to give a good answer. Short seems to be use it for single node multi GPU and you can save setting up a lot of other stuff.\n",
    "\n",
    "Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb53c14-da32-40ba-bf0e-10adcb155aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:49:45.765000 96358 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[W1114 14:49:45.211740000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:46.814254000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:47.601272000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:48.739299000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:50.640563000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:53.014873000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:57.436222000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:03.184587000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:13.861398000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:22.444711000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "^C\n",
      "[W1114 14:50:27.242578000 TCPStore.cpp:347] [c10d] TCP client failed to connect/validate to host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa:49218 - retrying (try=0, timeout=300000ms, delay=19962ms): Interrupted system call\n",
      "Exception raised from delay at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 56 (0x100bc7d7c in libc10.dylib)\n",
      "frame #1: c10d::detail::(anonymous namespace)::SocketConnectOp::tryConnect(int) + 4044 (0x1172e6588 in libtorch_cpu.dylib)\n",
      "frame #2: c10d::detail::Socket::connect(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, c10d::detail::SocketOptions const&) + 260 (0x1172e4f94 in libtorch_cpu.dylib)\n",
      "frame #3: c10d::detail::TCPClient::connect(c10d::detail::SocketAddress const&, c10d::TCPStoreOptions const&, std::__1::shared_ptr<c10d::Backoff>) + 184 (0x1172729bc in libtorch_cpu.dylib)\n",
      "frame #4: c10d::TCPStore::TCPStore(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, c10d::TCPStoreOptions const&) + 980 (0x117273164 in libtorch_cpu.dylib)\n",
      "frame #5: c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>::make<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&) + 168 (0x104af80e0 in libtorch_python.dylib)\n",
      "frame #6: std::__1::enable_if<std::is_void<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>>::value, pybind11::detail::void_type>::type pybind11::detail::argument_loader<pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool>::call<void, pybind11::detail::void_type, void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&>(void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&) && + 176 (0x104af7c54 in libtorch_python.dylib)\n",
      "frame #7: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&&, void (*)(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24])::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 92 (0x104af7b2c in libtorch_python.dylib)\n",
      "frame #8: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 4344 (0x103ee7a9c in libtorch_python.dylib)\n",
      "<omitting python frames>\n",
      "frame #15: pybind11_meta_call + 40 (0x103ee3620 in libtorch_python.dylib)\n",
      "frame #60: start + 6076 (0x18d40ab98 in dyld)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646b244-aa0d-495a-9e71-2e8d24a991de",
   "metadata": {},
   "source": [
    "Doesn't work, but no point in figuring that out, instead try on our single GPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c84f2-e926-4684-a1b4-4bb6a7882d68",
   "metadata": {},
   "source": [
    "### trying on single GPU machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bb95b3-57a0-4d61-b83d-f8227103d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 16, in <module>\n",
      "    from my_nanochat.my_tokenizer import get_tokenizer, get_token_bytes\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_tokenizer.py\", line 1, in <module>\n",
      "    import rust_tokenizer;\n",
      "ModuleNotFoundError: No module named 'rust_tokenizer'\n",
      "[W1114 19:57:36.434514993 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "E1114 19:57:36.615000 1773 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 1788) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_19:57:36\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1788)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464dd266-f676-4363-a5a5-9b36ea1735f4",
   "metadata": {},
   "source": [
    "The error about no module 'rust_tokenizer' reminds me that I should move that code out of challenge 7. Don't understand why I'm getting that error now but maybe because I did `uv sync` to get wandb here? Do this:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3b312f-ea1e-4c95-8cde-f20b0d99740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:11:34.693000 2585 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 735.62ms | tok/sec: 174 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 68.68ms | tok/sec: 1,863 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 68.09ms | tok/sec: 1,879 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 49.67ms | tok/sec: 2,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 41.51ms | tok/sec: 3,083 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 41.47ms | tok/sec: 3,086 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 41.37ms | tok/sec: 3,093 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 41.24ms | tok/sec: 3,103 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:11:41.760899202 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb6df1-5d0e-4f98-91c6-4de3333c7b0c",
   "metadata": {},
   "source": [
    "ok, seems good. What happens if tell it to use 2 GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd7e4ff-8411-40e4-899e-9130cffb10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:14:20.700000 2755 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 20:14:20.700000 2756 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 683.57ms | tok/sec: 187 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 80.73ms | tok/sec: 1,585 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 80.69ms | tok/sec: 1,586 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 78.92ms | tok/sec: 1,621 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 80.23ms | tok/sec: 1,595 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 80.46ms | tok/sec: 1,590 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 80.41ms | tok/sec: 1,591 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 78.17ms | tok/sec: 1,637 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 230, in <module>\n",
      "    save_checkpoint(\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_checkpoint_manager.py\", line 11, in save_checkpoint\n",
      "    assert int(os.environ.get('RANK', 0)) == 0\n",
      "AssertionError\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:14:27.771280111 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 20:14:28.004000 2736 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2755 closing signal SIGTERM\n",
      "E1114 20:14:28.118000 2736 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 2756) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_20:14:28\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2756)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c5436-d9d0-4364-a7ba-3f13efd5941a",
   "metadata": {},
   "source": [
    "^ failed, as expected, not sure if in the expected way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bacd8-b3ed-4061-b202-041bf4ffe940",
   "metadata": {},
   "source": [
    "### Trying with 2 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c70cee-5fda-4beb-a570-b7a998e67669",
   "metadata": {},
   "source": [
    "- Create a new 2 (low-powered) GPU machine in paperspace\n",
    "\n",
    "- Follow the instructions in `challenge-14-baby-pretrain-on-gpu/getting-ready.ipynb` to set it up.\n",
    "\n",
    "Chose 2xRTX4000 (single GPU machine was also RTX4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc841971-f9cc-4dd8-95f8-896e670c7185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
