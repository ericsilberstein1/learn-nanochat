{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329edd73-aaa3-4dcd-b215-c55a889f2fb2",
   "metadata": {},
   "source": [
    "I'll be going to a new machine and so I'll first need to download data. As before, do that with:\n",
    "\n",
    "`python -m nanochat.dataset -n 20` for example\n",
    "\n",
    "I'll then want to train the tokenizer but I never put that in a script.\n",
    "\n",
    "In `challenge-14-baby-pretrain-on-gpu` I did it in a notebook `train-tokenizer.ipynb`\n",
    "\n",
    "And then in `challenge-18-add-evaluate-bpb` I wrote/ran the code to cache the mapping from token to number of bytes.\n",
    "\n",
    "It's time to put all that into `my_tok_train.py` to keep things organized.\n",
    "\n",
    "An errow below reminded me I'll also need to do this on the new machine:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cafb85b-f0f0-4f8f-960f-e49363e43eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b024aeb8-5c07-47af-adb6-fdf6eeafc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe1a6-4788-48d7-ae0c-1c339f8a409c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87637ffa-5d04-454a-a2f7-c7890eb8436d",
   "metadata": {},
   "source": [
    "So far I've been running scripts with python. From looking at his [speedrun.sh](https://github.com/karpathy/nanochat/blob/master/speedrun.sh), it looks we use `torchrun` to use the torch distributed stuff. Let me first see if that can run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671a2ac-1545-48fd-b5ab-a3c852155ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:42:49.242000 96239 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE]\n",
      "                [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]\n",
      "                [--max-restarts MAX_RESTARTS]\n",
      "                [--monitor-interval MONITOR_INTERVAL]\n",
      "                [--start-method {spawn,fork,forkserver}]\n",
      "                [--event-log-handler EVENT_LOG_HANDLER] [--role ROLE] [-m]\n",
      "                [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--local-ranks-filter LOCAL_RANKS_FILTER]\n",
      "                [--node-rank NODE_RANK] [--master-addr MASTER_ADDR]\n",
      "                [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR]\n",
      "                [--logs-specs LOGS_SPECS]\n",
      "                [--numa-binding {node,socket,exclusive,core-complex}]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "!torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f628c7-c034-4a8d-b574-530227043d43",
   "metadata": {},
   "source": [
    "He calls it like this:\n",
    "\n",
    "`torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_train -- --depth=20 --run=$WANDB_RUN`\n",
    "\n",
    "What does the --standalone flag do?\n",
    "\n",
    "ChatGPT seems to give a good answer. Short seems to be use it for single node multi GPU and you can save setting up a lot of other stuff.\n",
    "\n",
    "Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb53c14-da32-40ba-bf0e-10adcb155aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:49:45.765000 96358 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[W1114 14:49:45.211740000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:46.814254000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:47.601272000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:48.739299000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:50.640563000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:53.014873000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:57.436222000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:03.184587000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:13.861398000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:22.444711000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "^C\n",
      "[W1114 14:50:27.242578000 TCPStore.cpp:347] [c10d] TCP client failed to connect/validate to host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa:49218 - retrying (try=0, timeout=300000ms, delay=19962ms): Interrupted system call\n",
      "Exception raised from delay at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 56 (0x100bc7d7c in libc10.dylib)\n",
      "frame #1: c10d::detail::(anonymous namespace)::SocketConnectOp::tryConnect(int) + 4044 (0x1172e6588 in libtorch_cpu.dylib)\n",
      "frame #2: c10d::detail::Socket::connect(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, c10d::detail::SocketOptions const&) + 260 (0x1172e4f94 in libtorch_cpu.dylib)\n",
      "frame #3: c10d::detail::TCPClient::connect(c10d::detail::SocketAddress const&, c10d::TCPStoreOptions const&, std::__1::shared_ptr<c10d::Backoff>) + 184 (0x1172729bc in libtorch_cpu.dylib)\n",
      "frame #4: c10d::TCPStore::TCPStore(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, c10d::TCPStoreOptions const&) + 980 (0x117273164 in libtorch_cpu.dylib)\n",
      "frame #5: c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>::make<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&) + 168 (0x104af80e0 in libtorch_python.dylib)\n",
      "frame #6: std::__1::enable_if<std::is_void<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>>::value, pybind11::detail::void_type>::type pybind11::detail::argument_loader<pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool>::call<void, pybind11::detail::void_type, void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&>(void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&) && + 176 (0x104af7c54 in libtorch_python.dylib)\n",
      "frame #7: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&&, void (*)(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24])::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 92 (0x104af7b2c in libtorch_python.dylib)\n",
      "frame #8: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 4344 (0x103ee7a9c in libtorch_python.dylib)\n",
      "<omitting python frames>\n",
      "frame #15: pybind11_meta_call + 40 (0x103ee3620 in libtorch_python.dylib)\n",
      "frame #60: start + 6076 (0x18d40ab98 in dyld)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646b244-aa0d-495a-9e71-2e8d24a991de",
   "metadata": {},
   "source": [
    "Doesn't work, but no point in figuring that out, instead try on our single GPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c84f2-e926-4684-a1b4-4bb6a7882d68",
   "metadata": {},
   "source": [
    "### trying on single GPU machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bb95b3-57a0-4d61-b83d-f8227103d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 16, in <module>\n",
      "    from my_nanochat.my_tokenizer import get_tokenizer, get_token_bytes\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_tokenizer.py\", line 1, in <module>\n",
      "    import rust_tokenizer;\n",
      "ModuleNotFoundError: No module named 'rust_tokenizer'\n",
      "[W1114 19:57:36.434514993 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "E1114 19:57:36.615000 1773 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 1788) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_19:57:36\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1788)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464dd266-f676-4363-a5a5-9b36ea1735f4",
   "metadata": {},
   "source": [
    "The error about no module 'rust_tokenizer' reminds me that I should move that code out of challenge 7. Don't understand why I'm getting that error now but maybe because I did `uv sync` to get wandb here? Do this:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3b312f-ea1e-4c95-8cde-f20b0d99740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:11:34.693000 2585 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 735.62ms | tok/sec: 174 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 68.68ms | tok/sec: 1,863 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 68.09ms | tok/sec: 1,879 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 49.67ms | tok/sec: 2,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 41.51ms | tok/sec: 3,083 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 41.47ms | tok/sec: 3,086 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 41.37ms | tok/sec: 3,093 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 41.24ms | tok/sec: 3,103 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:11:41.760899202 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb6df1-5d0e-4f98-91c6-4de3333c7b0c",
   "metadata": {},
   "source": [
    "ok, seems good. What happens if tell it to use 2 GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd7e4ff-8411-40e4-899e-9130cffb10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:14:20.700000 2755 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 20:14:20.700000 2756 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 683.57ms | tok/sec: 187 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 80.73ms | tok/sec: 1,585 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 80.69ms | tok/sec: 1,586 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 78.92ms | tok/sec: 1,621 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 80.23ms | tok/sec: 1,595 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 80.46ms | tok/sec: 1,590 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 80.41ms | tok/sec: 1,591 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 78.17ms | tok/sec: 1,637 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 230, in <module>\n",
      "    save_checkpoint(\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_checkpoint_manager.py\", line 11, in save_checkpoint\n",
      "    assert int(os.environ.get('RANK', 0)) == 0\n",
      "AssertionError\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:14:27.771280111 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 20:14:28.004000 2736 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2755 closing signal SIGTERM\n",
      "E1114 20:14:28.118000 2736 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 2756) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_20:14:28\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2756)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c5436-d9d0-4364-a7ba-3f13efd5941a",
   "metadata": {},
   "source": [
    "^ failed, as expected, not sure if in the expected way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bacd8-b3ed-4061-b202-041bf4ffe940",
   "metadata": {},
   "source": [
    "### Trying with 2 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c70cee-5fda-4beb-a570-b7a998e67669",
   "metadata": {},
   "source": [
    "- Create a new 2 (low-powered) GPU machine in paperspace\n",
    "\n",
    "- Follow the instructions in `challenge-14-baby-pretrain-on-gpu/getting-ready.ipynb` to set it up.\n",
    "\n",
    "Chose 2xRTX4000 (single GPU machine was also RTX4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0394f0-7754-47fd-bc92-77a4f6a682dc",
   "metadata": {},
   "source": [
    "I followed those instructions and now I'm on the new machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96afe05e-e119-426d-82cf-9d7f32a263ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 14 20:55:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                Off |   00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   31C    P8              2W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Quadro RTX 4000                Off |   00000000:00:06.0 Off |                  N/A |\n",
      "| 30%   33C    P8              6W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16747893-0d9e-4ce4-891e-2cdf910d5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b173c-5512-4ca3-9908-59405dca272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "buffers filled: 73\n",
      "buffers filled: 74\n",
      "buffers filled: 75\n",
      "buffers filled: 76\n",
      "buffers filled: 77\n",
      "buffers filled: 78\n",
      "buffers filled: 79\n",
      "buffers filled: 80\n",
      "buffers filled: 81\n",
      "buffers filled: 82\n",
      "buffers filled: 83\n",
      "buffers filled: 84\n",
      "buffers filled: 85\n",
      "buffers filled: 86\n",
      "buffers filled: 87\n",
      "buffers filled: 88\n",
      "buffers filled: 89\n",
      "buffers filled: 90\n",
      "buffers filled: 91\n",
      "buffers filled: 92\n",
      "buffers filled: 93\n",
      "buffers filled: 94\n",
      "buffers filled: 95\n",
      "buffers filled: 96\n",
      "buffers filled: 97\n",
      "buffers filled: 98\n",
      "buffers filled: 99\n",
      "buffers filled: 100\n",
      "buffers filled: 101\n",
      "buffers filled: 102\n",
      "buffers filled: 103\n",
      "buffers filled: 104\n",
      "buffers filled: 105\n",
      "buffers filled: 106\n",
      "buffers filled: 107\n",
      "buffers filled: 108\n",
      "buffers filled: 109\n",
      "buffers filled: 110\n",
      "buffers filled: 111\n",
      "buffers filled: 112\n",
      "buffers filled: 113\n",
      "buffers filled: 114\n",
      "buffers filled: 115\n",
      "buffers filled: 116\n",
      "buffers filled: 117\n",
      "buffers filled: 118\n",
      "buffers filled: 119\n",
      "buffers filled: 120\n",
      "buffers filled: 121\n",
      "buffers filled: 122\n",
      "buffers filled: 123\n",
      "buffers filled: 124\n",
      "buffers filled: 125\n",
      "buffers filled: 126\n",
      "buffers filled: 127\n",
      "buffers filled: 128\n",
      "buffers filled: 129\n",
      "buffers filled: 130\n",
      "buffers filled: 131\n",
      "buffers filled: 132\n",
      "buffers filled: 133\n",
      "buffers filled: 134\n",
      "buffers filled: 135\n",
      "buffers filled: 136\n",
      "buffers filled: 137\n",
      "buffers filled: 138\n",
      "buffers filled: 139\n",
      "buffers filled: 140\n",
      "buffers filled: 141\n",
      "buffers filled: 142\n",
      "buffers filled: 143\n",
      "buffers filled: 144\n",
      "buffers filled: 145\n",
      "buffers filled: 146\n",
      "buffers filled: 147\n",
      "buffers filled: 148\n",
      "buffers filled: 149\n",
      "buffers filled: 150\n",
      "buffers filled: 151\n",
      "buffers filled: 152\n",
      "buffers filled: 153\n",
      "buffers filled: 154\n",
      "buffers filled: 155\n",
      "buffers filled: 156\n",
      "buffers filled: 157\n",
      "buffers filled: 158\n",
      "buffers filled: 159\n",
      "buffers filled: 160\n",
      "buffers filled: 161\n",
      "buffers filled: 162\n",
      "buffers filled: 163\n",
      "buffers filled: 164\n",
      "buffers filled: 165\n",
      "buffers filled: 166\n",
      "buffers filled: 167\n",
      "buffers filled: 168\n",
      "buffers filled: 169\n",
      "buffers filled: 170\n",
      "buffers filled: 171\n",
      "buffers filled: 172\n",
      "buffers filled: 173\n",
      "buffers filled: 174\n",
      "buffers filled: 175\n",
      "buffers filled: 176\n",
      "buffers filled: 177\n",
      "buffers filled: 178\n",
      "buffers filled: 179\n",
      "buffers filled: 180\n",
      "buffers filled: 181\n",
      "buffers filled: 182\n",
      "buffers filled: 183\n",
      "buffers filled: 184\n",
      "buffers filled: 185\n",
      "buffers filled: 186\n",
      "buffers filled: 187\n",
      "buffers filled: 188\n",
      "buffers filled: 189\n",
      "buffers filled: 190\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /home/paperspace/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adb0575-c420-4f5a-810c-c35ffba2e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] \n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:36:32.228000 37125 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:36:32.245000 37126 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 869.65ms | tok/sec: 147 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 83.89ms | tok/sec: 1,525 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.988388 | grad norm: 3.0695 | lrm: 1.00 | dt: 74.97ms | tok/sec: 1,707 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.847008 | grad norm: 4.9443 | lrm: 1.00 | dt: 78.54ms | tok/sec: 1,629 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.685705 | grad norm: 4.6343 | lrm: 1.00 | dt: 76.45ms | tok/sec: 1,674 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.549452 | grad norm: 4.2740 | lrm: 1.00 | dt: 80.78ms | tok/sec: 1,584 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.094818 | grad norm: 5.6507 | lrm: 1.00 | dt: 78.69ms | tok/sec: 1,626 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.810596 | grad norm: 5.0067 | lrm: 1.00 | dt: 78.65ms | tok/sec: 1,627 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.552553 | grad norm: 3.7087 | lrm: 1.00 | dt: 78.84ms | tok/sec: 1,623 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.228461 | grad norm: 3.6521 | lrm: 0.50 | dt: 76.78ms | tok/sec: 1,667 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.7032\n",
      "<bos>The capital of France is a less, and the time, and the time\n",
      "<bos>The chemical symbol of gold is a less, and the time, and the time\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less, and the time, and the time\n",
      "<bos>The opposite of hot is a less, and the time, and the time\n",
      "<bos>The planets of the solar system are: 20, and the time, and the time\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 230, in <module>\n",
      "    save_checkpoint(\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_checkpoint_manager.py\", line 11, in save_checkpoint\n",
      "    assert int(os.environ.get('RANK', 0)) == 0\n",
      "AssertionError\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.7032\n",
      "[W1114 21:36:41.669889329 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 21:36:41.517000 37098 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 37125 closing signal SIGTERM\n",
      "E1114 21:36:41.632000 37098 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 37126) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_21:36:41\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 37126)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca745c-e068-4ee8-9c80-beca2eb71573",
   "metadata": {},
   "source": [
    "hit that assert I put in because left out code for saving optimizers with requires something special with ranks...go add\n",
    "\n",
    "but that also means it got all the way to saving...I guess that's good, but how do I know it was doing the righ thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8de80e-d738-493d-96d3-577b64376bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] \n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:48:17.145000 37741 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:48:17.170000 37740 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 810.14ms | tok/sec: 157 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 77.20ms | tok/sec: 1,658 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.988388 | grad norm: 3.0695 | lrm: 1.00 | dt: 80.15ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.847008 | grad norm: 4.9443 | lrm: 1.00 | dt: 81.19ms | tok/sec: 1,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.685705 | grad norm: 4.6343 | lrm: 1.00 | dt: 76.45ms | tok/sec: 1,674 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.549452 | grad norm: 4.2740 | lrm: 1.00 | dt: 78.95ms | tok/sec: 1,621 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.094818 | grad norm: 5.6507 | lrm: 1.00 | dt: 78.22ms | tok/sec: 1,636 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.810596 | grad norm: 5.0067 | lrm: 1.00 | dt: 78.99ms | tok/sec: 1,620 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.552553 | grad norm: 3.7087 | lrm: 1.00 | dt: 81.19ms | tok/sec: 1,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.228461 | grad norm: 3.6521 | lrm: 0.50 | dt: 78.29ms | tok/sec: 1,634 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.7032\n",
      "<bos>The capital of France is a less, and the time, and the time\n",
      "<bos>The chemical symbol of gold is a less, and the time, and the time\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less, and the time, and the time\n",
      "<bos>The opposite of hot is a less, and the time, and the time\n",
      "<bos>The planets of the solar system are: 20, and the time, and the time\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000010_rank0.pt\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.7032\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000010_rank0.pt\n",
      "[W1114 21:48:27.702025232 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1114 21:48:27.961290282 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263bdb5-f1ee-4b3c-9a99-04d85a23fbac",
   "metadata": {},
   "source": [
    "Added another print statement that prints in all ranks (not only master process). Want to make sure see both ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee71b9f-057f-4f0a-a8f7-b82d0211d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] \n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 256\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:53:59.081000 38156 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:53:59.114000 38157 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00002 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 791.44ms | tok/sec: 161 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00002 (50.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 61.59ms | tok/sec: 2,078 | mfu: -1.00 | total time: 0.00m\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00002 | Validation bpb: 3.1460\n",
      "<bos>The capital of France is an important consideration when planning UNCTAD, Air\n",
      "<bos>The chemical symbol of gold is an important consideration when planning UNCTAD, Air\n",
      "<bos>If yesterday was Friday, then tomorrow will be evaluated when planning UNCTAD, Air, Air\n",
      "<bos>The opposite of hot is an important consideration when planning UNCTAD, Air\n",
      "<bos>The planets of the solar system are: when planning UNCTAD, Air, Air,\n",
      "<bos>My favorite color is an important consideration when planning UNCTAD, Air\n",
      "<bos>If 5*x + 3 = 13, then x is an important consideration when planning UNCTAD, Air\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000002.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000002.json\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000002.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000002.json\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000002_rank0.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000002_rank0.pt\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.1460\n",
      "[W1114 21:54:08.709303464 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1114 21:54:08.768293558 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0a63a-9349-46f8-a532-5d9d1c98aed2",
   "metadata": {},
   "source": [
    "Seeing `This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1`, so must be missing something.\n",
    "\n",
    "Oh yeah, forgot to update this in `my_common.py`:\n",
    "```\n",
    "# return ddp, ddp_rank, ddp_local_rank, ddp_world_size\n",
    "def get_dist_info():\n",
    "    # for now\n",
    "    return False, 0, 0, 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0ae0de-016d-4dc6-bd59-669a6bc6595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] \n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 134, in <module>\n",
      "[rank1]:     optimizers = model.setup_optimizers(\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_gpt.py\", line 201, in setup_optimizers\n",
      "[rank1]:     adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
      "[rank1]: TypeError: 'NoneType' object is not callable\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 134, in <module>\n",
      "[rank0]:     optimizers = model.setup_optimizers(\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_gpt.py\", line 201, in setup_optimizers\n",
      "[rank0]:     adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
      "[rank0]: TypeError: 'NoneType' object is not callable\n",
      "[rank0]:[W1114 22:05:26.441608257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank1]:[W1114 22:05:28.620819630 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1114 22:05:28.681378349 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "E1114 22:05:28.514000 38726 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 38752) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-11-14_22:05:28\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 38753)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:05:28\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 38752)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aef819-19e3-4f5f-bad8-bb3e3665741c",
   "metadata": {},
   "source": [
    "ok, now failing due to `DistAdamW = None # for now so it will fail until I \"copy\" adamw.py`\n",
    "\n",
    "time to look at `adamw.py`\n",
    "\n",
    "I'm going to copy and paste it and go back and look later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9f9a19-a656-43ab-84d8-c58ef262df32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] \n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Muon: Grouping 16 params of shape torch.Size([256, 256]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([256, 1024]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([1024, 256]), device cuda:0, dtype torch.float32\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "[rank0]:W1114 22:30:59.236000 39542 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "[rank1]:W1114 22:30:59.264000 39543 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 2.9727\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank0]:     opt.step()\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank0]:     out = func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 33, in step\n",
      "[rank0]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank0]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank0]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank1]:     opt.step()\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank1]:     out = func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 33, in step\n",
      "[rank1]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank1]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank1]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]:[W1114 22:31:04.986828210 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W1114 22:31:05.273053922 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank1]:[W1114 22:31:06.486275983 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 22:31:06.149000 39515 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 39543 closing signal SIGTERM\n",
      "E1114 22:31:06.264000 39515 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 39542) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:31:06\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 39542)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e1880-d8fc-4d7f-8027-b09542cb5a28",
   "metadata": {},
   "source": [
    "Add temp debug printing in `adamw.py` to see shapes of output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15fe8502-a86e-447d-8653-f3c737f99b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] \n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Muon: Grouping 16 params of shape torch.Size([256, 256]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([256, 1024]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([1024, 256]), device cuda:0, dtype torch.float32\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "[rank1]:W1114 22:42:06.550000 40128 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "[rank0]:W1114 22:42:06.664000 40127 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 2.9727\n",
      "Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])\n",
      "Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank1]:     opt.step()\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank1]:     out = func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 34, in step\n",
      "[rank1]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank1]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank1]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank0]:     opt.step()\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank0]:     out = func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 34, in step\n",
      "[rank0]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank0]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank0]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]:[W1114 22:42:11.226552490 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W1114 22:42:12.228890129 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 22:42:13.106000 40101 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 40128 closing signal SIGTERM\n",
      "E1114 22:42:13.221000 40101 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 40127) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:42:13\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 40127)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa066a9-40b1-470c-a74a-7a02cbdb4450",
   "metadata": {},
   "source": [
    "`Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])` and `reduce_scatter_tensor()` doc says: \"input (Tensor): Input tensor to be reduced and scattered. Its size should be output tensor size times the world size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7f3a279-39e4-4bcb-8d74-bbfc0b60d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_tokenizer import get_tokenizer\n",
    "get_tokenizer().get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661165bb-fc17-40d3-b652-4008a7865166",
   "metadata": {},
   "source": [
    "Prob do need to understand adamw but just based on those numbers, does it somehow split up the lm_head params and if vocab_size % world_size is not 0 it's a prob? Is there something in the actual tokenizer training that forces vocab_size to be a multiple of 8 or something like that? Don't immediately see anything like that. I have 65536 + BOS = 65537. The real one has 9 special tokens including BOS so that's still going to be odd number. Hmm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e85b8-d009-4d04-bd43-1704214e18d6",
   "metadata": {},
   "source": [
    "Let me google and chatgpt \"ValueError: input tensor must be the same size as output size times world size\"\n",
    "\n",
    "Not immediately seeing an answer, but it looks like understanding scatter and related is important to understand DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d2928-f0d4-48aa-ba48-ffad7bcba12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4132c-5461-4ecd-97ca-bb9f0d51c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a0364-49f0-48d0-90bc-8ec444426af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44bcf09-a9ba-44f3-a99f-346372a3d927",
   "metadata": {},
   "source": [
    "Code added / updated as part of this challenge so far:\n",
    "\n",
    "- Added `my_tok_train.py`\n",
    " \n",
    "- Added / fixed code purposely left out earlier when ignoring DDP in `my_checkpoint_manager.py`, `my_common.py` and `my_gpt.py`\n",
    "\n",
    "- Directly copied the entire `adamw.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce00cc1-e059-4959-b59c-8b0eb632b901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
