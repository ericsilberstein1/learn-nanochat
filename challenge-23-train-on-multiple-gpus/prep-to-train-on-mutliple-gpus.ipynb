{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329edd73-aaa3-4dcd-b215-c55a889f2fb2",
   "metadata": {},
   "source": [
    "I'll be going to a new machine and so I'll first need to download data. As before, do that with:\n",
    "\n",
    "`python -m nanochat.dataset -n 20` for example\n",
    "\n",
    "I'll then want to train the tokenizer but I never put that in a script.\n",
    "\n",
    "In `challenge-14-baby-pretrain-on-gpu` I did it in a notebook `train-tokenizer.ipynb`\n",
    "\n",
    "And then in `challenge-18-add-evaluate-bpb` I wrote/ran the code to cache the mapping from token to number of bytes.\n",
    "\n",
    "It's time to put all that into `my_tok_train.py` to keep things organized.\n",
    "\n",
    "An errow below reminded me I'll also need to do this on the new machine:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cafb85b-f0f0-4f8f-960f-e49363e43eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b024aeb8-5c07-47af-adb6-fdf6eeafc4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe1a6-4788-48d7-ae0c-1c339f8a409c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87637ffa-5d04-454a-a2f7-c7890eb8436d",
   "metadata": {},
   "source": [
    "So far I've been running scripts with python. From looking at his [speedrun.sh](https://github.com/karpathy/nanochat/blob/master/speedrun.sh), it looks we use `torchrun` to use the torch distributed stuff. Let me first see if that can run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671a2ac-1545-48fd-b5ab-a3c852155ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:42:49.242000 96239 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE]\n",
      "                [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]\n",
      "                [--max-restarts MAX_RESTARTS]\n",
      "                [--monitor-interval MONITOR_INTERVAL]\n",
      "                [--start-method {spawn,fork,forkserver}]\n",
      "                [--event-log-handler EVENT_LOG_HANDLER] [--role ROLE] [-m]\n",
      "                [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--local-ranks-filter LOCAL_RANKS_FILTER]\n",
      "                [--node-rank NODE_RANK] [--master-addr MASTER_ADDR]\n",
      "                [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR]\n",
      "                [--logs-specs LOGS_SPECS]\n",
      "                [--numa-binding {node,socket,exclusive,core-complex}]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "!torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f628c7-c034-4a8d-b574-530227043d43",
   "metadata": {},
   "source": [
    "He calls it like this:\n",
    "\n",
    "`torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_train -- --depth=20 --run=$WANDB_RUN`\n",
    "\n",
    "What does the --standalone flag do?\n",
    "\n",
    "ChatGPT seems to give a good answer. Short seems to be use it for single node multi GPU and you can save setting up a lot of other stuff.\n",
    "\n",
    "Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb53c14-da32-40ba-bf0e-10adcb155aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 14:49:45.765000 96358 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[W1114 14:49:45.211740000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:46.814254000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:47.601272000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:48.739299000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:50.640563000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:53.014873000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:49:57.436222000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:03.184587000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:13.861398000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "[W1114 14:50:22.444711000 socket.cpp:767] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 49218) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).\n",
      "^C\n",
      "[W1114 14:50:27.242578000 TCPStore.cpp:347] [c10d] TCP client failed to connect/validate to host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa:49218 - retrying (try=0, timeout=300000ms, delay=19962ms): Interrupted system call\n",
      "Exception raised from delay at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 56 (0x100bc7d7c in libc10.dylib)\n",
      "frame #1: c10d::detail::(anonymous namespace)::SocketConnectOp::tryConnect(int) + 4044 (0x1172e6588 in libtorch_cpu.dylib)\n",
      "frame #2: c10d::detail::Socket::connect(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, c10d::detail::SocketOptions const&) + 260 (0x1172e4f94 in libtorch_cpu.dylib)\n",
      "frame #3: c10d::detail::TCPClient::connect(c10d::detail::SocketAddress const&, c10d::TCPStoreOptions const&, std::__1::shared_ptr<c10d::Backoff>) + 184 (0x1172729bc in libtorch_cpu.dylib)\n",
      "frame #4: c10d::TCPStore::TCPStore(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, c10d::TCPStoreOptions const&) + 980 (0x117273164 in libtorch_cpu.dylib)\n",
      "frame #5: c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>::make<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, c10d::TCPStoreOptions&) + 168 (0x104af80e0 in libtorch_python.dylib)\n",
      "frame #6: std::__1::enable_if<std::is_void<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>>::value, pybind11::detail::void_type>::type pybind11::detail::argument_loader<pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool>::call<void, pybind11::detail::void_type, void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&>(void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool)&) && + 176 (0x104af7c54 in libtorch_python.dylib)\n",
      "frame #7: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::factory<torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::$_50, pybind11::detail::void_type (*)(), c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::detail::void_type ()>::execute<pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24]) &&::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v, char [24]>(pybind11::class_<c10d::TCPStore, c10::intrusive_ptr<c10d::TCPStore, c10::detail::intrusive_target_default_null_type<c10d::TCPStore>>>&&, void (*)(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, unsigned short, std::__1::optional<int>, bool, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l>>, bool, bool, std::__1::optional<int>, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&, char const (&) [24])::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 92 (0x104af7b2c in libtorch_python.dylib)\n",
      "frame #8: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 4344 (0x103ee7a9c in libtorch_python.dylib)\n",
      "<omitting python frames>\n",
      "frame #15: pybind11_meta_call + 40 (0x103ee3620 in libtorch_python.dylib)\n",
      "frame #60: start + 6076 (0x18d40ab98 in dyld)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646b244-aa0d-495a-9e71-2e8d24a991de",
   "metadata": {},
   "source": [
    "Doesn't work, but no point in figuring that out, instead try on our single GPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c84f2-e926-4684-a1b4-4bb6a7882d68",
   "metadata": {},
   "source": [
    "### trying on single GPU machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bb95b3-57a0-4d61-b83d-f8227103d6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 16, in <module>\n",
      "    from my_nanochat.my_tokenizer import get_tokenizer, get_token_bytes\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_tokenizer.py\", line 1, in <module>\n",
      "    import rust_tokenizer;\n",
      "ModuleNotFoundError: No module named 'rust_tokenizer'\n",
      "[W1114 19:57:36.434514993 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "E1114 19:57:36.615000 1773 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 1788) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_19:57:36\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1788)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464dd266-f676-4363-a5a5-9b36ea1735f4",
   "metadata": {},
   "source": [
    "The error about no module 'rust_tokenizer' reminds me that I should move that code out of challenge 7. Don't understand why I'm getting that error now but maybe because I did `uv sync` to get wandb here? Do this:\n",
    "\n",
    "```\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3b312f-ea1e-4c95-8cde-f20b0d99740b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:11:34.693000 2585 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 735.62ms | tok/sec: 174 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 68.68ms | tok/sec: 1,863 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 68.09ms | tok/sec: 1,879 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 49.67ms | tok/sec: 2,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 41.51ms | tok/sec: 3,083 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 41.47ms | tok/sec: 3,086 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 41.37ms | tok/sec: 3,093 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 41.43ms | tok/sec: 3,089 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 41.24ms | tok/sec: 3,103 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:11:41.760899202 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb6df1-5d0e-4f98-91c6-4de3333c7b0c",
   "metadata": {},
   "source": [
    "ok, seems good. What happens if tell it to use 2 GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd7e4ff-8411-40e4-899e-9130cffb10d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 20:14:10.344000 2736 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 20:14:20.700000 2755 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 20:14:20.700000 2756 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5782 | lrm: 1.00 | dt: 683.57ms | tok/sec: 187 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.070559 | grad norm: 2.1845 | lrm: 1.00 | dt: 80.73ms | tok/sec: 1,585 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.986515 | grad norm: 3.0479 | lrm: 1.00 | dt: 80.69ms | tok/sec: 1,586 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.843448 | grad norm: 4.7966 | lrm: 1.00 | dt: 78.92ms | tok/sec: 1,621 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.691146 | grad norm: 4.5346 | lrm: 1.00 | dt: 80.23ms | tok/sec: 1,595 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.563588 | grad norm: 4.1584 | lrm: 1.00 | dt: 80.46ms | tok/sec: 1,590 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.110770 | grad norm: 5.6380 | lrm: 1.00 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.823182 | grad norm: 5.0905 | lrm: 1.00 | dt: 80.41ms | tok/sec: 1,591 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.557765 | grad norm: 3.6195 | lrm: 1.00 | dt: 78.17ms | tok/sec: 1,637 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.241593 | grad norm: 3.5451 | lrm: 0.50 | dt: 80.11ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.0731\n",
      "<bos>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 230, in <module>\n",
      "    save_checkpoint(\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_checkpoint_manager.py\", line 11, in save_checkpoint\n",
      "    assert int(os.environ.get('RANK', 0)) == 0\n",
      "AssertionError\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.0731\n",
      "[W1114 20:14:27.771280111 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 20:14:28.004000 2736 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2755 closing signal SIGTERM\n",
      "E1114 20:14:28.118000 2736 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 2756) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_20:14:28\n",
      "  host      : psdqm40xt8b3\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2756)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c5436-d9d0-4364-a7ba-3f13efd5941a",
   "metadata": {},
   "source": [
    "^ failed, as expected, not sure if in the expected way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bacd8-b3ed-4061-b202-041bf4ffe940",
   "metadata": {},
   "source": [
    "### Trying with 2 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c70cee-5fda-4beb-a570-b7a998e67669",
   "metadata": {},
   "source": [
    "- Create a new 2 (low-powered) GPU machine in paperspace\n",
    "\n",
    "- Follow the instructions in `challenge-14-baby-pretrain-on-gpu/getting-ready.ipynb` to set it up.\n",
    "\n",
    "Chose 2xRTX4000 (single GPU machine was also RTX4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0394f0-7754-47fd-bc92-77a4f6a682dc",
   "metadata": {},
   "source": [
    "I followed those instructions and now I'm on the new machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96afe05e-e119-426d-82cf-9d7f32a263ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 14 20:55:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                Off |   00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   31C    P8              2W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Quadro RTX 4000                Off |   00000000:00:06.0 Off |                  N/A |\n",
      "| 30%   33C    P8              6W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16747893-0d9e-4ce4-891e-2cdf910d5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b173c-5512-4ca3-9908-59405dca272a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "buffers filled: 73\n",
      "buffers filled: 74\n",
      "buffers filled: 75\n",
      "buffers filled: 76\n",
      "buffers filled: 77\n",
      "buffers filled: 78\n",
      "buffers filled: 79\n",
      "buffers filled: 80\n",
      "buffers filled: 81\n",
      "buffers filled: 82\n",
      "buffers filled: 83\n",
      "buffers filled: 84\n",
      "buffers filled: 85\n",
      "buffers filled: 86\n",
      "buffers filled: 87\n",
      "buffers filled: 88\n",
      "buffers filled: 89\n",
      "buffers filled: 90\n",
      "buffers filled: 91\n",
      "buffers filled: 92\n",
      "buffers filled: 93\n",
      "buffers filled: 94\n",
      "buffers filled: 95\n",
      "buffers filled: 96\n",
      "buffers filled: 97\n",
      "buffers filled: 98\n",
      "buffers filled: 99\n",
      "buffers filled: 100\n",
      "buffers filled: 101\n",
      "buffers filled: 102\n",
      "buffers filled: 103\n",
      "buffers filled: 104\n",
      "buffers filled: 105\n",
      "buffers filled: 106\n",
      "buffers filled: 107\n",
      "buffers filled: 108\n",
      "buffers filled: 109\n",
      "buffers filled: 110\n",
      "buffers filled: 111\n",
      "buffers filled: 112\n",
      "buffers filled: 113\n",
      "buffers filled: 114\n",
      "buffers filled: 115\n",
      "buffers filled: 116\n",
      "buffers filled: 117\n",
      "buffers filled: 118\n",
      "buffers filled: 119\n",
      "buffers filled: 120\n",
      "buffers filled: 121\n",
      "buffers filled: 122\n",
      "buffers filled: 123\n",
      "buffers filled: 124\n",
      "buffers filled: 125\n",
      "buffers filled: 126\n",
      "buffers filled: 127\n",
      "buffers filled: 128\n",
      "buffers filled: 129\n",
      "buffers filled: 130\n",
      "buffers filled: 131\n",
      "buffers filled: 132\n",
      "buffers filled: 133\n",
      "buffers filled: 134\n",
      "buffers filled: 135\n",
      "buffers filled: 136\n",
      "buffers filled: 137\n",
      "buffers filled: 138\n",
      "buffers filled: 139\n",
      "buffers filled: 140\n",
      "buffers filled: 141\n",
      "buffers filled: 142\n",
      "buffers filled: 143\n",
      "buffers filled: 144\n",
      "buffers filled: 145\n",
      "buffers filled: 146\n",
      "buffers filled: 147\n",
      "buffers filled: 148\n",
      "buffers filled: 149\n",
      "buffers filled: 150\n",
      "buffers filled: 151\n",
      "buffers filled: 152\n",
      "buffers filled: 153\n",
      "buffers filled: 154\n",
      "buffers filled: 155\n",
      "buffers filled: 156\n",
      "buffers filled: 157\n",
      "buffers filled: 158\n",
      "buffers filled: 159\n",
      "buffers filled: 160\n",
      "buffers filled: 161\n",
      "buffers filled: 162\n",
      "buffers filled: 163\n",
      "buffers filled: 164\n",
      "buffers filled: 165\n",
      "buffers filled: 166\n",
      "buffers filled: 167\n",
      "buffers filled: 168\n",
      "buffers filled: 169\n",
      "buffers filled: 170\n",
      "buffers filled: 171\n",
      "buffers filled: 172\n",
      "buffers filled: 173\n",
      "buffers filled: 174\n",
      "buffers filled: 175\n",
      "buffers filled: 176\n",
      "buffers filled: 177\n",
      "buffers filled: 178\n",
      "buffers filled: 179\n",
      "buffers filled: 180\n",
      "buffers filled: 181\n",
      "buffers filled: 182\n",
      "buffers filled: 183\n",
      "buffers filled: 184\n",
      "buffers filled: 185\n",
      "buffers filled: 186\n",
      "buffers filled: 187\n",
      "buffers filled: 188\n",
      "buffers filled: 189\n",
      "buffers filled: 190\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /home/paperspace/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adb0575-c420-4f5a-810c-c35ffba2e1b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] \n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:36:14.831000 37098 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:36:32.228000 37125 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:36:32.245000 37126 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 869.65ms | tok/sec: 147 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 83.89ms | tok/sec: 1,525 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.988388 | grad norm: 3.0695 | lrm: 1.00 | dt: 74.97ms | tok/sec: 1,707 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.847008 | grad norm: 4.9443 | lrm: 1.00 | dt: 78.54ms | tok/sec: 1,629 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.685705 | grad norm: 4.6343 | lrm: 1.00 | dt: 76.45ms | tok/sec: 1,674 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.549452 | grad norm: 4.2740 | lrm: 1.00 | dt: 80.78ms | tok/sec: 1,584 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.094818 | grad norm: 5.6507 | lrm: 1.00 | dt: 78.69ms | tok/sec: 1,626 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.810596 | grad norm: 5.0067 | lrm: 1.00 | dt: 78.65ms | tok/sec: 1,627 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.552553 | grad norm: 3.7087 | lrm: 1.00 | dt: 78.84ms | tok/sec: 1,623 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.228461 | grad norm: 3.6521 | lrm: 0.50 | dt: 76.78ms | tok/sec: 1,667 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.7032\n",
      "<bos>The capital of France is a less, and the time, and the time\n",
      "<bos>The chemical symbol of gold is a less, and the time, and the time\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less, and the time, and the time\n",
      "<bos>The opposite of hot is a less, and the time, and the time\n",
      "<bos>The planets of the solar system are: 20, and the time, and the time\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 230, in <module>\n",
      "    save_checkpoint(\n",
      "  File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_checkpoint_manager.py\", line 11, in save_checkpoint\n",
      "    assert int(os.environ.get('RANK', 0)) == 0\n",
      "AssertionError\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.7032\n",
      "[W1114 21:36:41.669889329 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 21:36:41.517000 37098 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 37125 closing signal SIGTERM\n",
      "E1114 21:36:41.632000 37098 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 37126) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_21:36:41\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 37126)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca745c-e068-4ee8-9c80-beca2eb71573",
   "metadata": {},
   "source": [
    "hit that assert I put in because left out code for saving optimizers with requires something special with ranks...go add\n",
    "\n",
    "but that also means it got all the way to saving...I guess that's good, but how do I know it was doing the righ thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8de80e-d738-493d-96d3-577b64376bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] \n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:48:04.863000 37714 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:48:17.145000 37741 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:48:17.170000 37740 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00010 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 810.14ms | tok/sec: 157 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 77.20ms | tok/sec: 1,658 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.988388 | grad norm: 3.0695 | lrm: 1.00 | dt: 80.15ms | tok/sec: 1,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.847008 | grad norm: 4.9443 | lrm: 1.00 | dt: 81.19ms | tok/sec: 1,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.685705 | grad norm: 4.6343 | lrm: 1.00 | dt: 76.45ms | tok/sec: 1,674 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.549452 | grad norm: 4.2740 | lrm: 1.00 | dt: 78.95ms | tok/sec: 1,621 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.094818 | grad norm: 5.6507 | lrm: 1.00 | dt: 78.22ms | tok/sec: 1,636 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.810596 | grad norm: 5.0067 | lrm: 1.00 | dt: 78.99ms | tok/sec: 1,620 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.552553 | grad norm: 3.7087 | lrm: 1.00 | dt: 81.19ms | tok/sec: 1,576 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.228461 | grad norm: 3.6521 | lrm: 0.50 | dt: 78.29ms | tok/sec: 1,634 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.7032\n",
      "<bos>The capital of France is a less, and the time, and the time\n",
      "<bos>The chemical symbol of gold is a less, and the time, and the time\n",
      "<bos>If yesterday was Friday, then tomorrow will be a less, and the time, and the time\n",
      "<bos>The opposite of hot is a less, and the time, and the time\n",
      "<bos>The planets of the solar system are: 20, and the time, and the time\n",
      "<bos>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<bos>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000010_rank0.pt\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.7032\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000010_rank0.pt\n",
      "[W1114 21:48:27.702025232 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1114 21:48:27.961290282 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263bdb5-f1ee-4b3c-9a99-04d85a23fbac",
   "metadata": {},
   "source": [
    "Added another print statement that prints in all ranks (not only master process). Want to make sure see both ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee71b9f-057f-4f0a-a8f7-b82d0211d81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] \n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] *****************************************\n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 21:53:47.112000 38130 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 256\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "W1114 21:53:59.081000 38156 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1114 21:53:59.114000 38157 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.1707\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00002 (0.00%) | loss: 11.090370 | grad norm: 1.5892 | lrm: 1.00 | dt: 791.44ms | tok/sec: 161 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00002 (50.00%) | loss: 11.073426 | grad norm: 2.1532 | lrm: 1.00 | dt: 61.59ms | tok/sec: 2,078 | mfu: -1.00 | total time: 0.00m\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00002 | Validation bpb: 3.1460\n",
      "<bos>The capital of France is an important consideration when planning UNCTAD, Air\n",
      "<bos>The chemical symbol of gold is an important consideration when planning UNCTAD, Air\n",
      "<bos>If yesterday was Friday, then tomorrow will be evaluated when planning UNCTAD, Air, Air\n",
      "<bos>The opposite of hot is an important consideration when planning UNCTAD, Air\n",
      "<bos>The planets of the solar system are: when planning UNCTAD, Air, Air,\n",
      "<bos>My favorite color is an important consideration when planning UNCTAD, Air\n",
      "<bos>If 5*x + 3 = 13, then x is an important consideration when planning UNCTAD, Air\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000002.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000002.json\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000002.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000002.json\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000002_rank0.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000002_rank0.pt\n",
      "Peak memory usage: 486.90MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.1460\n",
      "[W1114 21:54:08.709303464 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1114 21:54:08.768293558 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0a63a-9349-46f8-a532-5d9d1c98aed2",
   "metadata": {},
   "source": [
    "Seeing `This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1`, so must be missing something.\n",
    "\n",
    "Oh yeah, forgot to update this in `my_common.py`:\n",
    "```\n",
    "# return ddp, ddp_rank, ddp_local_rank, ddp_world_size\n",
    "def get_dist_info():\n",
    "    # for now\n",
    "    return False, 0, 0, 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0ae0de-016d-4dc6-bd59-669a6bc6595d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] \n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:05:18.973000 38726 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 134, in <module>\n",
      "[rank1]:     optimizers = model.setup_optimizers(\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_gpt.py\", line 201, in setup_optimizers\n",
      "[rank1]:     adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
      "[rank1]: TypeError: 'NoneType' object is not callable\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 134, in <module>\n",
      "[rank0]:     optimizers = model.setup_optimizers(\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/my_gpt.py\", line 201, in setup_optimizers\n",
      "[rank0]:     adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
      "[rank0]: TypeError: 'NoneType' object is not callable\n",
      "[rank0]:[W1114 22:05:26.441608257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank1]:[W1114 22:05:28.620819630 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1114 22:05:28.681378349 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "E1114 22:05:28.514000 38726 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 38752) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-11-14_22:05:28\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 38753)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:05:28\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 38752)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aef819-19e3-4f5f-bad8-bb3e3665741c",
   "metadata": {},
   "source": [
    "ok, now failing due to `DistAdamW = None # for now so it will fail until I \"copy\" adamw.py`\n",
    "\n",
    "time to look at `adamw.py`\n",
    "\n",
    "I'm going to copy and paste it and go back and look later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9f9a19-a656-43ab-84d8-c58ef262df32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] \n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:30:46.791000 39515 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Muon: Grouping 16 params of shape torch.Size([256, 256]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([256, 1024]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([1024, 256]), device cuda:0, dtype torch.float32\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "[rank0]:W1114 22:30:59.236000 39542 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "[rank1]:W1114 22:30:59.264000 39543 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 2.9727\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank0]:     opt.step()\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank0]:     out = func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 33, in step\n",
      "[rank0]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank0]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank0]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank1]:     opt.step()\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank1]:     out = func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 33, in step\n",
      "[rank1]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank1]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank1]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]:[W1114 22:31:04.986828210 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W1114 22:31:05.273053922 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank1]:[W1114 22:31:06.486275983 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 22:31:06.149000 39515 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 39543 closing signal SIGTERM\n",
      "E1114 22:31:06.264000 39515 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 39542) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:31:06\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 39542)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e1880-d8fc-4d7f-8027-b09542cb5a28",
   "metadata": {},
   "source": [
    "Add temp debug printing in `adamw.py` to see shapes of output and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15fe8502-a86e-447d-8653-f3c737f99b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] \n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] *****************************************\n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1114 22:41:54.509000 40101 torch/distributed/run.py:803] *****************************************\n",
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Muon: Grouping 16 params of shape torch.Size([256, 256]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([256, 1024]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([1024, 256]), device cuda:0, dtype torch.float32\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "[rank1]:W1114 22:42:06.550000 40128 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "[rank0]:W1114 22:42:06.664000 40127 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 2.9727\n",
      "Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])\n",
      "Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank1]:     opt.step()\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank1]:     out = func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 34, in step\n",
      "[rank1]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank1]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank1]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/scripts/my_base_train.py\", line 274, in <module>\n",
      "[rank0]:     opt.step()\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "[rank0]:     out = func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 68, in inner\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/my_nanochat/my_nanochat/adamw.py\", line 34, in step\n",
      "[rank0]:     reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4513, in reduce_scatter_tensor\n",
      "[rank0]:     work = group._reduce_scatter_base(output, input, opts)\n",
      "[rank0]: ValueError: input tensor must be the same size as output size times world size\n",
      "[rank0]:[W1114 22:42:11.226552490 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W1114 22:42:12.228890129 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1114 22:42:13.106000 40101 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 40128 closing signal SIGTERM\n",
      "E1114 22:42:13.221000 40101 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 40127) of binary: /home/paperspace/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-14_22:42:13\n",
      "  host      : ps2mtwj7p0ep\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 40127)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa066a9-40b1-470c-a74a-7a02cbdb4450",
   "metadata": {},
   "source": [
    "`Just before calling reduce_scatter_tensor, grad_slice (output) shape is torch.Size([32768, 256]) and grad (input) shape is torch.Size([65537, 256])` and `reduce_scatter_tensor()` doc says: \"input (Tensor): Input tensor to be reduced and scattered. Its size should be output tensor size times the world size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7f3a279-39e4-4bcb-8d74-bbfc0b60d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_tokenizer import get_tokenizer\n",
    "get_tokenizer().get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661165bb-fc17-40d3-b652-4008a7865166",
   "metadata": {},
   "source": [
    "Prob do need to understand adamw but just based on those numbers, does it somehow split up the lm_head params and if vocab_size % world_size is not 0 it's a prob? Is there something in the actual tokenizer training that forces vocab_size to be a multiple of 8 or something like that? Don't immediately see anything like that. I have 65536 + BOS = 65537. The real one has 9 special tokens including BOS so that's still going to be odd number. Hmm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e85b8-d009-4d04-bd43-1704214e18d6",
   "metadata": {},
   "source": [
    "Let me google and chatgpt \"ValueError: input tensor must be the same size as output size times world size\"\n",
    "\n",
    "Not immediately seeing an answer, but it looks like understanding scatter and related is important to understand DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9737c-440e-48ae-8c40-9ae41f8e7715",
   "metadata": {},
   "source": [
    "Think I should take a step back and think about the general idea.\n",
    "\n",
    "I believe each GPU has its own copy of the model, but each copy needs to have the same weights for all parameters. During training, when we do a forward pass, we divide our data among the GPUs. For example, if each GPU can handle a batch of 10 sequences, we have GPU #1 forward 10 sequences and GPU #2 forward another 10 sequences. We calculate loss, which will be different for the two batches because we have different data, and we calculate gradients for all the weights, which will also be different.\n",
    "\n",
    "So far each GPU is operating idependently. But now we can't update weights independently. If we do that we'll end up with different weights on each GPU and we're not getting the benefit of combined training on a larger amount of data, it would just be like training two different models.\n",
    "\n",
    "Let's forget about Adam for a second and assume our optimizer is just going to do weight - LR * gradient. If we have some type of sync point to wait until both GPUs calculate gradients, we can then take the average of the two gradients and adjust all weights by it. Like weight - LR/2 * (gradient_from_gpu_1 + gradient_from_gpu_2). BUT a) where do we sum/average the two gradients and b) do we then subtract in each GPU or, to be \"safe\", do we calculate the new weights on one GPU and transmit them to to the other.\n",
    "\n",
    "I don't see how we could sum the two gradients other than by first copying say the gradient from GPU 2 to GPU 1. GPU 1 could then sum, subtract, and send the new weights back to GPU 2. I imagine the torch dist has very efficient ways to move memory from one GPU, perhaps if both GPUs are on the same node by going through RAM, or even more sophisticated stuff, but it will still be much slower than directly working with GPU memory.\n",
    "\n",
    "I guess another strategy would be to place some layers of the network on one GPU and some on the other. Then it wouldn't be necessary to move all the gradients around, but interim tensors during the forward would need to moved from one to the other, and something similar in reverse during back prop. But nothing in the code I've worked on so far makes me think this could be happening.\n",
    "\n",
    "So back to moving the gradients and weights at the end of each step...if true, it starts to make sense that responsibility falls on the optimizers. At least for weight - LR * grad, without Adam or anything fancy, what reason could there be to deal in gradients with a dimension of original_dimension / world_size? I doubt this is it, but a way to share calculations would be say for GPU 1 to send half of the gradient to GPU 2, GPU 2 adds it to the gradient it has, updates weights, and then sends back, and the same thing happens in reverse for the other half of the gradient. Believe in that situation the total memory transmitted is the same but the calculations get distributed evenly.\n",
    "\n",
    "Ooh, what if we have more than two GPUs? Maybe it really does start to make sense for each GPU to \"own\" a portion of the gradient. But doesn't it also get very expensive to send so much information between so many GPUs?\n",
    "\n",
    "Maybe I should guess what \"reduce and scatter\" means and then look it up. I know it takes something like this...\n",
    "\n",
    "reduce_scatter_tensor(output, input-of-size-world-size-times-output-size, op=reduce-op-avg)\n",
    "\n",
    "so like\n",
    "\n",
    "`reduce_scatter_tensor(output=[ , ], input=[1,2,3,4], op=reduce-op-avg)`\n",
    "\n",
    "reduce could mean take the average of each world size group, like in this case 1,2 and 3,4, but what does that have to do with scatter and how is it distributed? OR could it mean this? Say it's called on two GPUs as follows:\n",
    "\n",
    "```\n",
    "GPU 1: reduce_scatter_tensor(output=[ , ], input=[1,2,3,4], op=reduce-op-avg)\n",
    "\n",
    "GPU 2: reduce_scatter_tensor(output=[ , ], input=[5,6,7,8], op=reduce-op-avg)\n",
    "\n",
    "The output on GPU 1 gets [(5+1)/2, (2+6)/2] and the output on GPU 2 gets [(3+7)/2, (4+8)/2]\n",
    "```\n",
    "\n",
    "In other words, the output on GPU 1 is determined by the first half of the inputs across both GPUs by taking the average. And the output on GPU 2 is determined by the second half of the inputs across both GPUs.\n",
    "\n",
    "This is very similar to what I was imagining above. Following the completion of something like that, each GPU will contain 1/world_size worth of the overall gradient.\n",
    "\n",
    "Feels right, but let me see what the doc says. Yes, I think my understanding is right. There is a similar example in the [doc](https://docs.pytorch.org/docs/stable/distributed.html)\n",
    "\n",
    "ok, to keep thinking about how this could work in a tiny example...\n",
    "\n",
    "```\n",
    "at start of step weights_on_gpu_0 and weights_on_gpu_1 are both say [10,20,30,40]\n",
    "\n",
    "after back prop, say gradient_on_gpu_1 is [4,3,1,2] and gradient_on_gpu_2 is [6,9,5,6]\n",
    "\n",
    "grad = [0,0]\n",
    "gpu 0: reduce_scatter_tensor(grad_on_gpu_0, [4,3,1,2], avg)\n",
    "gpu 1: reduce_scatter_tensor(grad_on_gpu_1, [6,9,5,6], avg)\n",
    "\n",
    "now grad_on_gpu_0 is [5, 6]\n",
    "and grad_on_gpu_1 is [3, 4]\n",
    "\n",
    "say LR = 1\n",
    "on both gpus do weights[rank:rank+world_size] = weights[rank:rank+world_size] - grad * LR\n",
    "\n",
    "now weights_on_gpu_0 is [5, 14, 30, 40]\n",
    "and weights_on_gpu_1 is [10, 20, 27, 36]\n",
    "the first two weight on GPU 0 are correct for the world, and the second two weights on GPU 1 are correct for the world\n",
    "\n",
    "(beginning to see why it's called world)\n",
    "\n",
    "now we need some way to \"scatter\" the correct parts out...say a function like:\n",
    "\n",
    "my_scatter(tensor, slice): copy this slice of this tensor to the same slice of the tensors on the other GPUs\n",
    "\n",
    "then both GPUs could do: my_scatter(weights, rank:rank+world_size)\n",
    "gpu 0: my_scatter([5, 14, 30, 40], (0:2))\n",
    "gpu 1: my_scatter([10, 20, 27, 36], (2:4))\n",
    "\n",
    "now weights_on_gpu_0 is [5, 14, 27, 36] and weights_on_gpu_1 is [5, 14, 27, 36]\n",
    "\n",
    "we're done and ready for the next step\n",
    "\n",
    "Let's see if torch.distributed has a function like that.\n",
    "\n",
    "```\n",
    "\n",
    "(There's ton's of interesting stuff in the [doc](https://docs.pytorch.org/docs/stable/distributed.html) including around the different ways communication between GPUs can happen. Come back to all that later.)\n",
    "\n",
    "There is a scatter function but it doesn't work like I imagined. It lets one rank send out a list of tensors, and one rank will receive the first, the next the second, etc. So like gpu 0 could do scatter(output, [[1,2],[3,4]) and output on gpu 0 will be [1,2] and output on gpu 1 will be [3,4]. Anyway, there are lots of functions in torch.distributed and lots of ways to get the weights distributed around.\n",
    "\n",
    "Look at adamw now.\n",
    "\n",
    "Just thinking...we need to move the gradients around, and we need to either move their average or updated weights around, but we don't need to move the moving averages around. Each GPU can be responsible for maintaining m and v for just their portion of the gradient. (Or one GPU could be responsible for the whole thing, but either way, we never need to move around M and V or keep multiple copies of it.)\n",
    "\n",
    "Looks like `dist.all_gather_into_tensor` is the function that distributes out the weights (parameters).\n",
    "\n",
    "Let's see what that does. Yes, this is similar to what I was imagining but the args work differently. Updating my example:\n",
    "\n",
    "```\n",
    "then both GPUs could do: all_gather_into_tensor(weights, weights[rank:rank+world_size])\n",
    "gpu 0: all_gather_into_tensor(weights, [5, 14])\n",
    "gpu 1: all_gather_into_tensor(weights, [27, 36])\n",
    "\n",
    "now now weights on both gpus are [5, 14, 27, 36]\n",
    "```\n",
    "\n",
    "ok, now back to the problem, this approach of using reduce_scatter_tensor and all_gather_into_tensor seems clean and elegant and removes a lot of bookkeeping, BUT how is it supposed to work when grad size is not divisible by world size?\n",
    "\n",
    "```\n",
    "Say our example weights above was not [10,20,30,40] but [10,20,30,40,50]\n",
    "\n",
    "And after back prop, say gradient_on_gpu_1 is [4,3,1,2,8] and gradient_on_gpu_2 is [6,9,5,6,2]\n",
    "\n",
    "We could pad the gradients to get to the next multiple of world_size, so:\n",
    "gradient_on_gpu_1 becomes [4,3,1,2,8,0]\n",
    "gradient_on_gpu_2 becomes [6,9,5,6,2,0]\n",
    "\n",
    "then proceed as before\n",
    "\n",
    "grad = [0,0,0]\n",
    "gpu 0: reduce_scatter_tensor(grad_on_gpu_0, [4,3,1,2,8,0], avg)\n",
    "gpu 1: reduce_scatter_tensor(grad_on_gpu_1, [6,9,5,6,2,0], avg)\n",
    "\n",
    "now grad_on_gpu_0 is [5, 6, 3]\n",
    "and grad_on_gpu_1 is [4, 5, 0]\n",
    "\n",
    "then on the highest rank, we slice off the padding, etc.\n",
    "```\n",
    "\n",
    "However, I don't see anything in adamw.py about padding.\n",
    "\n",
    "Although there is padding in DistMuon in muon.py.\n",
    "\n",
    "How could this be working? If he had 8 special tokens it would work by luck...(65536 + 8) % 8 == 0, but he has 9.\n",
    "\n",
    "BTW, maybe the reason we init some weights to 0 is so they'll start the same on all GPUs? But why do we not do that for all weights? Come back to that.\n",
    "\n",
    "Interesting that his default vocab_size in GPTConfig 50304 is also divisible by 8.\n",
    "\n",
    "But still, in base_train.py he uses vocab_size in model_config and vocab_size is tokenizer.get_vocab_size()\n",
    "\n",
    "FOR NOW, to move on, I'm going to train my tokenizer with 65535 so I'll end up with 65536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b492c56e-5805-406c-b21e-835723a78563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "base_dir = get_base_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559306e4-0562-4c62-a502-115cee562bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_checkpoints     shard_00009.parquet  shard_00020.parquet\n",
      "my-tokenizer.pkl     shard_00010.parquet  shard_00021.parquet\n",
      "shard_00000.parquet  shard_00011.parquet  shard_00022.parquet\n",
      "shard_00001.parquet  shard_00012.parquet  shard_00023.parquet\n",
      "shard_00002.parquet  shard_00013.parquet  shard_00024.parquet\n",
      "shard_00003.parquet  shard_00014.parquet  shard_00025.parquet\n",
      "shard_00004.parquet  shard_00015.parquet  shard_00026.parquet\n",
      "shard_00005.parquet  shard_00016.parquet  shard_00027.parquet\n",
      "shard_00006.parquet  shard_00017.parquet  shard_00028.parquet\n",
      "shard_00007.parquet  shard_00018.parquet  shard_00029.parquet\n",
      "shard_00008.parquet  shard_00019.parquet  token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!ls {base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be6a606-9fd9-4d8e-9496-fccdd3f45119",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {base_dir}/my-tokenizer.pkl {base_dir}/my-tokenizer-65537.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310c2f03-3cda-4945-9bbd-4ca3dc0e9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {base_dir}/token_bytes.pt {base_dir}/token_bytes-65537.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b68267-b725-40ca-b719-50ca3a421338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_checkpoints\tshard_00009.parquet  shard_00020.parquet\n",
      "my-tokenizer-65537.pkl\tshard_00010.parquet  shard_00021.parquet\n",
      "shard_00000.parquet\tshard_00011.parquet  shard_00022.parquet\n",
      "shard_00001.parquet\tshard_00012.parquet  shard_00023.parquet\n",
      "shard_00002.parquet\tshard_00013.parquet  shard_00024.parquet\n",
      "shard_00003.parquet\tshard_00014.parquet  shard_00025.parquet\n",
      "shard_00004.parquet\tshard_00015.parquet  shard_00026.parquet\n",
      "shard_00005.parquet\tshard_00016.parquet  shard_00027.parquet\n",
      "shard_00006.parquet\tshard_00017.parquet  shard_00028.parquet\n",
      "shard_00007.parquet\tshard_00018.parquet  shard_00029.parquet\n",
      "shard_00008.parquet\tshard_00019.parquet  token_bytes-65537.pt\n"
     ]
    }
   ],
   "source": [
    "!ls {base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c67f4f5-dbb3-4608-be21-d46d984037aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2a23d0-3c0a-47b7-b7c9-500ba582a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,535\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train --vocab_size=65535"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a809d83-b545-4c95-8bfb-9a2704d221d6",
   "metadata": {},
   "source": [
    "Ahh!!! Just as started realized maybe he subtracts special tokens from desired vocab size. That makes so much more sense so you get the vocab size you ask for and don't end up with a weird dimension for the lm head. And yes, he does that in tokenizer.py and I never copied that part. Let me go and fix that and then train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221e0e-0cdd-49d3-8b9f-44cef71eb1bf",
   "metadata": {},
   "source": [
    "ok, fixed, do a quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24d011a-8636-49c5-a7bb-7042b44f60e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 100,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 500\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /home/paperspace/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train --max_chars=100000 --vocab_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623bfac8-398c-4d5f-beb0-5dc2e667c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_tokenizer import get_tokenizer\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b19285-43dd-4b4e-953a-191e7c67615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358b8941-f0a7-4981-83bd-187a25055dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_bos_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deade3a5-6707-4c55-904c-11c952e2b75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|>A'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([499, 65])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec96c98-a7ee-4c6c-8e9b-4654316c05de",
   "metadata": {},
   "source": [
    "Now train the tokenizer for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0074703c-bd83-4d62-b2e7-ddd2f3c71ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "buffers filled: 73\n",
      "buffers filled: 74\n",
      "buffers filled: 75\n",
      "buffers filled: 76\n",
      "buffers filled: 77\n",
      "buffers filled: 78\n",
      "buffers filled: 79\n",
      "buffers filled: 80\n",
      "buffers filled: 81\n",
      "buffers filled: 82\n",
      "buffers filled: 83\n",
      "buffers filled: 84\n",
      "buffers filled: 85\n",
      "buffers filled: 86\n",
      "buffers filled: 87\n",
      "buffers filled: 88\n",
      "buffers filled: 89\n",
      "buffers filled: 90\n",
      "buffers filled: 91\n",
      "buffers filled: 92\n",
      "buffers filled: 93\n",
      "buffers filled: 94\n",
      "buffers filled: 95\n",
      "buffers filled: 96\n",
      "buffers filled: 97\n",
      "buffers filled: 98\n",
      "buffers filled: 99\n",
      "buffers filled: 100\n",
      "buffers filled: 101\n",
      "buffers filled: 102\n",
      "buffers filled: 103\n",
      "buffers filled: 104\n",
      "buffers filled: 105\n",
      "buffers filled: 106\n",
      "buffers filled: 107\n",
      "buffers filled: 108\n",
      "buffers filled: 109\n",
      "buffers filled: 110\n",
      "buffers filled: 111\n",
      "buffers filled: 112\n",
      "buffers filled: 113\n",
      "buffers filled: 114\n",
      "buffers filled: 115\n",
      "buffers filled: 116\n",
      "buffers filled: 117\n",
      "buffers filled: 118\n",
      "buffers filled: 119\n",
      "buffers filled: 120\n",
      "buffers filled: 121\n",
      "buffers filled: 122\n",
      "buffers filled: 123\n",
      "buffers filled: 124\n",
      "buffers filled: 125\n",
      "buffers filled: 126\n",
      "buffers filled: 127\n",
      "buffers filled: 128\n",
      "buffers filled: 129\n",
      "buffers filled: 130\n",
      "buffers filled: 131\n",
      "buffers filled: 132\n",
      "buffers filled: 133\n",
      "buffers filled: 134\n",
      "buffers filled: 135\n",
      "buffers filled: 136\n",
      "buffers filled: 137\n",
      "buffers filled: 138\n",
      "buffers filled: 139\n",
      "buffers filled: 140\n",
      "buffers filled: 141\n",
      "buffers filled: 142\n",
      "buffers filled: 143\n",
      "buffers filled: 144\n",
      "buffers filled: 145\n",
      "buffers filled: 146\n",
      "buffers filled: 147\n",
      "buffers filled: 148\n",
      "buffers filled: 149\n",
      "buffers filled: 150\n",
      "buffers filled: 151\n",
      "buffers filled: 152\n",
      "buffers filled: 153\n",
      "buffers filled: 154\n",
      "buffers filled: 155\n",
      "buffers filled: 156\n",
      "buffers filled: 157\n",
      "buffers filled: 158\n",
      "buffers filled: 159\n",
      "buffers filled: 160\n",
      "buffers filled: 161\n",
      "buffers filled: 162\n",
      "buffers filled: 163\n",
      "buffers filled: 164\n",
      "buffers filled: 165\n",
      "buffers filled: 166\n",
      "buffers filled: 167\n",
      "buffers filled: 168\n",
      "buffers filled: 169\n",
      "buffers filled: 170\n",
      "buffers filled: 171\n",
      "buffers filled: 172\n",
      "buffers filled: 173\n",
      "buffers filled: 174\n",
      "buffers filled: 175\n",
      "buffers filled: 176\n",
      "buffers filled: 177\n",
      "buffers filled: 178\n",
      "buffers filled: 179\n",
      "buffers filled: 180\n",
      "buffers filled: 181\n",
      "buffers filled: 182\n",
      "buffers filled: 183\n",
      "buffers filled: 184\n",
      "buffers filled: 185\n",
      "buffers filled: 186\n",
      "buffers filled: 187\n",
      "buffers filled: 188\n",
      "buffers filled: 189\n",
      "buffers filled: 190\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /home/paperspace/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c912c79-efba-4590-b2b4-79804bdaa578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dc700-a53a-45b9-a31d-4610d2d2c116",
   "metadata": {},
   "source": [
    "ok, now back to trying to train a model with 2 GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "025820fc-8f5a-4115-8fbc-fd90d4ca14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 15 13:40:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                Off |   00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   34C    P8              5W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Quadro RTX 4000                Off |   00000000:00:06.0 Off |                  N/A |\n",
      "| 30%   32C    P8              6W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7d9fd0-df00-4cb7-bd32-10abab9cf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1115 13:40:23.906000 81480 torch/distributed/run.py:803] \n",
      "W1115 13:40:23.906000 81480 torch/distributed/run.py:803] *****************************************\n",
      "W1115 13:40:23.906000 81480 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1115 13:40:23.906000 81480 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cudaoverriding depth = 4\n",
      "\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 2\n",
      "overriding total_batch_size = 256\n",
      "overriding eval_every = 100\n",
      "overriding eval_tokens = 1280\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 2, 'device_batch_size': 1, 'total_batch_size': 256, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 100, 'eval_tokens': 1280, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "This process is ddp_rank: 1, ddp_local_rank: 1, ddp_world_size: 2This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 2\n",
      "\n",
      "Vocab size: 65,536\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 256\n",
      "Total batch size 256 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65536, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65536, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,160\n",
      "Using user-provided number of iterations: 2\n",
      "Total number of training tokens: 512\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Muon: Grouping 16 params of shape torch.Size([256, 256]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([256, 1024]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 4 params of shape torch.Size([1024, 256]), device cuda:0, dtype torch.float32\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "[rank0]:W1115 13:40:37.667000 81506 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "[rank1]:W1115 13:40:37.698000 81507 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 2.9727\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00002 (0.00%) | loss: 11.090355 | grad norm: 1.5892 | lrm: 1.00 | dt: 877.17ms | tok/sec: 291 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00002 (50.00%) | loss: 11.071588 | grad norm: 2.2435 | lrm: 1.00 | dt: 66.70ms | tok/sec: 3,838 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002 | Validation bpb: 2.9280\n",
      "<|bos|>The capital of France is also used quite extensively for about 177.\n",
      "<|bos|>The chemical symbol of gold is also used quite extensively for about 177.\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be modified easily if someone with only one leg and weight\n",
      "<|bos|>The opposite of hot is also used quite extensively for about 177.\n",
      "<|bos|>The planets of the solar system are: when planning UNCTAD, these cases available on\n",
      "<|bos|>My favorite color is also used quite extensively for about 177.\n",
      "<|bos|>If 5*x + 3 = 13, then x is also used quite extensively for about 177.\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000002.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000002.json\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/optim_000002_rank0.pt\n",
      "Peak memory usage: 523.13MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.9280\n",
      "[W1115 13:40:45.769031944 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1115 13:40:45.797837882 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.my_base_train -- \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=2 \\\n",
    "    --total_batch_size=256 \\\n",
    "    --eval_every=100 \\\n",
    "    --eval_tokens=1280 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f1a23-3edb-4bdd-99e6-0636c680949e",
   "metadata": {},
   "source": [
    "it completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f9907-17de-4086-9fa9-85b441ca9b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44bcf09-a9ba-44f3-a99f-346372a3d927",
   "metadata": {},
   "source": [
    "Code added / updated as part of this challenge so far:\n",
    "\n",
    "- Added `my_tok_train.py`\n",
    " \n",
    "- Added / fixed code purposely left out earlier when ignoring DDP in `my_checkpoint_manager.py`, `my_common.py` and `my_gpt.py`\n",
    "\n",
    "- Directly copied the entire `adamw.py`\n",
    "\n",
    "- Fixed `my_tokenizer.py` so that we end up with the desired vocab size (not vocab size + number of special tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
