{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc40b0c2-b49e-40fd-b832-7e682653b538",
   "metadata": {},
   "source": [
    "While working on challenge 15 I realized I'm confused about what tensors need to be available and allocated for back prop. Do back prop by hand to build my intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c5eab-1c22-4554-ab4a-6f3e7bbcb364",
   "metadata": {},
   "source": [
    "Let me start with an example that uses scalars only and only one piece of data.\n",
    "\n",
    "```\n",
    "x = 10\n",
    "y = 20\n",
    "\n",
    "and my model is a function f parameterized by a single weight w: f(w) = w*x\n",
    "\n",
    "To keep in mind that x and y are fixed and we're interested in the derivative of the loss with respect to w (so we know how to adjust it), I'm going to write x and y as 10 and 20.\n",
    "\n",
    "f(w) = 10 * w             this is our model\n",
    "error(w) = 20 - f(w)          \n",
    "loss(w) = square(error(w)) \n",
    "\n",
    "Suppose w is 5\n",
    "\n",
    "I'll first calculate the derivative of loss with respect to w the way I learned in HS:\n",
    "\n",
    "loss(w) = (20 - 10w)^2\n",
    "loss(w) = 100w^2 - 400w + 400\n",
    "loss'(w) = 200w - 400\n",
    "\n",
    "loss'(5) = 600           this tells us increasing w will increase the loss\n",
    "\n",
    "Now I want to calculate the way I think back propagation works.\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "f(w) = 50\n",
    "error(w) = 20 - f(w) = -30\n",
    "loss(w) = square(error(w)) = square(-30) = 900\n",
    "\n",
    "Backward pass:\n",
    "\n",
    "loss'(w) = square'(error(w)) * error'(w) = 2(error(w)) * error'(w) = -60 *\n",
    "           error'(w) = -f'(w) =  -1 *\n",
    "           f'(w) = 10\n",
    "           = 600\n",
    "\n",
    "or maybe it's clearer written like this:\n",
    "\n",
    "loss'(w) = square'(error(w)) * error'(w)\n",
    "         = 2(error(w)) * (-f'(w))\n",
    "         = 2(-30) * (-10)\n",
    "         = -60 * -10\n",
    "         = 600\n",
    "\n",
    "calc rules that come into play here:\n",
    "d/dx f(g(x)) = f'(g(x)) * g'(x)\n",
    "d/dx x^2 = 2x\n",
    "d/dx c + f(x) = f'(x)\n",
    "d/dx cx = c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e04435-dfc8-479b-87e1-4ad80068b0fd",
   "metadata": {},
   "source": [
    "Now do it with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0585a00e-ce04-47a0-aba4-6e8da2334915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5d9f40-e7bb-4c9b-bb7d-7a04dc66c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(10, dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "output = x * w\n",
    "error = y - output\n",
    "loss = error ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1ac4b7-2a28-497d-b846-f5fd38a5d316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10.),\n",
       " tensor(20.),\n",
       " tensor(5., requires_grad=True),\n",
       " tensor(50., grad_fn=<MulBackward0>),\n",
       " tensor(-30., grad_fn=<SubBackward0>),\n",
       " tensor(900., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, w, output, error, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251353ff-793d-4fdb-85c7-c47d1cf61a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(600.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce47c64b-3bfa-4422-a78b-d73fcc7e3a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/6skvpz414td2dmgxz1v1_rxr0000gn/T/ipykernel_91946/250565712.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  output.grad\n"
     ]
    }
   ],
   "source": [
    "output.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3b990-315d-4cd9-89ae-67c709758842",
   "metadata": {},
   "source": [
    "To see the backward calculation at each tensor (is that the right way to think about it?) looks like we can tell torch to retain the grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff19c58-58bd-4491-95a6-dfbc3de71290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(-60.), tensor(60.), tensor(600.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(10, dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "output = x * w\n",
    "error = y - output\n",
    "loss = error ** 2\n",
    "output.retain_grad()\n",
    "error.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "loss.grad, error.grad, output.grad, w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3e595-5ce5-44c0-a974-2567aeb929c7",
   "metadata": {},
   "source": [
    "So it seems like back to the question in challenge 15 about how tensors get allocated during back prop, it probably creates a bunch but also can free them as soon as it no longer needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefd9a3-caff-4289-83f0-032fd6eed8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e9f7d7-8148-4008-bfb2-b99d8b89b0fc",
   "metadata": {},
   "source": [
    "Now let me try back prop by hand where we have 2 weights and so partials will come into play.\n",
    "\n",
    "```\n",
    "x = [10, 15]\n",
    "y = 20\n",
    "\n",
    "model is a function f parameterized by 2 weights W = [w1, w2]: f(W) = w1*x1 + w2*x2\n",
    "\n",
    "As above, write x and y as numbers to make it less confusing.\n",
    "\n",
    "f(W) = 10*w1 + 15*w2\n",
    "error(W) = 20 - f(W)          \n",
    "loss(W) = square(error(W)) \n",
    "\n",
    "Suppose our starting value for W is [5,6]\n",
    "\n",
    "I'll first calculate the derivative of loss with respect to W the HS way, plus knowing that to get the partial derivative with respect to one variable you treat the others like constants.\n",
    "\n",
    "loss(W) = (20 - (10w1 + 15w2))^2\n",
    "loss(W) = 100w1^2 + 300w1w2 - 400w1 + 225w2^2 - 600w2 + 400\n",
    "loss'(w1) = 200w1 + 300w2 - 400\n",
    "loss'(w2) = 300w1 + 450w2 - 600\n",
    "loss'([5,6]) = [2400, 3600]\n",
    "\n",
    "Now I want to calculate the way I think back propagation works.\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "f(W) = 10*5 + 15*6 = 140\n",
    "error(W) = 20 - f(W) = -120\n",
    "loss(W) = square(error(W)) = square(-120) = 14400\n",
    "\n",
    "Backward pass:\n",
    "\n",
    "loss'(W) = square'(error(W)) * error'(W) = 2(error(W)) * error'(W) = -240 *\n",
    "           error'(W) = -f'(W) = -1 *\n",
    "           f'(W) = [10, 15]\n",
    "           = [2400, 3600]\n",
    "\n",
    "Why is f'(W) = [10,15]? Becuase f'(w1) = 10 and f'(w2) = 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ce6da-3dbd-4ac3-8518-c0f0a1963abb",
   "metadata": {},
   "source": [
    "Now with torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3081ef72-deb3-44e7-b315-da74b0e2ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([10, 15], dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor([5, 6], dtype=torch.float32, requires_grad=True)\n",
    "output = x @ w\n",
    "error = y - output\n",
    "loss = error ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd460a3-6494-49b9-83cc-7f44354d9329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10., 15.]),\n",
       " tensor(20.),\n",
       " tensor([5., 6.], requires_grad=True),\n",
       " tensor(140., grad_fn=<DotBackward0>),\n",
       " tensor(-120., grad_fn=<SubBackward0>),\n",
       " tensor(14400., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, w, output, error, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243c28bf-ab37-4e48-b199-455cae397a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2400., 3600.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cfc30-fdd6-40d2-bc8d-60ceeae31979",
   "metadata": {},
   "source": [
    "And now using retain_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "073bf70c-2a63-4b8e-81f4-40fe1e1b2723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(-240.), tensor(240.), tensor([2400., 3600.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([10, 15], dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor([5, 6], dtype=torch.float32, requires_grad=True)\n",
    "output = x @ w\n",
    "error = y - output\n",
    "loss = error ** 2\n",
    "output.retain_grad()\n",
    "error.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "loss.grad, error.grad, output.grad, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b9449-64c1-4c08-bc0b-a5c6b6f5988a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c49dfbf5-ecdc-435a-abca-9e0a2c6fa6ff",
   "metadata": {},
   "source": [
    "Now try back prop by hand where we have 2 pieces of data and 2 weights. \n",
    "\n",
    "```\n",
    "x = [[10, 15], [30, 40]]\n",
    "y = [20, 25]\n",
    "\n",
    "f(W) = [10w1 + 15w2, 30w1 + 40w2]\n",
    "error(W) = [20 - f(W), 25 - f(W)]\n",
    "loss = mean(square(error(W)))\n",
    "\n",
    "suppose W = [5,6]\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "f(W) = [140, 390]\n",
    "error(W) = [-120, -365]\n",
    "square(error(W)) = [14400, 133225]\n",
    "mean(square(error(W))) = 73812.5\n",
    "\n",
    "Backward pass:\n",
    "\n",
    "loss'(W) = mean'(square(error(W)))\n",
    "         = 0.5 * (   square'(error(W))          +     square'(error(W))         )       †             \n",
    "         = 0.5 * (   2(error(W)) * error'(W)    +     2(error(W)) * error'(W)   )       †\n",
    "         = 0.5 * (   2(-120) * -f'(W)           +     2(-365) * -f'(W)          )       †\n",
    "         = 0.5 * (   -240 * (-1) * [10, 15])    +     (-730 * (-1) * [30, 40]   )       †\n",
    "         = 0.5 * (   [2400, 3600]               +     [21900, 29200]            )       †\n",
    "         = 0.5 * [24300, 32800]\n",
    "         = [12150, 16400]\n",
    "\n",
    "† - this notation is confusing / wrong, but each column is like the prior example, column 1\n",
    "    is for the first piece of data and column 2 for the second\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e706a7-9ddc-42a3-acb5-d85e0f126a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.),\n",
       " tensor([0.5000, 0.5000]),\n",
       " tensor([-120., -365.]),\n",
       " tensor([120., 365.]),\n",
       " tensor([12150., 16400.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[10, 15], [30, 40]], dtype=torch.float32)\n",
    "y = torch.tensor([20, 25], dtype=torch.float32)\n",
    "w = torch.tensor([5, 6], dtype=torch.float32, requires_grad=True)\n",
    "output = x @ w\n",
    "error = y - output\n",
    "error_squared = error ** 2\n",
    "loss = torch.mean(error_squared)\n",
    "output.retain_grad()\n",
    "error.retain_grad()\n",
    "error_squared.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "loss.grad, error_squared.grad, error.grad, output.grad, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b020c05-8839-442b-9ee2-e72cce2b833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([140., 390.], grad_fn=<MvBackward0>),\n",
       " tensor([-120., -365.], grad_fn=<SubBackward0>),\n",
       " tensor([ 14400., 133225.], grad_fn=<PowBackward0>),\n",
       " tensor(73812.5000, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, error, error_squared, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d86c9-8a07-4a49-8519-f30afd8acf23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
