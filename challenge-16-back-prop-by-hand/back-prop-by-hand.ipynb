{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc40b0c2-b49e-40fd-b832-7e682653b538",
   "metadata": {},
   "source": [
    "While working on challenge 15 I realized I'm confused about what tensors need to be available and allocated for back prop. Do back prop by hand to build my intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585211c-c3a3-43eb-b214-89f528f2709f",
   "metadata": {},
   "source": [
    "Let's first try an example that uses scalars only and doesn't even involve data or a loss function.\n",
    "\n",
    "```\n",
    "We're going to build a \"machine\" called \"f\" with 4 \"knobs.\" We're only allowed to turn the first knob which we call \"a\" but we can read the value off any of the knobs.\n",
    "\n",
    "[a knob] -> [b knob] -> [c knob] -> [d knob]\n",
    "\n",
    "the value of the \"b\" knob will be the square of its input\n",
    "the value of the \"c\" knob will be the cube of its input\n",
    "the value of the \"d\" knob will be the square of its input\n",
    "\n",
    "We set \"a\" to 2. We want to know how \"d\" will change if we turn \"a\" a little bit.\n",
    "\n",
    "a(x) = x\n",
    "b(x) = x^2\n",
    "c(x) = x^3\n",
    "d(x) = x^2\n",
    "f(x) = d(c(b(a(x))))\n",
    "\n",
    "This means that f(x) = x^12\n",
    "so f'(x) = 12x^11\n",
    "so f'(2) = 24,576\n",
    "\n",
    "If we turn \"a\" tiny bit, say +1/100,000, we expect the output \"d\" to increase by around 24,576/100,000 =~ .25. Let's see:\n",
    "\n",
    "(2 + 1/100,000)^12 - 2^12  =~ 0.246\n",
    "\n",
    "With the raise to the 12th power, it's no surprise that a tiny, tiny turn of \"a\" causes a much bigger increase in \"d\".\n",
    "\n",
    "Now let's calculate this again but with back propagation through the machine. In other words, instead of first simplfying the \"machine\" to x^12 and using the power rule in calculus once, we use the chain rule.\n",
    "\n",
    "A note about notation because I was a little sloppy at first and got myself confused:\n",
    "\n",
    "d/dx f(x) means the derivative of f with respect to x\n",
    "f'(x) means the derivative of f with respect to the input of f\n",
    "so d/dx f(x) = f'(x)\n",
    "but d/dx f(g(x)) ≠ f'(g(x))\n",
    "\n",
    "First we calculate forward:\n",
    "\n",
    "a = 2\n",
    "b(a) = a^2 = 2^2 = 4\n",
    "c(b(a)) = 4^3 = 64\n",
    "d(c(b(a))) = 64^2 = 4096\n",
    "\n",
    "Let's show these on our knobs:\n",
    "\n",
    "[a knob] -> [b knob] -> [c knob] -> [d knob]\n",
    "    2          4           64         4096\n",
    "\n",
    "Now backward:\n",
    "\n",
    "f'(a) = d/da d(c(b(a)))\n",
    "      = d'(c(b(a))) * d/da c(b(a))\n",
    "      = 2 * c(b(a)) * d/da c(b(a))        # becuase d'(x) = 2x\n",
    "      = 2 * 64 * d/da c(b(a))\n",
    "      = 128 * d/da c(b(a))                <--- 128 is \"c\" knob grad\n",
    "      = 128 * c'(b(a)) * d/da b(a)\n",
    "      = 128 * 3 * b(a)^2 * d/da b(a)      # because c'(x) = 3x^2\n",
    "      = 128 * 3 * 4^2 * d/da b(a)\n",
    "      = 128 * 3 * 16 * d/da b(a)\n",
    "      = 128 * 48 * d/da b(a)\n",
    "      = 6144 * d/da b(a)                  <--- 6144 is \"b\" knob grad\n",
    "      = 6144 * 2 * a                      # because b'(x) = 2x\n",
    "      = 6144 * 2 * 2\n",
    "      = 6144 * 4\n",
    "      = 24,576                            <--- 24,576 is \"a\" knob grad\n",
    "\n",
    "\n",
    "                    b(x)            c(x)            d(x)\n",
    "          [a knob]   ->   [b knob]   ->   [c knob]   ->   [d knob]\n",
    "forward:      2              4               64             4096\n",
    "backward:  24,576           6144             128              1\n",
    "                                             \n",
    "\n",
    "How to think about the \"c\" knob grad of 128? We know in this \"machine\" we aren't allowed to adjust the \"c\" knob by hand. But let's say we did. The 128 tell us that in the current state (where \"a\" is set to 2 and \"c\" is therefore set to 64), if we increase \"c\" slightly, we'll see an increase of 128 times that amount in \"d\":\n",
    "\n",
    "(64 + 1/1000)^2 - 4096 =~ 0.128\n",
    "\n",
    "How about the \"b\" knob grad of 6144? Same idea:\n",
    "\n",
    "(4 + 1/10,000)^6 - 4096 =~ 0.614\n",
    "\n",
    "Now going back to our original motivation for exploring this stuff from challenge 15, let's think about what information we need to do the backward pass.\n",
    "\n",
    "grad_of_d = 1\n",
    "grad_of_c = d_backwards(grad_of_d, value_of_c) = grad_of_d * d'(value_of_c) = 1 * 2(64) = 128\n",
    "grad_of_b = c_backwards(grad_of_c, value_of_b) = grad_of_c * c'(value_of_b) = 128 * 3(4^2) = 6144\n",
    "grad_of_a = b_backwards(grad_of_b, value_of_a) = grad_of_b * b'(value_of_a) = 6144 * 2(2) = 24576\n",
    "\n",
    "In other words, for each knob, calculate its derivative with respect to its input, plug in the input it received on the forward pass, multiply by its gradent, and that's the gradient of the knob before.\n",
    "\n",
    "Interesting, looks like we don't actually need to compute the value of \"d\" to determine the grad of \"a\".\n",
    "\n",
    "```\n",
    "\n",
    "Now let's do it in torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e19d74-45f3-44da-9553-c03b18ce7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: a:2.0 -> b:4.0 -> c:64.0 -> d:4096.0\n",
      "Backward: d:1.0 -> c:128.0 -> b:6144.0 -> a:24576.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor(2, dtype=torch.float32, requires_grad=True)\n",
    "b = a ** 2\n",
    "c = b ** 3\n",
    "d = c ** 2\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "d.retain_grad()\n",
    "d.backward()\n",
    "print(f\"Forward: a:{a.item()} -> b:{b.item()} -> c:{c.item()} -> d:{d.item()}\")\n",
    "print(f\"Backward: d:{d.grad.item()} -> c:{c.grad.item()} -> b:{b.grad.item()} -> a:{a.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d3342-bf94-479c-9ab0-ab33bc676018",
   "metadata": {},
   "source": [
    "^ The numbers match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c9b9f-5273-4394-9118-697b2514487e",
   "metadata": {},
   "source": [
    "To get a bit more practice, try where the input to the machine has two values (a vector of size 2), the output is a single number, and we're interested in the gradient of the input vector. This brings us a little closer to a situation where we're interested in the gradient of many weights and our final output is a single number (the loss).\n",
    "\n",
    "```\n",
    "[a1 knob] -> [b1 knob] -> [c1 knob] -> [d1 knob] \\\n",
    "                                                   -> [e knob]\n",
    "[a2 knob] -> [b2 knob] -> [c2 knob] -> [d2 knob] /\n",
    "\n",
    "This will be just like the example above except the \"e\" knob is set by the mean of its inputs.\n",
    "\n",
    "X is the vector [x1, x2]\n",
    "\n",
    "b(X) = X^2\n",
    "c(X) = X^3\n",
    "d(X) = X^2\n",
    "e(X) = 0.5(x1) + 0.5(x2)\n",
    "\n",
    "Let's set our A knobs to [2,3]\n",
    "\n",
    "                    b(X)            c(X)            d(X)                  e(X)\n",
    "          [a knob]   ->   [b knob]   ->   [c knob]   ->   [d knob]         ->    [e knob]\n",
    "\n",
    "forward:   [2,3]           [4,9]          [64,729]        [4096, 531441]         267768.5\n",
    "\n",
    "\n",
    "To calculate backward we need to know e'(X):\n",
    "e(X) = 0.5(x1) + 0.5(x2)\n",
    "∂/∂x1 e(X) = 0.5     (I believe another notation for this is e_x1)\n",
    "∂/∂x2 e(X) = 0.5\n",
    "e'(X) = [0.5, 0.5]\n",
    "\n",
    "e_backwards(grad_of_e, value_of_d) = grad_of_e * e'(value_of_d) = 1 * [0.5, 0.5] = [0.5, 0.5]\n",
    "\n",
    "\n",
    "So now we do things like before:\n",
    "\n",
    "                       b(X)                c(X)                  d(X)                  e(X)\n",
    "          [a knob]      ->    [b knob]      ->      [c knob]      ->   [d knob]         ->     [e knob]\n",
    "\n",
    "forward:   [2, 3]              [4, 9]                [64, 729]          [4096, 531441]          267768.5\n",
    "backward:  [12288, 1062882]    [3072, 177147]        [64, 729]          [0.5, 0.5]              1\n",
    "\n",
    "And as a sanity check, if we \"force\" the d2 knob up by 1, it makes sense that our output will increase by 0.5 because our output is the mean of d1 and d2. (Mean of 2 and 4 is 3, mean of 2 and 5 is 3.5.)\n",
    "\n",
    "```\n",
    "\n",
    "Now with torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c7c528-93c7-48d8-8ee0-94a5f4568c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: a:[2.0, 3.0] -> b:[4.0, 9.0] -> c:[64.0, 729.0] -> d:[4096.0, 531441.0] -> e:267768.5\n",
      "Backward: e:1.0 -> d:[0.5, 0.5] -> c:[64.0, 729.0] -> b:[3072.0, 177147.0] -> a:[12288.0, 1062882.0]\n"
     ]
    }
   ],
   "source": [
    "pp = lambda t: '[' + ', '.join([f\"{x:.1f}\" for x in t]) + ']'\n",
    "a = torch.tensor([2,3], dtype=torch.float32, requires_grad=True)\n",
    "b = a ** 2\n",
    "c = b ** 3\n",
    "d = c ** 2\n",
    "e = torch.mean(d)\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "d.retain_grad()\n",
    "e.retain_grad()\n",
    "e.backward()\n",
    "print(f\"Forward: a:{pp(a)} -> b:{pp(b)} -> c:{pp(c)} -> d:{pp(d)} -> e:{e.item()}\")\n",
    "print(f\"Backward: e:{e.grad.item()} -> d:{pp(d.grad)} -> c:{pp(c.grad)} -> b:{pp(b.grad)} -> a:{pp(a.grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c5eab-1c22-4554-ab4a-6f3e7bbcb364",
   "metadata": {},
   "source": [
    "Now try an example with data and a weight to be adjusted but only with scalars. (This was actually what I did first and then realized I wanted to start even simpler with the ones above.)\n",
    "\n",
    "```\n",
    "data:\n",
    "\n",
    "x = 10\n",
    "y = 20\n",
    "\n",
    "model:\n",
    "\n",
    "our prediction is parameterized by a single weight w: prediction(x,w) = w*x\n",
    "\n",
    "To keep in mind that x and y are fixed and we're interested in the derivative of the loss with respect to w (so we know how to adjust it), I'm going to write x and y as 10 and 20.\n",
    "\n",
    "prediction(w) = 10 * w                  this gives the prediction of our model for x = 10\n",
    "error(w) = 20 - prediction(w)           this gives the error for x = 10, y = 20 \n",
    "loss(w) = square(error(prediction(w))) \n",
    "\n",
    "Suppose w is 5\n",
    "\n",
    "I'll first calculate the derivative of loss with respect to w the way I learned in HS:\n",
    "\n",
    "loss(w) = (20 - 10w)^2\n",
    "loss(w) = 100w^2 - 400w + 400\n",
    "loss'(w) = 200w - 400\n",
    "\n",
    "loss'(5) = 600           this tells us increasing w will increase the loss\n",
    "\n",
    "Now I'll use the back propagation approach.\n",
    "\n",
    "First forward:\n",
    "\n",
    "           [w]   ->    [prediction]       ->      [error]       ->      [loss]\n",
    "Forward:    5             50                        -30                  900\n",
    "\n",
    "Before doing backward, calculate the derivatives we'll need (using z to be very clear this has nothing to do with x above):\n",
    "\n",
    "loss(z) = z^2 so loss'(z) = 2z\n",
    "error(z) = 20 - z so error'(z) = -1\n",
    "prediction(z) = 10z so prediction'(z) = 10\n",
    "\n",
    "           [w]   ->    [prediction]    ->    [error]       ->      [loss]\n",
    "Forward:    5             50                   -30                  900\n",
    "\n",
    "Backward:   60 * 10     -60 * -1           1 * 2(-30)                1\n",
    "            600            60                 -60                    1   \n",
    "\n",
    "\n",
    "Or we can write it out this way:\n",
    "\n",
    "loss'(w) = d/dw square(error(prediction(w)))\n",
    "         = square'(error(prediction(w))) * d/dw error(prediction(w))\n",
    "         = square'(-30) * d/dw error(prediction(w))\n",
    "         = -60 * d/dw error(prediction(w))\n",
    "         = -60 * error'(prediction(w)) * d/dw prediction(w)\n",
    "         = -60 * error'(50) * d/dw prediction(w)\n",
    "         = -60 * -1 * d/dw prediction(w)\n",
    "         = 60 * d/dw prediction(w)\n",
    "         = 60 * 10\n",
    "         = 600\n",
    "\n",
    "Looking at the prediction \"knob\", if we force it up our loss will increase, which we know is true because the \"actual\" value is 20.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e04435-dfc8-479b-87e1-4ad80068b0fd",
   "metadata": {},
   "source": [
    "Now do it with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5d9f40-e7bb-4c9b-bb7d-7a04dc66c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:  w:5.0 -> prediction:50.0 -> error:-30.0 -> loss:900.0\n",
      "Backward: loss:1.0 -> error:-60.0 -> prediction:60.0 -> w:600.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(10, dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "prediction = x * w\n",
    "error = y - prediction\n",
    "loss = error ** 2\n",
    "prediction.retain_grad()\n",
    "error.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(f\"Forward:  w:{w.item()} -> prediction:{prediction.item()} -> error:{error.item()} -> loss:{loss.item()}\")\n",
    "print(f\"Backward: loss:{loss.grad.item()} -> error:{error.grad.item()} -> prediction:{prediction.grad.item()} -> w:{w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70525a-d445-4e92-89a9-2e2aaebc89d6",
   "metadata": {},
   "source": [
    "What happens if we try to access the grad of say error without calling `.retain_grad()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38690d67-815f-4339-a8e3-6961798a2351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:  w:5.0 -> prediction:50.0 -> error:-30.0 -> loss:900.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/6skvpz414td2dmgxz1v1_rxr0000gn/T/ipykernel_14584/4184479387.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"Backward: loss:{loss.grad.item()} -> error:{error.grad.item()} -> prediction:{prediction.grad.item()} -> w:{w.grad.item()}\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward:  w:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> prediction:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> error:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward: loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> error:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> prediction:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> w:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(10, dtype=torch.float32)\n",
    "y = torch.tensor(20, dtype=torch.float32)\n",
    "w = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "prediction = x * w\n",
    "error = y - prediction\n",
    "loss = error ** 2\n",
    "prediction.retain_grad()\n",
    "error.retain_grad()\n",
    "# loss.retain_grad()\n",
    "loss.backward()\n",
    "print(f\"Forward:  w:{w.item()} -> prediction:{prediction.item()} -> error:{error.item()} -> loss:{loss.item()}\")\n",
    "print(f\"Backward: loss:{loss.grad.item()} -> error:{error.grad.item()} -> prediction:{prediction.grad.item()} -> w:{w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054d8fe-34b6-4342-aa7d-12b2eb8d48d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58cc234d-f1aa-4668-b024-42cb676bae18",
   "metadata": {},
   "source": [
    "Now let's try 3 pieces of data and 2 weights.\n",
    "\n",
    "```\n",
    "Data:\n",
    "\n",
    "X = [[10, 15], [3, 4], [7, 11]]\n",
    "Y = [20, 8, 14]\n",
    "\n",
    "prediction(W) = X @ W\n",
    "error(W) = Y - prediction(W)\n",
    "square_error(W) = square(error(W))\n",
    "loss(W) = mean(square(error(W))\n",
    "\n",
    "Suppose W is [5,6]\n",
    "\n",
    "\n",
    "     [W]             ->       [prediction]    ->    [error]            ->  [square_error]    -> [loss]\n",
    "\n",
    "Fwd: [5,6]                    [140, 39, 101]        [-120, -31, -87]       [14400, 961, 7569]   7643.3335\n",
    "\n",
    "Bwd: X.T @ [80, 20.6667, 58]  -1*[-80,-20.67,-58]   2/3*[-120, -31, -87]   1*[1/3,1/3,1/3]      1\n",
    "\n",
    "     [1268, 1920.67]          [80, 20.6667, 58]     [-80, -20.6667, -58]   [1/3, 1/3, 1/3]      1\n",
    "\n",
    "\n",
    "Everything is like above except for the last backwards step: X.T @ [80, 20.67, 58]. I don't understand that. Will explore below after doing this in torch.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b68aa7e5-5015-4c4e-964e-4be394051864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fwd: w:[5, 6] -> pred:[140, 39, 101] -> error:[-120, -31, -87] -> error_sq:[14400, 961, 7569] -> loss:7643.3\n",
      "Bwd: loss:1.0 -> error_sq:[0.3, 0.3, 0.3] -> error:[-80, -20.7, -58] -> pred:[80, 20.7, 58] -> w:[1268, 1920.7]\n"
     ]
    }
   ],
   "source": [
    "pp = lambda t: '[' + ', '.join([format(x, '.0f' if x % 1 == 0 else '.1f') for x in t]) + ']'\n",
    "x = torch.tensor([[10, 15], [3, 4], [7, 11]], dtype=torch.float32)\n",
    "y = torch.tensor([20, 8, 14], dtype=torch.float32)\n",
    "w = torch.tensor([5, 6], dtype=torch.float32, requires_grad=True)\n",
    "prediction = x @ w\n",
    "error = y - prediction\n",
    "error_squared = error ** 2\n",
    "loss = torch.mean(error_squared)\n",
    "prediction.retain_grad()\n",
    "error.retain_grad()\n",
    "error_squared.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(f\"Fwd: w:{pp(w)} -> pred:{pp(prediction)} -> error:{pp(error)} -> error_sq:{pp(error_squared)} -> loss:{loss.item():.1f}\")\n",
    "print(f\"Bwd: loss:{loss.grad.item()} -> error_sq:{pp(error_squared.grad)} -> error:{pp(error.grad)} -> pred:{pp(prediction.grad)} -> w:{pp(w.grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb38f7-afcc-4bb9-94fb-97895624bf0c",
   "metadata": {},
   "source": [
    "```\n",
    "Why in the last backward step do we do X.T @ [80, 20.67, 58]?\n",
    "\n",
    "We know the general idea is something like:\n",
    "\n",
    "W.grad = prediction.grad * prediction'(W)\n",
    "\n",
    "We also know that prediction'(W) will need to tell us things like if we increase w1 a little bit or if we increase w2 a little bit, how does the prediction for each of our 3 pieces of data change. Since this is all linear, we can see that increasing w1 by 1 and not touching w2 will increase the first prediction by 10, the second by 3, and the third by 7. Similarly, if we leave w1 alone and increase w2 by 1, we will increase our first prediction by 15, our second by 4, and our third by 11. So it makes sense that the function prediction'(W) has X in it, and I believe there's no way it could contain something with less information than X becuase then it wouldn't be possible to say how much each of our 3 predictions increases.\n",
    "\n",
    "Is the the derivative of the prediction with respect to w1 [10, 3, 7]? If so that too makes sense. But why is it valid to compute w1.grad as 10 * 80 + 3 * 20.7 + 7 * 58 = 1268? Why are we allowed to add those numbers together?\n",
    "\n",
    "Let me compute W.grad the non-back-prop, non-matrix way. Maybe that will give some insight.\n",
    "\n",
    "loss(W) = 1/3(20 - (10w1 + 15w2))^2 + 1/3(8 - (3w1 + 4w2))^2 + 1/3(14 - (7w1 + 11w2))^2\n",
    "        = (158w1^2)/3 + (478w1w2)/3 - (644w1)/3 + (362w2^2)/3 - 324w2 + 220\n",
    "\n",
    "∂loss/∂w1 = 2/3(158)w1 + 1/3(478)w2 - 1/3(644) = 1268\n",
    "\n",
    "∂loss/∂w2 = d/dw2 = 1/3(478)w1 + 2/3(362)w2 - 324 = 1920.67\n",
    "\n",
    "```\n",
    "\n",
    "That's good but I still don't get it. Giving up and asking ChatGPT. Let me at least try to ask the question clearly.\n",
    "\n",
    "me: Given matrices X, W and Y and X @ W = Y, when doing back propagation to determine W.grad, why is W.grad calculated by X.T @ Y.grad?\n",
    "\n",
    "ChatGPT said a lot. Not sure I appreciate it telling me \"Below is the clean, no-mystery answer, wrapped in imagery so it goes down easier.\"! Not going to try to digest it now.\n",
    "\n",
    "Also found these [Stanford CS class slides](https://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf) which seem helpful but want to move on. Perhaps revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e2440-c74b-4395-95bc-e0e669387f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
