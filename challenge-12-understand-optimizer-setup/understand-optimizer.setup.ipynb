{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b188b57-2a35-45ff-bf5c-3dc5714fc3e3",
   "metadata": {},
   "source": [
    "copied from [gpt.py](https://github.com/karpathy/nanochat/blob/master/nanochat/gpt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3becd6-3768-455a-b031-9e3dfbdd2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28466834-ada7-4af3-b268-06caaf709bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import get_dist_info\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af48f9b-1a3a-4907-a195-68944caf3630",
   "metadata": {},
   "source": [
    "### Understanding optimizers in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b18ac0-8a20-42c9-9a2e-e5842415c98f",
   "metadata": {},
   "source": [
    "First try to remember/reconstruct what an optimizer does. I think it decides what the learning rate should be at each training step. For example, in early training steps we want to multiply the gradient by a higher learning rate and in later training steps by a lower one, but there are countless ways to adjust the learning rate over time ranging from a linear predetermined schedule to one that takes into account \"clues\" such as how fast the loss is training, if we're converging, etc. I believe the optimizer encapsulates all that.\n",
    "\n",
    "Let's see what ChatGPT says: In deep learning, an optimizer is the algorithm that adjusts a model’s parameters (weights and biases) during training to minimize the loss function. It decides how to update the parameters based on gradients computed from backpropagation.\n",
    "\n",
    "So maybe it's even a bit more general than what I thought. It's job is not just or only to decide the learning rate, but in fact to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a55f8-246f-4984-ae85-5785137e5063",
   "metadata": {},
   "source": [
    "How do you use a torch optimizer? What's the contract? Let's try a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e355cbcb-cecf-4ff0-b3a2-4d29e5b69d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 2)\n",
    "y = (x[:,0] * 5 + x[:,1] * 7).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe595861-10b6-43e6-9e49-87b32782e382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2985,  2.5948],\n",
       "         [-0.1912,  0.6394],\n",
       "         [-1.4373,  1.1328],\n",
       "         [ 0.3734,  1.1152],\n",
       "         [ 1.0200,  0.6448]]),\n",
       " tensor([[19.6562],\n",
       "         [ 3.5200],\n",
       "         [ 0.7432],\n",
       "         [ 9.6734],\n",
       "         [ 9.6139]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d3d7e8-cbfd-48dd-b5ba-923432dbe282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3072],\n",
       "        [-0.4432],\n",
       "        [-1.2876],\n",
       "        [-0.4497],\n",
       "        [ 0.1077]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Linear(2,1,bias=False)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2fb5a8-939b-4100-9f83-26c372511d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4573, -0.5564]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286531e2-4d37-4a05-909c-baa152ce1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 130.42807006835938, learning rate: 0.001\n",
      "step: 1000, loss: 0.9276749491691589, learning rate: 0.001\n",
      "step: 2000, loss: 0.06006918102502823, learning rate: 0.001\n",
      "step: 3000, loss: 0.004045575857162476, learning rate: 0.001\n",
      "step: 4000, loss: 0.0002725429367274046, learning rate: 0.001\n",
      "step: 5000, loss: 1.8373380953562446e-05, learning rate: 0.001\n",
      "step: 6000, loss: 1.2555952935144887e-06, learning rate: 0.001\n",
      "step: 7000, loss: 1.0440614062190434e-07, learning rate: 0.001\n",
      "step: 8000, loss: 2.836850399035029e-08, learning rate: 0.001\n",
      "step: 9000, loss: 2.836850399035029e-08, learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters())\n",
    "for i in range(10_000):\n",
    "    optimizer.zero_grad()\n",
    "    F.mse_loss(model(x),y).backward()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"step: {i}, loss: {F.mse_loss(model(x),y).item()}, learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4dd6230-7b0e-4aff-935d-fc094bfd8d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[4.9998, 7.0001]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bee24-3639-4dd4-9a3d-cba8533e62c4",
   "metadata": {},
   "source": [
    "So judging by that at least, the contract seems to be that you give it the parameters and you tell it to take steps. It's reponsible for reading the gradient, multiplying it by the learning rate, and subtracting it from the weights, or whatever it chooses to do to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e3835-61f1-4e49-8568-04b4f55ea628",
   "metadata": {},
   "source": [
    "### Step through GPT.setup_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0208ccf0-bdfb-4cd9-9c87-7bd16474f72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(10, 16)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_q): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (c_k): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (c_v): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (c_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=16, out_features=64, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=16, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=16, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config = GPTConfig(\n",
    "    sequence_len=7,\n",
    "    vocab_size=10,\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_kv_head=2,\n",
    "    n_embd=16,\n",
    ")\n",
    "gpt = GPT(my_config)\n",
    "gpt.init_weights()\n",
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d704ebf1-ca21-4af1-bd6a-5545ae821225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters passed into the function with their defaults\n",
    "unembedding_lr = 0.004 # is this for the lm_head ? A synonym for that?\n",
    "embedding_lr = 0.2\n",
    "matrix_lr = 0.2\n",
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada2436b-3206-4df7-9878-d46ab766504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make it easier to later copy and paste this code into gpt.py, I'm going to:\n",
    "self = gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781794f4-3337-46bd-be8a-06ec0ab28efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dim = self.config.n_embd; model_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30269d5-7eaf-4d35-bccf-ca0826c4d793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0, 0, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp, rank, local_rank, world_size = get_dist_info()\n",
    "ddp, rank, local_rank, world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09136a4-4c0b-456c-9940-9fd51594bc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seperate params into 3 groups\n",
    "matrix_params = list(self.transformer.h.parameters()) # so \"matrix\" is all except wte and lm_head\n",
    "embedding_params = list(self.transformer.wte.parameters())\n",
    "lm_head_params = list(self.lm_head.parameters())\n",
    "len(matrix_params), len(embedding_params), len(lm_head_params) # expect 6*2=12, 1, 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12047964-6f06-455e-9fc0-cb74f7b1710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cfdc62-cc31-4790-a01a-370b63905374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.92820323027551"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the AdamW optimizer for the embedding and lm_head\n",
    "# scale the LR for by proportional to 1/sqrt(model_dim)\n",
    "# from his note, seems like the passed in / default LRs assume a 768 dim model so we scale the LR\n",
    "# if this model has different dimensions\n",
    "# not sure why the wording is \"scale ... for by proportional to\" vs \"scale by\" and if I'm missing\n",
    "# something or understanding ∝ wrong\n",
    "dmodel_lr_scale = (model_dim / 768) ** -0.5; dmodel_lr_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818acc6a-da6f-4f0e-914b-35633ed50163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(16/768) = 6.92820323027551\n"
     ]
    }
   ],
   "source": [
    "if rank == 0:\n",
    "    print(f\"Scaling the LR for the AdamW parameters proportional to 1/sqrt({model_dim}/768) = {dmodel_lr_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3302756-7174-4639-801b-a019455947e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_groups = [\n",
    "    dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),\n",
    "    dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fab89eb-1757-4336-8cb5-1629cf089cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume these 2 betas correspond to the 2 groups\n",
    "adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d6b5eef-d201-4315-8277-2a5c49f480f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistAdamW comes from his adamw.py and is used for distributed training, set to None for now\n",
    "# so we'll fail if we try to do distributed training and haven't yet \"copied\" it\n",
    "DistAdamW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6badb5d6-0e74-47a1-ae8f-735ffe1f3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cc190-9e1c-43ea-ad37-f6310a843227",
   "metadata": {},
   "source": [
    "What is fused? ChatGPT: multiple operations have been combined into a single, lower-level kernel (often a CUDA kernel) instead of running as many separate operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a12f2aa3-9451-4dfc-abf7-4f29fd4ee46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.8, 0.95)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-10\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 0.027712812921102038\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.8, 0.95)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-10\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 1.385640646055102\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
    "adamw_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3db83865-1f26-4908-8935-dba6fd8cbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Muon optimizer for the linear layers\n",
    "muon_kwargs = dict(lr=matrix_lr, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a8055-89f9-453b-b4ae-962c9773831d",
   "metadata": {},
   "source": [
    "Hmm, he implements Muon (not just DistMuon) in `muon.py`. This doesn't seem like the right time to go deep into that. FOR NOW, I'm going to break the no copy / paste rule and copy his muon.py wholesale so I can use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d91598c5-0578-4a12-8ecb-37a0ba078f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.muon import Muon, DistMuon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a3afcf-f1ae-4a79-962d-1941611a4568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Muon (\n",
       "Parameter Group 0\n",
       "    lr: 0.2\n",
       "    momentum: 0.95\n",
       "    nesterov: True\n",
       "    ns_steps: 5\n",
       "\n",
       "Parameter Group 1\n",
       "    lr: 0.2\n",
       "    momentum: 0.95\n",
       "    nesterov: True\n",
       "    ns_steps: 5\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MuonFactory = DistMoon if ddp else Muon\n",
    "muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)\n",
    "muon_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9caed0a6-d90a-41d9-bef0-68f076e8dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two optimizers into one list\n",
    "optimizers = [adamw_optimizer, muon_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c5d1cf5-fbd4-46ac-9712-a6fc4577a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"] # guessing for reporting (nope, or not ony, realized this in challenge 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1238eadd-ee97-4057-84dc-6a6e3d6beb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Muon (\n",
       "Parameter Group 0\n",
       "    initial_lr: 0.2\n",
       "    lr: 0.2\n",
       "    momentum: 0.95\n",
       "    nesterov: True\n",
       "    ns_steps: 5\n",
       "\n",
       "Parameter Group 1\n",
       "    initial_lr: 0.2\n",
       "    lr: 0.2\n",
       "    momentum: 0.95\n",
       "    nesterov: True\n",
       "    ns_steps: 5\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muon_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceebc3b-02fe-433e-bd59-0742df5042b1",
   "metadata": {},
   "source": [
    "Now add `setup_optimizers()` to `my_gpt.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94f43775-35e0-4110-b7d6-7a7c51b4807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278afe79-85ad-464d-88f1-5b0ad593246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(16/768) = 6.92820323027551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.8, 0.95)\n",
       "     capturable: False\n",
       "     decoupled_weight_decay: True\n",
       "     differentiable: False\n",
       "     eps: 1e-10\n",
       "     foreach: None\n",
       "     fused: True\n",
       "     initial_lr: 0.027712812921102038\n",
       "     lr: 0.027712812921102038\n",
       "     maximize: False\n",
       "     weight_decay: 0.0\n",
       " \n",
       " Parameter Group 1\n",
       "     amsgrad: False\n",
       "     betas: (0.8, 0.95)\n",
       "     capturable: False\n",
       "     decoupled_weight_decay: True\n",
       "     differentiable: False\n",
       "     eps: 1e-10\n",
       "     foreach: None\n",
       "     fused: True\n",
       "     initial_lr: 1.385640646055102\n",
       "     lr: 1.385640646055102\n",
       "     maximize: False\n",
       "     weight_decay: 0.0\n",
       " ),\n",
       " Muon (\n",
       " Parameter Group 0\n",
       "     initial_lr: 0.2\n",
       "     lr: 0.2\n",
       "     momentum: 0.95\n",
       "     nesterov: True\n",
       "     ns_steps: 5\n",
       " \n",
       " Parameter Group 1\n",
       "     initial_lr: 0.2\n",
       "     lr: 0.2\n",
       "     momentum: 0.95\n",
       "     nesterov: True\n",
       "     ns_steps: 5\n",
       " )]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.setup_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b901b-b2af-4526-8bd4-a5f2acc46ae4",
   "metadata": {},
   "source": [
    "### Understand AdamW optimizer\n",
    "\n",
    "Now that the code is in place, understand a little about AdamW. Ok, after asking ChatGPT and reading the [torch docs](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html) I get the idea. Wondering where it stores the first and second moments and if can see those in param groups once we start training. Try the simple example from above but using AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4b18a3e-99b1-40c9-806e-8acd999fb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 2)\n",
    "y = (x[:,0] * 5 + x[:,1] * 7).unsqueeze(-1)\n",
    "model = torch.nn.Linear(2,1, bias=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e538fa-564f-44e0-9711-81ed097ede42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4048, -0.1024]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76f8b5bf-b734-40e7-b720-87c0a1d2022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7d3a203-122d-4ddf-831c-4d3462f6f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 60.29609298706055, learning rate: 0.001\n",
      "step: 1000, loss: 44.197731018066406, learning rate: 0.001\n",
      "step: 2000, loss: 31.580692291259766, learning rate: 0.001\n",
      "step: 3000, loss: 21.744022369384766, learning rate: 0.001\n",
      "step: 4000, loss: 14.211898803710938, learning rate: 0.001\n",
      "step: 5000, loss: 8.635725975036621, learning rate: 0.001\n",
      "step: 6000, loss: 4.728221893310547, learning rate: 0.001\n",
      "step: 7000, loss: 2.219465732574463, learning rate: 0.001\n",
      "step: 8000, loss: 0.8228629231452942, learning rate: 0.001\n",
      "step: 9000, loss: 0.21109020709991455, learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "for i in range(10_000):\n",
    "    optimizer.zero_grad()\n",
    "    F.mse_loss(model(x),y).backward()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"step: {i}, loss: {F.mse_loss(model(x),y).item()}, learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ed8d46-f8e1-437b-8b80-f2c5404f22bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[4.9032, 6.8311]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0742c5-ed83-47ae-a900-0d941480ff05",
   "metadata": {},
   "source": [
    "Funny, at least on some runs ^ it's ending up in very good local minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8aa3272-54a7-4525-80a6-695d3c9abc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': tensor(10000.),\n",
       "   'exp_avg': tensor([[-0.0249, -0.3682]]),\n",
       "   'exp_avg_sq': tensor([[0.0260, 2.1372]])}},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0.01,\n",
       "   'amsgrad': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'capturable': False,\n",
       "   'differentiable': False,\n",
       "   'fused': None,\n",
       "   'decoupled_weight_decay': True,\n",
       "   'params': [0]}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f129b8-b471-49d3-af09-9a221c34277f",
   "metadata": {},
   "source": [
    "Yes, looks like the holds maintains the moving averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77da083a-f3e7-4525-81d5-206609d8637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 83.30, grad: tensor([[-14.5306, -14.7523]]), exp_avg: tensor([[-1.4533, -1.4755]])\n",
      "step: 1001, loss: 57.78, grad: tensor([[-11.8233, -12.4782]]), exp_avg: tensor([[-11.8491, -12.5000]])\n",
      "step: 2001, loss: 38.56, grad: tensor([[ -9.3578, -10.3839]]), exp_avg: tensor([[ -9.3814, -10.4041]])\n",
      "step: 3001, loss: 24.34, grad: tensor([[-7.1029, -8.4409]]), exp_avg: tensor([[-7.1245, -8.4596]])\n",
      "step: 4001, loss: 14.24, grad: tensor([[-5.0566, -6.6397]]), exp_avg: tensor([[-5.0760, -6.6570]])\n",
      "step: 5001, loss: 7.53, grad: tensor([[-3.2504, -4.9908]]), exp_avg: tensor([[-3.2671, -5.0064]])\n",
      "step: 6001, loss: 3.51, grad: tensor([[-1.7556, -3.5249]]), exp_avg: tensor([[-1.7687, -3.5385]])\n",
      "step: 7001, loss: 1.44, grad: tensor([[-0.6828, -2.2918]]), exp_avg: tensor([[-0.6911, -2.3028]])\n",
      "step: 8001, loss: 0.51, grad: tensor([[-0.1229, -1.3413]]), exp_avg: tensor([[-0.1260, -1.3493]])\n",
      "step: 9001, loss: 0.14, grad: tensor([[ 0.0154, -0.6806]]), exp_avg: tensor([[ 0.0152, -0.6859]])\n",
      "step: 10001, loss: 0.02, grad: tensor([[ 0.0004, -0.2768]]), exp_avg: tensor([[ 0.0007, -0.2796]])\n",
      "step: 11001, loss: 0.00, grad: tensor([[-0.0117, -0.0918]]), exp_avg: tensor([[-0.0116, -0.0928]])\n",
      "step: 12001, loss: 0.00, grad: tensor([[-0.0099, -0.0368]]), exp_avg: tensor([[-0.0099, -0.0370]])\n",
      "step: 13001, loss: 0.00, grad: tensor([[-0.0062, -0.0201]]), exp_avg: tensor([[-0.0062, -0.0202]])\n",
      "step: 14001, loss: 0.00, grad: tensor([[-0.0038, -0.0118]]), exp_avg: tensor([[-0.0038, -0.0119]])\n",
      "step: 15001, loss: 0.00, grad: tensor([[-0.0023, -0.0071]]), exp_avg: tensor([[-0.0023, -0.0071]])\n",
      "step: 16001, loss: 0.00, grad: tensor([[-0.0014, -0.0042]]), exp_avg: tensor([[-0.0014, -0.0043]])\n",
      "step: 17001, loss: 0.00, grad: tensor([[-0.0009, -0.0026]]), exp_avg: tensor([[-0.0009, -0.0026]])\n",
      "step: 18001, loss: 0.00, grad: tensor([[-0.0005, -0.0015]]), exp_avg: tensor([[-0.0005, -0.0016]])\n",
      "step: 19001, loss: 0.00, grad: tensor([[-0.0003, -0.0009]]), exp_avg: tensor([[-0.0003, -0.0009]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100, 2) # more data points, less likely to find such a good local minimum ?\n",
    "y = (x[:,0] * 5 + x[:,1] * 7).unsqueeze(-1)\n",
    "model = torch.nn.Linear(2,1, bias=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "for i in range(20_000):\n",
    "    optimizer.zero_grad()\n",
    "    F.mse_loss(model(x),y).backward()\n",
    "    if i % 1000 == 1:\n",
    "        print(f\"step: {i}, loss: {F.mse_loss(model(x),y).item():.2f}, grad: {model.weight.grad}, exp_avg: {optimizer.state_dict()['state'][0]['exp_avg']}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5dfc07b-95b8-44b7-8d5f-5fdfecaefb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[5.0000, 6.9997]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0568f4b-65a7-4f43-853a-12f39385a36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
