{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664fa57c-c8f0-4f21-b2c3-149f00a4eecc",
   "metadata": {},
   "source": [
    "## SFT train the d20 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5a10-828e-4589-a4da-2dd5f610171b",
   "metadata": {},
   "source": [
    "I'm going to approach this like `challenge-28-midtrain-d20/midtrain-d20.ipynb`. I'll use the 8xH100 in lambda cloud and run all the scripts from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc713876-efcf-4e99-aac3-2e29bcff10b4",
   "metadata": {},
   "source": [
    "Follow the instructions here to get the machine ready: `challenge-28-midtrain-d20/midtrain-d20.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2ba769-1a45-455f-b1f0-70d98a655633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 21 00:31:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0701be-b09f-45a6-a05a-cb6b9d78b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabff0a-b418-4e6c-9bb9-9332ecd6fc9c",
   "metadata": {},
   "source": [
    "#### Repeat CORE eval on base model\n",
    "\n",
    "For reasons discussed in `challenge-28-midtrain-d20/investigate-questions.ipynb`, I want to first do the CORE eval on the base model **with max-per-task=500**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aa96b9-9b9b-4919-97cd-834d6eeea2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 00:32:12.588000 14480 torch/distributed/run.py:803] \n",
      "W1121 00:32:12.588000 14480 torch/distributed/run.py:803] *****************************************\n",
      "W1121 00:32:12.588000 14480 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 00:32:12.588000 14480 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4320 | centered: 0.2427 | time: 1.40s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1320 | centered: 0.1320 | time: 1.01s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5500 | centered: 0.5500 | time: 1.02s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6640 | centered: 0.5520 | time: 1.25s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3840 | centered: 0.1787 | time: 1.06s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.22s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2160 | centered: 0.0200 | time: 0.98s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6920 | centered: 0.3840 | time: 1.17s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.88s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3960 | centered: 0.3960 | time: 0.98s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4520 | centered: 0.2693 | time: 1.69s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.42s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5220 | centered: 0.0440 | time: 0.74s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0980 | centered: 0.0980 | time: 1.09s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.62s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3580 | centered: 0.3580 | time: 1.01s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.43s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.06s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2360 | centered: 0.2360 | time: 1.30s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1800 | centered: 0.1800 | time: 1.06s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5240 | centered: -0.2526 | time: 1.40s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2680 | centered: 0.1947 | time: 2.23s\n",
      "CORE metric: 0.2084\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.2426666816075643,\n",
      "    \"jeopardy\": 0.13199999928474426,\n",
      "    \"bigbench_qa_wikidata\": 0.550000011920929,\n",
      "    \"arc_easy\": 0.5520000457763672,\n",
      "    \"arc_challenge\": 0.17866667111714682,\n",
      "    \"copa\": 0.3799999952316284,\n",
      "    \"commonsense_qa\": 0.020000007003545747,\n",
      "    \"piqa\": 0.3840000629425049,\n",
      "    \"openbook_qa\": 0.14666668574015299,\n",
      "    \"lambada_openai\": 0.3960000276565552,\n",
      "    \"hellaswag\": 0.2693333625793457,\n",
      "    \"winograd\": 0.28205132484436035,\n",
      "    \"winogrande\": 0.04400002956390381,\n",
      "    \"bigbench_dyck_languages\": 0.09800000488758087,\n",
      "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
      "    \"bigbench_cs_algorithms\": 0.3580000102519989,\n",
      "    \"bigbench_operators\": 0.1666666716337204,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.23600001633167267,\n",
      "    \"coqa\": 0.18000000715255737,\n",
      "    \"boolq\": -0.25263145095423645,\n",
      "    \"bigbench_language_identification\": 0.19471947929122135\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- \\\n",
    "--source=base --model-tag=d20 --max-per-task=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10362b8-ac7b-492d-9560-0d24e5faaea9",
   "metadata": {},
   "source": [
    "#### Redo mid training\n",
    "\n",
    "Because I left out updating muon momentum (noticed in `challenge-29-understand-sft/understand-sft.ipynb`), I want to redo the mid training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ed3484-88fc-4650-8b28-91822588ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 00:33:11.705000 39756 torch/distributed/run.py:803] \n",
      "W1121 00:33:11.705000 39756 torch/distributed/run.py:803] *****************************************\n",
      "W1121 00:33:11.705000 39756 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 00:33:11.705000 39756 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-30-1\n",
      "user_config: {'run': 'challenge-30-1', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-30-sft-train-d20/wandb/run-20251121_003325-79iz8auo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-30-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/79iz8auo\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
      "Tokens / micro-batch: 524,288\n",
      "Total batch size 524,288 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "README.md: 2.24kB [00:00, 6.95MB/s]\n",
      "data/train-00000-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 135MB/s]\n",
      "data/train-00001-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 159MB/s]\n",
      "data/train-00002-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231M/231M [00:01<00:00, 214MB/s]\n",
      "data/train-00003-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232M/232M [00:00<00:00, 257MB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.2M/48.2M [00:00<00:00, 109MB/s]\n",
      "Generating train split: 100%|â–ˆ| 460341/460341 [00:03<00:00, 123923.43 examples/s\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 24229/24229 [00:00<00:00, 132372.63 examples/s]\n",
      "README.md: 53.2kB [00:00, 93.6MB/s]\n",
      "dataset_infos.json: 138kB [00:00, 194MB/s]\n",
      "auxiliary_train/train-00000-of-00001.par(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 9\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 99842/99842 [00:00<00:00, 388341.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 16659.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 16468.87 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 16506.70 examples/s]\n",
      "README.md: 7.94kB [00:00, 21.6MB/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 16459.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 16373.24 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 15977.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 15626.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:06<00:00, 15424.09 examples/s]\n",
      "main/train-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆ| 2.31M/2.31M [00:00<00:00, 7.60MB/s]\n",
      "main/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419k/419k [00:00<00:00, 2.91MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 644978.78 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 377965.91 examples/s]\n",
      "all/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50M/3.50M [00:00<00:00, 14.4MB/s]\n",
      "all/validation-00000-of-00001.parquet: 100%|â–ˆ| 408k/408k [00:00<00:00, 2.87MB/s]\n",
      "all/dev-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76.5k/76.5k [00:00<00:00, 595kB/s]\n",
      "all/auxiliary_train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 7\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 14042/14042 [00:00<00:00, 733436.49 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 1531/1531 [00:00<00:00, 395362.60 examples/\n",
      "Generating dev split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 151716.80 examples/s]\n",
      "Generating auxiliary_train split: 100%|â–ˆ| 99842/99842 [00:00<00:00, 446161.10 ex\n",
      "step 00000 | Validation bpb: 0.6856\n",
      "step 00001 (0.23%) | loss: 1.400719 | lrm: 1.00 | dt: 60979.22ms | tok/sec: 8,597 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002 (0.37%) | loss: 1.747543 | lrm: 1.00 | dt: 679.84ms | tok/sec: 771,195 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003 (0.51%) | loss: 1.901648 | lrm: 1.00 | dt: 474.51ms | tok/sec: 1,104,894 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004 (0.63%) | loss: 1.962197 | lrm: 1.00 | dt: 632.08ms | tok/sec: 829,470 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005 (0.75%) | loss: 1.975422 | lrm: 1.00 | dt: 475.82ms | tok/sec: 1,101,860 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006 (0.86%) | loss: 1.973900 | lrm: 1.00 | dt: 486.09ms | tok/sec: 1,078,575 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007 (0.99%) | loss: 1.964316 | lrm: 1.00 | dt: 482.39ms | tok/sec: 1,086,846 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008 (1.12%) | loss: 1.966362 | lrm: 1.00 | dt: 486.05ms | tok/sec: 1,078,660 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009 (1.24%) | loss: 1.950576 | lrm: 1.00 | dt: 485.21ms | tok/sec: 1,080,544 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 (1.36%) | loss: 1.920264 | lrm: 1.00 | dt: 623.62ms | tok/sec: 840,717 | mfu: -1.00 | total time: 0.00m\n",
      "step 00011 (1.48%) | loss: 1.914273 | lrm: 1.00 | dt: 479.08ms | tok/sec: 1,094,374 | mfu: -1.00 | total time: 0.01m\n",
      "step 00012 (1.60%) | loss: 1.897573 | lrm: 1.00 | dt: 485.36ms | tok/sec: 1,080,213 | mfu: -1.00 | total time: 0.02m\n",
      "step 00013 (1.72%) | loss: 1.883286 | lrm: 1.00 | dt: 482.91ms | tok/sec: 1,085,695 | mfu: -1.00 | total time: 0.02m\n",
      "step 00014 (1.85%) | loss: 1.866036 | lrm: 1.00 | dt: 545.55ms | tok/sec: 961,030 | mfu: -1.00 | total time: 0.03m\n",
      "step 00015 (1.99%) | loss: 1.852852 | lrm: 1.00 | dt: 479.03ms | tok/sec: 1,094,476 | mfu: -1.00 | total time: 0.04m\n",
      "step 00016 (2.12%) | loss: 1.837827 | lrm: 1.00 | dt: 531.85ms | tok/sec: 985,790 | mfu: -1.00 | total time: 0.05m\n",
      "step 00017 (2.25%) | loss: 1.809278 | lrm: 1.00 | dt: 477.68ms | tok/sec: 1,097,568 | mfu: -1.00 | total time: 0.06m\n",
      "step 00018 (2.37%) | loss: 1.791478 | lrm: 1.00 | dt: 536.58ms | tok/sec: 977,093 | mfu: -1.00 | total time: 0.07m\n",
      "step 00019 (2.50%) | loss: 1.775533 | lrm: 1.00 | dt: 478.98ms | tok/sec: 1,094,601 | mfu: -1.00 | total time: 0.07m\n",
      "step 00020 (2.62%) | loss: 1.761104 | lrm: 1.00 | dt: 548.44ms | tok/sec: 955,969 | mfu: -1.00 | total time: 0.08m\n",
      "step 00021 (2.75%) | loss: 1.744577 | lrm: 1.00 | dt: 474.06ms | tok/sec: 1,105,957 | mfu: -1.00 | total time: 0.09m\n",
      "step 00022 (2.88%) | loss: 1.720850 | lrm: 1.00 | dt: 488.27ms | tok/sec: 1,073,773 | mfu: -1.00 | total time: 0.10m\n",
      "step 00023 (2.98%) | loss: 1.699280 | lrm: 1.00 | dt: 485.15ms | tok/sec: 1,080,666 | mfu: -1.00 | total time: 0.11m\n",
      "step 00024 (3.09%) | loss: 1.690404 | lrm: 1.00 | dt: 540.27ms | tok/sec: 970,420 | mfu: -1.00 | total time: 0.12m\n",
      "step 00025 (3.21%) | loss: 1.704776 | lrm: 1.00 | dt: 534.05ms | tok/sec: 981,714 | mfu: -1.00 | total time: 0.13m\n",
      "step 00026 (3.34%) | loss: 1.704942 | lrm: 1.00 | dt: 476.67ms | tok/sec: 1,099,898 | mfu: -1.00 | total time: 0.13m\n",
      "step 00027 (3.47%) | loss: 1.702428 | lrm: 1.00 | dt: 486.20ms | tok/sec: 1,078,335 | mfu: -1.00 | total time: 0.14m\n",
      "step 00028 (3.58%) | loss: 1.690109 | lrm: 1.00 | dt: 484.90ms | tok/sec: 1,081,221 | mfu: -1.00 | total time: 0.15m\n",
      "step 00029 (3.69%) | loss: 1.682750 | lrm: 1.00 | dt: 545.92ms | tok/sec: 960,368 | mfu: -1.00 | total time: 0.16m\n",
      "step 00030 (3.83%) | loss: 1.678103 | lrm: 1.00 | dt: 476.35ms | tok/sec: 1,100,628 | mfu: -1.00 | total time: 0.17m\n",
      "step 00031 (3.95%) | loss: 1.675003 | lrm: 1.00 | dt: 485.26ms | tok/sec: 1,080,421 | mfu: -1.00 | total time: 0.18m\n",
      "step 00032 (4.08%) | loss: 1.661055 | lrm: 1.00 | dt: 539.36ms | tok/sec: 972,056 | mfu: -1.00 | total time: 0.18m\n",
      "step 00033 (4.20%) | loss: 1.673263 | lrm: 1.00 | dt: 532.82ms | tok/sec: 983,985 | mfu: -1.00 | total time: 0.19m\n",
      "step 00034 (4.33%) | loss: 1.665313 | lrm: 1.00 | dt: 475.04ms | tok/sec: 1,103,663 | mfu: -1.00 | total time: 0.20m\n",
      "step 00035 (4.43%) | loss: 1.657174 | lrm: 1.00 | dt: 632.46ms | tok/sec: 828,969 | mfu: -1.00 | total time: 0.21m\n",
      "step 00036 (4.56%) | loss: 1.650833 | lrm: 1.00 | dt: 530.12ms | tok/sec: 989,005 | mfu: -1.00 | total time: 0.22m\n",
      "step 00037 (4.70%) | loss: 1.649592 | lrm: 1.00 | dt: 476.56ms | tok/sec: 1,100,146 | mfu: -1.00 | total time: 0.23m\n",
      "step 00038 (4.82%) | loss: 1.643682 | lrm: 1.00 | dt: 534.34ms | tok/sec: 981,193 | mfu: -1.00 | total time: 0.24m\n",
      "step 00039 (4.94%) | loss: 1.640820 | lrm: 1.00 | dt: 535.64ms | tok/sec: 978,804 | mfu: -1.00 | total time: 0.25m\n",
      "step 00040 (5.07%) | loss: 1.639306 | lrm: 1.00 | dt: 478.43ms | tok/sec: 1,095,840 | mfu: -1.00 | total time: 0.25m\n",
      "step 00041 (5.16%) | loss: 1.642815 | lrm: 1.00 | dt: 488.53ms | tok/sec: 1,073,191 | mfu: -1.00 | total time: 0.26m\n",
      "step 00042 (5.31%) | loss: 1.627022 | lrm: 1.00 | dt: 484.75ms | tok/sec: 1,081,563 | mfu: -1.00 | total time: 0.27m\n",
      "step 00043 (5.43%) | loss: 1.623773 | lrm: 1.00 | dt: 482.06ms | tok/sec: 1,087,594 | mfu: -1.00 | total time: 0.28m\n",
      "step 00044 (5.59%) | loss: 1.624358 | lrm: 1.00 | dt: 541.69ms | tok/sec: 967,867 | mfu: -1.00 | total time: 0.29m\n",
      "step 00045 (5.70%) | loss: 1.614907 | lrm: 1.00 | dt: 530.02ms | tok/sec: 989,187 | mfu: -1.00 | total time: 0.30m\n",
      "step 00046 (5.83%) | loss: 1.609982 | lrm: 1.00 | dt: 476.10ms | tok/sec: 1,101,209 | mfu: -1.00 | total time: 0.30m\n",
      "step 00047 (5.95%) | loss: 1.610232 | lrm: 1.00 | dt: 488.00ms | tok/sec: 1,074,370 | mfu: -1.00 | total time: 0.31m\n",
      "step 00048 (6.07%) | loss: 1.605316 | lrm: 1.00 | dt: 617.08ms | tok/sec: 849,626 | mfu: -1.00 | total time: 0.32m\n",
      "step 00049 (6.19%) | loss: 1.601197 | lrm: 1.00 | dt: 478.51ms | tok/sec: 1,095,674 | mfu: -1.00 | total time: 0.33m\n",
      "step 00050 (6.32%) | loss: 1.593956 | lrm: 1.00 | dt: 496.55ms | tok/sec: 1,055,854 | mfu: -1.00 | total time: 0.34m\n",
      "step 00051 (6.44%) | loss: 1.593323 | lrm: 1.00 | dt: 479.48ms | tok/sec: 1,093,459 | mfu: -1.00 | total time: 0.35m\n",
      "step 00052 (6.59%) | loss: 1.594238 | lrm: 1.00 | dt: 641.65ms | tok/sec: 817,088 | mfu: -1.00 | total time: 0.36m\n",
      "step 00053 (6.71%) | loss: 1.586125 | lrm: 1.00 | dt: 546.70ms | tok/sec: 958,998 | mfu: -1.00 | total time: 0.37m\n",
      "step 00054 (6.85%) | loss: 1.579805 | lrm: 1.00 | dt: 533.90ms | tok/sec: 982,005 | mfu: -1.00 | total time: 0.38m\n",
      "step 00055 (6.98%) | loss: 1.570506 | lrm: 1.00 | dt: 557.56ms | tok/sec: 940,331 | mfu: -1.00 | total time: 0.38m\n",
      "step 00056 (7.09%) | loss: 1.567192 | lrm: 1.00 | dt: 552.93ms | tok/sec: 948,198 | mfu: -1.00 | total time: 0.39m\n",
      "step 00057 (7.23%) | loss: 1.553430 | lrm: 1.00 | dt: 478.43ms | tok/sec: 1,095,843 | mfu: -1.00 | total time: 0.40m\n",
      "step 00058 (7.34%) | loss: 1.555856 | lrm: 1.00 | dt: 493.87ms | tok/sec: 1,061,583 | mfu: -1.00 | total time: 0.41m\n",
      "step 00059 (7.45%) | loss: 1.563157 | lrm: 1.00 | dt: 538.76ms | tok/sec: 973,131 | mfu: -1.00 | total time: 0.42m\n",
      "step 00060 (7.57%) | loss: 1.569147 | lrm: 1.00 | dt: 665.84ms | tok/sec: 787,406 | mfu: -1.00 | total time: 0.43m\n",
      "step 00061 (7.67%) | loss: 1.581470 | lrm: 1.00 | dt: 562.44ms | tok/sec: 932,160 | mfu: -1.00 | total time: 0.44m\n",
      "step 00062 (7.81%) | loss: 1.577776 | lrm: 1.00 | dt: 477.77ms | tok/sec: 1,097,365 | mfu: -1.00 | total time: 0.45m\n",
      "step 00063 (7.93%) | loss: 1.575227 | lrm: 1.00 | dt: 566.13ms | tok/sec: 926,089 | mfu: -1.00 | total time: 0.46m\n",
      "step 00064 (8.05%) | loss: 1.559449 | lrm: 1.00 | dt: 606.64ms | tok/sec: 864,250 | mfu: -1.00 | total time: 0.47m\n",
      "step 00065 (8.18%) | loss: 1.559075 | lrm: 1.00 | dt: 546.92ms | tok/sec: 958,624 | mfu: -1.00 | total time: 0.48m\n",
      "step 00066 (8.30%) | loss: 1.567108 | lrm: 1.00 | dt: 480.10ms | tok/sec: 1,092,033 | mfu: -1.00 | total time: 0.48m\n",
      "step 00067 (8.42%) | loss: 1.576801 | lrm: 1.00 | dt: 496.96ms | tok/sec: 1,054,994 | mfu: -1.00 | total time: 0.49m\n",
      "step 00068 (8.55%) | loss: 1.568072 | lrm: 1.00 | dt: 547.67ms | tok/sec: 957,298 | mfu: -1.00 | total time: 0.50m\n",
      "step 00069 (8.67%) | loss: 1.564255 | lrm: 1.00 | dt: 534.71ms | tok/sec: 980,500 | mfu: -1.00 | total time: 0.51m\n",
      "step 00070 (8.81%) | loss: 1.563847 | lrm: 1.00 | dt: 537.85ms | tok/sec: 974,783 | mfu: -1.00 | total time: 0.52m\n",
      "step 00071 (8.93%) | loss: 1.564517 | lrm: 1.00 | dt: 477.64ms | tok/sec: 1,097,663 | mfu: -1.00 | total time: 0.53m\n",
      "step 00072 (9.04%) | loss: 1.567824 | lrm: 1.00 | dt: 485.65ms | tok/sec: 1,079,551 | mfu: -1.00 | total time: 0.54m\n",
      "step 00073 (9.19%) | loss: 1.558227 | lrm: 1.00 | dt: 489.53ms | tok/sec: 1,071,013 | mfu: -1.00 | total time: 0.54m\n",
      "step 00074 (9.32%) | loss: 1.555613 | lrm: 1.00 | dt: 484.77ms | tok/sec: 1,081,528 | mfu: -1.00 | total time: 0.55m\n",
      "step 00075 (9.44%) | loss: 1.552502 | lrm: 1.00 | dt: 494.42ms | tok/sec: 1,060,402 | mfu: -1.00 | total time: 0.56m\n",
      "step 00076 (9.56%) | loss: 1.545099 | lrm: 1.00 | dt: 544.64ms | tok/sec: 962,637 | mfu: -1.00 | total time: 0.57m\n",
      "step 00077 (9.68%) | loss: 1.529854 | lrm: 1.00 | dt: 695.41ms | tok/sec: 753,931 | mfu: -1.00 | total time: 0.58m\n",
      "step 00078 (9.82%) | loss: 1.532343 | lrm: 1.00 | dt: 611.61ms | tok/sec: 857,229 | mfu: -1.00 | total time: 0.59m\n",
      "step 00079 (9.93%) | loss: 1.534875 | lrm: 1.00 | dt: 534.70ms | tok/sec: 980,523 | mfu: -1.00 | total time: 0.60m\n",
      "step 00080 (10.06%) | loss: 1.532913 | lrm: 1.00 | dt: 600.79ms | tok/sec: 872,661 | mfu: -1.00 | total time: 0.61m\n",
      "step 00081 (10.19%) | loss: 1.526526 | lrm: 1.00 | dt: 542.39ms | tok/sec: 966,624 | mfu: -1.00 | total time: 0.62m\n",
      "step 00082 (10.31%) | loss: 1.528895 | lrm: 1.00 | dt: 621.67ms | tok/sec: 843,349 | mfu: -1.00 | total time: 0.63m\n",
      "step 00083 (10.46%) | loss: 1.517424 | lrm: 1.00 | dt: 618.42ms | tok/sec: 847,789 | mfu: -1.00 | total time: 0.64m\n",
      "step 00084 (10.58%) | loss: 1.523184 | lrm: 1.00 | dt: 476.11ms | tok/sec: 1,101,195 | mfu: -1.00 | total time: 0.65m\n",
      "step 00085 (10.71%) | loss: 1.512941 | lrm: 1.00 | dt: 723.69ms | tok/sec: 724,461 | mfu: -1.00 | total time: 0.66m\n",
      "step 00086 (10.82%) | loss: 1.497095 | lrm: 1.00 | dt: 564.04ms | tok/sec: 929,529 | mfu: -1.00 | total time: 0.67m\n",
      "step 00087 (10.95%) | loss: 1.501954 | lrm: 1.00 | dt: 479.92ms | tok/sec: 1,092,441 | mfu: -1.00 | total time: 0.68m\n",
      "step 00088 (11.06%) | loss: 1.498009 | lrm: 1.00 | dt: 547.99ms | tok/sec: 956,754 | mfu: -1.00 | total time: 0.69m\n",
      "step 00089 (11.18%) | loss: 1.498327 | lrm: 1.00 | dt: 477.04ms | tok/sec: 1,099,052 | mfu: -1.00 | total time: 0.69m\n",
      "step 00090 (11.29%) | loss: 1.484557 | lrm: 1.00 | dt: 492.47ms | tok/sec: 1,064,617 | mfu: -1.00 | total time: 0.70m\n",
      "step 00091 (11.41%) | loss: 1.489061 | lrm: 1.00 | dt: 631.39ms | tok/sec: 830,375 | mfu: -1.00 | total time: 0.71m\n",
      "step 00092 (11.53%) | loss: 1.482723 | lrm: 1.00 | dt: 593.72ms | tok/sec: 883,051 | mfu: -1.00 | total time: 0.72m\n",
      "step 00093 (11.65%) | loss: 1.480411 | lrm: 1.00 | dt: 544.28ms | tok/sec: 963,275 | mfu: -1.00 | total time: 0.73m\n",
      "step 00094 (11.78%) | loss: 1.482894 | lrm: 1.00 | dt: 477.82ms | tok/sec: 1,097,241 | mfu: -1.00 | total time: 0.74m\n",
      "step 00095 (11.91%) | loss: 1.487620 | lrm: 1.00 | dt: 492.26ms | tok/sec: 1,065,073 | mfu: -1.00 | total time: 0.75m\n",
      "step 00096 (12.03%) | loss: 1.483488 | lrm: 1.00 | dt: 545.47ms | tok/sec: 961,171 | mfu: -1.00 | total time: 0.76m\n",
      "step 00097 (12.14%) | loss: 1.492453 | lrm: 1.00 | dt: 541.97ms | tok/sec: 967,371 | mfu: -1.00 | total time: 0.77m\n",
      "step 00098 (12.26%) | loss: 1.495517 | lrm: 1.00 | dt: 532.74ms | tok/sec: 984,133 | mfu: -1.00 | total time: 0.78m\n",
      "step 00099 (12.38%) | loss: 1.492579 | lrm: 1.00 | dt: 623.77ms | tok/sec: 840,514 | mfu: -1.00 | total time: 0.79m\n",
      "step 00100 (12.49%) | loss: 1.491708 | lrm: 1.00 | dt: 477.97ms | tok/sec: 1,096,896 | mfu: -1.00 | total time: 0.79m\n",
      "step 00101 (12.62%) | loss: 1.491310 | lrm: 1.00 | dt: 539.59ms | tok/sec: 971,640 | mfu: -1.00 | total time: 0.80m\n",
      "step 00102 (12.75%) | loss: 1.487657 | lrm: 1.00 | dt: 596.59ms | tok/sec: 878,803 | mfu: -1.00 | total time: 0.81m\n",
      "step 00103 (12.87%) | loss: 1.490072 | lrm: 1.00 | dt: 478.96ms | tok/sec: 1,094,641 | mfu: -1.00 | total time: 0.82m\n",
      "step 00104 (13.00%) | loss: 1.484265 | lrm: 1.00 | dt: 493.25ms | tok/sec: 1,062,926 | mfu: -1.00 | total time: 0.83m\n",
      "step 00105 (13.10%) | loss: 1.479806 | lrm: 1.00 | dt: 486.93ms | tok/sec: 1,076,718 | mfu: -1.00 | total time: 0.84m\n",
      "step 00106 (13.21%) | loss: 1.477609 | lrm: 1.00 | dt: 547.83ms | tok/sec: 957,024 | mfu: -1.00 | total time: 0.85m\n",
      "step 00107 (13.34%) | loss: 1.478771 | lrm: 1.00 | dt: 549.89ms | tok/sec: 953,447 | mfu: -1.00 | total time: 0.86m\n",
      "step 00108 (13.45%) | loss: 1.478924 | lrm: 1.00 | dt: 590.57ms | tok/sec: 887,761 | mfu: -1.00 | total time: 0.86m\n",
      "step 00109 (13.57%) | loss: 1.475358 | lrm: 1.00 | dt: 477.81ms | tok/sec: 1,097,263 | mfu: -1.00 | total time: 0.87m\n",
      "step 00110 (13.70%) | loss: 1.473958 | lrm: 1.00 | dt: 491.70ms | tok/sec: 1,066,278 | mfu: -1.00 | total time: 0.88m\n",
      "step 00111 (13.81%) | loss: 1.466653 | lrm: 1.00 | dt: 487.31ms | tok/sec: 1,075,886 | mfu: -1.00 | total time: 0.89m\n",
      "step 00112 (13.93%) | loss: 1.461704 | lrm: 1.00 | dt: 486.89ms | tok/sec: 1,076,800 | mfu: -1.00 | total time: 0.90m\n",
      "step 00113 (14.07%) | loss: 1.467495 | lrm: 1.00 | dt: 493.16ms | tok/sec: 1,063,111 | mfu: -1.00 | total time: 0.91m\n",
      "step 00114 (14.19%) | loss: 1.483249 | lrm: 1.00 | dt: 485.02ms | tok/sec: 1,080,971 | mfu: -1.00 | total time: 0.91m\n",
      "step 00115 (14.32%) | loss: 1.475713 | lrm: 1.00 | dt: 553.07ms | tok/sec: 947,967 | mfu: -1.00 | total time: 0.92m\n",
      "step 00116 (14.47%) | loss: 1.480081 | lrm: 1.00 | dt: 598.04ms | tok/sec: 876,675 | mfu: -1.00 | total time: 0.93m\n",
      "step 00117 (14.59%) | loss: 1.482503 | lrm: 1.00 | dt: 480.90ms | tok/sec: 1,090,214 | mfu: -1.00 | total time: 0.94m\n",
      "step 00118 (14.71%) | loss: 1.475164 | lrm: 1.00 | dt: 490.33ms | tok/sec: 1,069,247 | mfu: -1.00 | total time: 0.95m\n",
      "step 00119 (14.83%) | loss: 1.473496 | lrm: 1.00 | dt: 489.22ms | tok/sec: 1,071,686 | mfu: -1.00 | total time: 0.96m\n",
      "step 00120 (14.96%) | loss: 1.470840 | lrm: 1.00 | dt: 488.95ms | tok/sec: 1,072,269 | mfu: -1.00 | total time: 0.97m\n",
      "step 00121 (15.07%) | loss: 1.469312 | lrm: 1.00 | dt: 492.07ms | tok/sec: 1,065,464 | mfu: -1.00 | total time: 0.97m\n",
      "step 00122 (15.21%) | loss: 1.464204 | lrm: 1.00 | dt: 541.30ms | tok/sec: 968,576 | mfu: -1.00 | total time: 0.98m\n",
      "step 00123 (15.35%) | loss: 1.463130 | lrm: 1.00 | dt: 535.14ms | tok/sec: 979,727 | mfu: -1.00 | total time: 0.99m\n",
      "step 00124 (15.47%) | loss: 1.467167 | lrm: 1.00 | dt: 480.45ms | tok/sec: 1,091,232 | mfu: -1.00 | total time: 1.00m\n",
      "step 00125 (15.58%) | loss: 1.457832 | lrm: 1.00 | dt: 494.71ms | tok/sec: 1,059,798 | mfu: -1.00 | total time: 1.01m\n",
      "step 00126 (15.69%) | loss: 1.457518 | lrm: 1.00 | dt: 483.43ms | tok/sec: 1,084,522 | mfu: -1.00 | total time: 1.02m\n",
      "step 00127 (15.82%) | loss: 1.453390 | lrm: 1.00 | dt: 543.75ms | tok/sec: 964,202 | mfu: -1.00 | total time: 1.02m\n",
      "step 00128 (15.96%) | loss: 1.448741 | lrm: 1.00 | dt: 608.17ms | tok/sec: 862,078 | mfu: -1.00 | total time: 1.03m\n",
      "step 00129 (16.09%) | loss: 1.453500 | lrm: 1.00 | dt: 482.04ms | tok/sec: 1,087,643 | mfu: -1.00 | total time: 1.04m\n",
      "step 00130 (16.22%) | loss: 1.453365 | lrm: 1.00 | dt: 490.87ms | tok/sec: 1,068,077 | mfu: -1.00 | total time: 1.05m\n",
      "step 00131 (16.34%) | loss: 1.466407 | lrm: 1.00 | dt: 491.77ms | tok/sec: 1,066,116 | mfu: -1.00 | total time: 1.06m\n",
      "step 00132 (16.46%) | loss: 1.461120 | lrm: 1.00 | dt: 487.65ms | tok/sec: 1,075,128 | mfu: -1.00 | total time: 1.07m\n",
      "step 00133 (16.58%) | loss: 1.459530 | lrm: 1.00 | dt: 544.27ms | tok/sec: 963,278 | mfu: -1.00 | total time: 1.08m\n",
      "step 00134 (16.69%) | loss: 1.465198 | lrm: 1.00 | dt: 479.01ms | tok/sec: 1,094,521 | mfu: -1.00 | total time: 1.08m\n",
      "step 00135 (16.81%) | loss: 1.451786 | lrm: 1.00 | dt: 494.35ms | tok/sec: 1,060,561 | mfu: -1.00 | total time: 1.09m\n",
      "step 00136 (16.94%) | loss: 1.459545 | lrm: 1.00 | dt: 488.71ms | tok/sec: 1,072,799 | mfu: -1.00 | total time: 1.10m\n",
      "step 00137 (17.09%) | loss: 1.457266 | lrm: 1.00 | dt: 487.44ms | tok/sec: 1,075,602 | mfu: -1.00 | total time: 1.11m\n",
      "step 00138 (17.20%) | loss: 1.453988 | lrm: 1.00 | dt: 494.19ms | tok/sec: 1,060,910 | mfu: -1.00 | total time: 1.12m\n",
      "step 00139 (17.33%) | loss: 1.463206 | lrm: 1.00 | dt: 546.15ms | tok/sec: 959,971 | mfu: -1.00 | total time: 1.13m\n",
      "step 00140 (17.45%) | loss: 1.455607 | lrm: 1.00 | dt: 534.81ms | tok/sec: 980,317 | mfu: -1.00 | total time: 1.14m\n",
      "step 00141 (17.56%) | loss: 1.450475 | lrm: 1.00 | dt: 479.69ms | tok/sec: 1,092,963 | mfu: -1.00 | total time: 1.14m\n",
      "step 00142 (17.69%) | loss: 1.443916 | lrm: 1.00 | dt: 606.74ms | tok/sec: 864,102 | mfu: -1.00 | total time: 1.15m\n",
      "step 00143 (17.79%) | loss: 1.441184 | lrm: 1.00 | dt: 542.06ms | tok/sec: 967,214 | mfu: -1.00 | total time: 1.16m\n",
      "step 00144 (17.92%) | loss: 1.445284 | lrm: 1.00 | dt: 478.66ms | tok/sec: 1,095,333 | mfu: -1.00 | total time: 1.17m\n",
      "step 00145 (18.04%) | loss: 1.445926 | lrm: 1.00 | dt: 489.73ms | tok/sec: 1,070,560 | mfu: -1.00 | total time: 1.18m\n",
      "step 00146 (18.17%) | loss: 1.448942 | lrm: 1.00 | dt: 488.51ms | tok/sec: 1,073,236 | mfu: -1.00 | total time: 1.19m\n",
      "step 00147 (18.30%) | loss: 1.456621 | lrm: 1.00 | dt: 601.38ms | tok/sec: 871,801 | mfu: -1.00 | total time: 1.20m\n",
      "step 00148 (18.40%) | loss: 1.458360 | lrm: 1.00 | dt: 537.15ms | tok/sec: 976,062 | mfu: -1.00 | total time: 1.21m\n",
      "step 00149 (18.52%) | loss: 1.457530 | lrm: 1.00 | dt: 481.41ms | tok/sec: 1,089,070 | mfu: -1.00 | total time: 1.21m\n",
      "step 00150 (18.64%) | loss: 1.455147 | lrm: 1.00 | dt: 491.71ms | tok/sec: 1,066,263 | mfu: -1.00 | total time: 1.22m\n",
      "step 00150 | Validation bpb: 0.4541\n",
      "step 00151 (18.76%) | loss: 1.448231 | lrm: 1.00 | dt: 479.33ms | tok/sec: 1,093,801 | mfu: -1.00 | total time: 1.23m\n",
      "step 00152 (18.89%) | loss: 1.446764 | lrm: 1.00 | dt: 492.87ms | tok/sec: 1,063,739 | mfu: -1.00 | total time: 1.24m\n",
      "step 00153 (19.01%) | loss: 1.453526 | lrm: 1.00 | dt: 538.53ms | tok/sec: 973,555 | mfu: -1.00 | total time: 1.25m\n",
      "step 00154 (19.13%) | loss: 1.448553 | lrm: 1.00 | dt: 558.40ms | tok/sec: 938,918 | mfu: -1.00 | total time: 1.26m\n",
      "step 00155 (19.25%) | loss: 1.443776 | lrm: 1.00 | dt: 541.50ms | tok/sec: 968,213 | mfu: -1.00 | total time: 1.27m\n",
      "step 00156 (19.40%) | loss: 1.442407 | lrm: 1.00 | dt: 478.34ms | tok/sec: 1,096,067 | mfu: -1.00 | total time: 1.27m\n",
      "step 00157 (19.52%) | loss: 1.433151 | lrm: 1.00 | dt: 614.71ms | tok/sec: 852,896 | mfu: -1.00 | total time: 1.28m\n",
      "step 00158 (19.66%) | loss: 1.427409 | lrm: 1.00 | dt: 476.69ms | tok/sec: 1,099,853 | mfu: -1.00 | total time: 1.29m\n",
      "step 00159 (19.79%) | loss: 1.425247 | lrm: 1.00 | dt: 494.40ms | tok/sec: 1,060,445 | mfu: -1.00 | total time: 1.30m\n",
      "step 00160 (19.92%) | loss: 1.431838 | lrm: 1.00 | dt: 607.97ms | tok/sec: 862,361 | mfu: -1.00 | total time: 1.31m\n",
      "step 00161 (20.06%) | loss: 1.426224 | lrm: 1.00 | dt: 536.57ms | tok/sec: 977,113 | mfu: -1.00 | total time: 1.32m\n",
      "step 00162 (20.19%) | loss: 1.415017 | lrm: 1.00 | dt: 540.33ms | tok/sec: 970,317 | mfu: -1.00 | total time: 1.33m\n",
      "step 00163 (20.31%) | loss: 1.412912 | lrm: 1.00 | dt: 601.07ms | tok/sec: 872,250 | mfu: -1.00 | total time: 1.34m\n",
      "step 00164 (20.43%) | loss: 1.409545 | lrm: 1.00 | dt: 600.08ms | tok/sec: 873,699 | mfu: -1.00 | total time: 1.35m\n",
      "step 00165 (20.58%) | loss: 1.402775 | lrm: 1.00 | dt: 534.96ms | tok/sec: 980,048 | mfu: -1.00 | total time: 1.36m\n",
      "step 00166 (20.69%) | loss: 1.406765 | lrm: 1.00 | dt: 543.40ms | tok/sec: 964,833 | mfu: -1.00 | total time: 1.37m\n",
      "step 00167 (20.80%) | loss: 1.404271 | lrm: 1.00 | dt: 603.44ms | tok/sec: 868,828 | mfu: -1.00 | total time: 1.38m\n",
      "step 00168 (20.94%) | loss: 1.415080 | lrm: 1.00 | dt: 541.46ms | tok/sec: 968,293 | mfu: -1.00 | total time: 1.38m\n",
      "step 00169 (21.07%) | loss: 1.411241 | lrm: 1.00 | dt: 603.71ms | tok/sec: 868,441 | mfu: -1.00 | total time: 1.40m\n",
      "step 00170 (21.18%) | loss: 1.418324 | lrm: 1.00 | dt: 591.10ms | tok/sec: 886,971 | mfu: -1.00 | total time: 1.40m\n",
      "step 00171 (21.30%) | loss: 1.414538 | lrm: 1.00 | dt: 540.51ms | tok/sec: 969,989 | mfu: -1.00 | total time: 1.41m\n",
      "step 00172 (21.42%) | loss: 1.414594 | lrm: 1.00 | dt: 476.83ms | tok/sec: 1,099,531 | mfu: -1.00 | total time: 1.42m\n",
      "step 00173 (21.54%) | loss: 1.412608 | lrm: 1.00 | dt: 491.46ms | tok/sec: 1,066,800 | mfu: -1.00 | total time: 1.43m\n",
      "step 00174 (21.67%) | loss: 1.405457 | lrm: 1.00 | dt: 489.38ms | tok/sec: 1,071,330 | mfu: -1.00 | total time: 1.44m\n",
      "step 00175 (21.79%) | loss: 1.409134 | lrm: 1.00 | dt: 609.93ms | tok/sec: 859,592 | mfu: -1.00 | total time: 1.45m\n",
      "step 00176 (21.91%) | loss: 1.402333 | lrm: 1.00 | dt: 479.21ms | tok/sec: 1,094,076 | mfu: -1.00 | total time: 1.46m\n",
      "step 00177 (22.05%) | loss: 1.408489 | lrm: 1.00 | dt: 617.46ms | tok/sec: 849,105 | mfu: -1.00 | total time: 1.47m\n",
      "step 00178 (22.17%) | loss: 1.408276 | lrm: 1.00 | dt: 480.63ms | tok/sec: 1,090,828 | mfu: -1.00 | total time: 1.47m\n",
      "step 00179 (22.28%) | loss: 1.404039 | lrm: 1.00 | dt: 490.81ms | tok/sec: 1,068,212 | mfu: -1.00 | total time: 1.48m\n",
      "step 00180 (22.42%) | loss: 1.402241 | lrm: 1.00 | dt: 485.77ms | tok/sec: 1,079,299 | mfu: -1.00 | total time: 1.49m\n",
      "step 00181 (22.55%) | loss: 1.403186 | lrm: 1.00 | dt: 488.55ms | tok/sec: 1,073,146 | mfu: -1.00 | total time: 1.50m\n",
      "step 00182 (22.68%) | loss: 1.401245 | lrm: 1.00 | dt: 491.76ms | tok/sec: 1,066,147 | mfu: -1.00 | total time: 1.51m\n",
      "step 00183 (22.81%) | loss: 1.404459 | lrm: 1.00 | dt: 489.09ms | tok/sec: 1,071,962 | mfu: -1.00 | total time: 1.52m\n",
      "step 00184 (22.93%) | loss: 1.406230 | lrm: 1.00 | dt: 490.64ms | tok/sec: 1,068,585 | mfu: -1.00 | total time: 1.52m\n",
      "step 00185 (23.07%) | loss: 1.400298 | lrm: 1.00 | dt: 485.85ms | tok/sec: 1,079,117 | mfu: -1.00 | total time: 1.53m\n",
      "step 00186 (23.20%) | loss: 1.389228 | lrm: 1.00 | dt: 555.51ms | tok/sec: 943,803 | mfu: -1.00 | total time: 1.54m\n",
      "step 00187 (23.31%) | loss: 1.381536 | lrm: 1.00 | dt: 545.41ms | tok/sec: 961,270 | mfu: -1.00 | total time: 1.55m\n",
      "step 00188 (23.46%) | loss: 1.387914 | lrm: 1.00 | dt: 545.12ms | tok/sec: 961,791 | mfu: -1.00 | total time: 1.56m\n",
      "step 00189 (23.57%) | loss: 1.395684 | lrm: 1.00 | dt: 480.93ms | tok/sec: 1,090,147 | mfu: -1.00 | total time: 1.57m\n",
      "step 00190 (23.71%) | loss: 1.396429 | lrm: 1.00 | dt: 490.23ms | tok/sec: 1,069,472 | mfu: -1.00 | total time: 1.58m\n",
      "step 00191 (23.84%) | loss: 1.399288 | lrm: 1.00 | dt: 488.37ms | tok/sec: 1,073,553 | mfu: -1.00 | total time: 1.58m\n",
      "step 00192 (23.96%) | loss: 1.399685 | lrm: 1.00 | dt: 486.71ms | tok/sec: 1,077,204 | mfu: -1.00 | total time: 1.59m\n",
      "step 00193 (24.07%) | loss: 1.409330 | lrm: 1.00 | dt: 491.76ms | tok/sec: 1,066,141 | mfu: -1.00 | total time: 1.60m\n",
      "step 00194 (24.20%) | loss: 1.412380 | lrm: 1.00 | dt: 536.95ms | tok/sec: 976,412 | mfu: -1.00 | total time: 1.61m\n",
      "step 00195 (24.34%) | loss: 1.412983 | lrm: 1.00 | dt: 540.76ms | tok/sec: 969,538 | mfu: -1.00 | total time: 1.62m\n",
      "step 00196 (24.45%) | loss: 1.418228 | lrm: 1.00 | dt: 481.64ms | tok/sec: 1,088,543 | mfu: -1.00 | total time: 1.63m\n",
      "step 00197 (24.59%) | loss: 1.418031 | lrm: 1.00 | dt: 550.67ms | tok/sec: 952,087 | mfu: -1.00 | total time: 1.63m\n",
      "step 00198 (24.70%) | loss: 1.415943 | lrm: 1.00 | dt: 539.72ms | tok/sec: 971,412 | mfu: -1.00 | total time: 1.64m\n",
      "step 00199 (24.82%) | loss: 1.421686 | lrm: 1.00 | dt: 480.73ms | tok/sec: 1,090,600 | mfu: -1.00 | total time: 1.65m\n",
      "step 00200 (24.96%) | loss: 1.416021 | lrm: 1.00 | dt: 489.52ms | tok/sec: 1,071,032 | mfu: -1.00 | total time: 1.66m\n",
      "step 00201 (25.10%) | loss: 1.426493 | lrm: 1.00 | dt: 489.03ms | tok/sec: 1,072,088 | mfu: -1.00 | total time: 1.67m\n",
      "step 00202 (25.23%) | loss: 1.414469 | lrm: 1.00 | dt: 487.02ms | tok/sec: 1,076,521 | mfu: -1.00 | total time: 1.68m\n",
      "step 00203 (25.35%) | loss: 1.408402 | lrm: 1.00 | dt: 491.55ms | tok/sec: 1,066,602 | mfu: -1.00 | total time: 1.68m\n",
      "step 00204 (25.47%) | loss: 1.400852 | lrm: 1.00 | dt: 486.77ms | tok/sec: 1,077,076 | mfu: -1.00 | total time: 1.69m\n",
      "step 00205 (25.60%) | loss: 1.399256 | lrm: 1.00 | dt: 492.90ms | tok/sec: 1,063,676 | mfu: -1.00 | total time: 1.70m\n",
      "step 00206 (25.72%) | loss: 1.390129 | lrm: 1.00 | dt: 555.08ms | tok/sec: 944,529 | mfu: -1.00 | total time: 1.71m\n",
      "step 00207 (25.83%) | loss: 1.397491 | lrm: 1.00 | dt: 478.30ms | tok/sec: 1,096,151 | mfu: -1.00 | total time: 1.72m\n",
      "step 00208 (25.94%) | loss: 1.398471 | lrm: 1.00 | dt: 492.65ms | tok/sec: 1,064,218 | mfu: -1.00 | total time: 1.73m\n",
      "step 00209 (26.06%) | loss: 1.401104 | lrm: 1.00 | dt: 599.75ms | tok/sec: 874,176 | mfu: -1.00 | total time: 1.74m\n",
      "step 00210 (26.18%) | loss: 1.404324 | lrm: 1.00 | dt: 540.77ms | tok/sec: 969,525 | mfu: -1.00 | total time: 1.75m\n",
      "step 00211 (26.30%) | loss: 1.404671 | lrm: 1.00 | dt: 480.43ms | tok/sec: 1,091,297 | mfu: -1.00 | total time: 1.75m\n",
      "step 00212 (26.45%) | loss: 1.405022 | lrm: 1.00 | dt: 493.04ms | tok/sec: 1,063,377 | mfu: -1.00 | total time: 1.76m\n",
      "step 00213 (26.57%) | loss: 1.401028 | lrm: 1.00 | dt: 487.47ms | tok/sec: 1,075,526 | mfu: -1.00 | total time: 1.77m\n",
      "step 00214 (26.69%) | loss: 1.396712 | lrm: 1.00 | dt: 487.99ms | tok/sec: 1,074,383 | mfu: -1.00 | total time: 1.78m\n",
      "step 00215 (26.82%) | loss: 1.392717 | lrm: 1.00 | dt: 541.94ms | tok/sec: 967,431 | mfu: -1.00 | total time: 1.79m\n",
      "step 00216 (26.93%) | loss: 1.390032 | lrm: 1.00 | dt: 478.66ms | tok/sec: 1,095,329 | mfu: -1.00 | total time: 1.79m\n",
      "step 00217 (27.07%) | loss: 1.387132 | lrm: 1.00 | dt: 550.91ms | tok/sec: 951,678 | mfu: -1.00 | total time: 1.80m\n",
      "step 00218 (27.21%) | loss: 1.390170 | lrm: 1.00 | dt: 599.55ms | tok/sec: 874,462 | mfu: -1.00 | total time: 1.81m\n",
      "step 00219 (27.33%) | loss: 1.388076 | lrm: 1.00 | dt: 538.87ms | tok/sec: 972,939 | mfu: -1.00 | total time: 1.82m\n",
      "step 00220 (27.45%) | loss: 1.405632 | lrm: 1.00 | dt: 537.50ms | tok/sec: 975,419 | mfu: -1.00 | total time: 1.83m\n",
      "step 00221 (27.57%) | loss: 1.406033 | lrm: 1.00 | dt: 481.58ms | tok/sec: 1,088,690 | mfu: -1.00 | total time: 1.84m\n",
      "step 00222 (27.70%) | loss: 1.402295 | lrm: 1.00 | dt: 494.09ms | tok/sec: 1,061,119 | mfu: -1.00 | total time: 1.85m\n",
      "step 00223 (27.82%) | loss: 1.398865 | lrm: 1.00 | dt: 539.49ms | tok/sec: 971,825 | mfu: -1.00 | total time: 1.86m\n",
      "step 00224 (27.94%) | loss: 1.405380 | lrm: 1.00 | dt: 546.12ms | tok/sec: 960,029 | mfu: -1.00 | total time: 1.87m\n",
      "step 00225 (28.07%) | loss: 1.405055 | lrm: 1.00 | dt: 541.48ms | tok/sec: 968,253 | mfu: -1.00 | total time: 1.88m\n",
      "step 00226 (28.20%) | loss: 1.410825 | lrm: 1.00 | dt: 478.67ms | tok/sec: 1,095,298 | mfu: -1.00 | total time: 1.88m\n",
      "step 00227 (28.33%) | loss: 1.404120 | lrm: 1.00 | dt: 490.71ms | tok/sec: 1,068,423 | mfu: -1.00 | total time: 1.89m\n",
      "step 00228 (28.47%) | loss: 1.405333 | lrm: 1.00 | dt: 488.61ms | tok/sec: 1,073,016 | mfu: -1.00 | total time: 1.90m\n",
      "step 00229 (28.60%) | loss: 1.409371 | lrm: 1.00 | dt: 539.63ms | tok/sec: 971,578 | mfu: -1.00 | total time: 1.91m\n",
      "step 00230 (28.72%) | loss: 1.406468 | lrm: 1.00 | dt: 541.39ms | tok/sec: 968,411 | mfu: -1.00 | total time: 1.92m\n",
      "step 00231 (28.84%) | loss: 1.399661 | lrm: 1.00 | dt: 541.56ms | tok/sec: 968,111 | mfu: -1.00 | total time: 1.93m\n",
      "step 00232 (28.97%) | loss: 1.413254 | lrm: 1.00 | dt: 477.65ms | tok/sec: 1,097,634 | mfu: -1.00 | total time: 1.93m\n",
      "step 00233 (29.08%) | loss: 1.407146 | lrm: 1.00 | dt: 491.06ms | tok/sec: 1,067,659 | mfu: -1.00 | total time: 1.94m\n",
      "step 00234 (29.21%) | loss: 1.404341 | lrm: 1.00 | dt: 486.71ms | tok/sec: 1,077,209 | mfu: -1.00 | total time: 1.95m\n",
      "step 00235 (29.34%) | loss: 1.412486 | lrm: 1.00 | dt: 488.55ms | tok/sec: 1,073,140 | mfu: -1.00 | total time: 1.96m\n",
      "step 00236 (29.48%) | loss: 1.406134 | lrm: 1.00 | dt: 492.30ms | tok/sec: 1,064,967 | mfu: -1.00 | total time: 1.97m\n",
      "step 00237 (29.58%) | loss: 1.408850 | lrm: 1.00 | dt: 486.67ms | tok/sec: 1,077,302 | mfu: -1.00 | total time: 1.98m\n",
      "step 00238 (29.69%) | loss: 1.417978 | lrm: 1.00 | dt: 544.59ms | tok/sec: 962,720 | mfu: -1.00 | total time: 1.98m\n",
      "step 00239 (29.81%) | loss: 1.413220 | lrm: 1.00 | dt: 543.66ms | tok/sec: 964,367 | mfu: -1.00 | total time: 1.99m\n",
      "step 00240 (29.93%) | loss: 1.412945 | lrm: 1.00 | dt: 540.54ms | tok/sec: 969,933 | mfu: -1.00 | total time: 2.00m\n",
      "step 00241 (30.05%) | loss: 1.418118 | lrm: 1.00 | dt: 545.24ms | tok/sec: 961,576 | mfu: -1.00 | total time: 2.01m\n",
      "step 00242 (30.19%) | loss: 1.425164 | lrm: 1.00 | dt: 476.45ms | tok/sec: 1,100,407 | mfu: -1.00 | total time: 2.02m\n",
      "step 00243 (30.29%) | loss: 1.431544 | lrm: 1.00 | dt: 543.12ms | tok/sec: 965,319 | mfu: -1.00 | total time: 2.03m\n",
      "step 00244 (30.42%) | loss: 1.426576 | lrm: 1.00 | dt: 478.87ms | tok/sec: 1,094,844 | mfu: -1.00 | total time: 2.04m\n",
      "step 00245 (30.53%) | loss: 1.431517 | lrm: 1.00 | dt: 494.06ms | tok/sec: 1,061,178 | mfu: -1.00 | total time: 2.04m\n",
      "step 00246 (30.66%) | loss: 1.436916 | lrm: 1.00 | dt: 487.89ms | tok/sec: 1,074,611 | mfu: -1.00 | total time: 2.05m\n",
      "step 00247 (30.76%) | loss: 1.434393 | lrm: 1.00 | dt: 488.67ms | tok/sec: 1,072,887 | mfu: -1.00 | total time: 2.06m\n",
      "step 00248 (30.90%) | loss: 1.430022 | lrm: 1.00 | dt: 548.27ms | tok/sec: 956,260 | mfu: -1.00 | total time: 2.07m\n",
      "step 00249 (31.02%) | loss: 1.421562 | lrm: 1.00 | dt: 478.64ms | tok/sec: 1,095,374 | mfu: -1.00 | total time: 2.08m\n",
      "step 00250 (31.15%) | loss: 1.414191 | lrm: 1.00 | dt: 490.14ms | tok/sec: 1,069,667 | mfu: -1.00 | total time: 2.09m\n",
      "step 00251 (31.27%) | loss: 1.406320 | lrm: 1.00 | dt: 488.99ms | tok/sec: 1,072,175 | mfu: -1.00 | total time: 2.09m\n",
      "step 00252 (31.39%) | loss: 1.401393 | lrm: 1.00 | dt: 548.28ms | tok/sec: 956,249 | mfu: -1.00 | total time: 2.10m\n",
      "step 00253 (31.51%) | loss: 1.408542 | lrm: 1.00 | dt: 535.30ms | tok/sec: 979,425 | mfu: -1.00 | total time: 2.11m\n",
      "step 00254 (31.63%) | loss: 1.400324 | lrm: 1.00 | dt: 477.95ms | tok/sec: 1,096,948 | mfu: -1.00 | total time: 2.12m\n",
      "step 00255 (31.75%) | loss: 1.397770 | lrm: 1.00 | dt: 488.36ms | tok/sec: 1,073,572 | mfu: -1.00 | total time: 2.13m\n",
      "step 00256 (31.88%) | loss: 1.397363 | lrm: 1.00 | dt: 491.22ms | tok/sec: 1,067,320 | mfu: -1.00 | total time: 2.14m\n",
      "step 00257 (32.00%) | loss: 1.387112 | lrm: 1.00 | dt: 549.90ms | tok/sec: 953,418 | mfu: -1.00 | total time: 2.15m\n",
      "step 00258 (32.12%) | loss: 1.392849 | lrm: 1.00 | dt: 479.56ms | tok/sec: 1,093,275 | mfu: -1.00 | total time: 2.15m\n",
      "step 00259 (32.26%) | loss: 1.388853 | lrm: 1.00 | dt: 493.29ms | tok/sec: 1,062,833 | mfu: -1.00 | total time: 2.16m\n",
      "step 00260 (32.38%) | loss: 1.383648 | lrm: 1.00 | dt: 488.12ms | tok/sec: 1,074,100 | mfu: -1.00 | total time: 2.17m\n",
      "step 00261 (32.50%) | loss: 1.391944 | lrm: 1.00 | dt: 549.05ms | tok/sec: 954,898 | mfu: -1.00 | total time: 2.18m\n",
      "step 00262 (32.62%) | loss: 1.393289 | lrm: 1.00 | dt: 478.78ms | tok/sec: 1,095,059 | mfu: -1.00 | total time: 2.19m\n",
      "step 00263 (32.75%) | loss: 1.389731 | lrm: 1.00 | dt: 493.96ms | tok/sec: 1,061,394 | mfu: -1.00 | total time: 2.20m\n",
      "step 00264 (32.87%) | loss: 1.401024 | lrm: 1.00 | dt: 482.99ms | tok/sec: 1,085,495 | mfu: -1.00 | total time: 2.20m\n",
      "step 00265 (32.97%) | loss: 1.403197 | lrm: 1.00 | dt: 490.22ms | tok/sec: 1,069,502 | mfu: -1.00 | total time: 2.21m\n",
      "step 00266 (33.10%) | loss: 1.399025 | lrm: 1.00 | dt: 489.85ms | tok/sec: 1,070,311 | mfu: -1.00 | total time: 2.22m\n",
      "step 00267 (33.22%) | loss: 1.408114 | lrm: 1.00 | dt: 546.68ms | tok/sec: 959,031 | mfu: -1.00 | total time: 2.23m\n",
      "step 00268 (33.33%) | loss: 1.414348 | lrm: 1.00 | dt: 538.76ms | tok/sec: 973,140 | mfu: -1.00 | total time: 2.24m\n",
      "step 00269 (33.44%) | loss: 1.422509 | lrm: 1.00 | dt: 479.52ms | tok/sec: 1,093,352 | mfu: -1.00 | total time: 2.25m\n",
      "step 00270 (33.57%) | loss: 1.416411 | lrm: 1.00 | dt: 492.25ms | tok/sec: 1,065,085 | mfu: -1.00 | total time: 2.25m\n",
      "step 00271 (33.69%) | loss: 1.419342 | lrm: 1.00 | dt: 487.50ms | tok/sec: 1,075,454 | mfu: -1.00 | total time: 2.26m\n",
      "step 00272 (33.80%) | loss: 1.414351 | lrm: 1.00 | dt: 487.42ms | tok/sec: 1,075,628 | mfu: -1.00 | total time: 2.27m\n",
      "step 00273 (33.95%) | loss: 1.409328 | lrm: 1.00 | dt: 487.93ms | tok/sec: 1,074,524 | mfu: -1.00 | total time: 2.28m\n",
      "step 00274 (34.07%) | loss: 1.396962 | lrm: 1.00 | dt: 488.15ms | tok/sec: 1,074,037 | mfu: -1.00 | total time: 2.29m\n",
      "step 00275 (34.19%) | loss: 1.390859 | lrm: 1.00 | dt: 491.33ms | tok/sec: 1,067,080 | mfu: -1.00 | total time: 2.30m\n",
      "step 00276 (34.31%) | loss: 1.389914 | lrm: 1.00 | dt: 549.76ms | tok/sec: 953,663 | mfu: -1.00 | total time: 2.30m\n",
      "step 00277 (34.43%) | loss: 1.402505 | lrm: 1.00 | dt: 480.13ms | tok/sec: 1,091,981 | mfu: -1.00 | total time: 2.31m\n",
      "step 00278 (34.53%) | loss: 1.399668 | lrm: 1.00 | dt: 491.75ms | tok/sec: 1,066,171 | mfu: -1.00 | total time: 2.32m\n",
      "step 00279 (34.65%) | loss: 1.395089 | lrm: 1.00 | dt: 485.13ms | tok/sec: 1,080,720 | mfu: -1.00 | total time: 2.33m\n",
      "step 00280 (34.77%) | loss: 1.389518 | lrm: 1.00 | dt: 488.23ms | tok/sec: 1,073,858 | mfu: -1.00 | total time: 2.34m\n",
      "step 00281 (34.89%) | loss: 1.383274 | lrm: 1.00 | dt: 488.51ms | tok/sec: 1,073,249 | mfu: -1.00 | total time: 2.34m\n",
      "step 00282 (35.02%) | loss: 1.374380 | lrm: 1.00 | dt: 487.43ms | tok/sec: 1,075,611 | mfu: -1.00 | total time: 2.35m\n",
      "step 00283 (35.15%) | loss: 1.382419 | lrm: 1.00 | dt: 488.23ms | tok/sec: 1,073,861 | mfu: -1.00 | total time: 2.36m\n",
      "step 00284 (35.27%) | loss: 1.390734 | lrm: 1.00 | dt: 486.47ms | tok/sec: 1,077,729 | mfu: -1.00 | total time: 2.37m\n",
      "step 00285 (35.37%) | loss: 1.396826 | lrm: 1.00 | dt: 540.31ms | tok/sec: 970,344 | mfu: -1.00 | total time: 2.38m\n",
      "step 00286 (35.48%) | loss: 1.390654 | lrm: 1.00 | dt: 478.35ms | tok/sec: 1,096,031 | mfu: -1.00 | total time: 2.39m\n",
      "step 00287 (35.59%) | loss: 1.400431 | lrm: 1.00 | dt: 493.08ms | tok/sec: 1,063,299 | mfu: -1.00 | total time: 2.39m\n",
      "step 00288 (35.71%) | loss: 1.406180 | lrm: 1.00 | dt: 488.38ms | tok/sec: 1,073,533 | mfu: -1.00 | total time: 2.40m\n",
      "step 00289 (35.84%) | loss: 1.395668 | lrm: 1.00 | dt: 491.90ms | tok/sec: 1,065,834 | mfu: -1.00 | total time: 2.41m\n",
      "step 00290 (35.96%) | loss: 1.387566 | lrm: 1.00 | dt: 487.90ms | tok/sec: 1,074,580 | mfu: -1.00 | total time: 2.42m\n",
      "step 00291 (36.09%) | loss: 1.406812 | lrm: 1.00 | dt: 488.02ms | tok/sec: 1,074,320 | mfu: -1.00 | total time: 2.43m\n",
      "step 00292 (36.22%) | loss: 1.389002 | lrm: 1.00 | dt: 551.41ms | tok/sec: 950,815 | mfu: -1.00 | total time: 2.44m\n",
      "step 00293 (36.34%) | loss: 1.379258 | lrm: 1.00 | dt: 479.10ms | tok/sec: 1,094,321 | mfu: -1.00 | total time: 2.44m\n",
      "step 00294 (36.46%) | loss: 1.392097 | lrm: 1.00 | dt: 1549.55ms | tok/sec: 338,348 | mfu: -1.00 | total time: 2.47m\n",
      "step 00295 (36.60%) | loss: 1.392097 | lrm: 1.00 | dt: 478.67ms | tok/sec: 1,095,307 | mfu: -1.00 | total time: 2.48m\n",
      "step 00296 (36.73%) | loss: 1.382413 | lrm: 1.00 | dt: 490.99ms | tok/sec: 1,067,822 | mfu: -1.00 | total time: 2.49m\n",
      "step 00297 (36.86%) | loss: 1.377420 | lrm: 1.00 | dt: 483.39ms | tok/sec: 1,084,596 | mfu: -1.00 | total time: 2.49m\n",
      "step 00298 (37.00%) | loss: 1.386032 | lrm: 1.00 | dt: 488.22ms | tok/sec: 1,073,871 | mfu: -1.00 | total time: 2.50m\n",
      "step 00299 (37.10%) | loss: 1.378782 | lrm: 1.00 | dt: 552.08ms | tok/sec: 949,663 | mfu: -1.00 | total time: 2.51m\n",
      "step 00300 (37.23%) | loss: 1.378687 | lrm: 1.00 | dt: 539.28ms | tok/sec: 972,204 | mfu: -1.00 | total time: 2.52m\n",
      "step 00300 | Validation bpb: 0.4328\n",
      "step 00301 (37.34%) | loss: 1.369279 | lrm: 1.00 | dt: 476.92ms | tok/sec: 1,099,311 | mfu: -1.00 | total time: 2.53m\n",
      "step 00302 (37.45%) | loss: 1.364507 | lrm: 1.00 | dt: 535.06ms | tok/sec: 979,870 | mfu: -1.00 | total time: 2.54m\n",
      "step 00303 (37.58%) | loss: 1.368682 | lrm: 1.00 | dt: 479.13ms | tok/sec: 1,094,255 | mfu: -1.00 | total time: 2.55m\n",
      "step 00304 (37.71%) | loss: 1.365112 | lrm: 1.00 | dt: 488.81ms | tok/sec: 1,072,584 | mfu: -1.00 | total time: 2.55m\n",
      "step 00305 (37.84%) | loss: 1.363294 | lrm: 1.00 | dt: 485.93ms | tok/sec: 1,078,931 | mfu: -1.00 | total time: 2.56m\n",
      "step 00306 (37.97%) | loss: 1.370589 | lrm: 1.00 | dt: 483.48ms | tok/sec: 1,084,398 | mfu: -1.00 | total time: 2.57m\n",
      "step 00307 (38.09%) | loss: 1.373318 | lrm: 1.00 | dt: 490.10ms | tok/sec: 1,069,766 | mfu: -1.00 | total time: 2.58m\n",
      "step 00308 (38.20%) | loss: 1.383807 | lrm: 1.00 | dt: 483.15ms | tok/sec: 1,085,150 | mfu: -1.00 | total time: 2.59m\n",
      "step 00309 (38.31%) | loss: 1.392280 | lrm: 1.00 | dt: 490.27ms | tok/sec: 1,069,391 | mfu: -1.00 | total time: 2.59m\n",
      "step 00310 (38.44%) | loss: 1.392270 | lrm: 1.00 | dt: 487.10ms | tok/sec: 1,076,341 | mfu: -1.00 | total time: 2.60m\n",
      "step 00311 (38.54%) | loss: 1.383833 | lrm: 1.00 | dt: 487.21ms | tok/sec: 1,076,110 | mfu: -1.00 | total time: 2.61m\n",
      "step 00312 (38.66%) | loss: 1.380241 | lrm: 1.00 | dt: 485.47ms | tok/sec: 1,079,967 | mfu: -1.00 | total time: 2.62m\n",
      "step 00313 (38.79%) | loss: 1.369791 | lrm: 1.00 | dt: 488.14ms | tok/sec: 1,074,042 | mfu: -1.00 | total time: 2.63m\n",
      "step 00314 (38.90%) | loss: 1.378927 | lrm: 1.00 | dt: 487.46ms | tok/sec: 1,075,544 | mfu: -1.00 | total time: 2.63m\n",
      "step 00315 (39.01%) | loss: 1.370232 | lrm: 1.00 | dt: 488.37ms | tok/sec: 1,073,551 | mfu: -1.00 | total time: 2.64m\n",
      "step 00316 (39.12%) | loss: 1.378733 | lrm: 1.00 | dt: 484.42ms | tok/sec: 1,082,299 | mfu: -1.00 | total time: 2.65m\n",
      "step 00317 (39.25%) | loss: 1.376064 | lrm: 1.00 | dt: 486.68ms | tok/sec: 1,077,282 | mfu: -1.00 | total time: 2.66m\n",
      "step 00318 (39.35%) | loss: 1.380037 | lrm: 1.00 | dt: 488.20ms | tok/sec: 1,073,915 | mfu: -1.00 | total time: 2.67m\n",
      "step 00319 (39.46%) | loss: 1.374900 | lrm: 1.00 | dt: 488.47ms | tok/sec: 1,073,333 | mfu: -1.00 | total time: 2.68m\n",
      "step 00320 (39.59%) | loss: 1.372789 | lrm: 1.00 | dt: 486.85ms | tok/sec: 1,076,895 | mfu: -1.00 | total time: 2.68m\n",
      "step 00321 (39.72%) | loss: 1.367381 | lrm: 1.00 | dt: 549.44ms | tok/sec: 954,218 | mfu: -1.00 | total time: 2.69m\n",
      "step 00322 (39.83%) | loss: 1.363823 | lrm: 1.00 | dt: 479.95ms | tok/sec: 1,092,385 | mfu: -1.00 | total time: 2.70m\n",
      "step 00323 (39.94%) | loss: 1.367379 | lrm: 1.00 | dt: 552.04ms | tok/sec: 949,729 | mfu: -1.00 | total time: 2.71m\n",
      "step 00324 (40.06%) | loss: 1.371901 | lrm: 1.00 | dt: 552.91ms | tok/sec: 948,231 | mfu: -1.00 | total time: 2.72m\n",
      "step 00325 (40.19%) | loss: 1.359935 | lrm: 1.00 | dt: 478.22ms | tok/sec: 1,096,338 | mfu: -1.00 | total time: 2.73m\n",
      "step 00326 (40.31%) | loss: 1.365990 | lrm: 1.00 | dt: 488.93ms | tok/sec: 1,072,321 | mfu: -1.00 | total time: 2.74m\n",
      "step 00327 (40.43%) | loss: 1.379206 | lrm: 1.00 | dt: 484.17ms | tok/sec: 1,082,863 | mfu: -1.00 | total time: 2.74m\n",
      "step 00328 (40.56%) | loss: 1.378952 | lrm: 1.00 | dt: 484.78ms | tok/sec: 1,081,498 | mfu: -1.00 | total time: 2.75m\n",
      "step 00329 (40.68%) | loss: 1.366844 | lrm: 1.00 | dt: 487.04ms | tok/sec: 1,076,467 | mfu: -1.00 | total time: 2.76m\n",
      "step 00330 (40.80%) | loss: 1.359212 | lrm: 1.00 | dt: 485.66ms | tok/sec: 1,079,545 | mfu: -1.00 | total time: 2.77m\n",
      "step 00331 (40.91%) | loss: 1.352905 | lrm: 1.00 | dt: 490.67ms | tok/sec: 1,068,506 | mfu: -1.00 | total time: 2.78m\n",
      "step 00332 (41.02%) | loss: 1.354535 | lrm: 1.00 | dt: 483.41ms | tok/sec: 1,084,557 | mfu: -1.00 | total time: 2.78m\n",
      "step 00333 (41.15%) | loss: 1.345286 | lrm: 1.00 | dt: 489.95ms | tok/sec: 1,070,088 | mfu: -1.00 | total time: 2.79m\n",
      "step 00334 (41.28%) | loss: 1.346357 | lrm: 1.00 | dt: 486.22ms | tok/sec: 1,078,283 | mfu: -1.00 | total time: 2.80m\n",
      "step 00335 (41.40%) | loss: 1.338444 | lrm: 1.00 | dt: 486.59ms | tok/sec: 1,077,468 | mfu: -1.00 | total time: 2.81m\n",
      "step 00336 (41.54%) | loss: 1.343212 | lrm: 1.00 | dt: 485.21ms | tok/sec: 1,080,533 | mfu: -1.00 | total time: 2.82m\n",
      "step 00337 (41.68%) | loss: 1.343245 | lrm: 1.00 | dt: 486.87ms | tok/sec: 1,076,858 | mfu: -1.00 | total time: 2.82m\n",
      "step 00338 (41.79%) | loss: 1.340100 | lrm: 1.00 | dt: 552.94ms | tok/sec: 948,175 | mfu: -1.00 | total time: 2.83m\n",
      "step 00339 (41.93%) | loss: 1.339313 | lrm: 1.00 | dt: 477.36ms | tok/sec: 1,098,318 | mfu: -1.00 | total time: 2.84m\n",
      "step 00340 (42.06%) | loss: 1.349346 | lrm: 1.00 | dt: 487.86ms | tok/sec: 1,074,677 | mfu: -1.00 | total time: 2.85m\n",
      "step 00341 (42.19%) | loss: 1.343762 | lrm: 1.00 | dt: 485.43ms | tok/sec: 1,080,040 | mfu: -1.00 | total time: 2.86m\n",
      "step 00342 (42.30%) | loss: 1.353965 | lrm: 1.00 | dt: 486.22ms | tok/sec: 1,078,287 | mfu: -1.00 | total time: 2.87m\n",
      "step 00343 (42.43%) | loss: 1.346427 | lrm: 1.00 | dt: 547.15ms | tok/sec: 958,212 | mfu: -1.00 | total time: 2.87m\n",
      "step 00344 (42.57%) | loss: 1.340344 | lrm: 1.00 | dt: 476.98ms | tok/sec: 1,099,186 | mfu: -1.00 | total time: 2.88m\n",
      "step 00345 (42.70%) | loss: 1.336602 | lrm: 1.00 | dt: 551.94ms | tok/sec: 949,902 | mfu: -1.00 | total time: 2.89m\n",
      "step 00346 (42.82%) | loss: 1.334370 | lrm: 1.00 | dt: 538.96ms | tok/sec: 972,779 | mfu: -1.00 | total time: 2.90m\n",
      "step 00347 (42.94%) | loss: 1.336141 | lrm: 1.00 | dt: 477.91ms | tok/sec: 1,097,033 | mfu: -1.00 | total time: 2.91m\n",
      "step 00348 (43.07%) | loss: 1.333423 | lrm: 1.00 | dt: 487.02ms | tok/sec: 1,076,523 | mfu: -1.00 | total time: 2.92m\n",
      "step 00349 (43.19%) | loss: 1.329392 | lrm: 1.00 | dt: 489.95ms | tok/sec: 1,070,095 | mfu: -1.00 | total time: 2.93m\n",
      "step 00350 (43.31%) | loss: 1.333860 | lrm: 1.00 | dt: 546.25ms | tok/sec: 959,787 | mfu: -1.00 | total time: 2.93m\n",
      "step 00351 (43.44%) | loss: 1.337990 | lrm: 1.00 | dt: 477.70ms | tok/sec: 1,097,536 | mfu: -1.00 | total time: 2.94m\n",
      "step 00352 (43.57%) | loss: 1.329077 | lrm: 1.00 | dt: 489.44ms | tok/sec: 1,071,209 | mfu: -1.00 | total time: 2.95m\n",
      "step 00353 (43.70%) | loss: 1.330504 | lrm: 1.00 | dt: 486.48ms | tok/sec: 1,077,707 | mfu: -1.00 | total time: 2.96m\n",
      "step 00354 (43.80%) | loss: 1.353679 | lrm: 1.00 | dt: 484.64ms | tok/sec: 1,081,818 | mfu: -1.00 | total time: 2.97m\n",
      "step 00355 (43.94%) | loss: 1.350880 | lrm: 1.00 | dt: 488.88ms | tok/sec: 1,072,425 | mfu: -1.00 | total time: 2.97m\n",
      "step 00356 (44.05%) | loss: 1.347120 | lrm: 1.00 | dt: 485.29ms | tok/sec: 1,080,368 | mfu: -1.00 | total time: 2.98m\n",
      "step 00357 (44.15%) | loss: 1.334631 | lrm: 1.00 | dt: 488.10ms | tok/sec: 1,074,142 | mfu: -1.00 | total time: 2.99m\n",
      "step 00358 (44.28%) | loss: 1.319049 | lrm: 1.00 | dt: 486.60ms | tok/sec: 1,077,451 | mfu: -1.00 | total time: 3.00m\n",
      "step 00359 (44.42%) | loss: 1.314736 | lrm: 1.00 | dt: 486.49ms | tok/sec: 1,077,692 | mfu: -1.00 | total time: 3.01m\n",
      "step 00360 (44.55%) | loss: 1.316394 | lrm: 1.00 | dt: 548.57ms | tok/sec: 955,733 | mfu: -1.00 | total time: 3.02m\n",
      "step 00361 (44.68%) | loss: 1.314989 | lrm: 1.00 | dt: 475.92ms | tok/sec: 1,101,623 | mfu: -1.00 | total time: 3.02m\n",
      "step 00362 (44.82%) | loss: 1.315497 | lrm: 1.00 | dt: 489.29ms | tok/sec: 1,071,525 | mfu: -1.00 | total time: 3.03m\n",
      "step 00363 (44.94%) | loss: 1.313750 | lrm: 1.00 | dt: 487.02ms | tok/sec: 1,076,533 | mfu: -1.00 | total time: 3.04m\n",
      "step 00364 (45.09%) | loss: 1.320539 | lrm: 1.00 | dt: 485.49ms | tok/sec: 1,079,913 | mfu: -1.00 | total time: 3.05m\n",
      "step 00365 (45.20%) | loss: 1.317833 | lrm: 1.00 | dt: 488.60ms | tok/sec: 1,073,050 | mfu: -1.00 | total time: 3.06m\n",
      "step 00366 (45.32%) | loss: 1.311163 | lrm: 1.00 | dt: 486.12ms | tok/sec: 1,078,511 | mfu: -1.00 | total time: 3.06m\n",
      "step 00367 (45.45%) | loss: 1.309777 | lrm: 1.00 | dt: 487.63ms | tok/sec: 1,075,174 | mfu: -1.00 | total time: 3.07m\n",
      "step 00368 (45.59%) | loss: 1.322766 | lrm: 1.00 | dt: 484.78ms | tok/sec: 1,081,500 | mfu: -1.00 | total time: 3.08m\n",
      "step 00369 (45.71%) | loss: 1.332706 | lrm: 1.00 | dt: 489.41ms | tok/sec: 1,071,276 | mfu: -1.00 | total time: 3.09m\n",
      "step 00370 (45.83%) | loss: 1.335200 | lrm: 1.00 | dt: 486.68ms | tok/sec: 1,077,283 | mfu: -1.00 | total time: 3.10m\n",
      "step 00371 (45.98%) | loss: 1.326839 | lrm: 1.00 | dt: 485.74ms | tok/sec: 1,079,353 | mfu: -1.00 | total time: 3.11m\n",
      "step 00372 (46.11%) | loss: 1.320184 | lrm: 1.00 | dt: 487.89ms | tok/sec: 1,074,593 | mfu: -1.00 | total time: 3.11m\n",
      "step 00373 (46.24%) | loss: 1.324962 | lrm: 1.00 | dt: 485.58ms | tok/sec: 1,079,718 | mfu: -1.00 | total time: 3.12m\n",
      "step 00374 (46.37%) | loss: 1.333457 | lrm: 1.00 | dt: 485.33ms | tok/sec: 1,080,270 | mfu: -1.00 | total time: 3.13m\n",
      "step 00375 (46.49%) | loss: 1.332827 | lrm: 1.00 | dt: 489.89ms | tok/sec: 1,070,212 | mfu: -1.00 | total time: 3.14m\n",
      "step 00376 (46.60%) | loss: 1.337788 | lrm: 1.00 | dt: 486.85ms | tok/sec: 1,076,889 | mfu: -1.00 | total time: 3.15m\n",
      "step 00377 (46.74%) | loss: 1.332485 | lrm: 1.00 | dt: 487.70ms | tok/sec: 1,075,013 | mfu: -1.00 | total time: 3.15m\n",
      "step 00378 (46.87%) | loss: 1.343895 | lrm: 1.00 | dt: 483.28ms | tok/sec: 1,084,847 | mfu: -1.00 | total time: 3.16m\n",
      "step 00379 (47.01%) | loss: 1.347154 | lrm: 1.00 | dt: 487.90ms | tok/sec: 1,074,571 | mfu: -1.00 | total time: 3.17m\n",
      "step 00380 (47.13%) | loss: 1.349130 | lrm: 1.00 | dt: 485.40ms | tok/sec: 1,080,120 | mfu: -1.00 | total time: 3.18m\n",
      "step 00381 (47.26%) | loss: 1.352050 | lrm: 1.00 | dt: 486.51ms | tok/sec: 1,077,661 | mfu: -1.00 | total time: 3.19m\n",
      "step 00382 (47.39%) | loss: 1.349276 | lrm: 1.00 | dt: 489.35ms | tok/sec: 1,071,394 | mfu: -1.00 | total time: 3.19m\n",
      "step 00383 (47.51%) | loss: 1.341870 | lrm: 1.00 | dt: 487.22ms | tok/sec: 1,076,083 | mfu: -1.00 | total time: 3.20m\n",
      "step 00384 (47.64%) | loss: 1.351593 | lrm: 1.00 | dt: 487.30ms | tok/sec: 1,075,900 | mfu: -1.00 | total time: 3.21m\n",
      "step 00385 (47.78%) | loss: 1.362167 | lrm: 1.00 | dt: 489.66ms | tok/sec: 1,070,716 | mfu: -1.00 | total time: 3.22m\n",
      "step 00386 (47.92%) | loss: 1.359820 | lrm: 1.00 | dt: 485.94ms | tok/sec: 1,078,922 | mfu: -1.00 | total time: 3.23m\n",
      "step 00387 (48.06%) | loss: 1.361838 | lrm: 1.00 | dt: 489.46ms | tok/sec: 1,071,163 | mfu: -1.00 | total time: 3.24m\n",
      "step 00388 (48.17%) | loss: 1.351914 | lrm: 1.00 | dt: 486.45ms | tok/sec: 1,077,776 | mfu: -1.00 | total time: 3.24m\n",
      "step 00389 (48.29%) | loss: 1.369290 | lrm: 1.00 | dt: 489.60ms | tok/sec: 1,070,846 | mfu: -1.00 | total time: 3.25m\n",
      "step 00390 (48.43%) | loss: 1.362512 | lrm: 1.00 | dt: 482.99ms | tok/sec: 1,085,508 | mfu: -1.00 | total time: 3.26m\n",
      "step 00391 (48.55%) | loss: 1.356474 | lrm: 1.00 | dt: 488.90ms | tok/sec: 1,072,384 | mfu: -1.00 | total time: 3.27m\n",
      "step 00392 (48.66%) | loss: 1.356811 | lrm: 1.00 | dt: 487.00ms | tok/sec: 1,076,576 | mfu: -1.00 | total time: 3.28m\n",
      "step 00393 (48.78%) | loss: 1.358265 | lrm: 1.00 | dt: 486.50ms | tok/sec: 1,077,681 | mfu: -1.00 | total time: 3.28m\n",
      "step 00394 (48.90%) | loss: 1.361304 | lrm: 1.00 | dt: 489.43ms | tok/sec: 1,071,211 | mfu: -1.00 | total time: 3.29m\n",
      "step 00395 (49.03%) | loss: 1.362953 | lrm: 1.00 | dt: 484.43ms | tok/sec: 1,082,285 | mfu: -1.00 | total time: 3.30m\n",
      "step 00396 (49.17%) | loss: 1.351024 | lrm: 1.00 | dt: 488.71ms | tok/sec: 1,072,795 | mfu: -1.00 | total time: 3.31m\n",
      "step 00397 (49.30%) | loss: 1.348546 | lrm: 1.00 | dt: 486.13ms | tok/sec: 1,078,496 | mfu: -1.00 | total time: 3.32m\n",
      "step 00398 (49.44%) | loss: 1.352820 | lrm: 1.00 | dt: 486.40ms | tok/sec: 1,077,891 | mfu: -1.00 | total time: 3.32m\n",
      "step 00399 (49.55%) | loss: 1.351294 | lrm: 1.00 | dt: 485.82ms | tok/sec: 1,079,180 | mfu: -1.00 | total time: 3.33m\n",
      "step 00400 (49.67%) | loss: 1.362326 | lrm: 1.00 | dt: 489.34ms | tok/sec: 1,071,415 | mfu: -1.00 | total time: 3.34m\n",
      "step 00401 (49.81%) | loss: 1.357484 | lrm: 1.00 | dt: 485.16ms | tok/sec: 1,080,642 | mfu: -1.00 | total time: 3.35m\n",
      "step 00402 (49.94%) | loss: 1.348135 | lrm: 1.00 | dt: 489.10ms | tok/sec: 1,071,950 | mfu: -1.00 | total time: 3.36m\n",
      "step 00403 (50.06%) | loss: 1.335260 | lrm: 1.00 | dt: 486.06ms | tok/sec: 1,078,644 | mfu: -1.00 | total time: 3.37m\n",
      "step 00404 (50.18%) | loss: 1.344521 | lrm: 1.00 | dt: 488.92ms | tok/sec: 1,072,340 | mfu: -1.00 | total time: 3.37m\n",
      "step 00405 (50.29%) | loss: 1.330580 | lrm: 1.00 | dt: 487.09ms | tok/sec: 1,076,369 | mfu: -1.00 | total time: 3.38m\n",
      "step 00406 (50.41%) | loss: 1.331470 | lrm: 1.00 | dt: 486.12ms | tok/sec: 1,078,510 | mfu: -1.00 | total time: 3.39m\n",
      "step 00407 (50.54%) | loss: 1.328884 | lrm: 1.00 | dt: 487.48ms | tok/sec: 1,075,514 | mfu: -1.00 | total time: 3.40m\n",
      "step 00408 (50.65%) | loss: 1.327320 | lrm: 1.00 | dt: 485.74ms | tok/sec: 1,079,348 | mfu: -1.00 | total time: 3.41m\n",
      "step 00409 (50.78%) | loss: 1.329104 | lrm: 1.00 | dt: 487.12ms | tok/sec: 1,076,300 | mfu: -1.00 | total time: 3.41m\n",
      "step 00410 (50.90%) | loss: 1.323446 | lrm: 1.00 | dt: 485.26ms | tok/sec: 1,080,435 | mfu: -1.00 | total time: 3.42m\n",
      "step 00411 (51.01%) | loss: 1.322676 | lrm: 1.00 | dt: 485.46ms | tok/sec: 1,079,980 | mfu: -1.00 | total time: 3.43m\n",
      "step 00412 (51.13%) | loss: 1.324008 | lrm: 1.00 | dt: 489.10ms | tok/sec: 1,071,948 | mfu: -1.00 | total time: 3.44m\n",
      "step 00413 (51.25%) | loss: 1.321694 | lrm: 1.00 | dt: 484.85ms | tok/sec: 1,081,339 | mfu: -1.00 | total time: 3.45m\n",
      "step 00414 (51.38%) | loss: 1.321671 | lrm: 1.00 | dt: 489.38ms | tok/sec: 1,071,329 | mfu: -1.00 | total time: 3.45m\n",
      "step 00415 (51.49%) | loss: 1.317725 | lrm: 1.00 | dt: 486.55ms | tok/sec: 1,077,568 | mfu: -1.00 | total time: 3.46m\n",
      "step 00416 (51.60%) | loss: 1.313487 | lrm: 1.00 | dt: 486.43ms | tok/sec: 1,077,822 | mfu: -1.00 | total time: 3.47m\n",
      "step 00417 (51.72%) | loss: 1.312227 | lrm: 1.00 | dt: 486.57ms | tok/sec: 1,077,510 | mfu: -1.00 | total time: 3.48m\n",
      "step 00418 (51.83%) | loss: 1.311055 | lrm: 1.00 | dt: 489.75ms | tok/sec: 1,070,529 | mfu: -1.00 | total time: 3.49m\n",
      "step 00419 (51.97%) | loss: 1.304812 | lrm: 1.00 | dt: 481.99ms | tok/sec: 1,087,762 | mfu: -1.00 | total time: 3.50m\n",
      "step 00420 (52.09%) | loss: 1.297877 | lrm: 1.00 | dt: 487.80ms | tok/sec: 1,074,804 | mfu: -1.00 | total time: 3.50m\n",
      "step 00421 (52.19%) | loss: 1.298611 | lrm: 1.00 | dt: 485.86ms | tok/sec: 1,079,085 | mfu: -1.00 | total time: 3.51m\n",
      "step 00422 (52.33%) | loss: 1.307089 | lrm: 1.00 | dt: 487.43ms | tok/sec: 1,075,615 | mfu: -1.00 | total time: 3.52m\n",
      "step 00423 (52.46%) | loss: 1.306057 | lrm: 1.00 | dt: 486.02ms | tok/sec: 1,078,734 | mfu: -1.00 | total time: 3.53m\n",
      "step 00424 (52.59%) | loss: 1.298562 | lrm: 1.00 | dt: 485.16ms | tok/sec: 1,080,639 | mfu: -1.00 | total time: 3.54m\n",
      "step 00425 (52.71%) | loss: 1.304673 | lrm: 1.00 | dt: 488.74ms | tok/sec: 1,072,730 | mfu: -1.00 | total time: 3.54m\n",
      "step 00426 (52.81%) | loss: 1.311555 | lrm: 1.00 | dt: 486.59ms | tok/sec: 1,077,481 | mfu: -1.00 | total time: 3.55m\n",
      "step 00427 (52.91%) | loss: 1.310018 | lrm: 1.00 | dt: 483.44ms | tok/sec: 1,084,490 | mfu: -1.00 | total time: 3.56m\n",
      "step 00428 (53.04%) | loss: 1.306815 | lrm: 1.00 | dt: 486.77ms | tok/sec: 1,077,083 | mfu: -1.00 | total time: 3.57m\n",
      "step 00429 (53.15%) | loss: 1.307995 | lrm: 1.00 | dt: 487.44ms | tok/sec: 1,075,603 | mfu: -1.00 | total time: 3.58m\n",
      "step 00430 (53.27%) | loss: 1.320751 | lrm: 1.00 | dt: 485.48ms | tok/sec: 1,079,932 | mfu: -1.00 | total time: 3.58m\n",
      "step 00431 (53.38%) | loss: 1.309535 | lrm: 1.00 | dt: 486.98ms | tok/sec: 1,076,621 | mfu: -1.00 | total time: 3.59m\n",
      "step 00432 (53.49%) | loss: 1.312283 | lrm: 1.00 | dt: 489.34ms | tok/sec: 1,071,411 | mfu: -1.00 | total time: 3.60m\n",
      "step 00433 (53.61%) | loss: 1.318305 | lrm: 1.00 | dt: 487.98ms | tok/sec: 1,074,415 | mfu: -1.00 | total time: 3.61m\n",
      "step 00434 (53.72%) | loss: 1.314105 | lrm: 1.00 | dt: 487.43ms | tok/sec: 1,075,613 | mfu: -1.00 | total time: 3.62m\n",
      "step 00435 (53.85%) | loss: 1.305371 | lrm: 1.00 | dt: 484.51ms | tok/sec: 1,082,108 | mfu: -1.00 | total time: 3.62m\n",
      "step 00436 (53.96%) | loss: 1.301085 | lrm: 1.00 | dt: 488.47ms | tok/sec: 1,073,327 | mfu: -1.00 | total time: 3.63m\n",
      "step 00437 (54.10%) | loss: 1.305965 | lrm: 1.00 | dt: 486.69ms | tok/sec: 1,077,254 | mfu: -1.00 | total time: 3.64m\n",
      "step 00438 (54.21%) | loss: 1.302277 | lrm: 1.00 | dt: 484.30ms | tok/sec: 1,082,575 | mfu: -1.00 | total time: 3.65m\n",
      "step 00439 (54.33%) | loss: 1.299668 | lrm: 1.00 | dt: 486.52ms | tok/sec: 1,077,630 | mfu: -1.00 | total time: 3.66m\n",
      "step 00440 (54.45%) | loss: 1.306122 | lrm: 1.00 | dt: 488.77ms | tok/sec: 1,072,664 | mfu: -1.00 | total time: 3.67m\n",
      "step 00441 (54.58%) | loss: 1.313350 | lrm: 1.00 | dt: 487.69ms | tok/sec: 1,075,047 | mfu: -1.00 | total time: 3.67m\n",
      "step 00442 (54.68%) | loss: 1.315408 | lrm: 1.00 | dt: 485.44ms | tok/sec: 1,080,033 | mfu: -1.00 | total time: 3.68m\n",
      "step 00443 (54.78%) | loss: 1.306322 | lrm: 1.00 | dt: 486.21ms | tok/sec: 1,078,322 | mfu: -1.00 | total time: 3.69m\n",
      "step 00444 (54.88%) | loss: 1.308680 | lrm: 1.00 | dt: 485.88ms | tok/sec: 1,079,039 | mfu: -1.00 | total time: 3.70m\n",
      "step 00445 (55.02%) | loss: 1.303264 | lrm: 1.00 | dt: 487.60ms | tok/sec: 1,075,235 | mfu: -1.00 | total time: 3.71m\n",
      "step 00446 (55.13%) | loss: 1.312690 | lrm: 1.00 | dt: 482.37ms | tok/sec: 1,086,893 | mfu: -1.00 | total time: 3.71m\n",
      "step 00447 (55.25%) | loss: 1.305839 | lrm: 1.00 | dt: 489.09ms | tok/sec: 1,071,960 | mfu: -1.00 | total time: 3.72m\n",
      "step 00448 (55.36%) | loss: 1.304175 | lrm: 1.00 | dt: 485.09ms | tok/sec: 1,080,813 | mfu: -1.00 | total time: 3.73m\n",
      "step 00449 (55.49%) | loss: 1.304376 | lrm: 1.00 | dt: 486.58ms | tok/sec: 1,077,494 | mfu: -1.00 | total time: 3.74m\n",
      "step 00450 (55.60%) | loss: 1.301178 | lrm: 1.00 | dt: 487.05ms | tok/sec: 1,076,446 | mfu: -1.00 | total time: 3.75m\n",
      "step 00450 | Validation bpb: 0.4203\n",
      "step 00451 (55.72%) | loss: 1.308275 | lrm: 1.00 | dt: 478.37ms | tok/sec: 1,095,992 | mfu: -1.00 | total time: 3.75m\n",
      "step 00452 (55.85%) | loss: 1.302261 | lrm: 1.00 | dt: 492.28ms | tok/sec: 1,065,025 | mfu: -1.00 | total time: 3.76m\n",
      "step 00453 (55.95%) | loss: 1.297284 | lrm: 1.00 | dt: 480.34ms | tok/sec: 1,091,503 | mfu: -1.00 | total time: 3.77m\n",
      "step 00454 (56.08%) | loss: 1.297555 | lrm: 1.00 | dt: 485.83ms | tok/sec: 1,079,169 | mfu: -1.00 | total time: 3.78m\n",
      "step 00455 (56.20%) | loss: 1.293555 | lrm: 1.00 | dt: 484.60ms | tok/sec: 1,081,902 | mfu: -1.00 | total time: 3.79m\n",
      "step 00456 (56.32%) | loss: 1.292544 | lrm: 1.00 | dt: 485.51ms | tok/sec: 1,079,881 | mfu: -1.00 | total time: 3.79m\n",
      "step 00457 (56.46%) | loss: 1.301457 | lrm: 1.00 | dt: 484.01ms | tok/sec: 1,083,221 | mfu: -1.00 | total time: 3.80m\n",
      "step 00458 (56.58%) | loss: 1.306288 | lrm: 1.00 | dt: 485.72ms | tok/sec: 1,079,400 | mfu: -1.00 | total time: 3.81m\n",
      "step 00459 (56.69%) | loss: 1.307347 | lrm: 1.00 | dt: 486.39ms | tok/sec: 1,077,917 | mfu: -1.00 | total time: 3.82m\n",
      "step 00460 (56.81%) | loss: 1.306926 | lrm: 1.00 | dt: 486.02ms | tok/sec: 1,078,743 | mfu: -1.00 | total time: 3.83m\n",
      "step 00461 (56.94%) | loss: 1.313254 | lrm: 1.00 | dt: 485.22ms | tok/sec: 1,080,516 | mfu: -1.00 | total time: 3.84m\n",
      "step 00462 (57.06%) | loss: 1.315630 | lrm: 1.00 | dt: 486.19ms | tok/sec: 1,078,367 | mfu: -1.00 | total time: 3.84m\n",
      "step 00463 (57.17%) | loss: 1.317156 | lrm: 1.00 | dt: 484.57ms | tok/sec: 1,081,970 | mfu: -1.00 | total time: 3.85m\n",
      "step 00464 (57.30%) | loss: 1.313598 | lrm: 1.00 | dt: 486.61ms | tok/sec: 1,077,440 | mfu: -1.00 | total time: 3.86m\n",
      "step 00465 (57.44%) | loss: 1.307620 | lrm: 1.00 | dt: 486.81ms | tok/sec: 1,076,984 | mfu: -1.00 | total time: 3.87m\n",
      "step 00466 (57.57%) | loss: 1.309836 | lrm: 1.00 | dt: 484.26ms | tok/sec: 1,082,663 | mfu: -1.00 | total time: 3.88m\n",
      "step 00467 (57.70%) | loss: 1.318126 | lrm: 1.00 | dt: 487.66ms | tok/sec: 1,075,100 | mfu: -1.00 | total time: 3.88m\n",
      "step 00468 (57.82%) | loss: 1.318990 | lrm: 1.00 | dt: 485.20ms | tok/sec: 1,080,554 | mfu: -1.00 | total time: 3.89m\n",
      "step 00469 (57.93%) | loss: 1.313791 | lrm: 1.00 | dt: 485.59ms | tok/sec: 1,079,701 | mfu: -1.00 | total time: 3.90m\n",
      "step 00470 (58.04%) | loss: 1.318513 | lrm: 1.00 | dt: 485.09ms | tok/sec: 1,080,811 | mfu: -1.00 | total time: 3.91m\n",
      "step 00471 (58.15%) | loss: 1.317497 | lrm: 1.00 | dt: 488.25ms | tok/sec: 1,073,801 | mfu: -1.00 | total time: 3.92m\n",
      "step 00472 (58.27%) | loss: 1.311573 | lrm: 1.00 | dt: 484.34ms | tok/sec: 1,082,474 | mfu: -1.00 | total time: 3.92m\n",
      "step 00473 (58.39%) | loss: 1.314577 | lrm: 1.00 | dt: 486.82ms | tok/sec: 1,076,961 | mfu: -1.00 | total time: 3.93m\n",
      "step 00474 (58.51%) | loss: 1.306804 | lrm: 1.00 | dt: 485.17ms | tok/sec: 1,080,628 | mfu: -1.00 | total time: 3.94m\n",
      "step 00475 (58.64%) | loss: 1.302642 | lrm: 1.00 | dt: 483.77ms | tok/sec: 1,083,750 | mfu: -1.00 | total time: 3.95m\n",
      "step 00476 (58.77%) | loss: 1.306863 | lrm: 1.00 | dt: 487.39ms | tok/sec: 1,075,705 | mfu: -1.00 | total time: 3.96m\n",
      "step 00477 (58.89%) | loss: 1.310061 | lrm: 1.00 | dt: 485.77ms | tok/sec: 1,079,282 | mfu: -1.00 | total time: 3.96m\n",
      "step 00478 (59.02%) | loss: 1.296726 | lrm: 1.00 | dt: 485.69ms | tok/sec: 1,079,481 | mfu: -1.00 | total time: 3.97m\n",
      "step 00479 (59.13%) | loss: 1.303501 | lrm: 1.00 | dt: 486.82ms | tok/sec: 1,076,970 | mfu: -1.00 | total time: 3.98m\n",
      "step 00480 (59.28%) | loss: 1.311677 | lrm: 1.00 | dt: 486.66ms | tok/sec: 1,077,323 | mfu: -1.00 | total time: 3.99m\n",
      "step 00481 (59.39%) | loss: 1.308629 | lrm: 1.00 | dt: 483.23ms | tok/sec: 1,084,962 | mfu: -1.00 | total time: 4.00m\n",
      "step 00482 (59.50%) | loss: 1.304746 | lrm: 1.00 | dt: 487.19ms | tok/sec: 1,076,146 | mfu: -1.00 | total time: 4.01m\n",
      "step 00483 (59.63%) | loss: 1.323436 | lrm: 1.00 | dt: 485.56ms | tok/sec: 1,079,758 | mfu: -1.00 | total time: 4.01m\n",
      "step 00484 (59.75%) | loss: 1.313103 | lrm: 1.00 | dt: 485.57ms | tok/sec: 1,079,726 | mfu: -1.00 | total time: 4.02m\n",
      "step 00485 (59.88%) | loss: 1.306048 | lrm: 1.00 | dt: 485.83ms | tok/sec: 1,079,169 | mfu: -1.00 | total time: 4.03m\n",
      "step 00486 (60.01%) | loss: 1.300859 | lrm: 1.00 | dt: 487.10ms | tok/sec: 1,076,341 | mfu: -1.00 | total time: 4.04m\n",
      "step 00487 (60.13%) | loss: 1.300518 | lrm: 1.00 | dt: 484.14ms | tok/sec: 1,082,916 | mfu: -1.00 | total time: 4.05m\n",
      "step 00488 (60.25%) | loss: 1.292055 | lrm: 1.00 | dt: 486.99ms | tok/sec: 1,076,582 | mfu: -1.00 | total time: 4.05m\n",
      "step 00489 (60.40%) | loss: 1.286983 | lrm: 1.00 | dt: 486.29ms | tok/sec: 1,078,144 | mfu: -1.00 | total time: 4.06m\n",
      "step 00490 (60.52%) | loss: 1.290976 | lrm: 1.00 | dt: 484.25ms | tok/sec: 1,082,687 | mfu: -1.00 | total time: 4.07m\n",
      "step 00491 (60.65%) | loss: 1.298427 | lrm: 1.00 | dt: 488.43ms | tok/sec: 1,073,411 | mfu: -1.00 | total time: 4.08m\n",
      "step 00492 (60.78%) | loss: 1.305727 | lrm: 1.00 | dt: 484.10ms | tok/sec: 1,083,016 | mfu: -1.00 | total time: 4.09m\n",
      "step 00493 (60.90%) | loss: 1.308692 | lrm: 1.00 | dt: 485.21ms | tok/sec: 1,080,531 | mfu: -1.00 | total time: 4.09m\n",
      "step 00494 (61.04%) | loss: 1.303091 | lrm: 1.00 | dt: 486.59ms | tok/sec: 1,077,483 | mfu: -1.00 | total time: 4.10m\n",
      "step 00495 (61.18%) | loss: 1.305360 | lrm: 1.00 | dt: 557.28ms | tok/sec: 940,790 | mfu: -1.00 | total time: 4.11m\n",
      "step 00496 (61.32%) | loss: 1.302865 | lrm: 1.00 | dt: 473.36ms | tok/sec: 1,107,597 | mfu: -1.00 | total time: 4.12m\n",
      "step 00497 (61.45%) | loss: 1.306952 | lrm: 1.00 | dt: 493.95ms | tok/sec: 1,061,421 | mfu: -1.00 | total time: 4.13m\n",
      "step 00498 (61.56%) | loss: 1.308667 | lrm: 1.00 | dt: 481.32ms | tok/sec: 1,089,265 | mfu: -1.00 | total time: 4.14m\n",
      "step 00499 (61.70%) | loss: 1.310880 | lrm: 1.00 | dt: 486.87ms | tok/sec: 1,076,845 | mfu: -1.00 | total time: 4.14m\n",
      "step 00500 (61.81%) | loss: 1.321067 | lrm: 1.00 | dt: 488.59ms | tok/sec: 1,073,073 | mfu: -1.00 | total time: 4.15m\n",
      "step 00501 (61.95%) | loss: 1.314352 | lrm: 1.00 | dt: 483.90ms | tok/sec: 1,083,459 | mfu: -1.00 | total time: 4.16m\n",
      "step 00502 (62.07%) | loss: 1.310116 | lrm: 1.00 | dt: 486.79ms | tok/sec: 1,077,022 | mfu: -1.00 | total time: 4.17m\n",
      "step 00503 (62.19%) | loss: 1.319127 | lrm: 1.00 | dt: 486.78ms | tok/sec: 1,077,054 | mfu: -1.00 | total time: 4.18m\n",
      "step 00504 (62.32%) | loss: 1.324361 | lrm: 1.00 | dt: 486.19ms | tok/sec: 1,078,354 | mfu: -1.00 | total time: 4.18m\n",
      "step 00505 (62.45%) | loss: 1.315482 | lrm: 1.00 | dt: 487.39ms | tok/sec: 1,075,707 | mfu: -1.00 | total time: 4.19m\n",
      "step 00506 (62.59%) | loss: 1.315102 | lrm: 1.00 | dt: 489.46ms | tok/sec: 1,071,149 | mfu: -1.00 | total time: 4.20m\n",
      "step 00507 (62.71%) | loss: 1.311973 | lrm: 1.00 | dt: 484.33ms | tok/sec: 1,082,496 | mfu: -1.00 | total time: 4.21m\n",
      "step 00508 (62.83%) | loss: 1.313901 | lrm: 1.00 | dt: 568.08ms | tok/sec: 922,915 | mfu: -1.00 | total time: 4.22m\n",
      "step 00509 (62.95%) | loss: 1.310386 | lrm: 1.00 | dt: 476.35ms | tok/sec: 1,100,635 | mfu: -1.00 | total time: 4.23m\n",
      "step 00510 (63.07%) | loss: 1.302866 | lrm: 1.00 | dt: 569.04ms | tok/sec: 921,354 | mfu: -1.00 | total time: 4.24m\n",
      "step 00511 (63.20%) | loss: 1.303938 | lrm: 1.00 | dt: 552.97ms | tok/sec: 948,126 | mfu: -1.00 | total time: 4.25m\n",
      "step 00512 (63.32%) | loss: 1.305178 | lrm: 1.00 | dt: 477.80ms | tok/sec: 1,097,297 | mfu: -1.00 | total time: 4.25m\n",
      "step 00513 (63.44%) | loss: 1.302090 | lrm: 1.00 | dt: 494.31ms | tok/sec: 1,060,648 | mfu: -1.00 | total time: 4.26m\n",
      "step 00514 (63.55%) | loss: 1.312991 | lrm: 1.00 | dt: 479.81ms | tok/sec: 1,092,698 | mfu: -1.00 | total time: 4.27m\n",
      "step 00515 (63.69%) | loss: 1.318539 | lrm: 1.00 | dt: 491.07ms | tok/sec: 1,067,638 | mfu: -1.00 | total time: 4.28m\n",
      "step 00516 (63.81%) | loss: 1.322946 | lrm: 1.00 | dt: 483.12ms | tok/sec: 1,085,212 | mfu: -1.00 | total time: 4.29m\n",
      "step 00517 (63.94%) | loss: 1.329667 | lrm: 1.00 | dt: 489.88ms | tok/sec: 1,070,230 | mfu: -1.00 | total time: 4.29m\n",
      "step 00518 (64.06%) | loss: 1.322242 | lrm: 1.00 | dt: 480.69ms | tok/sec: 1,090,696 | mfu: -1.00 | total time: 4.30m\n",
      "step 00519 (64.18%) | loss: 1.313497 | lrm: 1.00 | dt: 557.92ms | tok/sec: 939,712 | mfu: -1.00 | total time: 4.31m\n",
      "step 00520 (64.29%) | loss: 1.317812 | lrm: 1.00 | dt: 474.62ms | tok/sec: 1,104,657 | mfu: -1.00 | total time: 4.32m\n",
      "step 00521 (64.41%) | loss: 1.313893 | lrm: 1.00 | dt: 562.95ms | tok/sec: 931,326 | mfu: -1.00 | total time: 4.33m\n",
      "step 00522 (64.52%) | loss: 1.316078 | lrm: 1.00 | dt: 550.94ms | tok/sec: 951,625 | mfu: -1.00 | total time: 4.34m\n",
      "step 00523 (64.64%) | loss: 1.315998 | lrm: 1.00 | dt: 479.70ms | tok/sec: 1,092,959 | mfu: -1.00 | total time: 4.35m\n",
      "step 00524 (64.77%) | loss: 1.310963 | lrm: 1.00 | dt: 496.81ms | tok/sec: 1,055,307 | mfu: -1.00 | total time: 4.35m\n",
      "step 00525 (64.91%) | loss: 1.305998 | lrm: 1.00 | dt: 478.82ms | tok/sec: 1,094,950 | mfu: -1.00 | total time: 4.36m\n",
      "step 00526 (65.03%) | loss: 1.309934 | lrm: 1.00 | dt: 562.37ms | tok/sec: 932,289 | mfu: -1.00 | total time: 4.37m\n",
      "step 00527 (65.17%) | loss: 1.309219 | lrm: 1.00 | dt: 553.59ms | tok/sec: 947,069 | mfu: -1.00 | total time: 4.38m\n",
      "step 00528 (65.29%) | loss: 1.303311 | lrm: 1.00 | dt: 477.44ms | tok/sec: 1,098,112 | mfu: -1.00 | total time: 4.39m\n",
      "step 00529 (65.42%) | loss: 1.301180 | lrm: 1.00 | dt: 493.37ms | tok/sec: 1,062,660 | mfu: -1.00 | total time: 4.40m\n",
      "step 00530 (65.54%) | loss: 1.302550 | lrm: 1.00 | dt: 479.50ms | tok/sec: 1,093,398 | mfu: -1.00 | total time: 4.40m\n",
      "step 00531 (65.65%) | loss: 1.306970 | lrm: 1.00 | dt: 489.55ms | tok/sec: 1,070,967 | mfu: -1.00 | total time: 4.41m\n",
      "step 00532 (65.78%) | loss: 1.301343 | lrm: 1.00 | dt: 484.01ms | tok/sec: 1,083,226 | mfu: -1.00 | total time: 4.42m\n",
      "step 00533 (65.89%) | loss: 1.315314 | lrm: 1.00 | dt: 489.96ms | tok/sec: 1,070,059 | mfu: -1.00 | total time: 4.43m\n",
      "step 00534 (66.04%) | loss: 1.315042 | lrm: 1.00 | dt: 483.79ms | tok/sec: 1,083,712 | mfu: -1.00 | total time: 4.44m\n",
      "step 00535 (66.17%) | loss: 1.315480 | lrm: 1.00 | dt: 487.93ms | tok/sec: 1,074,523 | mfu: -1.00 | total time: 4.45m\n",
      "step 00536 (66.28%) | loss: 1.324528 | lrm: 1.00 | dt: 482.82ms | tok/sec: 1,085,892 | mfu: -1.00 | total time: 4.45m\n",
      "step 00537 (66.40%) | loss: 1.322669 | lrm: 1.00 | dt: 486.94ms | tok/sec: 1,076,703 | mfu: -1.00 | total time: 4.46m\n",
      "step 00538 (66.54%) | loss: 1.323688 | lrm: 1.00 | dt: 486.61ms | tok/sec: 1,077,421 | mfu: -1.00 | total time: 4.47m\n",
      "step 00539 (66.67%) | loss: 1.324007 | lrm: 1.00 | dt: 557.29ms | tok/sec: 940,787 | mfu: -1.00 | total time: 4.48m\n",
      "step 00540 (66.80%) | loss: 1.314502 | lrm: 1.00 | dt: 476.65ms | tok/sec: 1,099,940 | mfu: -1.00 | total time: 4.49m\n",
      "step 00541 (66.93%) | loss: 1.311237 | lrm: 1.00 | dt: 496.98ms | tok/sec: 1,054,939 | mfu: -1.00 | total time: 4.49m\n",
      "step 00542 (67.06%) | loss: 1.302598 | lrm: 1.00 | dt: 481.15ms | tok/sec: 1,089,646 | mfu: -1.00 | total time: 4.50m\n",
      "step 00543 (67.16%) | loss: 1.307079 | lrm: 1.00 | dt: 492.18ms | tok/sec: 1,065,226 | mfu: -1.00 | total time: 4.51m\n",
      "step 00544 (67.28%) | loss: 1.294385 | lrm: 1.00 | dt: 481.78ms | tok/sec: 1,088,235 | mfu: -1.00 | total time: 4.52m\n",
      "step 00545 (67.42%) | loss: 1.293247 | lrm: 1.00 | dt: 490.94ms | tok/sec: 1,067,928 | mfu: -1.00 | total time: 4.53m\n",
      "step 00546 (67.53%) | loss: 1.289170 | lrm: 1.00 | dt: 482.05ms | tok/sec: 1,087,621 | mfu: -1.00 | total time: 4.54m\n",
      "step 00547 (67.67%) | loss: 1.289388 | lrm: 1.00 | dt: 563.85ms | tok/sec: 929,841 | mfu: -1.00 | total time: 4.54m\n",
      "step 00548 (67.80%) | loss: 1.286304 | lrm: 1.00 | dt: 476.74ms | tok/sec: 1,099,744 | mfu: -1.00 | total time: 4.55m\n",
      "step 00549 (67.94%) | loss: 1.293874 | lrm: 1.00 | dt: 563.98ms | tok/sec: 929,617 | mfu: -1.00 | total time: 4.56m\n",
      "step 00550 (68.08%) | loss: 1.282983 | lrm: 1.00 | dt: 477.65ms | tok/sec: 1,097,629 | mfu: -1.00 | total time: 4.57m\n",
      "step 00551 (68.21%) | loss: 1.289979 | lrm: 1.00 | dt: 496.60ms | tok/sec: 1,055,764 | mfu: -1.00 | total time: 4.58m\n",
      "step 00552 (68.34%) | loss: 1.299728 | lrm: 1.00 | dt: 478.65ms | tok/sec: 1,095,343 | mfu: -1.00 | total time: 4.59m\n",
      "step 00553 (68.46%) | loss: 1.290910 | lrm: 1.00 | dt: 492.69ms | tok/sec: 1,064,136 | mfu: -1.00 | total time: 4.59m\n",
      "step 00554 (68.60%) | loss: 1.296211 | lrm: 1.00 | dt: 480.94ms | tok/sec: 1,090,136 | mfu: -1.00 | total time: 4.60m\n",
      "step 00555 (68.73%) | loss: 1.298804 | lrm: 1.00 | dt: 487.33ms | tok/sec: 1,075,830 | mfu: -1.00 | total time: 4.61m\n",
      "step 00556 (68.85%) | loss: 1.301491 | lrm: 1.00 | dt: 485.17ms | tok/sec: 1,080,622 | mfu: -1.00 | total time: 4.62m\n",
      "step 00557 (68.98%) | loss: 1.301459 | lrm: 1.00 | dt: 489.55ms | tok/sec: 1,070,960 | mfu: -1.00 | total time: 4.63m\n",
      "step 00558 (69.11%) | loss: 1.311542 | lrm: 1.00 | dt: 480.90ms | tok/sec: 1,090,217 | mfu: -1.00 | total time: 4.64m\n",
      "step 00559 (69.22%) | loss: 1.318589 | lrm: 1.00 | dt: 488.20ms | tok/sec: 1,073,918 | mfu: -1.00 | total time: 4.64m\n",
      "step 00560 (69.34%) | loss: 1.321465 | lrm: 1.00 | dt: 485.75ms | tok/sec: 1,079,345 | mfu: -1.00 | total time: 4.65m\n",
      "step 00561 (69.48%) | loss: 1.326819 | lrm: 1.00 | dt: 483.89ms | tok/sec: 1,083,487 | mfu: -1.00 | total time: 4.66m\n",
      "step 00562 (69.62%) | loss: 1.327302 | lrm: 1.00 | dt: 486.94ms | tok/sec: 1,076,703 | mfu: -1.00 | total time: 4.67m\n",
      "step 00563 (69.73%) | loss: 1.321776 | lrm: 1.00 | dt: 482.00ms | tok/sec: 1,087,735 | mfu: -1.00 | total time: 4.68m\n",
      "step 00564 (69.86%) | loss: 1.319431 | lrm: 1.00 | dt: 567.80ms | tok/sec: 923,371 | mfu: -1.00 | total time: 4.68m\n",
      "step 00565 (69.98%) | loss: 1.309657 | lrm: 1.00 | dt: 477.24ms | tok/sec: 1,098,579 | mfu: -1.00 | total time: 4.69m\n",
      "step 00566 (70.10%) | loss: 1.307748 | lrm: 1.00 | dt: 490.98ms | tok/sec: 1,067,840 | mfu: -1.00 | total time: 4.70m\n",
      "step 00567 (70.23%) | loss: 1.311368 | lrm: 1.00 | dt: 479.66ms | tok/sec: 1,093,036 | mfu: -1.00 | total time: 4.71m\n",
      "step 00568 (70.36%) | loss: 1.315329 | lrm: 1.00 | dt: 491.17ms | tok/sec: 1,067,422 | mfu: -1.00 | total time: 4.72m\n",
      "step 00569 (70.51%) | loss: 1.314232 | lrm: 1.00 | dt: 552.47ms | tok/sec: 948,992 | mfu: -1.00 | total time: 4.73m\n",
      "step 00570 (70.62%) | loss: 1.306802 | lrm: 1.00 | dt: 478.88ms | tok/sec: 1,094,824 | mfu: -1.00 | total time: 4.73m\n",
      "step 00571 (70.72%) | loss: 1.297678 | lrm: 1.00 | dt: 492.86ms | tok/sec: 1,063,771 | mfu: -1.00 | total time: 4.74m\n",
      "step 00572 (70.84%) | loss: 1.310156 | lrm: 1.00 | dt: 482.23ms | tok/sec: 1,087,212 | mfu: -1.00 | total time: 4.75m\n",
      "step 00573 (70.97%) | loss: 1.310642 | lrm: 1.00 | dt: 491.74ms | tok/sec: 1,066,199 | mfu: -1.00 | total time: 4.76m\n",
      "step 00574 (71.08%) | loss: 1.307872 | lrm: 1.00 | dt: 561.77ms | tok/sec: 933,286 | mfu: -1.00 | total time: 4.77m\n",
      "step 00575 (71.19%) | loss: 1.314162 | lrm: 1.00 | dt: 554.68ms | tok/sec: 945,213 | mfu: -1.00 | total time: 4.78m\n",
      "step 00576 (71.30%) | loss: 1.313684 | lrm: 1.00 | dt: 478.04ms | tok/sec: 1,096,755 | mfu: -1.00 | total time: 4.79m\n",
      "step 00577 (71.42%) | loss: 1.308613 | lrm: 1.00 | dt: 494.77ms | tok/sec: 1,059,666 | mfu: -1.00 | total time: 4.79m\n",
      "step 00578 (71.55%) | loss: 1.301113 | lrm: 1.00 | dt: 478.39ms | tok/sec: 1,095,949 | mfu: -1.00 | total time: 4.80m\n",
      "step 00579 (71.69%) | loss: 1.302384 | lrm: 1.00 | dt: 490.92ms | tok/sec: 1,067,975 | mfu: -1.00 | total time: 4.81m\n",
      "step 00580 (71.82%) | loss: 1.311208 | lrm: 1.00 | dt: 560.55ms | tok/sec: 935,312 | mfu: -1.00 | total time: 4.82m\n",
      "step 00581 (71.95%) | loss: 1.305965 | lrm: 1.00 | dt: 549.79ms | tok/sec: 953,616 | mfu: -1.00 | total time: 4.83m\n",
      "step 00582 (72.07%) | loss: 1.308134 | lrm: 1.00 | dt: 480.58ms | tok/sec: 1,090,939 | mfu: -1.00 | total time: 4.84m\n",
      "step 00583 (72.18%) | loss: 1.314274 | lrm: 1.00 | dt: 492.27ms | tok/sec: 1,065,032 | mfu: -1.00 | total time: 4.84m\n",
      "step 00584 (72.29%) | loss: 1.319741 | lrm: 1.00 | dt: 481.26ms | tok/sec: 1,089,411 | mfu: -1.00 | total time: 4.85m\n",
      "step 00585 (72.43%) | loss: 1.308493 | lrm: 1.00 | dt: 490.32ms | tok/sec: 1,069,285 | mfu: -1.00 | total time: 4.86m\n",
      "step 00586 (72.55%) | loss: 1.305965 | lrm: 1.00 | dt: 555.75ms | tok/sec: 943,388 | mfu: -1.00 | total time: 4.87m\n",
      "step 00587 (72.67%) | loss: 1.303296 | lrm: 1.00 | dt: 555.51ms | tok/sec: 943,800 | mfu: -1.00 | total time: 4.88m\n",
      "step 00588 (72.81%) | loss: 1.314591 | lrm: 1.00 | dt: 479.57ms | tok/sec: 1,093,244 | mfu: -1.00 | total time: 4.89m\n",
      "step 00589 (72.92%) | loss: 1.322890 | lrm: 1.00 | dt: 493.06ms | tok/sec: 1,063,340 | mfu: -1.00 | total time: 4.90m\n",
      "step 00590 (73.05%) | loss: 1.324221 | lrm: 1.00 | dt: 481.29ms | tok/sec: 1,089,330 | mfu: -1.00 | total time: 4.90m\n",
      "step 00591 (73.19%) | loss: 1.326425 | lrm: 1.00 | dt: 490.49ms | tok/sec: 1,068,905 | mfu: -1.00 | total time: 4.91m\n",
      "step 00592 (73.31%) | loss: 1.313816 | lrm: 1.00 | dt: 481.99ms | tok/sec: 1,087,767 | mfu: -1.00 | total time: 4.92m\n",
      "step 00593 (73.42%) | loss: 1.312748 | lrm: 1.00 | dt: 556.54ms | tok/sec: 942,056 | mfu: -1.00 | total time: 4.93m\n",
      "step 00594 (73.54%) | loss: 1.324368 | lrm: 1.00 | dt: 549.02ms | tok/sec: 954,946 | mfu: -1.00 | total time: 4.94m\n",
      "step 00595 (73.67%) | loss: 1.324757 | lrm: 1.00 | dt: 476.40ms | tok/sec: 1,100,520 | mfu: -1.00 | total time: 4.95m\n",
      "step 00596 (73.77%) | loss: 1.336924 | lrm: 1.00 | dt: 496.92ms | tok/sec: 1,055,066 | mfu: -1.00 | total time: 4.95m\n",
      "step 00597 (73.93%) | loss: 1.338186 | lrm: 1.00 | dt: 480.59ms | tok/sec: 1,090,916 | mfu: -1.00 | total time: 4.96m\n",
      "step 00598 (74.05%) | loss: 1.342462 | lrm: 1.00 | dt: 492.28ms | tok/sec: 1,065,017 | mfu: -1.00 | total time: 4.97m\n",
      "step 00599 (74.16%) | loss: 1.338511 | lrm: 1.00 | dt: 556.91ms | tok/sec: 941,423 | mfu: -1.00 | total time: 4.98m\n",
      "step 00600 (74.26%) | loss: 1.330578 | lrm: 1.00 | dt: 476.12ms | tok/sec: 1,101,158 | mfu: -1.00 | total time: 4.99m\n",
      "step 00600 | Validation bpb: 0.4119\n",
      "step 00601 (74.39%) | loss: 1.326915 | lrm: 1.00 | dt: 478.43ms | tok/sec: 1,095,858 | mfu: -1.00 | total time: 5.00m\n",
      "step 00602 (74.50%) | loss: 1.318362 | lrm: 1.00 | dt: 492.56ms | tok/sec: 1,064,424 | mfu: -1.00 | total time: 5.00m\n",
      "step 00603 (74.62%) | loss: 1.323389 | lrm: 1.00 | dt: 482.55ms | tok/sec: 1,086,505 | mfu: -1.00 | total time: 5.01m\n",
      "step 00604 (74.72%) | loss: 1.312567 | lrm: 1.00 | dt: 490.55ms | tok/sec: 1,068,784 | mfu: -1.00 | total time: 5.02m\n",
      "step 00605 (74.84%) | loss: 1.313770 | lrm: 1.00 | dt: 486.06ms | tok/sec: 1,078,645 | mfu: -1.00 | total time: 5.03m\n",
      "step 00606 (74.98%) | loss: 1.328184 | lrm: 1.00 | dt: 489.31ms | tok/sec: 1,071,485 | mfu: -1.00 | total time: 5.04m\n",
      "step 00607 (75.12%) | loss: 1.319761 | lrm: 1.00 | dt: 488.28ms | tok/sec: 1,073,740 | mfu: -1.00 | total time: 5.04m\n",
      "step 00608 (75.23%) | loss: 1.301699 | lrm: 1.00 | dt: 489.44ms | tok/sec: 1,071,201 | mfu: -1.00 | total time: 5.05m\n",
      "step 00609 (75.34%) | loss: 1.297497 | lrm: 1.00 | dt: 487.19ms | tok/sec: 1,076,157 | mfu: -1.00 | total time: 5.06m\n",
      "step 00610 (75.45%) | loss: 1.302567 | lrm: 1.00 | dt: 483.37ms | tok/sec: 1,084,656 | mfu: -1.00 | total time: 5.07m\n",
      "step 00611 (75.58%) | loss: 1.295351 | lrm: 1.00 | dt: 486.16ms | tok/sec: 1,078,434 | mfu: -1.00 | total time: 5.08m\n",
      "step 00612 (75.70%) | loss: 1.283540 | lrm: 1.00 | dt: 481.80ms | tok/sec: 1,088,175 | mfu: -1.00 | total time: 5.09m\n",
      "step 00613 (75.83%) | loss: 1.275523 | lrm: 1.00 | dt: 484.40ms | tok/sec: 1,082,354 | mfu: -1.00 | total time: 5.09m\n",
      "step 00614 (75.93%) | loss: 1.277720 | lrm: 1.00 | dt: 484.08ms | tok/sec: 1,083,053 | mfu: -1.00 | total time: 5.10m\n",
      "step 00615 (76.08%) | loss: 1.277632 | lrm: 1.00 | dt: 486.25ms | tok/sec: 1,078,216 | mfu: -1.00 | total time: 5.11m\n",
      "step 00616 (76.19%) | loss: 1.279674 | lrm: 1.00 | dt: 485.77ms | tok/sec: 1,079,282 | mfu: -1.00 | total time: 5.12m\n",
      "step 00617 (76.31%) | loss: 1.281107 | lrm: 1.00 | dt: 483.85ms | tok/sec: 1,083,572 | mfu: -1.00 | total time: 5.13m\n",
      "step 00618 (76.44%) | loss: 1.283827 | lrm: 1.00 | dt: 486.74ms | tok/sec: 1,077,139 | mfu: -1.00 | total time: 5.13m\n",
      "step 00619 (76.55%) | loss: 1.293165 | lrm: 1.00 | dt: 563.07ms | tok/sec: 931,119 | mfu: -1.00 | total time: 5.14m\n",
      "step 00620 (76.68%) | loss: 1.302867 | lrm: 1.00 | dt: 478.37ms | tok/sec: 1,095,994 | mfu: -1.00 | total time: 5.15m\n",
      "step 00621 (76.79%) | loss: 1.299095 | lrm: 1.00 | dt: 568.73ms | tok/sec: 921,861 | mfu: -1.00 | total time: 5.16m\n",
      "step 00622 (76.92%) | loss: 1.297297 | lrm: 1.00 | dt: 475.82ms | tok/sec: 1,101,858 | mfu: -1.00 | total time: 5.17m\n",
      "step 00623 (77.03%) | loss: 1.305734 | lrm: 1.00 | dt: 496.91ms | tok/sec: 1,055,086 | mfu: -1.00 | total time: 5.18m\n",
      "step 00624 (77.16%) | loss: 1.300077 | lrm: 1.00 | dt: 477.16ms | tok/sec: 1,098,771 | mfu: -1.00 | total time: 5.18m\n",
      "step 00625 (77.27%) | loss: 1.303156 | lrm: 1.00 | dt: 489.03ms | tok/sec: 1,072,104 | mfu: -1.00 | total time: 5.19m\n",
      "step 00626 (77.39%) | loss: 1.294386 | lrm: 1.00 | dt: 570.04ms | tok/sec: 919,738 | mfu: -1.00 | total time: 5.20m\n",
      "step 00627 (77.50%) | loss: 1.289162 | lrm: 1.00 | dt: 478.94ms | tok/sec: 1,094,690 | mfu: -1.00 | total time: 5.21m\n",
      "step 00628 (77.64%) | loss: 1.292945 | lrm: 1.00 | dt: 566.92ms | tok/sec: 924,800 | mfu: -1.00 | total time: 5.22m\n",
      "step 00629 (77.77%) | loss: 1.291361 | lrm: 1.00 | dt: 559.42ms | tok/sec: 937,191 | mfu: -1.00 | total time: 5.23m\n",
      "step 00630 (77.90%) | loss: 1.292601 | lrm: 1.00 | dt: 545.99ms | tok/sec: 960,245 | mfu: -1.00 | total time: 5.24m\n",
      "step 00631 (78.02%) | loss: 1.283572 | lrm: 1.00 | dt: 478.26ms | tok/sec: 1,096,244 | mfu: -1.00 | total time: 5.25m\n",
      "step 00632 (78.13%) | loss: 1.286576 | lrm: 1.00 | dt: 494.55ms | tok/sec: 1,060,131 | mfu: -1.00 | total time: 5.25m\n",
      "step 00633 (78.25%) | loss: 1.282879 | lrm: 1.00 | dt: 480.82ms | tok/sec: 1,090,399 | mfu: -1.00 | total time: 5.26m\n",
      "step 00634 (78.37%) | loss: 1.275411 | lrm: 1.00 | dt: 491.25ms | tok/sec: 1,067,246 | mfu: -1.00 | total time: 5.27m\n",
      "step 00635 (78.49%) | loss: 1.271640 | lrm: 1.00 | dt: 477.96ms | tok/sec: 1,096,939 | mfu: -1.00 | total time: 5.28m\n",
      "step 00636 (78.61%) | loss: 1.274170 | lrm: 1.00 | dt: 488.59ms | tok/sec: 1,073,066 | mfu: -1.00 | total time: 5.29m\n",
      "step 00637 (78.72%) | loss: 1.276816 | lrm: 1.00 | dt: 553.87ms | tok/sec: 946,591 | mfu: -1.00 | total time: 5.30m\n",
      "step 00638 (78.84%) | loss: 1.276124 | lrm: 1.00 | dt: 556.71ms | tok/sec: 941,753 | mfu: -1.00 | total time: 5.31m\n",
      "step 00639 (78.95%) | loss: 1.271034 | lrm: 1.00 | dt: 477.55ms | tok/sec: 1,097,862 | mfu: -1.00 | total time: 5.31m\n",
      "step 00640 (79.08%) | loss: 1.272156 | lrm: 1.00 | dt: 494.41ms | tok/sec: 1,060,432 | mfu: -1.00 | total time: 5.32m\n",
      "step 00641 (79.19%) | loss: 1.278419 | lrm: 1.00 | dt: 480.41ms | tok/sec: 1,091,330 | mfu: -1.00 | total time: 5.33m\n",
      "step 00642 (79.33%) | loss: 1.285971 | lrm: 1.00 | dt: 491.46ms | tok/sec: 1,066,807 | mfu: -1.00 | total time: 5.34m\n",
      "step 00643 (79.47%) | loss: 1.283564 | lrm: 1.00 | dt: 561.08ms | tok/sec: 934,424 | mfu: -1.00 | total time: 5.35m\n",
      "step 00644 (79.58%) | loss: 1.281875 | lrm: 1.00 | dt: 553.35ms | tok/sec: 947,479 | mfu: -1.00 | total time: 5.36m\n",
      "step 00645 (79.69%) | loss: 1.279927 | lrm: 1.00 | dt: 477.94ms | tok/sec: 1,096,985 | mfu: -1.00 | total time: 5.36m\n",
      "step 00646 (79.84%) | loss: 1.276512 | lrm: 1.00 | dt: 493.84ms | tok/sec: 1,061,662 | mfu: -1.00 | total time: 5.37m\n",
      "step 00647 (79.95%) | loss: 1.278511 | lrm: 1.00 | dt: 568.74ms | tok/sec: 921,834 | mfu: -1.00 | total time: 5.38m\n",
      "step 00648 (80.07%) | loss: 1.274028 | lrm: 1.00 | dt: 552.16ms | tok/sec: 949,515 | mfu: -1.00 | total time: 5.39m\n",
      "step 00649 (80.18%) | loss: 1.269232 | lrm: 0.99 | dt: 476.78ms | tok/sec: 1,099,636 | mfu: -1.00 | total time: 5.40m\n",
      "step 00650 (80.30%) | loss: 1.262308 | lrm: 0.99 | dt: 496.60ms | tok/sec: 1,055,749 | mfu: -1.00 | total time: 5.41m\n",
      "step 00651 (80.41%) | loss: 1.259802 | lrm: 0.98 | dt: 478.78ms | tok/sec: 1,095,044 | mfu: -1.00 | total time: 5.42m\n",
      "step 00652 (80.54%) | loss: 1.251581 | lrm: 0.97 | dt: 492.11ms | tok/sec: 1,065,394 | mfu: -1.00 | total time: 5.42m\n",
      "step 00653 (80.66%) | loss: 1.255992 | lrm: 0.97 | dt: 480.41ms | tok/sec: 1,091,331 | mfu: -1.00 | total time: 5.43m\n",
      "step 00654 (80.79%) | loss: 1.259654 | lrm: 0.96 | dt: 492.54ms | tok/sec: 1,064,448 | mfu: -1.00 | total time: 5.44m\n",
      "step 00655 (80.91%) | loss: 1.267423 | lrm: 0.95 | dt: 479.06ms | tok/sec: 1,094,416 | mfu: -1.00 | total time: 5.45m\n",
      "step 00656 (81.03%) | loss: 1.262666 | lrm: 0.95 | dt: 490.93ms | tok/sec: 1,067,956 | mfu: -1.00 | total time: 5.46m\n",
      "step 00657 (81.15%) | loss: 1.256937 | lrm: 0.94 | dt: 483.78ms | tok/sec: 1,083,722 | mfu: -1.00 | total time: 5.46m\n",
      "step 00658 (81.27%) | loss: 1.267245 | lrm: 0.94 | dt: 484.73ms | tok/sec: 1,081,609 | mfu: -1.00 | total time: 5.47m\n",
      "step 00659 (81.39%) | loss: 1.270506 | lrm: 0.93 | dt: 483.29ms | tok/sec: 1,084,832 | mfu: -1.00 | total time: 5.48m\n",
      "step 00660 (81.52%) | loss: 1.266940 | lrm: 0.92 | dt: 486.36ms | tok/sec: 1,077,981 | mfu: -1.00 | total time: 5.49m\n",
      "step 00661 (81.65%) | loss: 1.265043 | lrm: 0.92 | dt: 484.98ms | tok/sec: 1,081,044 | mfu: -1.00 | total time: 5.50m\n",
      "step 00662 (81.77%) | loss: 1.260545 | lrm: 0.91 | dt: 485.31ms | tok/sec: 1,080,326 | mfu: -1.00 | total time: 5.50m\n",
      "step 00663 (81.90%) | loss: 1.262208 | lrm: 0.90 | dt: 488.00ms | tok/sec: 1,074,354 | mfu: -1.00 | total time: 5.51m\n",
      "step 00664 (82.03%) | loss: 1.278250 | lrm: 0.90 | dt: 485.82ms | tok/sec: 1,079,188 | mfu: -1.00 | total time: 5.52m\n",
      "step 00665 (82.14%) | loss: 1.287484 | lrm: 0.89 | dt: 487.27ms | tok/sec: 1,075,960 | mfu: -1.00 | total time: 5.53m\n",
      "step 00666 (82.27%) | loss: 1.281251 | lrm: 0.89 | dt: 483.88ms | tok/sec: 1,083,514 | mfu: -1.00 | total time: 5.54m\n",
      "step 00667 (82.40%) | loss: 1.284619 | lrm: 0.88 | dt: 492.37ms | tok/sec: 1,064,827 | mfu: -1.00 | total time: 5.54m\n",
      "step 00668 (82.52%) | loss: 1.282081 | lrm: 0.87 | dt: 482.93ms | tok/sec: 1,085,628 | mfu: -1.00 | total time: 5.55m\n",
      "step 00669 (82.62%) | loss: 1.304954 | lrm: 0.87 | dt: 489.13ms | tok/sec: 1,071,885 | mfu: -1.00 | total time: 5.56m\n",
      "step 00670 (82.75%) | loss: 1.301993 | lrm: 0.86 | dt: 483.90ms | tok/sec: 1,083,463 | mfu: -1.00 | total time: 5.57m\n",
      "step 00671 (82.87%) | loss: 1.304115 | lrm: 0.86 | dt: 487.11ms | tok/sec: 1,076,334 | mfu: -1.00 | total time: 5.58m\n",
      "step 00672 (82.99%) | loss: 1.304414 | lrm: 0.85 | dt: 573.52ms | tok/sec: 914,158 | mfu: -1.00 | total time: 5.59m\n",
      "step 00673 (83.11%) | loss: 1.289127 | lrm: 0.84 | dt: 477.64ms | tok/sec: 1,097,655 | mfu: -1.00 | total time: 5.59m\n",
      "step 00674 (83.22%) | loss: 1.278490 | lrm: 0.84 | dt: 496.94ms | tok/sec: 1,055,022 | mfu: -1.00 | total time: 5.60m\n",
      "step 00675 (83.34%) | loss: 1.257618 | lrm: 0.83 | dt: 478.11ms | tok/sec: 1,096,578 | mfu: -1.00 | total time: 5.61m\n",
      "step 00676 (83.46%) | loss: 1.260247 | lrm: 0.83 | dt: 491.12ms | tok/sec: 1,067,524 | mfu: -1.00 | total time: 5.62m\n",
      "step 00677 (83.59%) | loss: 1.263419 | lrm: 0.82 | dt: 484.64ms | tok/sec: 1,081,812 | mfu: -1.00 | total time: 5.63m\n",
      "step 00678 (83.71%) | loss: 1.269992 | lrm: 0.81 | dt: 486.63ms | tok/sec: 1,077,375 | mfu: -1.00 | total time: 5.64m\n",
      "step 00679 (83.84%) | loss: 1.277323 | lrm: 0.81 | dt: 485.23ms | tok/sec: 1,080,485 | mfu: -1.00 | total time: 5.64m\n",
      "step 00680 (83.95%) | loss: 1.281496 | lrm: 0.80 | dt: 488.05ms | tok/sec: 1,074,256 | mfu: -1.00 | total time: 5.65m\n",
      "step 00681 (84.06%) | loss: 1.281213 | lrm: 0.80 | dt: 481.77ms | tok/sec: 1,088,242 | mfu: -1.00 | total time: 5.66m\n",
      "step 00682 (84.20%) | loss: 1.284738 | lrm: 0.79 | dt: 488.51ms | tok/sec: 1,073,236 | mfu: -1.00 | total time: 5.67m\n",
      "step 00683 (84.31%) | loss: 1.278470 | lrm: 0.78 | dt: 483.69ms | tok/sec: 1,083,931 | mfu: -1.00 | total time: 5.68m\n",
      "step 00684 (84.43%) | loss: 1.282023 | lrm: 0.78 | dt: 486.66ms | tok/sec: 1,077,320 | mfu: -1.00 | total time: 5.68m\n",
      "step 00685 (84.54%) | loss: 1.285869 | lrm: 0.77 | dt: 485.88ms | tok/sec: 1,079,059 | mfu: -1.00 | total time: 5.69m\n",
      "step 00686 (84.66%) | loss: 1.286625 | lrm: 0.77 | dt: 487.45ms | tok/sec: 1,075,579 | mfu: -1.00 | total time: 5.70m\n",
      "step 00687 (84.76%) | loss: 1.285439 | lrm: 0.76 | dt: 485.43ms | tok/sec: 1,080,041 | mfu: -1.00 | total time: 5.71m\n",
      "step 00688 (84.88%) | loss: 1.309669 | lrm: 0.76 | dt: 486.65ms | tok/sec: 1,077,334 | mfu: -1.00 | total time: 5.72m\n",
      "step 00689 (85.01%) | loss: 1.295812 | lrm: 0.75 | dt: 485.90ms | tok/sec: 1,079,013 | mfu: -1.00 | total time: 5.72m\n",
      "step 00690 (85.15%) | loss: 1.291085 | lrm: 0.74 | dt: 485.32ms | tok/sec: 1,080,295 | mfu: -1.00 | total time: 5.73m\n",
      "step 00691 (85.26%) | loss: 1.284691 | lrm: 0.74 | dt: 486.53ms | tok/sec: 1,077,613 | mfu: -1.00 | total time: 5.74m\n",
      "step 00692 (85.37%) | loss: 1.293591 | lrm: 0.73 | dt: 485.96ms | tok/sec: 1,078,875 | mfu: -1.00 | total time: 5.75m\n",
      "step 00693 (85.50%) | loss: 1.298334 | lrm: 0.72 | dt: 486.10ms | tok/sec: 1,078,557 | mfu: -1.00 | total time: 5.76m\n",
      "step 00694 (85.63%) | loss: 1.299672 | lrm: 0.72 | dt: 485.83ms | tok/sec: 1,079,153 | mfu: -1.00 | total time: 5.76m\n",
      "step 00695 (85.77%) | loss: 1.306575 | lrm: 0.71 | dt: 487.16ms | tok/sec: 1,076,217 | mfu: -1.00 | total time: 5.77m\n",
      "step 00696 (85.88%) | loss: 1.311272 | lrm: 0.71 | dt: 483.91ms | tok/sec: 1,083,439 | mfu: -1.00 | total time: 5.78m\n",
      "step 00697 (86.02%) | loss: 1.299352 | lrm: 0.70 | dt: 487.68ms | tok/sec: 1,075,058 | mfu: -1.00 | total time: 5.79m\n",
      "step 00698 (86.16%) | loss: 1.302274 | lrm: 0.69 | dt: 483.78ms | tok/sec: 1,083,735 | mfu: -1.00 | total time: 5.80m\n",
      "step 00699 (86.29%) | loss: 1.306352 | lrm: 0.69 | dt: 484.46ms | tok/sec: 1,082,216 | mfu: -1.00 | total time: 5.81m\n",
      "step 00700 (86.41%) | loss: 1.301868 | lrm: 0.68 | dt: 487.58ms | tok/sec: 1,075,295 | mfu: -1.00 | total time: 5.81m\n",
      "step 00701 (86.51%) | loss: 1.291486 | lrm: 0.67 | dt: 486.74ms | tok/sec: 1,077,147 | mfu: -1.00 | total time: 5.82m\n",
      "step 00702 (86.65%) | loss: 1.288503 | lrm: 0.67 | dt: 483.94ms | tok/sec: 1,083,381 | mfu: -1.00 | total time: 5.83m\n",
      "step 00703 (86.79%) | loss: 1.281966 | lrm: 0.66 | dt: 485.68ms | tok/sec: 1,079,488 | mfu: -1.00 | total time: 5.84m\n",
      "step 00704 (86.90%) | loss: 1.282461 | lrm: 0.65 | dt: 487.09ms | tok/sec: 1,076,359 | mfu: -1.00 | total time: 5.85m\n",
      "step 00705 (87.03%) | loss: 1.286623 | lrm: 0.65 | dt: 484.50ms | tok/sec: 1,082,118 | mfu: -1.00 | total time: 5.85m\n",
      "step 00706 (87.15%) | loss: 1.284377 | lrm: 0.64 | dt: 487.80ms | tok/sec: 1,074,807 | mfu: -1.00 | total time: 5.86m\n",
      "step 00707 (87.27%) | loss: 1.284954 | lrm: 0.64 | dt: 486.20ms | tok/sec: 1,078,337 | mfu: -1.00 | total time: 5.87m\n",
      "step 00708 (87.39%) | loss: 1.280590 | lrm: 0.63 | dt: 484.53ms | tok/sec: 1,082,062 | mfu: -1.00 | total time: 5.88m\n",
      "step 00709 (87.49%) | loss: 1.275229 | lrm: 0.63 | dt: 486.80ms | tok/sec: 1,077,017 | mfu: -1.00 | total time: 5.89m\n",
      "step 00710 (87.62%) | loss: 1.265017 | lrm: 0.62 | dt: 485.87ms | tok/sec: 1,079,061 | mfu: -1.00 | total time: 5.89m\n",
      "step 00711 (87.74%) | loss: 1.261249 | lrm: 0.61 | dt: 482.68ms | tok/sec: 1,086,209 | mfu: -1.00 | total time: 5.90m\n",
      "step 00712 (87.85%) | loss: 1.259732 | lrm: 0.61 | dt: 488.46ms | tok/sec: 1,073,344 | mfu: -1.00 | total time: 5.91m\n",
      "step 00713 (87.97%) | loss: 1.264454 | lrm: 0.60 | dt: 485.59ms | tok/sec: 1,079,692 | mfu: -1.00 | total time: 5.92m\n",
      "step 00714 (88.10%) | loss: 1.261700 | lrm: 0.60 | dt: 485.72ms | tok/sec: 1,079,405 | mfu: -1.00 | total time: 5.93m\n",
      "step 00715 (88.23%) | loss: 1.268786 | lrm: 0.59 | dt: 486.86ms | tok/sec: 1,076,878 | mfu: -1.00 | total time: 5.94m\n",
      "step 00716 (88.36%) | loss: 1.261265 | lrm: 0.58 | dt: 487.25ms | tok/sec: 1,076,009 | mfu: -1.00 | total time: 5.94m\n",
      "step 00717 (88.49%) | loss: 1.258972 | lrm: 0.58 | dt: 484.32ms | tok/sec: 1,082,517 | mfu: -1.00 | total time: 5.95m\n",
      "step 00718 (88.62%) | loss: 1.261017 | lrm: 0.57 | dt: 486.08ms | tok/sec: 1,078,607 | mfu: -1.00 | total time: 5.96m\n",
      "step 00719 (88.73%) | loss: 1.253058 | lrm: 0.56 | dt: 486.17ms | tok/sec: 1,078,415 | mfu: -1.00 | total time: 5.97m\n",
      "step 00720 (88.85%) | loss: 1.257026 | lrm: 0.56 | dt: 483.32ms | tok/sec: 1,084,759 | mfu: -1.00 | total time: 5.98m\n",
      "step 00721 (88.96%) | loss: 1.253102 | lrm: 0.55 | dt: 487.57ms | tok/sec: 1,075,314 | mfu: -1.00 | total time: 5.98m\n",
      "step 00722 (89.10%) | loss: 1.254438 | lrm: 0.55 | dt: 486.74ms | tok/sec: 1,077,148 | mfu: -1.00 | total time: 5.99m\n",
      "step 00723 (89.22%) | loss: 1.250665 | lrm: 0.54 | dt: 485.02ms | tok/sec: 1,080,955 | mfu: -1.00 | total time: 6.00m\n",
      "step 00724 (89.34%) | loss: 1.252896 | lrm: 0.53 | dt: 486.54ms | tok/sec: 1,077,584 | mfu: -1.00 | total time: 6.01m\n",
      "step 00725 (89.45%) | loss: 1.242588 | lrm: 0.53 | dt: 485.65ms | tok/sec: 1,079,566 | mfu: -1.00 | total time: 6.02m\n",
      "step 00726 (89.56%) | loss: 1.246085 | lrm: 0.52 | dt: 485.64ms | tok/sec: 1,079,576 | mfu: -1.00 | total time: 6.02m\n",
      "step 00727 (89.67%) | loss: 1.253647 | lrm: 0.52 | dt: 486.07ms | tok/sec: 1,078,634 | mfu: -1.00 | total time: 6.03m\n",
      "step 00728 (89.81%) | loss: 1.265342 | lrm: 0.51 | dt: 486.83ms | tok/sec: 1,076,943 | mfu: -1.00 | total time: 6.04m\n",
      "step 00729 (89.92%) | loss: 1.256884 | lrm: 0.50 | dt: 483.90ms | tok/sec: 1,083,462 | mfu: -1.00 | total time: 6.05m\n",
      "step 00730 (90.05%) | loss: 1.264013 | lrm: 0.50 | dt: 487.78ms | tok/sec: 1,074,839 | mfu: -1.00 | total time: 6.06m\n",
      "step 00731 (90.18%) | loss: 1.269293 | lrm: 0.49 | dt: 485.37ms | tok/sec: 1,080,173 | mfu: -1.00 | total time: 6.06m\n",
      "step 00732 (90.31%) | loss: 1.266783 | lrm: 0.48 | dt: 486.65ms | tok/sec: 1,077,335 | mfu: -1.00 | total time: 6.07m\n",
      "step 00733 (90.43%) | loss: 1.265170 | lrm: 0.48 | dt: 484.92ms | tok/sec: 1,081,186 | mfu: -1.00 | total time: 6.08m\n",
      "step 00734 (90.56%) | loss: 1.258086 | lrm: 0.47 | dt: 487.48ms | tok/sec: 1,075,497 | mfu: -1.00 | total time: 6.09m\n",
      "step 00735 (90.69%) | loss: 1.270811 | lrm: 0.47 | dt: 483.59ms | tok/sec: 1,084,147 | mfu: -1.00 | total time: 6.10m\n",
      "step 00736 (90.83%) | loss: 1.266720 | lrm: 0.46 | dt: 487.67ms | tok/sec: 1,075,082 | mfu: -1.00 | total time: 6.11m\n",
      "step 00737 (90.96%) | loss: 1.260440 | lrm: 0.45 | dt: 485.41ms | tok/sec: 1,080,095 | mfu: -1.00 | total time: 6.11m\n",
      "step 00738 (91.08%) | loss: 1.261785 | lrm: 0.45 | dt: 485.14ms | tok/sec: 1,080,699 | mfu: -1.00 | total time: 6.12m\n",
      "step 00739 (91.19%) | loss: 1.255038 | lrm: 0.44 | dt: 486.89ms | tok/sec: 1,076,807 | mfu: -1.00 | total time: 6.13m\n",
      "step 00740 (91.31%) | loss: 1.243829 | lrm: 0.43 | dt: 485.33ms | tok/sec: 1,080,260 | mfu: -1.00 | total time: 6.14m\n",
      "step 00741 (91.42%) | loss: 1.242908 | lrm: 0.43 | dt: 485.01ms | tok/sec: 1,080,979 | mfu: -1.00 | total time: 6.15m\n",
      "step 00742 (91.55%) | loss: 1.241098 | lrm: 0.42 | dt: 486.48ms | tok/sec: 1,077,720 | mfu: -1.00 | total time: 6.15m\n",
      "step 00743 (91.70%) | loss: 1.246772 | lrm: 0.42 | dt: 486.19ms | tok/sec: 1,078,356 | mfu: -1.00 | total time: 6.16m\n",
      "step 00744 (91.81%) | loss: 1.244762 | lrm: 0.41 | dt: 485.19ms | tok/sec: 1,080,582 | mfu: -1.00 | total time: 6.17m\n",
      "step 00745 (91.93%) | loss: 1.241710 | lrm: 0.40 | dt: 486.60ms | tok/sec: 1,077,451 | mfu: -1.00 | total time: 6.18m\n",
      "step 00746 (92.05%) | loss: 1.235587 | lrm: 0.40 | dt: 486.22ms | tok/sec: 1,078,295 | mfu: -1.00 | total time: 6.19m\n",
      "step 00747 (92.17%) | loss: 1.238454 | lrm: 0.39 | dt: 486.33ms | tok/sec: 1,078,043 | mfu: -1.00 | total time: 6.19m\n",
      "step 00748 (92.29%) | loss: 1.227783 | lrm: 0.39 | dt: 484.66ms | tok/sec: 1,081,774 | mfu: -1.00 | total time: 6.20m\n",
      "step 00749 (92.40%) | loss: 1.233317 | lrm: 0.38 | dt: 485.50ms | tok/sec: 1,079,897 | mfu: -1.00 | total time: 6.21m\n",
      "step 00750 (92.51%) | loss: 1.237041 | lrm: 0.37 | dt: 485.23ms | tok/sec: 1,080,489 | mfu: -1.00 | total time: 6.22m\n",
      "step 00750 | Validation bpb: 0.3999\n",
      "step 00751 (92.63%) | loss: 1.232918 | lrm: 0.37 | dt: 481.15ms | tok/sec: 1,089,653 | mfu: -1.00 | total time: 6.23m\n",
      "step 00752 (92.76%) | loss: 1.253917 | lrm: 0.36 | dt: 488.76ms | tok/sec: 1,072,683 | mfu: -1.00 | total time: 6.23m\n",
      "step 00753 (92.88%) | loss: 1.251177 | lrm: 0.36 | dt: 481.89ms | tok/sec: 1,087,979 | mfu: -1.00 | total time: 6.24m\n",
      "step 00754 (93.00%) | loss: 1.252896 | lrm: 0.35 | dt: 489.27ms | tok/sec: 1,071,574 | mfu: -1.00 | total time: 6.25m\n",
      "step 00755 (93.11%) | loss: 1.247792 | lrm: 0.34 | dt: 482.74ms | tok/sec: 1,086,057 | mfu: -1.00 | total time: 6.26m\n",
      "step 00756 (93.25%) | loss: 1.237362 | lrm: 0.34 | dt: 485.49ms | tok/sec: 1,079,918 | mfu: -1.00 | total time: 6.27m\n",
      "step 00757 (93.37%) | loss: 1.237092 | lrm: 0.33 | dt: 482.04ms | tok/sec: 1,087,654 | mfu: -1.00 | total time: 6.27m\n",
      "step 00758 (93.47%) | loss: 1.226557 | lrm: 0.33 | dt: 485.78ms | tok/sec: 1,079,263 | mfu: -1.00 | total time: 6.28m\n",
      "step 00759 (93.59%) | loss: 1.223984 | lrm: 0.32 | dt: 485.03ms | tok/sec: 1,080,938 | mfu: -1.00 | total time: 6.29m\n",
      "step 00760 (93.72%) | loss: 1.231828 | lrm: 0.31 | dt: 484.71ms | tok/sec: 1,081,650 | mfu: -1.00 | total time: 6.30m\n",
      "step 00761 (93.85%) | loss: 1.237126 | lrm: 0.31 | dt: 486.67ms | tok/sec: 1,077,306 | mfu: -1.00 | total time: 6.31m\n",
      "step 00762 (93.96%) | loss: 1.228178 | lrm: 0.30 | dt: 486.02ms | tok/sec: 1,078,735 | mfu: -1.00 | total time: 6.32m\n",
      "step 00763 (94.07%) | loss: 1.232626 | lrm: 0.30 | dt: 483.29ms | tok/sec: 1,084,839 | mfu: -1.00 | total time: 6.32m\n",
      "step 00764 (94.19%) | loss: 1.231542 | lrm: 0.29 | dt: 483.17ms | tok/sec: 1,085,111 | mfu: -1.00 | total time: 6.33m\n",
      "step 00765 (94.30%) | loss: 1.230213 | lrm: 0.29 | dt: 485.09ms | tok/sec: 1,080,807 | mfu: -1.00 | total time: 6.34m\n",
      "step 00766 (94.43%) | loss: 1.230014 | lrm: 0.28 | dt: 480.31ms | tok/sec: 1,091,562 | mfu: -1.00 | total time: 6.35m\n",
      "step 00767 (94.57%) | loss: 1.232835 | lrm: 0.27 | dt: 485.39ms | tok/sec: 1,080,134 | mfu: -1.00 | total time: 6.36m\n",
      "step 00768 (94.69%) | loss: 1.223256 | lrm: 0.27 | dt: 484.68ms | tok/sec: 1,081,728 | mfu: -1.00 | total time: 6.36m\n",
      "step 00769 (94.81%) | loss: 1.218480 | lrm: 0.26 | dt: 485.22ms | tok/sec: 1,080,507 | mfu: -1.00 | total time: 6.37m\n",
      "step 00770 (94.94%) | loss: 1.225073 | lrm: 0.25 | dt: 485.93ms | tok/sec: 1,078,931 | mfu: -1.00 | total time: 6.38m\n",
      "step 00771 (95.05%) | loss: 1.222826 | lrm: 0.25 | dt: 482.02ms | tok/sec: 1,087,688 | mfu: -1.00 | total time: 6.39m\n",
      "step 00772 (95.18%) | loss: 1.226807 | lrm: 0.24 | dt: 484.14ms | tok/sec: 1,082,926 | mfu: -1.00 | total time: 6.40m\n",
      "step 00773 (95.30%) | loss: 1.231397 | lrm: 0.24 | dt: 485.58ms | tok/sec: 1,079,712 | mfu: -1.00 | total time: 6.40m\n",
      "step 00774 (95.42%) | loss: 1.223510 | lrm: 0.23 | dt: 486.80ms | tok/sec: 1,077,016 | mfu: -1.00 | total time: 6.41m\n",
      "step 00775 (95.56%) | loss: 1.228528 | lrm: 0.22 | dt: 482.78ms | tok/sec: 1,085,984 | mfu: -1.00 | total time: 6.42m\n",
      "step 00776 (95.68%) | loss: 1.226195 | lrm: 0.22 | dt: 485.65ms | tok/sec: 1,079,570 | mfu: -1.00 | total time: 6.43m\n",
      "step 00777 (95.78%) | loss: 1.219978 | lrm: 0.21 | dt: 486.11ms | tok/sec: 1,078,542 | mfu: -1.00 | total time: 6.44m\n",
      "step 00778 (95.91%) | loss: 1.217480 | lrm: 0.20 | dt: 482.37ms | tok/sec: 1,086,898 | mfu: -1.00 | total time: 6.44m\n",
      "step 00779 (96.03%) | loss: 1.214365 | lrm: 0.20 | dt: 485.88ms | tok/sec: 1,079,039 | mfu: -1.00 | total time: 6.45m\n",
      "step 00780 (96.14%) | loss: 1.220049 | lrm: 0.19 | dt: 485.99ms | tok/sec: 1,078,809 | mfu: -1.00 | total time: 6.46m\n",
      "step 00781 (96.28%) | loss: 1.242923 | lrm: 0.19 | dt: 550.77ms | tok/sec: 951,918 | mfu: -1.00 | total time: 6.47m\n",
      "step 00782 (96.39%) | loss: 1.238676 | lrm: 0.18 | dt: 474.93ms | tok/sec: 1,103,923 | mfu: -1.00 | total time: 6.48m\n",
      "step 00783 (96.52%) | loss: 1.234151 | lrm: 0.17 | dt: 483.17ms | tok/sec: 1,085,090 | mfu: -1.00 | total time: 6.49m\n",
      "step 00784 (96.65%) | loss: 1.231867 | lrm: 0.17 | dt: 488.72ms | tok/sec: 1,072,776 | mfu: -1.00 | total time: 6.49m\n",
      "step 00785 (96.78%) | loss: 1.236752 | lrm: 0.16 | dt: 486.16ms | tok/sec: 1,078,428 | mfu: -1.00 | total time: 6.50m\n",
      "step 00786 (96.89%) | loss: 1.240734 | lrm: 0.16 | dt: 485.61ms | tok/sec: 1,079,647 | mfu: -1.00 | total time: 6.51m\n",
      "step 00787 (97.02%) | loss: 1.228029 | lrm: 0.15 | dt: 482.63ms | tok/sec: 1,086,308 | mfu: -1.00 | total time: 6.52m\n",
      "step 00788 (97.15%) | loss: 1.231246 | lrm: 0.14 | dt: 491.20ms | tok/sec: 1,067,351 | mfu: -1.00 | total time: 6.53m\n",
      "step 00789 (97.28%) | loss: 1.234320 | lrm: 0.14 | dt: 480.26ms | tok/sec: 1,091,682 | mfu: -1.00 | total time: 6.53m\n",
      "step 00790 (97.41%) | loss: 1.230934 | lrm: 0.13 | dt: 487.46ms | tok/sec: 1,075,545 | mfu: -1.00 | total time: 6.54m\n",
      "step 00791 (97.55%) | loss: 1.228340 | lrm: 0.12 | dt: 484.06ms | tok/sec: 1,083,100 | mfu: -1.00 | total time: 6.55m\n",
      "step 00792 (97.67%) | loss: 1.236377 | lrm: 0.12 | dt: 486.78ms | tok/sec: 1,077,057 | mfu: -1.00 | total time: 6.56m\n",
      "step 00793 (97.79%) | loss: 1.228253 | lrm: 0.11 | dt: 481.14ms | tok/sec: 1,089,676 | mfu: -1.00 | total time: 6.57m\n",
      "step 00794 (97.92%) | loss: 1.235522 | lrm: 0.10 | dt: 486.41ms | tok/sec: 1,077,879 | mfu: -1.00 | total time: 6.57m\n",
      "step 00795 (98.04%) | loss: 1.226751 | lrm: 0.10 | dt: 486.16ms | tok/sec: 1,078,423 | mfu: -1.00 | total time: 6.58m\n",
      "step 00796 (98.16%) | loss: 1.226663 | lrm: 0.09 | dt: 484.90ms | tok/sec: 1,081,222 | mfu: -1.00 | total time: 6.59m\n",
      "step 00797 (98.30%) | loss: 1.226902 | lrm: 0.09 | dt: 486.96ms | tok/sec: 1,076,662 | mfu: -1.00 | total time: 6.60m\n",
      "step 00798 (98.41%) | loss: 1.224225 | lrm: 0.08 | dt: 485.46ms | tok/sec: 1,079,973 | mfu: -1.00 | total time: 6.61m\n",
      "step 00799 (98.53%) | loss: 1.219806 | lrm: 0.07 | dt: 483.86ms | tok/sec: 1,083,552 | mfu: -1.00 | total time: 6.62m\n",
      "step 00800 (98.64%) | loss: 1.221545 | lrm: 0.07 | dt: 543.00ms | tok/sec: 965,543 | mfu: -1.00 | total time: 6.62m\n",
      "step 00801 (98.78%) | loss: 1.223178 | lrm: 0.06 | dt: 545.24ms | tok/sec: 961,565 | mfu: -1.00 | total time: 6.63m\n",
      "step 00802 (98.91%) | loss: 1.215993 | lrm: 0.05 | dt: 476.43ms | tok/sec: 1,100,446 | mfu: -1.00 | total time: 6.64m\n",
      "step 00803 (99.04%) | loss: 1.219343 | lrm: 0.05 | dt: 491.94ms | tok/sec: 1,065,763 | mfu: -1.00 | total time: 6.65m\n",
      "step 00804 (99.18%) | loss: 1.220347 | lrm: 0.04 | dt: 483.31ms | tok/sec: 1,084,786 | mfu: -1.00 | total time: 6.66m\n",
      "step 00805 (99.31%) | loss: 1.239009 | lrm: 0.03 | dt: 480.67ms | tok/sec: 1,090,735 | mfu: -1.00 | total time: 6.67m\n",
      "step 00806 (99.44%) | loss: 1.241959 | lrm: 0.03 | dt: 488.38ms | tok/sec: 1,073,531 | mfu: -1.00 | total time: 6.67m\n",
      "step 00807 (99.58%) | loss: 1.237423 | lrm: 0.02 | dt: 481.02ms | tok/sec: 1,089,954 | mfu: -1.00 | total time: 6.68m\n",
      "step 00808 (99.71%) | loss: 1.238895 | lrm: 0.01 | dt: 486.65ms | tok/sec: 1,077,333 | mfu: -1.00 | total time: 6.69m\n",
      "step 00809 (99.85%) | loss: 1.232868 | lrm: 0.01 | dt: 481.99ms | tok/sec: 1,087,749 | mfu: -1.00 | total time: 6.70m\n",
      "step 00809 | Validation bpb: 0.3961\n",
      "[W1121 00:42:46.645552174 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/mid_checkpoints/d20/model_000809.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/mid_checkpoints/d20/meta_000809.json\n",
      "[W1121 00:42:46.942206264 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:42:46.169893942 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:42:46.171317592 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:42:46.200681665 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved optimizer to /home/ubuntu/mynanochat/mid_checkpoints/d20/optim_000809_rank0.pt\n",
      "Peak memory usage: 75422.02MiB\n",
      "Total training time: 6.70m\n",
      "Minimum validation bpb: 0.3961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt â–ˆâ–â–â–‚â–‡â–‚â–‚â–‚â–„â–†â–‚â–„â–‚â–„â–‚â–„â–„â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–„â–‚â–‚â–â–â–‚â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss â–ˆâ–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–…â–„â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec â–‚â–ˆâ–‡â–â–ƒâ–ˆâ–‡â–‡â–‡â–ˆâ–…â–…â–…â–‡â–‡â–‡â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–ˆâ–ˆâ–ˆâ–„â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb â–ˆâ–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step 809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time 401.88123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt 0.543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.22155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm 0.06793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec 965543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb 0.3961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-30-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/79iz8auo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_003325-79iz8auo/logs\u001b[0m\n",
      "[W1121 00:42:52.458535323 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:42:55.754914354 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:42:55.283112114 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- \\\n",
    "--model_tag=d20 --run=challenge-30-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61481eda-f74f-4db3-8d3b-cc7b1674f245",
   "metadata": {},
   "source": [
    "#### Do chat eval on mid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753502c1-276b-47c5-9ef2-4373f0395fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 00:43:10.581000 49333 torch/distributed/run.py:803] \n",
      "W1121 00:43:10.581000 49333 torch/distributed/run.py:803] *****************************************\n",
      "W1121 00:43:10.581000 49333 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 00:43:10.581000 49333 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 9.00kB [00:00, 30.5MB/s]\n",
      "ARC-Easy/train-00000-of-00001.parquet: 100%|â–ˆ| 331k/331k [00:00<00:00, 1.40MB/s]\n",
      "ARC-Easy/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆ| 346k/346k [00:00<00:00, 2.26MB/s]\n",
      "ARC-Easy/validation-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 86.1k/86.1k [00:00<00:00, 6\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2251/2251 [00:00<00:00, 346104.27 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 535069.33 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 570/570 [00:00<00:00, 249296.48 examples/s]\n",
      "final: 1026/2376 (43.18%)\n",
      "ARC-Easy accuracy: 43.18%\n",
      "ARC-Challenge/train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 190k/190k [00:00<00:00, 775\n",
      "ARC-Challenge/test-00000-of-00001.parque(â€¦): 100%|â–ˆ| 204k/204k [00:00<00:00, 908\n",
      "ARC-Challenge/validation-00000-of-00001.(â€¦): 100%|â–ˆ| 55.7k/55.7k [00:00<00:00, 3\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1119/1119 [00:00<00:00, 226823.23 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 323339.10 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 299/299 [00:00<00:00, 141052.40 examples/s]\n",
      "final: 389/1172 (33.19%)\n",
      "ARC-Challenge accuracy: 33.19%\n",
      "final: 4644/14042 (33.07%)\n",
      "MMLU accuracy: 33.07%\n",
      "\u001b[KRank 4 | 2/165 (1.21%)]\n",
      "\u001b[KRank 0 | 4/165 (2.42%)]\n",
      "\u001b[KRank 7 | 3/164 (1.83%)]\n",
      "\u001b[KRank 5 | 3/165 (1.82%)]\n",
      "\u001b[KRank 3 | 7/165 (4.24%)]\n",
      "\u001b[KRank 2 | 1/165 (0.61%)]\n",
      "\u001b[KRank 6 | 4/165 (2.42%)]\n",
      "\u001b[KRank 1 | 5/165 (3.03%)]\n",
      "==================================================\n",
      "final: 29/1319 (2.20%)\n",
      "GSM8K accuracy: 2.20%\n",
      "README.md: 6.52kB [00:00, 18.6MB/s]\n",
      "openai_humaneval/test-00000-of-00001.par(â€¦): 100%|â–ˆ| 83.9k/83.9k [00:00<00:00, 2\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 41032.32 examples/s]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 2 | 3/21 (14.29%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 7 | 1/20 (5.00%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "==================================================\n",
      "final: 11/164 (6.71%)\n",
      "HumanEval accuracy: 6.71%\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 5 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 0 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 31/32 (96.88%)]\n",
      "\u001b[KRank 6 | 31/32 (96.88%)]\n",
      "==================================================\n",
      "final: 249/256 (97.27%)\n",
      "SpellingBee accuracy: 97.27%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f93f6-11f1-4f10-95ee-e2964fc6048a",
   "metadata": {},
   "source": [
    "#### SFT train\n",
    "\n",
    "First do a few iterations to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825d661a-f50e-49c0-a9c7-a67c6ebd1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 00:55:22.014000 53214 torch/distributed/run.py:803] \n",
      "W1121 00:55:22.014000 53214 torch/distributed/run.py:803] *****************************************\n",
      "W1121 00:55:22.014000 53214 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 00:55:22.014000 53214 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding num_iterations = 10\n",
      "overriding run = challenge-30-2\n",
      "user_config: {'run': 'challenge-30-2', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': 10, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 100, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 8p49aupt (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run 8p49aupt (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-30-sft-train-d20/wandb/run-20251121_005534-8p49aupt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-30-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/8p49aupt\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00010 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00010 | Training loss: 0.765428| lrm: 0.900000| num_tokens: 13,979\n",
      "Step 00002/00010 | Training loss: 1.093667| lrm: 0.800000| num_tokens: 11,283\n",
      "Step 00003/00010 | Training loss: 1.216391| lrm: 0.700000| num_tokens: 12,682\n",
      "Step 00004/00010 | Training loss: 0.785807| lrm: 0.600000| num_tokens: 9,509\n",
      "Step 00005/00010 | Training loss: 0.847113| lrm: 0.500000| num_tokens: 8,344\n",
      "Step 00006/00010 | Training loss: 0.690363| lrm: 0.400000| num_tokens: 8,763\n",
      "Step 00007/00010 | Training loss: 0.418127| lrm: 0.300000| num_tokens: 10,664\n",
      "Step 00008/00010 | Training loss: 1.030365| lrm: 0.200000| num_tokens: 11,584\n",
      "Step 00009 | Validation loss: 1.011211\n",
      "final: 328/1024 (32.03%)\n",
      "final: 440/1024 (42.97%)\n",
      "Step 00009 | mmlu_acc: 0.320312, arc_easy_acc: 0.429688\n",
      "[W1121 00:56:09.103144661 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:09.122847405 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:09.382355628 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:09.554983264 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:10.589510049 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000009.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000009.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 11-11, summary, console lines 21-26 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–‡â–†â–…â–…â–„â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–ƒâ–ˆâ–…â–†â–‚â–â–‚â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–„â–„â–‡â–ˆâ–„â–…â–ƒâ–â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.42969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.32031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 1.03037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-30-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/8p49aupt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_005534-8p49aupt/logs\u001b[0m\n",
      "[W1121 00:56:14.866416812 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:14.949138255 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:56:14.135131489 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --num_iterations=10 --run=challenge-30-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f007a40-2c5b-4042-83c8-ca532a659643",
   "metadata": {},
   "source": [
    "Then do for real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9acfc53-f03a-4b43-b7b8-a7919291eacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 00:56:40.397000 54574 torch/distributed/run.py:803] \n",
      "W1121 00:56:40.397000 54574 torch/distributed/run.py:803] *****************************************\n",
      "W1121 00:56:40.397000 54574 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 00:56:40.397000 54574 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-30-3\n",
      "user_config: {'run': 'challenge-30-3', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 100, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-30-sft-train-d20/wandb/run-20251121_005652-ey2nnc2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-30-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/ey2nnc2n\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00701 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00701 | Training loss: 0.765084| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 1.093738| lrm: 0.997147| num_tokens: 11,283\n",
      "Step 00003/00701 | Training loss: 1.215614| lrm: 0.995720| num_tokens: 12,682\n",
      "Step 00004/00701 | Training loss: 0.787125| lrm: 0.994294| num_tokens: 9,509\n",
      "Step 00005/00701 | Training loss: 0.847228| lrm: 0.992867| num_tokens: 8,344\n",
      "Step 00006/00701 | Training loss: 0.691814| lrm: 0.991441| num_tokens: 8,763\n",
      "Step 00007/00701 | Training loss: 0.417909| lrm: 0.990014| num_tokens: 10,664\n",
      "Step 00008/00701 | Training loss: 1.030622| lrm: 0.988588| num_tokens: 11,584\n",
      "Step 00009/00701 | Training loss: 0.566032| lrm: 0.987161| num_tokens: 9,920\n",
      "Step 00010/00701 | Training loss: 0.403417| lrm: 0.985735| num_tokens: 15,002\n",
      "Step 00011/00701 | Training loss: 1.074474| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.878491| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.956995| lrm: 0.981455| num_tokens: 10,802\n",
      "Step 00014/00701 | Training loss: 0.775062| lrm: 0.980029| num_tokens: 7,541\n",
      "Step 00015/00701 | Training loss: 0.880040| lrm: 0.978602| num_tokens: 10,515\n",
      "Step 00016/00701 | Training loss: 1.073907| lrm: 0.977175| num_tokens: 14,469\n",
      "Step 00017/00701 | Training loss: 0.539771| lrm: 0.975749| num_tokens: 7,715\n",
      "Step 00018/00701 | Training loss: 1.229020| lrm: 0.974322| num_tokens: 11,655\n",
      "Step 00019/00701 | Training loss: 0.591814| lrm: 0.972896| num_tokens: 11,228\n",
      "Step 00020/00701 | Training loss: 0.415564| lrm: 0.971469| num_tokens: 10,678\n",
      "Step 00021/00701 | Training loss: 0.760080| lrm: 0.970043| num_tokens: 12,324\n",
      "Step 00022/00701 | Training loss: 1.112936| lrm: 0.968616| num_tokens: 9,331\n",
      "Step 00023/00701 | Training loss: 0.691731| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.844405| lrm: 0.965763| num_tokens: 10,715\n",
      "Step 00025/00701 | Training loss: 0.630036| lrm: 0.964337| num_tokens: 12,383\n",
      "Step 00026/00701 | Training loss: 1.035601| lrm: 0.962910| num_tokens: 12,576\n",
      "Step 00027/00701 | Training loss: 0.918852| lrm: 0.961484| num_tokens: 8,359\n",
      "Step 00028/00701 | Training loss: 0.933215| lrm: 0.960057| num_tokens: 10,178\n",
      "Step 00029/00701 | Training loss: 0.937503| lrm: 0.958631| num_tokens: 10,629\n",
      "Step 00030/00701 | Training loss: 0.588875| lrm: 0.957204| num_tokens: 9,143\n",
      "Step 00031/00701 | Training loss: 1.144195| lrm: 0.955777| num_tokens: 7,481\n",
      "Step 00032/00701 | Training loss: 0.860826| lrm: 0.954351| num_tokens: 9,972\n",
      "Step 00033/00701 | Training loss: 0.840129| lrm: 0.952924| num_tokens: 11,270\n",
      "Step 00034/00701 | Training loss: 1.043275| lrm: 0.951498| num_tokens: 11,177\n",
      "Step 00035/00701 | Training loss: 0.845871| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.930535| lrm: 0.948645| num_tokens: 9,343\n",
      "Step 00037/00701 | Training loss: 1.066761| lrm: 0.947218| num_tokens: 10,954\n",
      "Step 00038/00701 | Training loss: 0.721495| lrm: 0.945792| num_tokens: 6,793\n",
      "Step 00039/00701 | Training loss: 0.609949| lrm: 0.944365| num_tokens: 5,849\n",
      "Step 00040/00701 | Training loss: 0.940755| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.619763| lrm: 0.941512| num_tokens: 14,344\n",
      "Step 00042/00701 | Training loss: 1.182020| lrm: 0.940086| num_tokens: 11,223\n",
      "Step 00043/00701 | Training loss: 0.998740| lrm: 0.938659| num_tokens: 11,061\n",
      "Step 00044/00701 | Training loss: 1.083742| lrm: 0.937233| num_tokens: 12,666\n",
      "Step 00045/00701 | Training loss: 1.066249| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.688943| lrm: 0.934379| num_tokens: 9,630\n",
      "Step 00047/00701 | Training loss: 1.078131| lrm: 0.932953| num_tokens: 12,891\n",
      "Step 00048/00701 | Training loss: 0.733716| lrm: 0.931526| num_tokens: 10,059\n",
      "Step 00049/00701 | Training loss: 0.538099| lrm: 0.930100| num_tokens: 11,237\n",
      "Step 00050/00701 | Training loss: 0.759019| lrm: 0.928673| num_tokens: 9,079\n",
      "Step 00051/00701 | Training loss: 0.277585| lrm: 0.927247| num_tokens: 9,590\n",
      "Step 00052/00701 | Training loss: 0.831264| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 1.004039| lrm: 0.924394| num_tokens: 7,924\n",
      "Step 00054/00701 | Training loss: 0.373861| lrm: 0.922967| num_tokens: 9,631\n",
      "Step 00055/00701 | Training loss: 0.896715| lrm: 0.921541| num_tokens: 6,508\n",
      "Step 00056/00701 | Training loss: 1.394012| lrm: 0.920114| num_tokens: 10,513\n",
      "Step 00057/00701 | Training loss: 1.008256| lrm: 0.918688| num_tokens: 9,782\n",
      "Step 00058/00701 | Training loss: 1.360182| lrm: 0.917261| num_tokens: 13,372\n",
      "Step 00059/00701 | Training loss: 1.060322| lrm: 0.915835| num_tokens: 7,944\n",
      "Step 00060/00701 | Training loss: 1.142571| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.256508| lrm: 0.912981| num_tokens: 13,711\n",
      "Step 00062/00701 | Training loss: 0.938936| lrm: 0.911555| num_tokens: 12,159\n",
      "Step 00063/00701 | Training loss: 0.570050| lrm: 0.910128| num_tokens: 10,617\n",
      "Step 00064/00701 | Training loss: 0.670877| lrm: 0.908702| num_tokens: 8,098\n",
      "Step 00065/00701 | Training loss: 0.488175| lrm: 0.907275| num_tokens: 11,033\n",
      "Step 00066/00701 | Training loss: 1.025907| lrm: 0.905849| num_tokens: 9,531\n",
      "Step 00067/00701 | Training loss: 0.555352| lrm: 0.904422| num_tokens: 11,573\n",
      "Step 00068/00701 | Training loss: 1.318044| lrm: 0.902996| num_tokens: 12,923\n",
      "Step 00069/00701 | Training loss: 0.845084| lrm: 0.901569| num_tokens: 7,943\n",
      "Step 00070/00701 | Training loss: 0.477587| lrm: 0.900143| num_tokens: 10,775\n",
      "Step 00071/00701 | Training loss: 0.732602| lrm: 0.898716| num_tokens: 9,193\n",
      "Step 00072/00701 | Training loss: 0.947480| lrm: 0.897290| num_tokens: 9,378\n",
      "Step 00073/00701 | Training loss: 1.220773| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.723801| lrm: 0.894437| num_tokens: 7,685\n",
      "Step 00075/00701 | Training loss: 0.699717| lrm: 0.893010| num_tokens: 6,807\n",
      "Step 00076/00701 | Training loss: 0.831906| lrm: 0.891583| num_tokens: 7,530\n",
      "Step 00077/00701 | Training loss: 0.855439| lrm: 0.890157| num_tokens: 9,319\n",
      "Step 00078/00701 | Training loss: 0.613415| lrm: 0.888730| num_tokens: 11,560\n",
      "Step 00079/00701 | Training loss: 0.583771| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080/00701 | Training loss: 1.001566| lrm: 0.885877| num_tokens: 11,674\n",
      "Step 00081/00701 | Training loss: 0.516849| lrm: 0.884451| num_tokens: 7,783\n",
      "Step 00082/00701 | Training loss: 0.684699| lrm: 0.883024| num_tokens: 14,544\n",
      "Step 00083/00701 | Training loss: 0.539491| lrm: 0.881598| num_tokens: 11,069\n",
      "Step 00084/00701 | Training loss: 0.748649| lrm: 0.880171| num_tokens: 13,568\n",
      "Step 00085/00701 | Training loss: 1.146381| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.991919| lrm: 0.877318| num_tokens: 9,908\n",
      "Step 00087/00701 | Training loss: 1.020619| lrm: 0.875892| num_tokens: 9,627\n",
      "Step 00088/00701 | Training loss: 0.862062| lrm: 0.874465| num_tokens: 11,064\n",
      "Step 00089/00701 | Training loss: 1.391732| lrm: 0.873039| num_tokens: 9,523\n",
      "Step 00090/00701 | Training loss: 0.959316| lrm: 0.871612| num_tokens: 10,566\n",
      "Step 00091/00701 | Training loss: 0.751611| lrm: 0.870185| num_tokens: 10,761\n",
      "Step 00092/00701 | Training loss: 0.868606| lrm: 0.868759| num_tokens: 10,418\n",
      "Step 00093/00701 | Training loss: 0.866138| lrm: 0.867332| num_tokens: 9,591\n",
      "Step 00094/00701 | Training loss: 0.729135| lrm: 0.865906| num_tokens: 9,464\n",
      "Step 00095/00701 | Training loss: 0.956402| lrm: 0.864479| num_tokens: 9,395\n",
      "Step 00096/00701 | Training loss: 1.120157| lrm: 0.863053| num_tokens: 13,239\n",
      "Step 00097/00701 | Training loss: 1.008525| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.755439| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 1.138432| lrm: 0.858773| num_tokens: 8,989\n",
      "Step 00100 | Validation loss: 1.015328\n",
      "Step 00100/00701 | Training loss: 1.958903| lrm: 0.857347| num_tokens: 10,527\n",
      "Step 00101/00701 | Training loss: 0.437290| lrm: 0.855920| num_tokens: 9,838\n",
      "Step 00102/00701 | Training loss: 0.566663| lrm: 0.854494| num_tokens: 10,665\n",
      "Step 00103/00701 | Training loss: 0.716551| lrm: 0.853067| num_tokens: 11,272\n",
      "Step 00104/00701 | Training loss: 0.712181| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.763239| lrm: 0.850214| num_tokens: 10,153\n",
      "Step 00106/00701 | Training loss: 0.827196| lrm: 0.848787| num_tokens: 11,156\n",
      "Step 00107/00701 | Training loss: 1.047147| lrm: 0.847361| num_tokens: 10,161\n",
      "Step 00108/00701 | Training loss: 0.941858| lrm: 0.845934| num_tokens: 8,469\n",
      "Step 00109/00701 | Training loss: 0.682140| lrm: 0.844508| num_tokens: 8,740\n",
      "Step 00110/00701 | Training loss: 0.816670| lrm: 0.843081| num_tokens: 12,241\n",
      "Step 00111/00701 | Training loss: 0.704741| lrm: 0.841655| num_tokens: 10,737\n",
      "Step 00112/00701 | Training loss: 1.482601| lrm: 0.840228| num_tokens: 14,866\n",
      "Step 00113/00701 | Training loss: 0.758488| lrm: 0.838802| num_tokens: 16,098\n",
      "Step 00114/00701 | Training loss: 1.106492| lrm: 0.837375| num_tokens: 14,309\n",
      "Step 00115/00701 | Training loss: 0.317848| lrm: 0.835949| num_tokens: 11,090\n",
      "Step 00116/00701 | Training loss: 0.795623| lrm: 0.834522| num_tokens: 9,127\n",
      "Step 00117/00701 | Training loss: 0.924010| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.644112| lrm: 0.831669| num_tokens: 9,686\n",
      "Step 00119/00701 | Training loss: 0.644908| lrm: 0.830243| num_tokens: 14,012\n",
      "Step 00120/00701 | Training loss: 0.973647| lrm: 0.828816| num_tokens: 12,993\n",
      "Step 00121/00701 | Training loss: 1.082420| lrm: 0.827389| num_tokens: 16,514\n",
      "Step 00122/00701 | Training loss: 0.820608| lrm: 0.825963| num_tokens: 7,264\n",
      "Step 00123/00701 | Training loss: 0.875020| lrm: 0.824536| num_tokens: 6,636\n",
      "Step 00124/00701 | Training loss: 0.693665| lrm: 0.823110| num_tokens: 11,608\n",
      "Step 00125/00701 | Training loss: 0.982668| lrm: 0.821683| num_tokens: 5,773\n",
      "Step 00126/00701 | Training loss: 0.629157| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 1.057260| lrm: 0.818830| num_tokens: 10,148\n",
      "Step 00128/00701 | Training loss: 0.452682| lrm: 0.817404| num_tokens: 10,627\n",
      "Step 00129/00701 | Training loss: 0.826937| lrm: 0.815977| num_tokens: 8,873\n",
      "Step 00130/00701 | Training loss: 0.953214| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.938390| lrm: 0.813124| num_tokens: 11,228\n",
      "Step 00132/00701 | Training loss: 0.808779| lrm: 0.811698| num_tokens: 12,012\n",
      "Step 00133/00701 | Training loss: 0.909837| lrm: 0.810271| num_tokens: 16,435\n",
      "Step 00134/00701 | Training loss: 0.700515| lrm: 0.808845| num_tokens: 10,741\n",
      "Step 00135/00701 | Training loss: 1.374817| lrm: 0.807418| num_tokens: 13,492\n",
      "Step 00136/00701 | Training loss: 0.507819| lrm: 0.805991| num_tokens: 10,194\n",
      "Step 00137/00701 | Training loss: 0.884268| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.743246| lrm: 0.803138| num_tokens: 11,043\n",
      "Step 00139/00701 | Training loss: 0.959753| lrm: 0.801712| num_tokens: 12,147\n",
      "Step 00140/00701 | Training loss: 0.965308| lrm: 0.800285| num_tokens: 13,088\n",
      "Step 00141/00701 | Training loss: 0.807813| lrm: 0.798859| num_tokens: 13,647\n",
      "Step 00142/00701 | Training loss: 0.470622| lrm: 0.797432| num_tokens: 10,286\n",
      "Step 00143/00701 | Training loss: 1.173398| lrm: 0.796006| num_tokens: 8,956\n",
      "Step 00144/00701 | Training loss: 1.255822| lrm: 0.794579| num_tokens: 14,847\n",
      "Step 00145/00701 | Training loss: 0.959426| lrm: 0.793153| num_tokens: 14,649\n",
      "Step 00146/00701 | Training loss: 0.575693| lrm: 0.791726| num_tokens: 11,999\n",
      "Step 00147/00701 | Training loss: 0.644399| lrm: 0.790300| num_tokens: 8,022\n",
      "Step 00148/00701 | Training loss: 1.042178| lrm: 0.788873| num_tokens: 11,348\n",
      "Step 00149/00701 | Training loss: 1.078557| lrm: 0.787447| num_tokens: 14,182\n",
      "Step 00150/00701 | Training loss: 0.625291| lrm: 0.786020| num_tokens: 10,776\n",
      "Step 00151/00701 | Training loss: 0.673901| lrm: 0.784593| num_tokens: 14,039\n",
      "Step 00152/00701 | Training loss: 0.787467| lrm: 0.783167| num_tokens: 8,526\n",
      "Step 00153/00701 | Training loss: 0.827275| lrm: 0.781740| num_tokens: 12,782\n",
      "Step 00154/00701 | Training loss: 0.761340| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.833909| lrm: 0.778887| num_tokens: 10,624\n",
      "Step 00156/00701 | Training loss: 0.576011| lrm: 0.777461| num_tokens: 13,575\n",
      "Step 00157/00701 | Training loss: 1.127683| lrm: 0.776034| num_tokens: 13,319\n",
      "Step 00158/00701 | Training loss: 0.543063| lrm: 0.774608| num_tokens: 7,492\n",
      "Step 00159/00701 | Training loss: 0.720461| lrm: 0.773181| num_tokens: 9,274\n",
      "Step 00160/00701 | Training loss: 0.646766| lrm: 0.771755| num_tokens: 9,399\n",
      "Step 00161/00701 | Training loss: 0.982518| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.984915| lrm: 0.768902| num_tokens: 11,779\n",
      "Step 00163/00701 | Training loss: 0.493484| lrm: 0.767475| num_tokens: 7,485\n",
      "Step 00164/00701 | Training loss: 0.417228| lrm: 0.766049| num_tokens: 9,081\n",
      "Step 00165/00701 | Training loss: 0.752925| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.956490| lrm: 0.763195| num_tokens: 12,309\n",
      "Step 00167/00701 | Training loss: 0.902270| lrm: 0.761769| num_tokens: 10,547\n",
      "Step 00168/00701 | Training loss: 0.702792| lrm: 0.760342| num_tokens: 9,717\n",
      "Step 00169/00701 | Training loss: 0.880850| lrm: 0.758916| num_tokens: 9,577\n",
      "Step 00170/00701 | Training loss: 1.304231| lrm: 0.757489| num_tokens: 13,464\n",
      "Step 00171/00701 | Training loss: 0.680150| lrm: 0.756063| num_tokens: 10,156\n",
      "Step 00172/00701 | Training loss: 0.960426| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.971035| lrm: 0.753210| num_tokens: 13,931\n",
      "Step 00174/00701 | Training loss: 0.943938| lrm: 0.751783| num_tokens: 9,132\n",
      "Step 00175/00701 | Training loss: 1.048836| lrm: 0.750357| num_tokens: 9,835\n",
      "Step 00176/00701 | Training loss: 0.991278| lrm: 0.748930| num_tokens: 11,244\n",
      "Step 00177/00701 | Training loss: 0.983139| lrm: 0.747504| num_tokens: 14,670\n",
      "Step 00178/00701 | Training loss: 0.633161| lrm: 0.746077| num_tokens: 9,624\n",
      "Step 00179/00701 | Training loss: 0.953218| lrm: 0.744650| num_tokens: 11,085\n",
      "Step 00180/00701 | Training loss: 0.545334| lrm: 0.743224| num_tokens: 11,989\n",
      "Step 00181/00701 | Training loss: 0.875279| lrm: 0.741797| num_tokens: 15,749\n",
      "Step 00182/00701 | Training loss: 1.182448| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.485704| lrm: 0.738944| num_tokens: 12,476\n",
      "Step 00184/00701 | Training loss: 0.966782| lrm: 0.737518| num_tokens: 13,569\n",
      "Step 00185/00701 | Training loss: 0.663386| lrm: 0.736091| num_tokens: 11,265\n",
      "Step 00186/00701 | Training loss: 0.843901| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.480824| lrm: 0.733238| num_tokens: 13,609\n",
      "Step 00188/00701 | Training loss: 0.963596| lrm: 0.731812| num_tokens: 9,421\n",
      "Step 00189/00701 | Training loss: 0.972269| lrm: 0.730385| num_tokens: 9,300\n",
      "Step 00190/00701 | Training loss: 0.460047| lrm: 0.728959| num_tokens: 15,818\n",
      "Step 00191/00701 | Training loss: 0.906100| lrm: 0.727532| num_tokens: 12,340\n",
      "Step 00192/00701 | Training loss: 0.532396| lrm: 0.726106| num_tokens: 9,962\n",
      "Step 00193/00701 | Training loss: 1.101410| lrm: 0.724679| num_tokens: 10,650\n",
      "Step 00194/00701 | Training loss: 1.214571| lrm: 0.723252| num_tokens: 8,192\n",
      "Step 00195/00701 | Training loss: 0.546762| lrm: 0.721826| num_tokens: 14,326\n",
      "Step 00196/00701 | Training loss: 0.761817| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 1.041702| lrm: 0.718973| num_tokens: 10,264\n",
      "Step 00198/00701 | Training loss: 0.583133| lrm: 0.717546| num_tokens: 9,718\n",
      "Step 00199/00701 | Training loss: 0.616575| lrm: 0.716120| num_tokens: 10,337\n",
      "Step 00200 | Validation loss: 1.014387\n",
      "final: 337/1024 (32.91%)\n",
      "final: 421/1024 (41.11%)\n",
      "Step 00200 | mmlu_acc: 0.329102, arc_easy_acc: 0.411133\n",
      "Step 00200/00701 | Training loss: 0.845461| lrm: 0.714693| num_tokens: 17,787\n",
      "Step 00201/00701 | Training loss: 0.880365| lrm: 0.713267| num_tokens: 9,676\n",
      "Step 00202/00701 | Training loss: 1.929716| lrm: 0.711840| num_tokens: 15,694\n",
      "Step 00203/00701 | Training loss: 1.075365| lrm: 0.710414| num_tokens: 8,344\n",
      "Step 00204/00701 | Training loss: 0.990828| lrm: 0.708987| num_tokens: 11,398\n",
      "Step 00205/00701 | Training loss: 0.654476| lrm: 0.707561| num_tokens: 11,165\n",
      "Step 00206/00701 | Training loss: 0.902601| lrm: 0.706134| num_tokens: 10,515\n",
      "Step 00207/00701 | Training loss: 0.640342| lrm: 0.704708| num_tokens: 11,281\n",
      "Step 00208/00701 | Training loss: 1.289678| lrm: 0.703281| num_tokens: 13,195\n",
      "Step 00209/00701 | Training loss: 0.570659| lrm: 0.701854| num_tokens: 10,620\n",
      "Step 00210/00701 | Training loss: 0.716900| lrm: 0.700428| num_tokens: 11,855\n",
      "Step 00211/00701 | Training loss: 1.063880| lrm: 0.699001| num_tokens: 11,439\n",
      "Step 00212/00701 | Training loss: 1.012751| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.735154| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 1.103364| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.676727| lrm: 0.693295| num_tokens: 10,049\n",
      "Step 00216/00701 | Training loss: 0.755634| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 0.992571| lrm: 0.690442| num_tokens: 8,208\n",
      "Step 00218/00701 | Training loss: 0.479463| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.885634| lrm: 0.687589| num_tokens: 14,707\n",
      "Step 00220/00701 | Training loss: 0.931339| lrm: 0.686163| num_tokens: 8,471\n",
      "Step 00221/00701 | Training loss: 0.770094| lrm: 0.684736| num_tokens: 7,856\n",
      "Step 00222/00701 | Training loss: 1.105054| lrm: 0.683310| num_tokens: 7,401\n",
      "Step 00223/00701 | Training loss: 1.151728| lrm: 0.681883| num_tokens: 11,899\n",
      "Step 00224/00701 | Training loss: 0.713125| lrm: 0.680456| num_tokens: 10,060\n",
      "Step 00225/00701 | Training loss: 1.127406| lrm: 0.679030| num_tokens: 8,888\n",
      "Step 00226/00701 | Training loss: 0.646301| lrm: 0.677603| num_tokens: 10,747\n",
      "Step 00227/00701 | Training loss: 0.596178| lrm: 0.676177| num_tokens: 10,038\n",
      "Step 00228/00701 | Training loss: 0.703604| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 1.138293| lrm: 0.673324| num_tokens: 9,397\n",
      "Step 00230/00701 | Training loss: 1.014665| lrm: 0.671897| num_tokens: 7,844\n",
      "Step 00231/00701 | Training loss: 1.210574| lrm: 0.670471| num_tokens: 11,685\n",
      "Step 00232/00701 | Training loss: 1.648922| lrm: 0.669044| num_tokens: 10,805\n",
      "Step 00233/00701 | Training loss: 0.739726| lrm: 0.667618| num_tokens: 10,203\n",
      "Step 00234/00701 | Training loss: 0.882960| lrm: 0.666191| num_tokens: 14,225\n",
      "Step 00235/00701 | Training loss: 1.032285| lrm: 0.664765| num_tokens: 9,748\n",
      "Step 00236/00701 | Training loss: 1.010737| lrm: 0.663338| num_tokens: 7,453\n",
      "Step 00237/00701 | Training loss: 0.842926| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 1.007081| lrm: 0.660485| num_tokens: 13,482\n",
      "Step 00239/00701 | Training loss: 1.262272| lrm: 0.659058| num_tokens: 12,515\n",
      "Step 00240/00701 | Training loss: 0.559930| lrm: 0.657632| num_tokens: 12,885\n",
      "Step 00241/00701 | Training loss: 1.007358| lrm: 0.656205| num_tokens: 11,284\n",
      "Step 00242/00701 | Training loss: 0.899789| lrm: 0.654779| num_tokens: 8,141\n",
      "Step 00243/00701 | Training loss: 0.666988| lrm: 0.653352| num_tokens: 10,581\n",
      "Step 00244/00701 | Training loss: 0.468304| lrm: 0.651926| num_tokens: 6,552\n",
      "Step 00245/00701 | Training loss: 1.032043| lrm: 0.650499| num_tokens: 12,896\n",
      "Step 00246/00701 | Training loss: 0.324738| lrm: 0.649073| num_tokens: 8,779\n",
      "Step 00247/00701 | Training loss: 0.559328| lrm: 0.647646| num_tokens: 10,640\n",
      "Step 00248/00701 | Training loss: 0.807468| lrm: 0.646220| num_tokens: 14,873\n",
      "Step 00249/00701 | Training loss: 0.707996| lrm: 0.644793| num_tokens: 8,755\n",
      "Step 00250/00701 | Training loss: 1.023973| lrm: 0.643367| num_tokens: 11,347\n",
      "Step 00251/00701 | Training loss: 1.206210| lrm: 0.641940| num_tokens: 13,773\n",
      "Step 00252/00701 | Training loss: 0.693790| lrm: 0.640514| num_tokens: 10,831\n",
      "Step 00253/00701 | Training loss: 0.820509| lrm: 0.639087| num_tokens: 14,171\n",
      "Step 00254/00701 | Training loss: 0.861663| lrm: 0.637660| num_tokens: 8,633\n",
      "Step 00255/00701 | Training loss: 0.743553| lrm: 0.636234| num_tokens: 10,649\n",
      "Step 00256/00701 | Training loss: 1.408461| lrm: 0.634807| num_tokens: 10,565\n",
      "Step 00257/00701 | Training loss: 0.537194| lrm: 0.633381| num_tokens: 8,375\n",
      "Step 00258/00701 | Training loss: 1.019293| lrm: 0.631954| num_tokens: 11,893\n",
      "Step 00259/00701 | Training loss: 0.909792| lrm: 0.630528| num_tokens: 6,289\n",
      "Step 00260/00701 | Training loss: 0.453441| lrm: 0.629101| num_tokens: 12,731\n",
      "Step 00261/00701 | Training loss: 0.677712| lrm: 0.627675| num_tokens: 11,069\n",
      "Step 00262/00701 | Training loss: 0.696361| lrm: 0.626248| num_tokens: 10,292\n",
      "Step 00263/00701 | Training loss: 0.622252| lrm: 0.624822| num_tokens: 6,973\n",
      "Step 00264/00701 | Training loss: 1.045600| lrm: 0.623395| num_tokens: 9,513\n",
      "Step 00265/00701 | Training loss: 0.819941| lrm: 0.621969| num_tokens: 11,233\n",
      "Step 00266/00701 | Training loss: 0.844081| lrm: 0.620542| num_tokens: 11,266\n",
      "Step 00267/00701 | Training loss: 0.547330| lrm: 0.619116| num_tokens: 8,955\n",
      "Step 00268/00701 | Training loss: 0.921792| lrm: 0.617689| num_tokens: 12,249\n",
      "Step 00269/00701 | Training loss: 0.757132| lrm: 0.616262| num_tokens: 9,576\n",
      "Step 00270/00701 | Training loss: 1.142734| lrm: 0.614836| num_tokens: 12,109\n",
      "Step 00271/00701 | Training loss: 1.393438| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.856354| lrm: 0.611983| num_tokens: 13,072\n",
      "Step 00273/00701 | Training loss: 0.622260| lrm: 0.610556| num_tokens: 9,778\n",
      "Step 00274/00701 | Training loss: 0.653962| lrm: 0.609130| num_tokens: 12,238\n",
      "Step 00275/00701 | Training loss: 0.583345| lrm: 0.607703| num_tokens: 10,854\n",
      "Step 00276/00701 | Training loss: 0.707124| lrm: 0.606277| num_tokens: 10,720\n",
      "Step 00277/00701 | Training loss: 0.571048| lrm: 0.604850| num_tokens: 7,570\n",
      "Step 00278/00701 | Training loss: 0.823057| lrm: 0.603424| num_tokens: 11,628\n",
      "Step 00279/00701 | Training loss: 0.891235| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280/00701 | Training loss: 0.776685| lrm: 0.600571| num_tokens: 8,970\n",
      "Step 00281/00701 | Training loss: 1.005744| lrm: 0.599144| num_tokens: 14,104\n",
      "Step 00282/00701 | Training loss: 0.992313| lrm: 0.597718| num_tokens: 10,950\n",
      "Step 00283/00701 | Training loss: 0.619132| lrm: 0.596291| num_tokens: 8,906\n",
      "Step 00284/00701 | Training loss: 0.430709| lrm: 0.594864| num_tokens: 12,614\n",
      "Step 00285/00701 | Training loss: 0.862250| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.598950| lrm: 0.592011| num_tokens: 11,914\n",
      "Step 00287/00701 | Training loss: 0.263390| lrm: 0.590585| num_tokens: 9,547\n",
      "Step 00288/00701 | Training loss: 0.678737| lrm: 0.589158| num_tokens: 5,007\n",
      "Step 00289/00701 | Training loss: 0.683976| lrm: 0.587732| num_tokens: 9,433\n",
      "Step 00290/00701 | Training loss: 1.071664| lrm: 0.586305| num_tokens: 11,035\n",
      "Step 00291/00701 | Training loss: 0.997424| lrm: 0.584879| num_tokens: 10,852\n",
      "Step 00292/00701 | Training loss: 0.949528| lrm: 0.583452| num_tokens: 11,659\n",
      "Step 00293/00701 | Training loss: 0.829581| lrm: 0.582026| num_tokens: 15,038\n",
      "Step 00294/00701 | Training loss: 0.725043| lrm: 0.580599| num_tokens: 9,439\n",
      "Step 00295/00701 | Training loss: 1.108504| lrm: 0.579173| num_tokens: 6,998\n",
      "Step 00296/00701 | Training loss: 1.222355| lrm: 0.577746| num_tokens: 12,267\n",
      "Step 00297/00701 | Training loss: 0.578472| lrm: 0.576320| num_tokens: 17,571\n",
      "Step 00298/00701 | Training loss: 1.035961| lrm: 0.574893| num_tokens: 10,103\n",
      "Step 00299/00701 | Training loss: 0.640279| lrm: 0.573466| num_tokens: 7,731\n",
      "Step 00300 | Validation loss: 1.014177\n",
      "Step 00300/00701 | Training loss: 0.750367| lrm: 0.572040| num_tokens: 8,914\n",
      "Step 00301/00701 | Training loss: 0.829816| lrm: 0.570613| num_tokens: 12,830\n",
      "Step 00302/00701 | Training loss: 0.767474| lrm: 0.569187| num_tokens: 12,209\n",
      "Step 00303/00701 | Training loss: 0.957773| lrm: 0.567760| num_tokens: 11,589\n",
      "Step 00304/00701 | Training loss: 0.611312| lrm: 0.566334| num_tokens: 8,228\n",
      "Step 00305/00701 | Training loss: 0.863311| lrm: 0.564907| num_tokens: 14,637\n",
      "Step 00306/00701 | Training loss: 0.887113| lrm: 0.563481| num_tokens: 11,570\n",
      "Step 00307/00701 | Training loss: 1.094733| lrm: 0.562054| num_tokens: 12,881\n",
      "Step 00308/00701 | Training loss: 0.666774| lrm: 0.560628| num_tokens: 6,839\n",
      "Step 00309/00701 | Training loss: 0.939561| lrm: 0.559201| num_tokens: 11,009\n",
      "Step 00310/00701 | Training loss: 0.582417| lrm: 0.557775| num_tokens: 12,118\n",
      "Step 00311/00701 | Training loss: 1.269598| lrm: 0.556348| num_tokens: 10,305\n",
      "Step 00312/00701 | Training loss: 0.747044| lrm: 0.554922| num_tokens: 11,427\n",
      "Step 00313/00701 | Training loss: 0.741287| lrm: 0.553495| num_tokens: 8,436\n",
      "Step 00314/00701 | Training loss: 1.103581| lrm: 0.552068| num_tokens: 11,858\n",
      "Step 00315/00701 | Training loss: 0.532871| lrm: 0.550642| num_tokens: 14,168\n",
      "Step 00316/00701 | Training loss: 1.129597| lrm: 0.549215| num_tokens: 6,451\n",
      "Step 00317/00701 | Training loss: 0.407079| lrm: 0.547789| num_tokens: 11,643\n",
      "Step 00318/00701 | Training loss: 0.709746| lrm: 0.546362| num_tokens: 10,043\n",
      "Step 00319/00701 | Training loss: 1.214774| lrm: 0.544936| num_tokens: 11,387\n",
      "Step 00320/00701 | Training loss: 0.615413| lrm: 0.543509| num_tokens: 6,437\n",
      "Step 00321/00701 | Training loss: 1.083300| lrm: 0.542083| num_tokens: 8,880\n",
      "Step 00322/00701 | Training loss: 0.750680| lrm: 0.540656| num_tokens: 4,147\n",
      "Step 00323/00701 | Training loss: 0.794005| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.978947| lrm: 0.537803| num_tokens: 11,807\n",
      "Step 00325/00701 | Training loss: 0.634220| lrm: 0.536377| num_tokens: 9,002\n",
      "Step 00326/00701 | Training loss: 0.998818| lrm: 0.534950| num_tokens: 7,345\n",
      "Step 00327/00701 | Training loss: 0.707678| lrm: 0.533524| num_tokens: 12,006\n",
      "Step 00328/00701 | Training loss: 1.044683| lrm: 0.532097| num_tokens: 12,714\n",
      "Step 00329/00701 | Training loss: 0.653810| lrm: 0.530670| num_tokens: 10,357\n",
      "Step 00330/00701 | Training loss: 1.009555| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.806137| lrm: 0.527817| num_tokens: 7,793\n",
      "Step 00332/00701 | Training loss: 0.728215| lrm: 0.526391| num_tokens: 10,629\n",
      "Step 00333/00701 | Training loss: 1.274387| lrm: 0.524964| num_tokens: 9,785\n",
      "Step 00334/00701 | Training loss: 0.376988| lrm: 0.523538| num_tokens: 12,100\n",
      "Step 00335/00701 | Training loss: 0.442277| lrm: 0.522111| num_tokens: 7,312\n",
      "Step 00336/00701 | Training loss: 1.149877| lrm: 0.520685| num_tokens: 12,905\n",
      "Step 00337/00701 | Training loss: 0.767269| lrm: 0.519258| num_tokens: 11,898\n",
      "Step 00338/00701 | Training loss: 0.378860| lrm: 0.517832| num_tokens: 10,202\n",
      "Step 00339/00701 | Training loss: 0.821467| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340/00701 | Training loss: 0.774973| lrm: 0.514979| num_tokens: 11,148\n",
      "Step 00341/00701 | Training loss: 1.192931| lrm: 0.513552| num_tokens: 8,979\n",
      "Step 00342/00701 | Training loss: 0.706850| lrm: 0.512126| num_tokens: 6,240\n",
      "Step 00343/00701 | Training loss: 1.131393| lrm: 0.510699| num_tokens: 11,617\n",
      "Step 00344/00701 | Training loss: 0.560441| lrm: 0.509272| num_tokens: 7,701\n",
      "Step 00345/00701 | Training loss: 0.800126| lrm: 0.507846| num_tokens: 12,529\n",
      "Step 00346/00701 | Training loss: 0.887818| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.678529| lrm: 0.504993| num_tokens: 11,169\n",
      "Step 00348/00701 | Training loss: 1.320755| lrm: 0.503566| num_tokens: 9,517\n",
      "Step 00349/00701 | Training loss: 0.666048| lrm: 0.502140| num_tokens: 10,291\n",
      "Step 00350/00701 | Training loss: 0.848818| lrm: 0.500713| num_tokens: 9,018\n",
      "Step 00351/00701 | Training loss: 0.765853| lrm: 0.499287| num_tokens: 11,729\n",
      "Step 00352/00701 | Training loss: 1.033631| lrm: 0.497860| num_tokens: 12,675\n",
      "Step 00353/00701 | Training loss: 0.863884| lrm: 0.496434| num_tokens: 8,383\n",
      "Step 00354/00701 | Training loss: 0.757238| lrm: 0.495007| num_tokens: 10,059\n",
      "Step 00355/00701 | Training loss: 0.834382| lrm: 0.493581| num_tokens: 10,087\n",
      "Step 00356/00701 | Training loss: 0.656791| lrm: 0.492154| num_tokens: 8,333\n",
      "Step 00357/00701 | Training loss: 0.653254| lrm: 0.490728| num_tokens: 8,266\n",
      "Step 00358/00701 | Training loss: 0.681418| lrm: 0.489301| num_tokens: 9,973\n",
      "Step 00359/00701 | Training loss: 0.709127| lrm: 0.487874| num_tokens: 9,013\n",
      "Step 00360/00701 | Training loss: 0.533700| lrm: 0.486448| num_tokens: 8,617\n",
      "Step 00361/00701 | Training loss: 0.743804| lrm: 0.485021| num_tokens: 11,847\n",
      "Step 00362/00701 | Training loss: 1.221250| lrm: 0.483595| num_tokens: 11,199\n",
      "Step 00363/00701 | Training loss: 0.464363| lrm: 0.482168| num_tokens: 11,212\n",
      "Step 00364/00701 | Training loss: 0.674821| lrm: 0.480742| num_tokens: 13,298\n",
      "Step 00365/00701 | Training loss: 0.521399| lrm: 0.479315| num_tokens: 7,758\n",
      "Step 00366/00701 | Training loss: 0.951434| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.769916| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 1.136258| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.822047| lrm: 0.473609| num_tokens: 13,353\n",
      "Step 00370/00701 | Training loss: 0.746739| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.923972| lrm: 0.470756| num_tokens: 11,940\n",
      "Step 00372/00701 | Training loss: 0.935224| lrm: 0.469330| num_tokens: 11,410\n",
      "Step 00373/00701 | Training loss: 0.551670| lrm: 0.467903| num_tokens: 8,816\n",
      "Step 00374/00701 | Training loss: 0.851986| lrm: 0.466476| num_tokens: 13,098\n",
      "Step 00375/00701 | Training loss: 1.159198| lrm: 0.465050| num_tokens: 13,724\n",
      "Step 00376/00701 | Training loss: 1.184594| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.672412| lrm: 0.462197| num_tokens: 13,890\n",
      "Step 00378/00701 | Training loss: 0.665286| lrm: 0.460770| num_tokens: 11,856\n",
      "Step 00379/00701 | Training loss: 0.897652| lrm: 0.459344| num_tokens: 7,960\n",
      "Step 00380/00701 | Training loss: 1.287477| lrm: 0.457917| num_tokens: 8,519\n",
      "Step 00381/00701 | Training loss: 0.906648| lrm: 0.456491| num_tokens: 14,234\n",
      "Step 00382/00701 | Training loss: 0.645375| lrm: 0.455064| num_tokens: 13,353\n",
      "Step 00383/00701 | Training loss: 0.529233| lrm: 0.453638| num_tokens: 15,146\n",
      "Step 00384/00701 | Training loss: 0.641959| lrm: 0.452211| num_tokens: 12,589\n",
      "Step 00385/00701 | Training loss: 0.433636| lrm: 0.450785| num_tokens: 10,105\n",
      "Step 00386/00701 | Training loss: 1.023193| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 1.039386| lrm: 0.447932| num_tokens: 9,869\n",
      "Step 00388/00701 | Training loss: 0.914872| lrm: 0.446505| num_tokens: 11,292\n",
      "Step 00389/00701 | Training loss: 0.834147| lrm: 0.445078| num_tokens: 15,855\n",
      "Step 00390/00701 | Training loss: 0.506380| lrm: 0.443652| num_tokens: 12,768\n",
      "Step 00391/00701 | Training loss: 0.675353| lrm: 0.442225| num_tokens: 13,893\n",
      "Step 00392/00701 | Training loss: 0.354597| lrm: 0.440799| num_tokens: 10,495\n",
      "Step 00393/00701 | Training loss: 1.291672| lrm: 0.439372| num_tokens: 13,459\n",
      "Step 00394/00701 | Training loss: 0.992735| lrm: 0.437946| num_tokens: 13,515\n",
      "Step 00395/00701 | Training loss: 0.849719| lrm: 0.436519| num_tokens: 10,006\n",
      "Step 00396/00701 | Training loss: 0.962162| lrm: 0.435093| num_tokens: 11,009\n",
      "Step 00397/00701 | Training loss: 1.088743| lrm: 0.433666| num_tokens: 8,178\n",
      "Step 00398/00701 | Training loss: 0.998517| lrm: 0.432240| num_tokens: 8,070\n",
      "Step 00399/00701 | Training loss: 0.898805| lrm: 0.430813| num_tokens: 8,559\n",
      "Step 00400 | Validation loss: 1.013926\n",
      "final: 343/1024 (33.50%)\n",
      "final: 436/1024 (42.58%)\n",
      "Step 00400 | mmlu_acc: 0.334961, arc_easy_acc: 0.425781\n",
      "Step 00400/00701 | Training loss: 0.454365| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.742998| lrm: 0.427960| num_tokens: 6,689\n",
      "Step 00402/00701 | Training loss: 0.665460| lrm: 0.426534| num_tokens: 7,153\n",
      "Step 00403/00701 | Training loss: 1.253089| lrm: 0.425107| num_tokens: 15,333\n",
      "Step 00404/00701 | Training loss: 0.861789| lrm: 0.423680| num_tokens: 8,839\n",
      "Step 00405/00701 | Training loss: 0.696022| lrm: 0.422254| num_tokens: 10,865\n",
      "Step 00406/00701 | Training loss: 0.684231| lrm: 0.420827| num_tokens: 9,689\n",
      "Step 00407/00701 | Training loss: 0.558484| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.622027| lrm: 0.417974| num_tokens: 12,826\n",
      "Step 00409/00701 | Training loss: 0.599109| lrm: 0.416548| num_tokens: 8,557\n",
      "Step 00410/00701 | Training loss: 0.611217| lrm: 0.415121| num_tokens: 13,374\n",
      "Step 00411/00701 | Training loss: 0.886060| lrm: 0.413695| num_tokens: 12,917\n",
      "Step 00412/00701 | Training loss: 0.750359| lrm: 0.412268| num_tokens: 11,922\n",
      "Step 00413/00701 | Training loss: 0.860991| lrm: 0.410842| num_tokens: 7,315\n",
      "Step 00414/00701 | Training loss: 0.986311| lrm: 0.409415| num_tokens: 14,723\n",
      "Step 00415/00701 | Training loss: 0.745569| lrm: 0.407989| num_tokens: 13,804\n",
      "Step 00416/00701 | Training loss: 1.086437| lrm: 0.406562| num_tokens: 11,956\n",
      "Step 00417/00701 | Training loss: 0.660601| lrm: 0.405136| num_tokens: 12,018\n",
      "Step 00418/00701 | Training loss: 0.374447| lrm: 0.403709| num_tokens: 3,817\n",
      "Step 00419/00701 | Training loss: 0.973600| lrm: 0.402282| num_tokens: 9,950\n",
      "Step 00420/00701 | Training loss: 1.278507| lrm: 0.400856| num_tokens: 10,386\n",
      "Step 00421/00701 | Training loss: 0.828302| lrm: 0.399429| num_tokens: 12,796\n",
      "Step 00422/00701 | Training loss: 0.635186| lrm: 0.398003| num_tokens: 13,674\n",
      "Step 00423/00701 | Training loss: 0.580038| lrm: 0.396576| num_tokens: 15,302\n",
      "Step 00424/00701 | Training loss: 0.716406| lrm: 0.395150| num_tokens: 14,939\n",
      "Step 00425/00701 | Training loss: 0.860707| lrm: 0.393723| num_tokens: 7,669\n",
      "Step 00426/00701 | Training loss: 0.919262| lrm: 0.392297| num_tokens: 6,524\n",
      "Step 00427/00701 | Training loss: 0.866022| lrm: 0.390870| num_tokens: 8,753\n",
      "Step 00428/00701 | Training loss: 0.787993| lrm: 0.389444| num_tokens: 9,846\n",
      "Step 00429/00701 | Training loss: 0.690861| lrm: 0.388017| num_tokens: 10,688\n",
      "Step 00430/00701 | Training loss: 0.807599| lrm: 0.386591| num_tokens: 11,206\n",
      "Step 00431/00701 | Training loss: 1.610296| lrm: 0.385164| num_tokens: 8,919\n",
      "Step 00432/00701 | Training loss: 1.026293| lrm: 0.383738| num_tokens: 13,388\n",
      "Step 00433/00701 | Training loss: 0.595353| lrm: 0.382311| num_tokens: 10,474\n",
      "Step 00434/00701 | Training loss: 0.863686| lrm: 0.380884| num_tokens: 11,897\n",
      "Step 00435/00701 | Training loss: 0.673307| lrm: 0.379458| num_tokens: 12,165\n",
      "Step 00436/00701 | Training loss: 0.787738| lrm: 0.378031| num_tokens: 13,113\n",
      "Step 00437/00701 | Training loss: 0.947270| lrm: 0.376605| num_tokens: 12,838\n",
      "Step 00438/00701 | Training loss: 1.267431| lrm: 0.375178| num_tokens: 10,430\n",
      "Step 00439/00701 | Training loss: 0.820259| lrm: 0.373752| num_tokens: 11,832\n",
      "Step 00440/00701 | Training loss: 0.650451| lrm: 0.372325| num_tokens: 10,469\n",
      "Step 00441/00701 | Training loss: 0.782330| lrm: 0.370899| num_tokens: 11,808\n",
      "Step 00442/00701 | Training loss: 0.820891| lrm: 0.369472| num_tokens: 12,062\n",
      "Step 00443/00701 | Training loss: 0.765689| lrm: 0.368046| num_tokens: 10,750\n",
      "Step 00444/00701 | Training loss: 0.689519| lrm: 0.366619| num_tokens: 13,119\n",
      "Step 00445/00701 | Training loss: 0.799069| lrm: 0.365193| num_tokens: 9,910\n",
      "Step 00446/00701 | Training loss: 0.515249| lrm: 0.363766| num_tokens: 13,290\n",
      "Step 00447/00701 | Training loss: 1.072151| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.924978| lrm: 0.360913| num_tokens: 10,547\n",
      "Step 00449/00701 | Training loss: 0.823427| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450/00701 | Training loss: 1.063795| lrm: 0.358060| num_tokens: 15,847\n",
      "Step 00451/00701 | Training loss: 0.881583| lrm: 0.356633| num_tokens: 10,672\n",
      "Step 00452/00701 | Training loss: 0.808259| lrm: 0.355207| num_tokens: 9,304\n",
      "Step 00453/00701 | Training loss: 0.773178| lrm: 0.353780| num_tokens: 12,538\n",
      "Step 00454/00701 | Training loss: 0.933223| lrm: 0.352354| num_tokens: 7,953\n",
      "Step 00455/00701 | Training loss: 0.887118| lrm: 0.350927| num_tokens: 7,755\n",
      "Step 00456/00701 | Training loss: 1.001259| lrm: 0.349501| num_tokens: 10,549\n",
      "Step 00457/00701 | Training loss: 0.605444| lrm: 0.348074| num_tokens: 10,105\n",
      "Step 00458/00701 | Training loss: 0.838363| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.748580| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460/00701 | Training loss: 0.748132| lrm: 0.343795| num_tokens: 8,420\n",
      "Step 00461/00701 | Training loss: 0.807549| lrm: 0.342368| num_tokens: 12,615\n",
      "Step 00462/00701 | Training loss: 0.566788| lrm: 0.340942| num_tokens: 9,887\n",
      "Step 00463/00701 | Training loss: 1.127252| lrm: 0.339515| num_tokens: 7,312\n",
      "Step 00464/00701 | Training loss: 0.689198| lrm: 0.338088| num_tokens: 9,797\n",
      "Step 00465/00701 | Training loss: 1.062171| lrm: 0.336662| num_tokens: 8,338\n",
      "Step 00466/00701 | Training loss: 0.932409| lrm: 0.335235| num_tokens: 11,435\n",
      "Step 00467/00701 | Training loss: 0.654183| lrm: 0.333809| num_tokens: 10,835\n",
      "Step 00468/00701 | Training loss: 0.751071| lrm: 0.332382| num_tokens: 9,671\n",
      "Step 00469/00701 | Training loss: 0.443155| lrm: 0.330956| num_tokens: 8,338\n",
      "Step 00470/00701 | Training loss: 1.076118| lrm: 0.329529| num_tokens: 8,700\n",
      "Step 00471/00701 | Training loss: 0.941965| lrm: 0.328103| num_tokens: 15,219\n",
      "Step 00472/00701 | Training loss: 1.888145| lrm: 0.326676| num_tokens: 13,448\n",
      "Step 00473/00701 | Training loss: 0.658547| lrm: 0.325250| num_tokens: 9,585\n",
      "Step 00474/00701 | Training loss: 1.146420| lrm: 0.323823| num_tokens: 14,725\n",
      "Step 00475/00701 | Training loss: 0.921680| lrm: 0.322397| num_tokens: 12,688\n",
      "Step 00476/00701 | Training loss: 1.141465| lrm: 0.320970| num_tokens: 11,268\n",
      "Step 00477/00701 | Training loss: 1.008115| lrm: 0.319544| num_tokens: 13,478\n",
      "Step 00478/00701 | Training loss: 1.318456| lrm: 0.318117| num_tokens: 6,838\n",
      "Step 00479/00701 | Training loss: 0.759181| lrm: 0.316690| num_tokens: 13,516\n",
      "Step 00480/00701 | Training loss: 0.405249| lrm: 0.315264| num_tokens: 13,587\n",
      "Step 00481/00701 | Training loss: 1.081552| lrm: 0.313837| num_tokens: 7,015\n",
      "Step 00482/00701 | Training loss: 0.982089| lrm: 0.312411| num_tokens: 9,863\n",
      "Step 00483/00701 | Training loss: 0.688252| lrm: 0.310984| num_tokens: 11,048\n",
      "Step 00484/00701 | Training loss: 0.495565| lrm: 0.309558| num_tokens: 10,661\n",
      "Step 00485/00701 | Training loss: 1.128206| lrm: 0.308131| num_tokens: 10,617\n",
      "Step 00486/00701 | Training loss: 0.391807| lrm: 0.306705| num_tokens: 9,656\n",
      "Step 00487/00701 | Training loss: 0.664093| lrm: 0.305278| num_tokens: 8,590\n",
      "Step 00488/00701 | Training loss: 0.437688| lrm: 0.303852| num_tokens: 11,260\n",
      "Step 00489/00701 | Training loss: 0.947656| lrm: 0.302425| num_tokens: 12,388\n",
      "Step 00490/00701 | Training loss: 0.892280| lrm: 0.300999| num_tokens: 8,471\n",
      "Step 00491/00701 | Training loss: 0.927028| lrm: 0.299572| num_tokens: 8,826\n",
      "Step 00492/00701 | Training loss: 1.446177| lrm: 0.298146| num_tokens: 14,321\n",
      "Step 00493/00701 | Training loss: 0.573082| lrm: 0.296719| num_tokens: 7,284\n",
      "Step 00494/00701 | Training loss: 0.537140| lrm: 0.295292| num_tokens: 8,592\n",
      "Step 00495/00701 | Training loss: 0.869891| lrm: 0.293866| num_tokens: 10,854\n",
      "Step 00496/00701 | Training loss: 0.761368| lrm: 0.292439| num_tokens: 10,975\n",
      "Step 00497/00701 | Training loss: 0.724154| lrm: 0.291013| num_tokens: 11,925\n",
      "Step 00498/00701 | Training loss: 0.978635| lrm: 0.289586| num_tokens: 11,744\n",
      "Step 00499/00701 | Training loss: 0.294620| lrm: 0.288160| num_tokens: 14,263\n",
      "Step 00500 | Validation loss: 1.013912\n",
      "Step 00500/00701 | Training loss: 0.895040| lrm: 0.286733| num_tokens: 13,673\n",
      "Step 00501/00701 | Training loss: 0.632100| lrm: 0.285307| num_tokens: 9,954\n",
      "Step 00502/00701 | Training loss: 0.441999| lrm: 0.283880| num_tokens: 9,783\n",
      "Step 00503/00701 | Training loss: 1.389607| lrm: 0.282454| num_tokens: 13,075\n",
      "Step 00504/00701 | Training loss: 1.557498| lrm: 0.281027| num_tokens: 9,767\n",
      "Step 00505/00701 | Training loss: 0.832099| lrm: 0.279601| num_tokens: 8,895\n",
      "Step 00506/00701 | Training loss: 1.359221| lrm: 0.278174| num_tokens: 9,652\n",
      "Step 00507/00701 | Training loss: 0.592164| lrm: 0.276748| num_tokens: 6,756\n",
      "Step 00508/00701 | Training loss: 1.329718| lrm: 0.275321| num_tokens: 18,637\n",
      "Step 00509/00701 | Training loss: 0.861715| lrm: 0.273894| num_tokens: 9,678\n",
      "Step 00510/00701 | Training loss: 0.770280| lrm: 0.272468| num_tokens: 9,942\n",
      "Step 00511/00701 | Training loss: 0.579831| lrm: 0.271041| num_tokens: 5,586\n",
      "Step 00512/00701 | Training loss: 1.011439| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.790862| lrm: 0.268188| num_tokens: 8,743\n",
      "Step 00514/00701 | Training loss: 0.857125| lrm: 0.266762| num_tokens: 9,904\n",
      "Step 00515/00701 | Training loss: 1.182578| lrm: 0.265335| num_tokens: 10,734\n",
      "Step 00516/00701 | Training loss: 1.217284| lrm: 0.263909| num_tokens: 12,039\n",
      "Step 00517/00701 | Training loss: 0.550063| lrm: 0.262482| num_tokens: 10,739\n",
      "Step 00518/00701 | Training loss: 1.134809| lrm: 0.261056| num_tokens: 10,288\n",
      "Step 00519/00701 | Training loss: 0.850125| lrm: 0.259629| num_tokens: 8,447\n",
      "Step 00520/00701 | Training loss: 0.863014| lrm: 0.258203| num_tokens: 9,737\n",
      "Step 00521/00701 | Training loss: 0.971639| lrm: 0.256776| num_tokens: 14,760\n",
      "Step 00522/00701 | Training loss: 1.076437| lrm: 0.255350| num_tokens: 9,452\n",
      "Step 00523/00701 | Training loss: 0.654045| lrm: 0.253923| num_tokens: 11,752\n",
      "Step 00524/00701 | Training loss: 0.977240| lrm: 0.252496| num_tokens: 7,305\n",
      "Step 00525/00701 | Training loss: 1.536572| lrm: 0.251070| num_tokens: 10,430\n",
      "Step 00526/00701 | Training loss: 1.126051| lrm: 0.249643| num_tokens: 9,485\n",
      "Step 00527/00701 | Training loss: 1.167151| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 1.188938| lrm: 0.246790| num_tokens: 14,142\n",
      "Step 00529/00701 | Training loss: 1.139770| lrm: 0.245364| num_tokens: 14,963\n",
      "Step 00530/00701 | Training loss: 0.464711| lrm: 0.243937| num_tokens: 8,920\n",
      "Step 00531/00701 | Training loss: 0.615094| lrm: 0.242511| num_tokens: 10,497\n",
      "Step 00532/00701 | Training loss: 1.153061| lrm: 0.241084| num_tokens: 8,047\n",
      "Step 00533/00701 | Training loss: 1.047886| lrm: 0.239658| num_tokens: 11,665\n",
      "Step 00534/00701 | Training loss: 1.163792| lrm: 0.238231| num_tokens: 12,661\n",
      "Step 00535/00701 | Training loss: 0.643745| lrm: 0.236805| num_tokens: 9,332\n",
      "Step 00536/00701 | Training loss: 0.510786| lrm: 0.235378| num_tokens: 10,270\n",
      "Step 00537/00701 | Training loss: 0.697449| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.647171| lrm: 0.232525| num_tokens: 10,264\n",
      "Step 00539/00701 | Training loss: 0.809618| lrm: 0.231098| num_tokens: 13,034\n",
      "Step 00540/00701 | Training loss: 0.815122| lrm: 0.229672| num_tokens: 7,601\n",
      "Step 00541/00701 | Training loss: 1.027586| lrm: 0.228245| num_tokens: 15,932\n",
      "Step 00542/00701 | Training loss: 1.242524| lrm: 0.226819| num_tokens: 10,867\n",
      "Step 00543/00701 | Training loss: 0.368537| lrm: 0.225392| num_tokens: 11,887\n",
      "Step 00544/00701 | Training loss: 0.968391| lrm: 0.223966| num_tokens: 11,865\n",
      "Step 00545/00701 | Training loss: 0.764482| lrm: 0.222539| num_tokens: 13,438\n",
      "Step 00546/00701 | Training loss: 0.793318| lrm: 0.221113| num_tokens: 13,844\n",
      "Step 00547/00701 | Training loss: 1.264237| lrm: 0.219686| num_tokens: 6,023\n",
      "Step 00548/00701 | Training loss: 0.740336| lrm: 0.218260| num_tokens: 13,321\n",
      "Step 00549/00701 | Training loss: 1.072808| lrm: 0.216833| num_tokens: 8,358\n",
      "Step 00550/00701 | Training loss: 0.635213| lrm: 0.215407| num_tokens: 13,487\n",
      "Step 00551/00701 | Training loss: 0.762806| lrm: 0.213980| num_tokens: 11,180\n",
      "Step 00552/00701 | Training loss: 0.772764| lrm: 0.212553| num_tokens: 8,538\n",
      "Step 00553/00701 | Training loss: 0.257441| lrm: 0.211127| num_tokens: 10,572\n",
      "Step 00554/00701 | Training loss: 0.975797| lrm: 0.209700| num_tokens: 12,982\n",
      "Step 00555/00701 | Training loss: 0.892577| lrm: 0.208274| num_tokens: 11,588\n",
      "Step 00556/00701 | Training loss: 0.911105| lrm: 0.206847| num_tokens: 7,432\n",
      "Step 00557/00701 | Training loss: 0.654040| lrm: 0.205421| num_tokens: 9,354\n",
      "Step 00558/00701 | Training loss: 0.881186| lrm: 0.203994| num_tokens: 10,187\n",
      "Step 00559/00701 | Training loss: 1.006259| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560/00701 | Training loss: 0.511543| lrm: 0.201141| num_tokens: 13,597\n",
      "Step 00561/00701 | Training loss: 1.173938| lrm: 0.199715| num_tokens: 6,689\n",
      "Step 00562/00701 | Training loss: 0.242466| lrm: 0.198288| num_tokens: 8,682\n",
      "Step 00563/00701 | Training loss: 0.941020| lrm: 0.196862| num_tokens: 12,881\n",
      "Step 00564/00701 | Training loss: 0.948333| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 1.069259| lrm: 0.194009| num_tokens: 13,874\n",
      "Step 00566/00701 | Training loss: 0.997252| lrm: 0.192582| num_tokens: 12,213\n",
      "Step 00567/00701 | Training loss: 0.920084| lrm: 0.191155| num_tokens: 18,532\n",
      "Step 00568/00701 | Training loss: 0.703739| lrm: 0.189729| num_tokens: 10,003\n",
      "Step 00569/00701 | Training loss: 0.891854| lrm: 0.188302| num_tokens: 9,476\n",
      "Step 00570/00701 | Training loss: 1.258264| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.722613| lrm: 0.185449| num_tokens: 13,088\n",
      "Step 00572/00701 | Training loss: 0.691846| lrm: 0.184023| num_tokens: 7,260\n",
      "Step 00573/00701 | Training loss: 1.088633| lrm: 0.182596| num_tokens: 10,391\n",
      "Step 00574/00701 | Training loss: 0.743819| lrm: 0.181170| num_tokens: 14,540\n",
      "Step 00575/00701 | Training loss: 0.423666| lrm: 0.179743| num_tokens: 7,583\n",
      "Step 00576/00701 | Training loss: 1.753318| lrm: 0.178317| num_tokens: 8,834\n",
      "Step 00577/00701 | Training loss: 0.724228| lrm: 0.176890| num_tokens: 13,422\n",
      "Step 00578/00701 | Training loss: 0.578548| lrm: 0.175464| num_tokens: 8,190\n",
      "Step 00579/00701 | Training loss: 0.489242| lrm: 0.174037| num_tokens: 10,220\n",
      "Step 00580/00701 | Training loss: 0.994902| lrm: 0.172611| num_tokens: 9,082\n",
      "Step 00581/00701 | Training loss: 1.045395| lrm: 0.171184| num_tokens: 13,484\n",
      "Step 00582/00701 | Training loss: 1.097157| lrm: 0.169757| num_tokens: 8,858\n",
      "Step 00583/00701 | Training loss: 0.587941| lrm: 0.168331| num_tokens: 12,740\n",
      "Step 00584/00701 | Training loss: 0.824692| lrm: 0.166904| num_tokens: 13,825\n",
      "Step 00585/00701 | Training loss: 0.676627| lrm: 0.165478| num_tokens: 14,923\n",
      "Step 00586/00701 | Training loss: 0.978690| lrm: 0.164051| num_tokens: 9,923\n",
      "Step 00587/00701 | Training loss: 0.766578| lrm: 0.162625| num_tokens: 7,792\n",
      "Step 00588/00701 | Training loss: 0.931722| lrm: 0.161198| num_tokens: 12,331\n",
      "Step 00589/00701 | Training loss: 0.471542| lrm: 0.159772| num_tokens: 9,040\n",
      "Step 00590/00701 | Training loss: 0.543971| lrm: 0.158345| num_tokens: 10,334\n",
      "Step 00591/00701 | Training loss: 0.755062| lrm: 0.156919| num_tokens: 8,577\n",
      "Step 00592/00701 | Training loss: 0.981742| lrm: 0.155492| num_tokens: 11,743\n",
      "Step 00593/00701 | Training loss: 1.070625| lrm: 0.154066| num_tokens: 11,795\n",
      "Step 00594/00701 | Training loss: 0.243285| lrm: 0.152639| num_tokens: 11,224\n",
      "Step 00595/00701 | Training loss: 0.463198| lrm: 0.151213| num_tokens: 8,408\n",
      "Step 00596/00701 | Training loss: 0.745139| lrm: 0.149786| num_tokens: 5,658\n",
      "Step 00597/00701 | Training loss: 1.319093| lrm: 0.148359| num_tokens: 10,836\n",
      "Step 00598/00701 | Training loss: 0.748260| lrm: 0.146933| num_tokens: 9,759\n",
      "Step 00599/00701 | Training loss: 1.426071| lrm: 0.145506| num_tokens: 8,267\n",
      "Step 00600 | Validation loss: 1.013741\n",
      "final: 369/1024 (36.04%)\n",
      "final: 461/1024 (45.02%)\n",
      "Step 00600 | mmlu_acc: 0.360352, arc_easy_acc: 0.450195\n",
      "Step 00600/00701 | Training loss: 0.723584| lrm: 0.144080| num_tokens: 10,446\n",
      "Step 00601/00701 | Training loss: 1.064890| lrm: 0.142653| num_tokens: 11,003\n",
      "Step 00602/00701 | Training loss: 0.782937| lrm: 0.141227| num_tokens: 8,234\n",
      "Step 00603/00701 | Training loss: 1.099410| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.966727| lrm: 0.138374| num_tokens: 13,517\n",
      "Step 00605/00701 | Training loss: 0.762993| lrm: 0.136947| num_tokens: 10,453\n",
      "Step 00606/00701 | Training loss: 0.927216| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.644153| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.613206| lrm: 0.132668| num_tokens: 12,833\n",
      "Step 00609/00701 | Training loss: 1.123768| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610/00701 | Training loss: 0.498273| lrm: 0.129815| num_tokens: 14,179\n",
      "Step 00611/00701 | Training loss: 0.971896| lrm: 0.128388| num_tokens: 9,321\n",
      "Step 00612/00701 | Training loss: 0.847085| lrm: 0.126961| num_tokens: 9,523\n",
      "Step 00613/00701 | Training loss: 1.078206| lrm: 0.125535| num_tokens: 16,542\n",
      "Step 00614/00701 | Training loss: 0.553554| lrm: 0.124108| num_tokens: 15,348\n",
      "Step 00615/00701 | Training loss: 0.922114| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.616314| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.719755| lrm: 0.119829| num_tokens: 11,180\n",
      "Step 00618/00701 | Training loss: 0.566490| lrm: 0.118402| num_tokens: 8,842\n",
      "Step 00619/00701 | Training loss: 0.705564| lrm: 0.116976| num_tokens: 9,213\n",
      "Step 00620/00701 | Training loss: 0.639287| lrm: 0.115549| num_tokens: 12,340\n",
      "Step 00621/00701 | Training loss: 0.486365| lrm: 0.114123| num_tokens: 7,718\n",
      "Step 00622/00701 | Training loss: 0.963887| lrm: 0.112696| num_tokens: 7,350\n",
      "Step 00623/00701 | Training loss: 0.903794| lrm: 0.111270| num_tokens: 14,519\n",
      "Step 00624/00701 | Training loss: 1.415153| lrm: 0.109843| num_tokens: 12,368\n",
      "Step 00625/00701 | Training loss: 0.581019| lrm: 0.108417| num_tokens: 6,842\n",
      "Step 00626/00701 | Training loss: 0.732188| lrm: 0.106990| num_tokens: 11,577\n",
      "Step 00627/00701 | Training loss: 0.488634| lrm: 0.105563| num_tokens: 8,498\n",
      "Step 00628/00701 | Training loss: 0.768955| lrm: 0.104137| num_tokens: 6,965\n",
      "Step 00629/00701 | Training loss: 0.606374| lrm: 0.102710| num_tokens: 12,391\n",
      "Step 00630/00701 | Training loss: 0.562397| lrm: 0.101284| num_tokens: 10,252\n",
      "Step 00631/00701 | Training loss: 0.768108| lrm: 0.099857| num_tokens: 12,194\n",
      "Step 00632/00701 | Training loss: 0.644821| lrm: 0.098431| num_tokens: 10,964\n",
      "Step 00633/00701 | Training loss: 0.911435| lrm: 0.097004| num_tokens: 11,198\n",
      "Step 00634/00701 | Training loss: 1.004985| lrm: 0.095578| num_tokens: 7,808\n",
      "Step 00635/00701 | Training loss: 0.765791| lrm: 0.094151| num_tokens: 12,053\n",
      "Step 00636/00701 | Training loss: 0.800440| lrm: 0.092725| num_tokens: 8,138\n",
      "Step 00637/00701 | Training loss: 0.860528| lrm: 0.091298| num_tokens: 13,101\n",
      "Step 00638/00701 | Training loss: 0.563064| lrm: 0.089872| num_tokens: 8,339\n",
      "Step 00639/00701 | Training loss: 0.688751| lrm: 0.088445| num_tokens: 13,057\n",
      "Step 00640/00701 | Training loss: 0.970644| lrm: 0.087019| num_tokens: 12,703\n",
      "Step 00641/00701 | Training loss: 0.911181| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.348230| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.890224| lrm: 0.082739| num_tokens: 8,878\n",
      "Step 00644/00701 | Training loss: 0.841309| lrm: 0.081312| num_tokens: 8,861\n",
      "Step 00645/00701 | Training loss: 1.237996| lrm: 0.079886| num_tokens: 13,446\n",
      "Step 00646/00701 | Training loss: 0.757870| lrm: 0.078459| num_tokens: 12,789\n",
      "Step 00647/00701 | Training loss: 1.062829| lrm: 0.077033| num_tokens: 8,241\n",
      "Step 00648/00701 | Training loss: 0.857043| lrm: 0.075606| num_tokens: 10,330\n",
      "Step 00649/00701 | Training loss: 0.662846| lrm: 0.074180| num_tokens: 12,278\n",
      "Step 00650/00701 | Training loss: 1.329875| lrm: 0.072753| num_tokens: 7,245\n",
      "Step 00651/00701 | Training loss: 1.298386| lrm: 0.071327| num_tokens: 13,989\n",
      "Step 00652/00701 | Training loss: 1.142712| lrm: 0.069900| num_tokens: 11,516\n",
      "Step 00653/00701 | Training loss: 0.643388| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.798842| lrm: 0.067047| num_tokens: 8,702\n",
      "Step 00655/00701 | Training loss: 0.891710| lrm: 0.065621| num_tokens: 11,510\n",
      "Step 00656/00701 | Training loss: 1.254047| lrm: 0.064194| num_tokens: 11,144\n",
      "Step 00657/00701 | Training loss: 0.388240| lrm: 0.062767| num_tokens: 10,336\n",
      "Step 00658/00701 | Training loss: 1.089044| lrm: 0.061341| num_tokens: 9,779\n",
      "Step 00659/00701 | Training loss: 0.745089| lrm: 0.059914| num_tokens: 8,105\n",
      "Step 00660/00701 | Training loss: 0.794590| lrm: 0.058488| num_tokens: 10,476\n",
      "Step 00661/00701 | Training loss: 1.117360| lrm: 0.057061| num_tokens: 11,310\n",
      "Step 00662/00701 | Training loss: 0.641218| lrm: 0.055635| num_tokens: 7,871\n",
      "Step 00663/00701 | Training loss: 0.708222| lrm: 0.054208| num_tokens: 9,855\n",
      "Step 00664/00701 | Training loss: 0.668389| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.654920| lrm: 0.051355| num_tokens: 9,697\n",
      "Step 00666/00701 | Training loss: 1.035984| lrm: 0.049929| num_tokens: 10,710\n",
      "Step 00667/00701 | Training loss: 0.849809| lrm: 0.048502| num_tokens: 13,511\n",
      "Step 00668/00701 | Training loss: 0.642161| lrm: 0.047076| num_tokens: 11,917\n",
      "Step 00669/00701 | Training loss: 1.120616| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670/00701 | Training loss: 0.760606| lrm: 0.044223| num_tokens: 9,513\n",
      "Step 00671/00701 | Training loss: 0.790111| lrm: 0.042796| num_tokens: 10,895\n",
      "Step 00672/00701 | Training loss: 0.654604| lrm: 0.041369| num_tokens: 10,537\n",
      "Step 00673/00701 | Training loss: 0.693335| lrm: 0.039943| num_tokens: 9,859\n",
      "Step 00674/00701 | Training loss: 0.625317| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675/00701 | Training loss: 0.609220| lrm: 0.037090| num_tokens: 10,400\n",
      "Step 00676/00701 | Training loss: 0.856832| lrm: 0.035663| num_tokens: 10,101\n",
      "Step 00677/00701 | Training loss: 1.020502| lrm: 0.034237| num_tokens: 16,334\n",
      "Step 00678/00701 | Training loss: 0.340019| lrm: 0.032810| num_tokens: 12,035\n",
      "Step 00679/00701 | Training loss: 0.793614| lrm: 0.031384| num_tokens: 8,362\n",
      "Step 00680/00701 | Training loss: 1.207589| lrm: 0.029957| num_tokens: 12,050\n",
      "Step 00681/00701 | Training loss: 0.679295| lrm: 0.028531| num_tokens: 9,425\n",
      "Step 00682/00701 | Training loss: 0.720442| lrm: 0.027104| num_tokens: 14,504\n",
      "Step 00683/00701 | Training loss: 0.925368| lrm: 0.025678| num_tokens: 9,745\n",
      "Step 00684/00701 | Training loss: 1.596272| lrm: 0.024251| num_tokens: 12,770\n",
      "Step 00685/00701 | Training loss: 0.594653| lrm: 0.022825| num_tokens: 9,268\n",
      "Step 00686/00701 | Training loss: 0.529436| lrm: 0.021398| num_tokens: 10,168\n",
      "Step 00687/00701 | Training loss: 0.782242| lrm: 0.019971| num_tokens: 13,350\n",
      "Step 00688/00701 | Training loss: 0.705134| lrm: 0.018545| num_tokens: 10,458\n",
      "Step 00689/00701 | Training loss: 0.507080| lrm: 0.017118| num_tokens: 10,652\n",
      "Step 00690/00701 | Training loss: 0.424714| lrm: 0.015692| num_tokens: 8,627\n",
      "Step 00691/00701 | Training loss: 0.546477| lrm: 0.014265| num_tokens: 10,642\n",
      "Step 00692/00701 | Training loss: 0.910404| lrm: 0.012839| num_tokens: 11,930\n",
      "Step 00693/00701 | Training loss: 1.139586| lrm: 0.011412| num_tokens: 9,686\n",
      "Step 00694/00701 | Training loss: 1.217760| lrm: 0.009986| num_tokens: 10,109\n",
      "Step 00695/00701 | Training loss: 1.040032| lrm: 0.008559| num_tokens: 8,528\n",
      "Step 00696/00701 | Training loss: 0.529449| lrm: 0.007133| num_tokens: 12,544\n",
      "Step 00697/00701 | Training loss: 1.214769| lrm: 0.005706| num_tokens: 10,314\n",
      "Step 00698/00701 | Training loss: 0.545476| lrm: 0.004280| num_tokens: 8,859\n",
      "Step 00699/00701 | Training loss: 0.529429| lrm: 0.002853| num_tokens: 11,147\n",
      "Step 00700 | Validation loss: 1.013749\n",
      "final: 361/1024 (35.25%)\n",
      "final: 450/1024 (43.95%)\n",
      "Step 00700 | mmlu_acc: 0.352539, arc_easy_acc: 0.439453\n",
      "[W1121 00:59:01.639800055 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:01.760711762 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:01.766061877 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:01.903404287 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:01.960625186 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 703-711, summary, console lines 719-732 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–â–„â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–â–‚â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–‚â–‚â–ƒâ–„â–„â–…â–„â–„â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–…â–â–„â–ƒâ–…â–â–‚â–†â–†â–…â–„â–…â–„â–…â–‚â–ˆâ–…â–„â–„â–‚â–‚â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–ƒâ–…â–…â–…â–„â–‡â–…â–„â–‡â–„â–ƒâ–„â–†â–†â–ƒâ–…â–„â–‚â–…â–…â–…â–„â–…â–…â–ƒâ–â–†â–…â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–„â–â–ˆâ–ƒâ–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–ˆâ–†â–†â–†â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.43945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.35254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 0.52943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-30-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/ey2nnc2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_005652-ey2nnc2n/logs\u001b[0m\n",
      "[W1121 00:59:06.613742243 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:06.650019825 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 00:59:06.918674016 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --run=challenge-30-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83683073-59e4-4f12-807a-5fb056f1d568",
   "metadata": {},
   "source": [
    "^ Did not seem like it trained properly from the loss jumping all over the place. Number of tokens per step also seems very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a5bd-46fd-49cf-a196-e046755ead44",
   "metadata": {},
   "source": [
    "#### Do chat eval on sft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa3b021-9682-406c-895c-ee8a4f8f3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 01:10:47.798000 57235 torch/distributed/run.py:803] \n",
      "W1121 01:10:47.798000 57235 torch/distributed/run.py:803] *****************************************\n",
      "W1121 01:10:47.798000 57235 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 01:10:47.798000 57235 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1047/2376 (44.07%)\n",
      "ARC-Easy accuracy: 44.07%\n",
      "final: 372/1172 (31.74%)\n",
      "ARC-Challenge accuracy: 31.74%\n",
      "final: 4541/14042 (32.34%)\n",
      "MMLU accuracy: 32.34%\n",
      "\u001b[KRank 6 | 2/165 (1.21%)]\n",
      "\u001b[KRank 7 | 5/164 (3.05%)]\n",
      "\u001b[KRank 0 | 9/165 (5.45%)]\n",
      "\u001b[KRank 2 | 6/165 (3.64%)]\n",
      "\u001b[KRank 4 | 5/165 (3.03%)]\n",
      "\u001b[KRank 5 | 5/165 (3.03%)]\n",
      "\u001b[KRank 1 | 9/165 (5.45%)]\n",
      "\u001b[KRank 3 | 8/165 (4.85%)]\n",
      "==================================================\n",
      "final: 49/1319 (3.71%)\n",
      "GSM8K accuracy: 3.71%\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 5 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "==================================================\n",
      "final: 10/164 (6.10%)\n",
      "HumanEval accuracy: 6.10%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 6 | 31/32 (96.88%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 0 | 29/32 (90.62%)]\n",
      "\u001b[KRank 5 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 245/256 (95.70%)\n",
      "SpellingBee accuracy: 95.70%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8c587-cb33-41b7-989e-2ae7b39e251c",
   "metadata": {},
   "source": [
    "(while chat eval is running, remember to sftp the mid and sft checkpoints to my mac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da50529-b568-4f20-8958-7fad0560bced",
   "metadata": {},
   "source": [
    "### Back on my mac\n",
    "\n",
    "Terminated the lambda cloud machine. Used it for 0.88 hr at a cost of $21.02."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be3bdf-da76-477b-907d-ef8ca9db8373",
   "metadata": {},
   "source": [
    "There are a lot of things to look into. I'm also thinking I should add the reporting stuff soon because it's painful trying to compare accuracy numbers, etc. by looking back at output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cec644-c829-41c8-8f79-be839cd55df1",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- The CORE metric eval on the base model with max-per-task=500 came out to 0.2084. This was expected and confirms the answer to question 2 in `challenge-28-midtrain-d20/investigate-questions.ipynb`.\n",
    "\n",
    "- Final validation bpb on the retrained mid model was 0.3961. This is worse than the 0.3947 from previous training in `challenge-28-midtrain-d20/midtrain-d20.ipynb`.\n",
    "\n",
    "- Here's what the chat eval was on previous mid from `challenge-28-midtrain-d20/midtrain-d20.ipynb`) and retrained mid and sft trained from this notebook:\n",
    "\n",
    "```\n",
    "                  previous mid     retrained mid        sft\n",
    "\n",
    "ARC-Easy          42.80            43.18                44.07\n",
    "ARC-Challenge     31.31            33.19                31.74\n",
    "MMLU              32.53            33.07                32.34\n",
    "GSM8K             2.81             2.20                 3.71\n",
    "HumanEval         9.76             6.71                 6.10\n",
    "SpellingBee       96.88*           97.27                95.70\n",
    "\n",
    "* different test set due to fixing train/test split bug\n",
    "```\n",
    "\n",
    "Observations on the SFT train:\n",
    "\n",
    "- num_tokens per step seems very small compared to the ~500,000 we were seeing before. Not sure it's wrong but should git into it.\n",
    " \n",
    "- Training loss jumps around a lot and doesn't seem to decrease or increase overall. Again, not sure it's wrong, but look into it.\n",
    "\n",
    "- Validation loss is 1.011 at step 0 then jumps to 1.015 at step 100 and declines to 1.04 at the end. Is this expected?\n",
    "\n",
    "- Accuracy of ARC-Easy, which is part of the sft training data, goes up compared to mid. But the accuracy of ARC-Challenge, also part of the sft training data, goes down.\n",
    "\n",
    "- Accuracy of MMLU which is not part of the sft training data goes down.\n",
    "\n",
    "- Accuracy of GSM8K which is part of the sft training data goes up.\n",
    "\n",
    "- Accuracy of HumanEval, which is not part of the sft training data, goes down. (Reminder to look at examples that are right for HumanEval. I'm surprised it can even 6% right.)\n",
    "\n",
    "- Accuracy of SpellingBee, which is part of the sft training data (but a very small part especially compared to what a big percent it is in mid training), went down.\n",
    "\n",
    "Also, look into the warning: UserWarning: Please use the new API settings to control TF32 behavior\n",
    "\n",
    "In shor there's a lot to look into!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eace6d-9898-4363-85de-6db8881ec711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
