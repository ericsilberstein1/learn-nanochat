{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6875cde4-753c-4705-8735-cf5d6553808a",
   "metadata": {},
   "source": [
    "### Getting ready to train on 8xH100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9808ed4-3f55-4388-92e0-45823235867a",
   "metadata": {},
   "source": [
    "I'm requesting the \"right\" to use 8xH100 on paperspace on a Sat afternoon. Not sure how long it will take.\n",
    "\n",
    "No response as of Sunday morning. I see Lambda is cheaper. See what it looks like. Play with that in `trying-labmda-cloud.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c7c41-f5b3-4bca-ba95-20295be8c5f1",
   "metadata": {},
   "source": [
    "#### add special tokens to tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c685d1-28ae-4b76-a3f4-9f0559b6b9a6",
   "metadata": {},
   "source": [
    "Let me adjust the tokenizer to include the special tokens. Even though I won't need them for pretrain, I hope not to need to repeat this (expensive and time consuming) pretrain with a different vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2da4d-8679-4fa3-b12b-1168b81584dc",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f930884c-c4b5-4ff5-bbaa-b2c1d80754f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58e3f77-28f0-46b6-963d-6687b5ecfa7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 10,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 65,536\n",
      "starting to train tokenizer\n",
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "buffers filled: 67\n",
      "buffers filled: 68\n",
      "buffers filled: 69\n",
      "buffers filled: 70\n",
      "buffers filled: 71\n",
      "buffers filled: 72\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n",
      "finished training tokenizer\n",
      "Saved tokenizer to /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "Saved token_bytes to /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4414ab-547b-42bd-a1ac-51472591ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541b4316-a36b-414a-b9f9-f401d38c47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6381585-3fa4-41f9-9783-4dde445f42b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c58fbf-ecc1-4a13-b471-d62ac020b0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|assistant_end|>',\n",
       " '<|assistant_start|>',\n",
       " '<|bos|>',\n",
       " '<|output_end|>',\n",
       " '<|output_start|>',\n",
       " '<|python_end|>',\n",
       " '<|python_start|>',\n",
       " '<|user_end|>',\n",
       " '<|user_start|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f157ea9f-6aa3-4c52-ad01-059c4cc05b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65527"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_bos_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8400b3-b1f3-4fd0-905c-65bbd60d0202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65531"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_special('<|assistant_end|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78a92ee-bb44-4b17-87c8-f7219e2fef3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "b'<|foo|>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_special\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|foo|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#expect error\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_tokenizer.py:80\u001b[0m, in \u001b[0;36mMyTokenizer.encode_special\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m) \u001b[38;5;66;03m# will take his word that encode_single_token is \"slow\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_special\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_single_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/tiktoken/core.py:256\u001b[0m, in \u001b[0;36mEncoding.encode_single_token\u001b[0;34m(self, text_or_bytes)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_or_bytes, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    255\u001b[0m     text_or_bytes \u001b[38;5;241m=\u001b[39m text_or_bytes\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_single_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_or_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: b'<|foo|>'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_special('<|foo|>') #expect error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f4a854-06b9-433e-bd74-6e8a3078ab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|>A<|assistant_end|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([65527, 65, 65531])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0c1fb-be89-470a-9b6d-9d7e8ef463e5",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Consolidating instructions here so I don't have to look back at challenge 14.\n",
    "\n",
    "```\n",
    "ssh paperspace@[public_ip]\n",
    "\n",
    "# ssh key for git\n",
    "ssh-keygen -t ed25519 -C \"paperspace-vm\"\n",
    "cat ~/.ssh/id_ed25519.pub\n",
    "copy into github UI (https://github.com/settings/keys)\n",
    "\n",
    "git config --global user.email \"ericsilberstein@gmail.com\"\n",
    "git config --global user.name \"Eric Silberstein\"\n",
    "\n",
    "# clone this repo\n",
    "git clone git@github.com:ericsilberstein1/nanogpt-learning.git\n",
    "\n",
    "# Basic packages\n",
    "sudo apt-get update && sudo apt-get install -y build-essential git curl wget ca-certificates\n",
    "\n",
    "# confirm / install smi\n",
    "nvidia-detector\n",
    "nvidia-smi\n",
    "sudo apt-get install nvidia-driver-580 nvidia-utils-580\n",
    "sudo modprobe nvidia # (before did sudo shutdown -r now)\n",
    "nvidia-smi\n",
    "\n",
    "# later, after working on challenge 15, looked into why torch was failing to compile the model\n",
    "# and realized needed to do this so python headers get installed\n",
    "# the specific error was: /tmp/tmp0normshd/cuda_utils.c:6:10: fatal error: Python.h: No such file \n",
    "# or directory 6 | #include <Python.h>\n",
    "sudo apt-get install python3.10-dev\n",
    "\n",
    "# UV\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "source .zshrc\n",
    "\n",
    "# rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "echo '. \"$HOME/.cargo/env\"' >> .zshrc\n",
    "source .zshrc\n",
    "\n",
    "cd nanogpt-learning\n",
    "\n",
    "uv sync\n",
    "source .venv/bin/activate\n",
    "\n",
    "# download data files\n",
    "PYTHONPATH=my_nanochat python -m my_nanochat.my_dataset --num-files 8\n",
    "\n",
    "# download another 240 in the background, check this completes before starting base_train\n",
    "PYTHONPATH=my_nanochat python -m my_nanochat.my_dataset --num-files 240 &\n",
    "\n",
    "# for now until organize this better\n",
    "uv tool install maturin\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "cd -\n",
    "\n",
    "# in challenge 23 I created my_tok_train.py so now can train the tokenizer here rather\n",
    "# than from a notebook\n",
    "PYTHONPATH=my_nanochat python -m scripts.my_tok_train\n",
    "\n",
    "# while that's running\n",
    "\n",
    "# in .zshrc add\n",
    "# export WANDB_API_KEY=\"XXX\"\n",
    "\n",
    "uv run jupyter lab\n",
    "\n",
    "# ON MY LAPTOP make port 8889 a tunnel to jupyter\n",
    "ssh -N -L 8889:localhost:8888 paperspace@[public_ip]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22b33-3a00-485a-8135-7e3eec517162",
   "metadata": {},
   "source": [
    "In tmux shell:\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "cd challenge-25-pretrain-d20\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_train -- --depth=20 --run=challenge-25-1 > train_output_001.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee70579-b8e6-45a7-8e55-932d4612a630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
