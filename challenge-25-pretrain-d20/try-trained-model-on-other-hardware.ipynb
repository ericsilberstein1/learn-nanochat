{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdab72c7-6279-4e6f-aad2-915d3b6e872a",
   "metadata": {},
   "source": [
    "### Try trained model on other hardware\n",
    "\n",
    "As long as the trained d20 model can fit on MPS / mac / RTX4000 I believe it should work, at least up to a certain sequence length because the KV cache will need to keep getting bigger and bigger. \n",
    "\n",
    "Total parms is 560,988,160. Say they all took 4 bytes, which they don't, that's ~2 GiB which is well under the RTX4000 where we had 7+ GiB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb1d27a-19fd-4f04-873f-1befeccd861d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.08984375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "560_988_160 * 4 / 1024 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df50cd-9116-4805-b02e-141b67a4276a",
   "metadata": {},
   "source": [
    "#### Let's try MPS on mac first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3256b5-2819-4f11-bd50-fa4572d96a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683d2fa-e7e1-493e-8feb-d3677196c350",
   "metadata": {},
   "source": [
    "Copied model files to the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa38b43-6f97-402c-b257-7cd288ee54ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4851336\n",
      "-rw-r--r--  1 ericsilberstein  staff   847B Nov 16 14:35 meta_021400.json\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.9G Nov 16 14:35 model_021400.pt\n",
      "-rw-r--r--  1 ericsilberstein  staff   389M Nov 16 14:35 optim_021400_rank0.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/base_checkpoints/d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb2d3a-f8ea-46e0-93b5-096dc6d91180",
   "metadata": {},
   "source": [
    "Copied tokenizer files to the right place (later, forgot to do this at first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7af977-0d25-4e76-abec-5e22a7b1a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 ericsilberstein  staff   826K Nov 16 15:51 /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "-rw-r--r--  1 ericsilberstein  staff   258K Nov 16 15:51 /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/*token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592e353c-73c5-49a9-8fb3-df03da500015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "device_type = autodetect_device_type()\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4da8f3c-2536-4602-819e-1b786f9bb4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d20\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=21400, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92760c2c-2679-4606-a03a-ed34aae3e607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7223a794-f379-4c38-8e47-92c5c04dcfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 21400,\n",
       " 'val_bpb': -1,\n",
       " 'model_config': {'sequence_len': 2048,\n",
       "  'vocab_size': 65536,\n",
       "  'n_layer': 20,\n",
       "  'n_head': 10,\n",
       "  'n_kv_head': 10,\n",
       "  'n_embd': 1280},\n",
       " 'user_config': {'run': 'challenge-25-4',\n",
       "  'device_type': '',\n",
       "  'depth': 20,\n",
       "  'max_seq_len': 2048,\n",
       "  'num_iterations': -1,\n",
       "  'target_param_data_ratio': 20,\n",
       "  'device_batch_size': 32,\n",
       "  'total_batch_size': 524288,\n",
       "  'embedding_lr': 0.2,\n",
       "  'unembedding_lr': 0.004,\n",
       "  'weight_decay': 0.0,\n",
       "  'matrix_lr': 0.02,\n",
       "  'grad_clip': 1.0,\n",
       "  'warmup_ratio': 0.0,\n",
       "  'warmdown_ratio': 0.2,\n",
       "  'final_lr_frac': 0.0,\n",
       "  'eval_every': 250,\n",
       "  'eval_tokens': 10485760,\n",
       "  'core_metric_every': 2000,\n",
       "  'core_metric_max_per_task': 500,\n",
       "  'sample_every': 2000,\n",
       "  'model_tag': ''},\n",
       " 'device_batch_size': 32,\n",
       " 'max_seq_len': 2048}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a386a2-c0b2-4b9d-be38-4713a783c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"The chemical symbol of gold is\",\n",
    "    \"If yesterday was Friday, then tomorrow will be\",\n",
    "    \"The opposite of hot is\",\n",
    "    \"The planets of the solar system are:\",\n",
    "    \"My favorite color is\",\n",
    "    \"If 5*x + 3 = 13, then x is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb427bb-c128-442a-a628-3c858fa1fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_engine import Engine\n",
    "engine = Engine(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43544527-b7ae-4255-8d56-c83e818908d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
      "<|bos|>My favorite color is red. It is the color of blood, of\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\n",
    "    sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=10, temperature=0)\n",
    "    print(tokenizer.decode(sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cf51c-5d7a-4d71-b14b-860bb01bf86e",
   "metadata": {},
   "source": [
    "Do these match the sample from the final step during training?\n",
    "```\n",
    "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
    "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n",
    "```\n",
    "\n",
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c0d9c-b25f-4c8d-8a66-9923c3714830",
   "metadata": {},
   "source": [
    "For fun, generate a few longer samples for each prompt with a higher temperature and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d76353-1b75-4bdc-8cc9-24727c2a312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris, and it is the second largest city in the country. It is also the most populous city\n",
      "<|bos|>The capital of France is Paris, and the capital of France is Paris. Paris is the capital of France. Paris is the\n",
      "<|bos|>The capital of France is Paris. It is a city that is known for its beautiful architecture, its food, its music,\n",
      "<|bos|>The capital of France is Paris and it is the largest city in the country. The city is also the most populous city in\n",
      "<|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe. It is the\n",
      "\n",
      "<|bos|>The chemical symbol of gold is Au, and it is a soft, malleable, ductile, and ductilely reactive metal. It is\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable metal that is a bright yellow color. Gold is the most\n",
      "<|bos|>The chemical symbol of gold is Au. The atomic number of gold is 79. The atomic weight of gold is 199\n",
      "<|bos|>The chemical symbol of gold is Au and it is a soft, silvery metal. Gold is a very soft metal and it is also\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile metal that is malleable and ductile at room temperature.\n",
      "\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on.\n",
      "If you are a student, then you should be able to use this\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on. We can also say that a day is a unit of time. The\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will be Wednesday. If tomorrow is Wednesday, then tomorrow\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on. The only thing that will change is the order of the days. If\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If yesterday was Saturday, then tomorrow will be Sunday. If yesterday was Monday, then tomorrow\n",
      "\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of cold\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of hot\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of hot\n",
      "<|bos|>The opposite of hot is cold. That is, hot is cold and cold is hot. The opposite of hot is cold.\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot.\n",
      "The opposite of hot is cold. The opposite of hot\n",
      "\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "\n",
      "<|bos|>My favorite color is red. It is the color of blood, of life, of love, of blood, of blood\n",
      "<|bos|>My favorite color is red. I know that I have a hard time seeing the color red. I have a hard time\n",
      "<|bos|>My favorite color is red. I love red. I love red. I love red. Red is my favorite color.\n",
      "<|bos|>My favorite color is red. That’s why I love it so much. I love the way it makes me feel.\n",
      "<|bos|>My favorite color is red. It’s the color of blood, of course, but it’s also the color of the\n",
      "\n",
      "<|bos|>If 5*x + 3 = 13, then x is a square number. If 3*x + 5 = 13, then x is a\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 2. If 5*x + 3 = 10, then x\n",
      "<|bos|>If 5*x + 3 = 13, then x is a multiple of 5. If x is a multiple of 3, then x is a multiple\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 4. If 5*x + 3 = 19, then x\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 3. If 5*x + 3 = 13, then x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\n",
    "    samples, _ = engine.generate_batch(tokens, num_samples=5, max_tokens=20, temperature=0.3)\n",
    "    for sample in samples:\n",
    "        print(tokenizer.decode(sample))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd34aa-416d-46de-80e5-bcdc84dae21d",
   "metadata": {},
   "source": [
    "Can we get it to OOM? Probably not just by max tokens because it will hit the assert on 10x max seq length for precomputed sin/cos before OOM, but perhaps by asking for lots of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0627614b-4433-4b9b-bd29-83b0a4cbf7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris, and it is the second largest city in the country. It is also the most populous city in France. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "samples, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=1000, temperature=0.3)\n",
    "print(tokenizer.decode(samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb908b67-7728-4295-a215-e5c4243825b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20479\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:186\u001b[0m, in \u001b[0;36mEngine.generate_batch\u001b[0;34m(self, tokens, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m masks \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m    185\u001b[0m completed \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_samples\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_column, token_masks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(token_column, token_masks)):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m completed[i]:\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=20479, temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a42ad-9c12-4a77-bcae-8fcb9d6849fc",
   "metadata": {},
   "source": [
    "It's been maybe 10 minutes and it's still going. Going to cancel and use the non batch version so can watch progress and see if it's even worth waiting. My guess is at first the times per token generated will be consistent because the self attention stuff is just one part of all the operations, but as the sequence grows, the non-self-attention stuff stays constant but the self attention will dominate as the KV cache grows and it needs to do those big matrix multiplications. But will it slow linearly or with say the square of something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03494e07-09e9-4fd4-90eb-80caf491ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2015cf2b-5eec-436e-a49e-8949eccb7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s 0\n",
      "6s 100\n",
      "5s 200\n",
      "5s 300\n",
      "5s 400\n",
      "6s 500\n",
      "6s 600\n",
      "6s 700\n",
      "8s 800\n",
      "7s 900\n",
      "7s 1000\n",
      "8s 1100\n",
      "12s 1200\n",
      "8s 1300\n",
      "7s 1400\n",
      "20s 1500\n",
      "11s 1600\n",
      "9s 1700\n",
      "9s 1800\n",
      "8s 1900\n",
      "8s 2000\n",
      "9s 2100\n",
      "8s 2200\n",
      "8s 2300\n",
      "11s 2400\n",
      "9s 2500\n",
      "9s 2600\n",
      "10s 2700\n",
      "9s 2800\n",
      "10s 2900\n",
      "10s 3000\n",
      "10s 3100\n",
      "10s 3200\n",
      "10s 3300\n",
      "10s 3400\n",
      "10s 3500\n",
      "11s 3600\n",
      "11s 3700\n",
      "11s 3800\n",
      "11s 3900\n",
      "11s 4000\n",
      "12s 4100\n",
      "12s 4200\n",
      "12s 4300\n",
      "12s 4400\n",
      "12s 4500\n",
      "13s 4600\n",
      "13s 4700\n",
      "13s 4800\n",
      "13s 4900\n",
      "13s 5000\n",
      "14s 5100\n",
      "14s 5200\n",
      "13s 5300\n",
      "14s 5400\n",
      "15s 5500\n",
      "14s 5600\n",
      "14s 5700\n",
      "14s 5800\n",
      "15s 5900\n",
      "15s 6000\n",
      "15s 6100\n",
      "15s 6200\n",
      "15s 6300\n",
      "15s 6400\n",
      "15s 6500\n",
      "15s 6600\n",
      "16s 6700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=1, max_tokens=20479, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cd34c-28e0-4ada-9644-769e01074738",
   "metadata": {},
   "source": [
    "I'm surprised it's not slowing faster. But not sure how much it's worth digging into this and how mac and MPS works around this stuff. Going to interrupt and try many samples for fun and then move to trying things on RTX4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a434d3ed-ba6e-4c42-bd53-e6454bb390af",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 47.74 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 47.74 GiB"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=50, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3deba-d28a-4c12-8e48-fbf6825298d9",
   "metadata": {},
   "source": [
    "^ interesting, failed immediately when first allocating the KV cache. Try with fewer samples. This also makes me think that either it will fail right away, or it will fail at one of the points where it needs to grow the cache. Not really sure about that though and also not sure what \"Invalid buffer size\" is vs. OOM. Also prob better to restart kernel and then repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77607caa-003f-437d-938e-79558144bff6",
   "metadata": {},
   "source": [
    "-- restarted kernel and ran appropriate cells above --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722155c5-8279-46c6-a4ef-f146893cb858",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 47.74 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts[\u001b[38;5;241m0\u001b[39m], prepend\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_bos_token_id())\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 47.74 GiB"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=50, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f6cf2-4b12-4e96-a58d-ef312a671eec",
   "metadata": {},
   "source": [
    "-- restarted kernel and ran appropriate cells above --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee209f9-1a2f-4fd9-af19-e40f8fbb0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s 0\n",
      "7s 100\n",
      "7s 200\n",
      "8s 300\n",
      "9s 400\n",
      "9s 500\n",
      "10s 600\n",
      "10s 700\n",
      "11s 800\n",
      "13s 900\n",
      "13s 1000\n",
      "22s 1100\n",
      "18s 1200\n",
      "16s 1300\n",
      "16s 1400\n",
      "17s 1500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts[\u001b[38;5;241m0\u001b[39m], prepend\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_bos_token_id())\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=5, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc202e1a-b611-419e-8e9c-930b5d22a822",
   "metadata": {},
   "source": [
    "Interrupted. Perhaps come back to this later. Move on to trying on RTX4000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de67456-27b9-4a1e-812f-2cfab20439e3",
   "metadata": {},
   "source": [
    "### Try on RTX4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e4e59-ed03-460f-b81c-6120494abb03",
   "metadata": {},
   "source": [
    "Now on the paperspace rtx400 machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025ed9bd-fca2-4b8d-a493-a5fe2034fea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 16 21:57:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                Off |   00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   33C    P8              8W /  125W |       1MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7a716-cdad-4c30-96db-069f96dd48c2",
   "metadata": {},
   "source": [
    "scp'd tokenizer files and model files from my laptop to this machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdf7eb4-21f1-4cfe-be44-291a45c40fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00af7735-48c3-4847-bfcc-3a8cc4d66ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.4G\n",
      "-rw-r--r-- 1 paperspace paperspace  847 Nov 16 22:02 meta_021400.json\n",
      "-rw-r--r-- 1 paperspace paperspace 2.0G Nov 16 22:02 model_021400.pt\n",
      "-rw-r--r-- 1 paperspace paperspace 389M Nov 16 22:03 optim_021400_rank0.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/base_checkpoints/d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f91c414-33da-4bb1-8bac-e25d4555e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 paperspace paperspace 827K Nov 16 21:59 /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n",
      "-rw-r--r-- 1 paperspace paperspace 258K Nov 16 21:59 /home/paperspace/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/*token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30ccb910-eddd-4c38-9278-5620851e90df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "device_type = autodetect_device_type()\n",
    "device = torch.device(device_type)\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a4d839-7cc9-43ca-8544-78e804a8a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d20\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=21400, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef992f5-0bdb-4e8c-8520-a7fe50cea4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4327ab8-4dee-425f-a4aa-56bbf3fff900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 21400,\n",
       " 'val_bpb': -1,\n",
       " 'model_config': {'sequence_len': 2048,\n",
       "  'vocab_size': 65536,\n",
       "  'n_layer': 20,\n",
       "  'n_head': 10,\n",
       "  'n_kv_head': 10,\n",
       "  'n_embd': 1280},\n",
       " 'user_config': {'run': 'challenge-25-4',\n",
       "  'device_type': '',\n",
       "  'depth': 20,\n",
       "  'max_seq_len': 2048,\n",
       "  'num_iterations': -1,\n",
       "  'target_param_data_ratio': 20,\n",
       "  'device_batch_size': 32,\n",
       "  'total_batch_size': 524288,\n",
       "  'embedding_lr': 0.2,\n",
       "  'unembedding_lr': 0.004,\n",
       "  'weight_decay': 0.0,\n",
       "  'matrix_lr': 0.02,\n",
       "  'grad_clip': 1.0,\n",
       "  'warmup_ratio': 0.0,\n",
       "  'warmdown_ratio': 0.2,\n",
       "  'final_lr_frac': 0.0,\n",
       "  'eval_every': 250,\n",
       "  'eval_tokens': 10485760,\n",
       "  'core_metric_every': 2000,\n",
       "  'core_metric_max_per_task': 500,\n",
       "  'sample_every': 2000,\n",
       "  'model_tag': ''},\n",
       " 'device_batch_size': 32,\n",
       " 'max_seq_len': 2048}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdcbf8c6-e7fd-41af-b0a7-5640fe7f9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"The chemical symbol of gold is\",\n",
    "    \"If yesterday was Friday, then tomorrow will be\",\n",
    "    \"The opposite of hot is\",\n",
    "    \"The planets of the solar system are:\",\n",
    "    \"My favorite color is\",\n",
    "    \"If 5*x + 3 = 13, then x is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bd8fa1-7543-4fab-886b-609a0360624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_engine import Engine\n",
    "engine = Engine(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e323102b-947d-42e7-89b2-4504e7efa486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
      "<|bos|>My favorite color is red. It is the color of blood, of\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\n",
    "    with autocast_ctx:\n",
    "        sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=10, temperature=0)\n",
    "    print(tokenizer.decode(sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374d601-eeb0-41f3-9366-3fe773299790",
   "metadata": {},
   "source": [
    "Looks like it matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916be47a-e034-44aa-9999-022164762610",
   "metadata": {},
   "source": [
    "Can we get this to OOM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a66bc071-d3f5-42c1-bf66-ba307575d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s 0\n",
      "2s 100\n",
      "2s 200\n",
      "2s 300\n",
      "2s 400\n",
      "2s 500\n",
      "2s 600\n",
      "2s 700\n",
      "2s 800\n",
      "2s 900\n",
      "2s 1000\n",
      "2s 1100\n",
      "2s 1200\n",
      "2s 1300\n",
      "2s 1400\n",
      "2s 1500\n",
      "2s 1600\n",
      "2s 1700\n",
      "2s 1800\n",
      "2s 1900\n",
      "2s 2000\n",
      "2s 2100\n",
      "2s 2200\n",
      "2s 2300\n",
      "2s 2400\n",
      "2s 2500\n",
      "2s 2600\n",
      "2s 2700\n",
      "2s 2800\n",
      "2s 2900\n",
      "2s 3000\n",
      "2s 3100\n",
      "2s 3200\n",
      "2s 3300\n",
      "2s 3400\n",
      "2s 3500\n",
      "2s 3600\n",
      "2s 3700\n",
      "2s 3800\n",
      "2s 3900\n",
      "2s 4000\n",
      "2s 4100\n",
      "2s 4200\n",
      "2s 4300\n",
      "2s 4400\n",
      "2s 4500\n",
      "2s 4600\n",
      "2s 4700\n",
      "2s 4800\n",
      "2s 4900\n",
      "2s 5000\n",
      "2s 5100\n",
      "2s 5200\n",
      "2s 5300\n",
      "2s 5400\n",
      "2s 5500\n",
      "2s 5600\n",
      "2s 5700\n",
      "2s 5800\n",
      "2s 5900\n",
      "2s 6000\n",
      "2s 6100\n",
      "2s 6200\n",
      "2s 6300\n",
      "2s 6400\n",
      "2s 6500\n",
      "2s 6600\n",
      "2s 6700\n",
      "2s 6800\n",
      "2s 6900\n",
      "2s 7000\n",
      "2s 7100\n",
      "2s 7200\n",
      "2s 7300\n",
      "2s 7400\n",
      "2s 7500\n",
      "2s 7600\n",
      "2s 7700\n",
      "2s 7800\n",
      "2s 7900\n",
      "2s 8000\n",
      "2s 8100\n",
      "2s 8200\n",
      "2s 8300\n",
      "2s 8400\n",
      "2s 8500\n",
      "2s 8600\n",
      "2s 8700\n",
      "2s 8800\n",
      "2s 8900\n",
      "2s 9000\n",
      "3s 9100\n",
      "3s 9200\n",
      "3s 9300\n",
      "3s 9400\n",
      "3s 9500\n",
      "3s 9600\n",
      "3s 9700\n",
      "3s 9800\n",
      "3s 9900\n",
      "3s 10000\n",
      "3s 10100\n",
      "3s 10200\n",
      "3s 10300\n",
      "3s 10400\n",
      "3s 10500\n",
      "3s 10600\n",
      "3s 10700\n",
      "3s 10800\n",
      "3s 10900\n",
      "3s 11000\n",
      "3s 11100\n",
      "3s 11200\n",
      "3s 11300\n",
      "3s 11400\n",
      "3s 11500\n",
      "3s 11600\n",
      "3s 11700\n",
      "3s 11800\n",
      "3s 11900\n",
      "3s 12000\n",
      "3s 12100\n",
      "3s 12200\n",
      "3s 12300\n",
      "3s 12400\n",
      "3s 12500\n",
      "3s 12600\n",
      "3s 12700\n",
      "3s 12800\n",
      "3s 12900\n",
      "3s 13000\n",
      "3s 13100\n",
      "3s 13200\n",
      "3s 13300\n",
      "3s 13400\n",
      "3s 13500\n",
      "3s 13600\n",
      "4s 13700\n",
      "4s 13800\n",
      "4s 13900\n",
      "4s 14000\n",
      "4s 14100\n",
      "4s 14200\n",
      "4s 14300\n",
      "4s 14400\n",
      "4s 14500\n",
      "4s 14600\n",
      "4s 14700\n",
      "4s 14800\n",
      "4s 14900\n",
      "4s 15000\n",
      "4s 15100\n",
      "4s 15200\n",
      "4s 15300\n",
      "4s 15400\n",
      "4s 15500\n",
      "4s 15600\n",
      "4s 15700\n",
      "4s 15800\n",
      "4s 15900\n",
      "4s 16000\n",
      "4s 16100\n",
      "4s 16200\n",
      "4s 16300\n",
      "4s 16400\n",
      "4s 16500\n",
      "4s 16600\n",
      "4s 16700\n",
      "4s 16800\n",
      "4s 16900\n",
      "4s 17000\n",
      "4s 17100\n",
      "4s 17200\n",
      "4s 17300\n",
      "4s 17400\n",
      "4s 17500\n",
      "4s 17600\n",
      "4s 17700\n",
      "4s 17800\n",
      "4s 17900\n",
      "4s 18000\n",
      "4s 18100\n",
      "4s 18200\n",
      "5s 18300\n",
      "5s 18400\n",
      "5s 18500\n",
      "5s 18600\n",
      "5s 18700\n",
      "5s 18800\n",
      "5s 18900\n",
      "5s 19000\n",
      "5s 19100\n",
      "5s 19200\n",
      "5s 19300\n",
      "5s 19400\n",
      "5s 19500\n",
      "5s 19600\n",
      "5s 19700\n",
      "5s 19800\n",
      "5s 19900\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "with autocast_ctx:\n",
    "    for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=1, max_tokens=20479, temperature=0.3)):\n",
    "        if i % 100 == 0:\n",
    "            t1 = time.time()\n",
    "            delta = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f05965-8775-4628-9ea8-b5ae70d57f7a",
   "metadata": {},
   "source": [
    "That completed. Prob need to look more closely at where it needs to allocate more memory. But for fun, try with multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21cb72ac-339a-447a-ab78-b507f4f83dd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 19.54 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.69 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 90.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m             t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 19.54 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.69 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 90.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "with autocast_ctx:\n",
    "    for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=10, max_tokens=20479, temperature=0.3)):\n",
    "        if i % 100 == 0:\n",
    "            t1 = time.time()\n",
    "            delta = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bfd457d-2dcf-49c5-aeca-768a154b7109",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.77 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.69 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 91.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m             t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.77 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.69 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 91.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "with autocast_ctx:\n",
    "    for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=5, max_tokens=20479, temperature=0.3)):\n",
    "        if i % 100 == 0:\n",
    "            t1 = time.time()\n",
    "            delta = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc6640c8-7605-442a-b246-8e4375b5f85a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.86 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.68 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 92.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m             t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.86 GiB. GPU 0 has a total capacity of 7.78 GiB of which 4.68 GiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 92.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "with autocast_ctx:\n",
    "    for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=3, max_tokens=20479, temperature=0.3)):\n",
    "        if i % 100 == 0:\n",
    "            t1 = time.time()\n",
    "            delta = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdf8cffa-ef42-4df2-86bf-31bc24b73199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s 0\n",
      "4s 100\n",
      "4s 200\n",
      "4s 300\n",
      "4s 400\n",
      "4s 500\n",
      "4s 600\n",
      "4s 700\n",
      "4s 800\n",
      "4s 900\n",
      "4s 1000\n",
      "4s 1100\n",
      "4s 1200\n",
      "4s 1300\n",
      "4s 1400\n",
      "4s 1500\n",
      "4s 1600\n",
      "4s 1700\n",
      "4s 1800\n",
      "4s 1900\n",
      "4s 2000\n",
      "5s 2100\n",
      "5s 2200\n",
      "5s 2300\n",
      "5s 2400\n",
      "5s 2500\n",
      "5s 2600\n",
      "5s 2700\n",
      "5s 2800\n",
      "5s 2900\n",
      "5s 3000\n",
      "5s 3100\n",
      "5s 3200\n",
      "5s 3300\n",
      "5s 3400\n",
      "5s 3500\n",
      "5s 3600\n",
      "5s 3700\n",
      "5s 3800\n",
      "5s 3900\n",
      "5s 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m             t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:150\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    148\u001b[0m     first_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache_decode\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, vocab_size) # CHECK ids\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_gpt.py:232\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    230\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 232\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    235\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_gpt.py:120\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(norm(x), cos_sin, kv_cache)\n\u001b[0;32m--> 120\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_gpt.py:108\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[0;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msquare()\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1697\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1697\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "with autocast_ctx:\n",
    "    for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=2, max_tokens=20479, temperature=0.3)):\n",
    "        if i % 100 == 0:\n",
    "            t1 = time.time()\n",
    "            delta = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a48e6-bf90-4e9f-80be-44ed10b8fb0f",
   "metadata": {},
   "source": [
    "Going to interrupt. Looking at the KVCache class, I forgot that if max_tokens is specified, we use that to initially size the cache. So it's not going to run out of memory while running. It's going to do it right away or not at all, I think. Anyway, overall it makes sense that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee428b5b-a972-49c1-b3c0-cd13716e9ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
