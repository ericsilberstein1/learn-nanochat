{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190b1f5f-eb24-436f-a513-df2b57bc42f1",
   "metadata": {},
   "source": [
    "### Full d20 pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0f021-93e5-46e3-b235-f99c19dfd104",
   "metadata": {},
   "source": [
    "I'm now going to try the full d20 pretrain on an 8xH100 machine on lambda cloud.\n",
    "\n",
    "First I'll follow the instructions in `trying-lambda-cloud.ipynb` to get ready.\n",
    "\n",
    "If I'm able to get a machine in the same region as my test (us-south-2) then I can use the same storage and the data files and tokenizer should already be there. If not, I'll download the data files and scp the tokenizer from my mac:\n",
    "\n",
    "```\n",
    "cd tokenizer-from-challenge-25\n",
    "scp my-tokenizer.pkl ubuntu@[ip]:/home/ubuntu/mynanochat/\n",
    "scp token_bytes.pt ubuntu@[ip]:/home/ubuntu/mynanochat/\n",
    "```\n",
    "\n",
    "I was able to use the same region. It says the machine could take up to 20 minutes to boot. Took < 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca930d-b465-4334-a4c7-b8ee3c8a9a78",
   "metadata": {},
   "source": [
    "```\n",
    "ubuntu@192-222-53-36:~$ nvidia-smi \n",
    "Sun Nov 16 16:08:09 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
    "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
    "| N/A   27C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
    "| N/A   29C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
    "| N/A   26C    P0             76W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
    "| N/A   25C    P0            N/A  /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
    "| N/A   28C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
    "| N/A   27C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
    "| N/A   25C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|  No running processes found                                                             |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264a55a-1742-445d-a057-d9a8a06ae031",
   "metadata": {},
   "source": [
    "In tmux shell:\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "cd challenge-25-pretrain-d20\n",
    "\n",
    "export PYTHONPATH=../my_nanochat/\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_train -- --depth=20 --run=challenge-25-4 > train_output_004.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645cc35-f08b-4ddd-b13d-9a0aae0dfb90",
   "metadata": {},
   "source": [
    "I started with --proc_per_nod=1 by mistake. (That's what I called run=challenge-25-3.) Killed as soon as I noticed tok/sec was similar to before and started as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632e803-8a1e-4f2e-9c96-bb04ccd48e13",
   "metadata": {},
   "source": [
    "Now seeing over 1M tokens per second.\n",
    "\n",
    "```\n",
    "step 00026/21400 (0.12%) | loss: 6.478619 | grad norm: 1.4101 | lrm: 1.00 | dt: 486.55ms | tok/sec: 1,077,563 | mfu: -1.00 | total time: 0.13m\n",
    "step 00027/21400 (0.13%) | loss: 6.464063 | grad norm: 1.0951 | lrm: 1.00 | dt: 489.48ms | tok/sec: 1,071,108 | mfu: -1.00 | total time: 0.14m\n",
    "```\n",
    "\n",
    "```\n",
    "Number of parameters: 560,988,160\n",
    "Calculated num iterations from target_param_data_ratio as 21,400\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef331a-cda1-491c-90e9-d149f9526e95",
   "metadata": {},
   "source": [
    "Here's the firsrt sampling at step 2000. Definitely better than anything before.\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris. The capital of France is Paris. The\n",
    "<|bos|>The chemical symbol of gold is 1. It is a soft metal, and\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Thursday, and if tomorrow is Wednesday, then tomorrow\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. I think it‚Äôs a good color for\n",
    "<|bos|>If 5*x + 3 = 13, then x is 13. If 5*x + \n",
    "```\n",
    "\n",
    "Here's the next at step 4000.\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, the capital of France. It is the\n",
    "<|bos|>The chemical symbol of gold is Au. The symbol of silver is Au. The\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be tomorrow. If you are a student, then you\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Jupiter, Saturn, Uranus, Neptune, and Pluto\n",
    "<|bos|>My favorite color is red. I love it because it is the color\n",
    "<|bos|>If 5*x + 3 = 13, then x is 5. If 5*x + \n",
    "```\n",
    "\n",
    "at 6000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, the capital of the United States is Washington\n",
    "<|bos|>The chemical symbol of gold is 24. Gold is a soft, malleable metal\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If you are a parent, you will\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the sky,\n",
    "<|bos|>If 5*x + 3 = 13, then x is a square. If 5*x + \n",
    "```\n",
    "\n",
    "at 8000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, the capital of France is Paris, the\n",
    "<|bos|>The chemical symbol of gold is Au, and the chemical symbol of silver is Ag\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Sunday. If tomorrow is Sunday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red, and I love the way it is.\n",
    "<|bos|>If 5*x + 3 = 13, then x is the square root of 5. If 5\n",
    "```\n",
    "\n",
    "at 10,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, and the city is the most populous in\n",
    "<|bos|>The chemical symbol of gold is Au. The symbol of gold is Au is a\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Friday. If today is Friday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of love, of\n",
    "<|bos|>If 5*x + 3 = 13, then x is 5*x + 3 = 13\n",
    "```\n",
    "\n",
    "at 12,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris. The capital of France is Paris. The\n",
    "<|bos|>The chemical symbol of gold is Au. The name of the metal is derived from\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Friday. This is a good way to keep track\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Earth, Mars, Jupiter, Saturn, Uranus,\n",
    "<|bos|>My favorite color is red. I love red. I love red.\n",
    "<|bos|>If 5*x + 3 = 13, then x is 3 times 13. So, 5\n",
    "```\n",
    "\n",
    "At 14,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, and the capital of France is Paris.\n",
    "<|bos|>The chemical symbol of gold is Au. The atomic number of gold is 79\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If yesterday was Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a multiple of 3. The answer is \n",
    "```\n",
    "\n",
    "(The atomic number of gold is 79!)\n",
    "\n",
    "At 16,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris, and the capital of France is Paris.\n",
    "<|bos|>The chemical symbol of gold is Au. The chemical symbol of gold is Au.\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Tuesday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It‚Äôs the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is 13. If 5*x + \n",
    "```\n",
    "\n",
    "It's pretty crazy that some of the early people to see results like this thought if they keep scaling up the model will be able to do advanced \"thinking.\" These samples scream \"I'm getting pretty good at completing prompts\" not \"I'm understanding\" and this was before we realized just how much \"thinking\" or \"understanding\" is about completing prompts.\n",
    "\n",
    "At 18,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris. It is the capital of France. It\n",
    "<|bos|>The chemical symbol of gold is Au. The chemical symbol of silver is Ag.\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is 13. If 5*x + \n",
    "```\n",
    "\n",
    "At 20,000:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris. It is the second largest city in France\n",
    "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on. The only way to\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It‚Äôs the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a factor of 2. If 5*\n",
    "```\n",
    "\n",
    "Paris is not the second largest city.\n",
    "\n",
    "At 21,400:\n",
    "\n",
    "```\n",
    "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
    "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a36453-d6e3-40f5-8fe8-512848273c54",
   "metadata": {},
   "source": [
    "Last 100 lines of output from job:\n",
    "```\n",
    "step 21374/21400 (99.88%) | loss: 2.741960 | grad norm: 0.1850 | lrm: 0.01 | dt: 480.02ms | tok/sec: 1,092,213 | mfu: -1.00 | total time: 171.83m\n",
    "step 21375/21400 (99.88%) | loss: 2.742765 | grad norm: 0.1700 | lrm: 0.01 | dt: 479.68ms | tok/sec: 1,093,004 | mfu: -1.00 | total time: 171.84m\n",
    "step 21376/21400 (99.89%) | loss: 2.749703 | grad norm: 0.1950 | lrm: 0.01 | dt: 479.89ms | tok/sec: 1,092,508 | mfu: -1.00 | total time: 171.85m\n",
    "step 21377/21400 (99.89%) | loss: 2.750738 | grad norm: 0.1698 | lrm: 0.01 | dt: 479.51ms | tok/sec: 1,093,382 | mfu: -1.00 | total time: 171.86m\n",
    "step 21378/21400 (99.90%) | loss: 2.750943 | grad norm: 0.1935 | lrm: 0.01 | dt: 478.80ms | tok/sec: 1,095,013 | mfu: -1.00 | total time: 171.86m\n",
    "step 21379/21400 (99.90%) | loss: 2.752251 | grad norm: 0.1763 | lrm: 0.00 | dt: 480.70ms | tok/sec: 1,090,665 | mfu: -1.00 | total time: 171.87m\n",
    "step 21380/21400 (99.91%) | loss: 2.756554 | grad norm: 0.1785 | lrm: 0.00 | dt: 479.59ms | tok/sec: 1,093,201 | mfu: -1.00 | total time: 171.88m\n",
    "step 21381/21400 (99.91%) | loss: 2.748836 | grad norm: 0.1720 | lrm: 0.00 | dt: 479.94ms | tok/sec: 1,092,413 | mfu: -1.00 | total time: 171.89m\n",
    "step 21382/21400 (99.92%) | loss: 2.753904 | grad norm: 0.1738 | lrm: 0.00 | dt: 480.36ms | tok/sec: 1,091,445 | mfu: -1.00 | total time: 171.90m\n",
    "step 21383/21400 (99.92%) | loss: 2.753519 | grad norm: 0.1712 | lrm: 0.00 | dt: 478.78ms | tok/sec: 1,095,056 | mfu: -1.00 | total time: 171.90m\n",
    "step 21384/21400 (99.93%) | loss: 2.751234 | grad norm: 0.1663 | lrm: 0.00 | dt: 480.38ms | tok/sec: 1,091,394 | mfu: -1.00 | total time: 171.91m\n",
    "step 21385/21400 (99.93%) | loss: 2.743604 | grad norm: 0.1646 | lrm: 0.00 | dt: 479.27ms | tok/sec: 1,093,924 | mfu: -1.00 | total time: 171.92m\n",
    "step 21386/21400 (99.93%) | loss: 2.732429 | grad norm: 0.1636 | lrm: 0.00 | dt: 478.18ms | tok/sec: 1,096,426 | mfu: -1.00 | total time: 171.93m\n",
    "step 21387/21400 (99.94%) | loss: 2.741124 | grad norm: 0.1752 | lrm: 0.00 | dt: 480.91ms | tok/sec: 1,090,190 | mfu: -1.00 | total time: 171.94m\n",
    "step 21388/21400 (99.94%) | loss: 2.735194 | grad norm: 0.1660 | lrm: 0.00 | dt: 478.97ms | tok/sec: 1,094,612 | mfu: -1.00 | total time: 171.94m\n",
    "step 21389/21400 (99.95%) | loss: 2.731359 | grad norm: 0.1669 | lrm: 0.00 | dt: 480.26ms | tok/sec: 1,091,675 | mfu: -1.00 | total time: 171.95m\n",
    "step 21390/21400 (99.95%) | loss: 2.742557 | grad norm: 0.1905 | lrm: 0.00 | dt: 478.04ms | tok/sec: 1,096,737 | mfu: -1.00 | total time: 171.96m\n",
    "step 21391/21400 (99.96%) | loss: 2.740270 | grad norm: 0.1763 | lrm: 0.00 | dt: 481.01ms | tok/sec: 1,089,977 | mfu: -1.00 | total time: 171.97m\n",
    "step 21392/21400 (99.96%) | loss: 2.704566 | grad norm: 0.1630 | lrm: 0.00 | dt: 480.92ms | tok/sec: 1,090,174 | mfu: -1.00 | total time: 171.98m\n",
    "step 21393/21400 (99.97%) | loss: 2.707926 | grad norm: 0.1676 | lrm: 0.00 | dt: 479.31ms | tok/sec: 1,093,829 | mfu: -1.00 | total time: 171.98m\n",
    "step 21394/21400 (99.97%) | loss: 2.718694 | grad norm: 0.1748 | lrm: 0.00 | dt: 479.41ms | tok/sec: 1,093,608 | mfu: -1.00 | total time: 171.99m\n",
    "step 21395/21400 (99.98%) | loss: 2.728112 | grad norm: 0.1696 | lrm: 0.00 | dt: 477.57ms | tok/sec: 1,097,815 | mfu: -1.00 | total time: 172.00m\n",
    "step 21396/21400 (99.98%) | loss: 2.722741 | grad norm: 0.1687 | lrm: 0.00 | dt: 480.46ms | tok/sec: 1,091,218 | mfu: -1.00 | total time: 172.01m\n",
    "step 21397/21400 (99.99%) | loss: 2.729883 | grad norm: 0.1745 | lrm: 0.00 | dt: 482.27ms | tok/sec: 1,087,122 | mfu: -1.00 | total time: 172.02m\n",
    "step 21398/21400 (99.99%) | loss: 2.733281 | grad norm: 0.1718 | lrm: 0.00 | dt: 481.44ms | tok/sec: 1,088,994 | mfu: -1.00 | total time: 172.02m\n",
    "step 21399/21400 (100.00%) | loss: 2.746857 | grad norm: 0.1814 | lrm: 0.00 | dt: 477.22ms | tok/sec: 1,098,640 | mfu: -1.00 | total time: 172.03m\n",
    "step 21400 | Validation bpb: 0.8135\n",
    "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4320 | centered: 0.2427 | time: 0.72s\n",
    "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1320 | centered: 0.1320 | time: 0.70s\n",
    "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5500 | centered: 0.5500 | time: 0.71s\n",
    "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6640 | centered: 0.5520 | time: 0.78s\n",
    "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3840 | centered: 0.1787 | time: 0.83s\n",
    "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.14s\n",
    "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2160 | centered: 0.0200 | time: 0.86s\n",
    "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6920 | centered: 0.3840 | time: 0.71s\n",
    "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.62s\n",
    "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3960 | centered: 0.3960 | time: 0.65s\n",
    "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4520 | centered: 0.2693 | time: 1.38s\n",
    "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.33s\n",
    "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5220 | centered: 0.0440 | time: 0.62s\n",
    "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0980 | centered: 0.0980 | time: 0.70s\n",
    "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.59s\n",
    "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3580 | centered: 0.3580 | time: 0.68s\n",
    "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.29s\n",
    "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.04s\n",
    "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2360 | centered: 0.2360 | time: 1.07s\n",
    "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1800 | centered: 0.1800 | time: 0.75s\n",
    "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5240 | centered: -0.2526 | time: 1.29s\n",
    "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2680 | centered: 0.1947 | time: 2.12s\n",
    "Step 21400: CORE metric: 0.2084\n",
    "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
    "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n",
    "saved model to /home/ubuntu/mynanochat/base_checkpoints/d20/model_021400.pt\n",
    "saved metadata to /home/ubuntu/mynanochat/base_checkpoints/d20/meta_021400.json\n",
    "[W1116 19:16:47.976035058 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:47.151341352 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:47.370174970 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:47.392311868 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:47.524902948 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "saved optimizer to /home/ubuntu/mynanochat/base_checkpoints/d20/optim_021400_rank0.pt\n",
    "Peak memory usage: 75422.02MiB\n",
    "Total training time: 172.03m\n",
    "Minimum validation bpb: 0.8135\n",
    "wandb: updating run metadata\n",
    "wandb: uploading history steps 311-311, summary, console lines 21838-21859\n",
    "wandb: \n",
    "wandb: Run history:\n",
    "wandb:         core_metric ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà\n",
    "wandb:                step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "wandb: total_training_time ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà\n",
    "wandb:            train/dt ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñá‚ñá‚ñá‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÇ‚ñÜ‚ñá\n",
    "wandb:     train/grad_norm ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
    "wandb:          train/loss ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
    "wandb:           train/lrm ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ\n",
    "wandb:   train/tok_per_sec ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "wandb:             val/bpb ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
    "wandb: \n",
    "wandb: Run summary:\n",
    "wandb:         core_metric 0.20842\n",
    "wandb:                step 21400\n",
    "wandb: total_training_time 10321.90252\n",
    "wandb:            train/dt 0.48043\n",
    "wandb:     train/grad_norm 0.17181\n",
    "wandb:          train/loss 2.71189\n",
    "wandb:           train/lrm 0.02336\n",
    "wandb:   train/tok_per_sec 1091299\n",
    "wandb:             val/bpb 0.81348\n",
    "wandb: \n",
    "wandb: üöÄ View run challenge-25-4 at: https://wandb.ai/ericsilberstein-self/my-nanochat/runs/i5o3evvu\n",
    "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat\n",
    "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "wandb: Find logs at: ./wandb/run-20251116_161520-i5o3evvu/logs\n",
    "[W1116 19:16:53.795706450 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:53.310833985 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "[W1116 19:16:55.520616915 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be95abb-de8f-4405-85c0-4a5ce04e21e5",
   "metadata": {},
   "source": [
    "### Copying trained model to my laptop\n",
    "\n",
    "I want to start getting more organized about where I keep things.\n",
    "\n",
    "On my laptop, at root of this repo:\n",
    "\n",
    "```\n",
    "mkdir files_copied_from_servers\n",
    "cd files_copied_from_servers\n",
    "mkdir challenge-25-pretrain-d20\n",
    "cd challenge-25-pretrain-d20\n",
    "mv ~/Documents/tokenizer-from-challenge-25/* .\n",
    "rmdir ~/Documents/tokenizer-from-challenge-25\n",
    "ls -lh\n",
    "total 2232\n",
    "-rw-r--r--  1 ericsilberstein  staff   826K Nov 16 08:52 my-tokenizer.pkl\n",
    "-rw-r--r--  1 ericsilberstein  staff   258K Nov 16 08:52 token_bytes.pt\n",
    "-rw-r--r--  1 ericsilberstein  staff    25K Nov 16 09:00 train_output_001.txt\n",
    "```\n",
    "\n",
    "Added to `.gitignore`\n",
    "\n",
    "When the model is done training I'll scp it to there:\n",
    "\n",
    "```\n",
    "scp -r ubuntu@[ip]:/home/ubuntu/mynanochat/base_checkpoints .\n",
    "scp -r ubuntu@[ip]:/home/ubuntu/nanogpt-learning/challenge-25-pretrain-d20/train_output_004.txt .\n",
    "\n",
    "ls -lh base_checkpoints/d20 \n",
    "total 4884856\n",
    "-rw-r--r--  1 ericsilberstein  staff   847B Nov 16 14:18 meta_021400.json\n",
    "-rw-r--r--  1 ericsilberstein  staff   1.9G Nov 16 14:20 model_021400.pt\n",
    "-rw-r--r--  1 ericsilberstein  staff   389M Nov 16 14:21 optim_021400_rank0.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134dbf44-415f-4a11-970b-625b3d38fe17",
   "metadata": {},
   "source": [
    "Now terminate machine.\n",
    "\n",
    "That was 3.31 hours @ \\\\$23.92 / hr = \\\\$79.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df71b7e-e445-4cbb-924b-ced75cba9258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66496036-949a-460b-aa90-1b5cdb6620dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
