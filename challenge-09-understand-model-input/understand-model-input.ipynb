{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1493b25-38c1-4ae2-aef6-284ae4850bf5",
   "metadata": {},
   "source": [
    "To do this challenge, I'm going to need to enhance `my_tokenizer.py` from challenge 7. I don't want to touch that code, though, and as I keep going, it's going to get confusing to keep code only in these different challenge folders. It's time to start recreating the entire structure of `nanochat` under `my_nanochat`. So copy `my_tokenizer.py` from challenge 7 to its \"permanent\" home and edit it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a66810a-33c5-4e37-8739-20a445c5a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcf5c6a-cc9e-4d3f-a6be-db2543dc4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_dataset import parquets_iter_batched\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b47a6ae-34fb-4ebb-b547-90a2f0f9664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed5db9-093b-48d4-aece-99f3868bd49f",
   "metadata": {},
   "source": [
    "### Understand code in [dataloader.py](https://github.com/karpathy/nanochat/blob/master/nanochat/dataloader.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa6ef64-41e7-46c2-b810-9565c8cbe28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2 # batch size\n",
    "T = 5 # sequence length (is that sequence length or max sequence length?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd313612-ae7f-495b-b011-469962807270",
   "metadata": {},
   "source": [
    "The the dataloder gets these values:\n",
    "\n",
    "`ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()`\n",
    "\n",
    "I'm guesing `ddp_rank` is the rank (~ GPU #) of \"this\" parallel process, but then I'm not sure what `ddp_local_rank` is.\n",
    "\n",
    "`ddp_world_size` which we came across earlier in a comment about why you want start/step arguments to `parquets_iter_batched()` must be the total number of parallel processes = GPUs = ranks.\n",
    "\n",
    "Not sure what `ddp` is (but peeking at `common.get_dist_info()` I see it's a boolean if we're doing ddp or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a85b97-c2fd-4533-b413-ee2ff3bbef12",
   "metadata": {},
   "source": [
    "Asking ChatGPT about rank vs local rank:\n",
    "\n",
    "rank = The global ID of a process in the entire distributed system.\n",
    "local rank = The ID of the process within its local machine (node).\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have 2 machines (nodes) with 4 GPUs each → 8 total processes.\n",
    "\n",
    "* Node 0 has GPUs 0–3 → local ranks 0, 1, 2, 3.\n",
    "\n",
    "* Node 1 has GPUs 0–3 → local ranks 0, 1, 2, 3.\n",
    "\n",
    "Global rank 4 (which is the first process on node 1) has local rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5052c128-e577-4426-84b3-4ef1a89751e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for this notebook...\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size = False, 0, 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360ca32b-016a-4a29-bc3a-bb7bfc5860b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed_tokens = B * T + 1; needed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb0418c-dcb6-4fa0-a4d8-dd6c15533b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.load_from_file(\"../challenge-08-train-tokenizer/my-tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b668655a-b92e-4f5d-ab23-533b60643eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52902, 882]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdbf4d1-aba6-4208-95f1-c44d3078fd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_token = tokenizer.get_bos_token_id(); bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7195aa07-6c81-4898-8599-94f6c4afa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_buffer = deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90fb4e93-b5fd-40dd-b521-fa295181311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, deque([2, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand deque\n",
    "foo = deque()\n",
    "foo.extend([1,2,3])\n",
    "foo.popleft(), foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb66f67-6962-4f59-9e13-15f54f6cbd82",
   "metadata": {},
   "source": [
    "Also realizing it's going to get odd and annoying to keep the parquet data files in the challenge 8 folder and have `my_dataset` know to look there. Put them in `~/.cache/my_nanochat` and create `my_common.py` and a `get_base_dir()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf7c76e-c137-4fe0-85b2-9e2a4ef17764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericsilberstein/.cache/my_nanochat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_base_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7152c19d-006b-4e4b-aba7-b033f66281fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-tokenizer.pkl    shard_00002.parquet shard_00005.parquet shard_00008.parquet\n",
      "shard_00000.parquet shard_00003.parquet shard_00006.parquet shard_00009.parquet\n",
      "shard_00001.parquet shard_00004.parquet shard_00007.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls {get_base_dir()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a259ac9a-2865-4ec2-b5e9-bff497d6081c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shipment & Transport-Sea, Air,'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm can read files\n",
    "next(parquets_iter_batched(\"train\"))[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a7517-2960-4ca1-b732-55da5247bf4c",
   "metadata": {},
   "source": [
    "convoluted going of document_batch -> tokenizer_batch -> token_lists -> token_buffer -> tokens below is just to get a feel for the code in `dataloader.py` where in actuality it's yielding the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8ee852f-6e92-4010-9819-228ce4b3c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_batch = next(parquets_iter_batched(\"train\", start=ddp_rank, step=ddp_world_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c864ae-69b6-45b6-ac71-0fe5af65e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_batch_size = 2 # 128 in his code, but 2 should be more than enough to get the 11 tokens we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04beb00f-f926-4480-939d-eb4ee52082a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_batch = document_batch[:tokenizer_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "781fecb7-fd46-4ec7-987a-497731abe04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'Shipment &', '12. Defini')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_batch), tokenizer_batch[0][:10], tokenizer_batch[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46771714-17ad-46aa-b085-bc6c6297a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[65536, 52902, 882], [65536, 31563, 882]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I see now that it's time to enhance my_tokenizer.encode() to accept (and return) a list\n",
    "# and accept a prepend argument\n",
    "tokenizer.encode([\"hello world\", \"bye world\"], prepend=bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2611ec44-d0ea-4f15-8044-6d5b483e671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(token_buffer) < needed_tokens:\n",
    "    token_lists = tokenizer.encode(tokenizer_batch, prepend=bos_token)\n",
    "    for tokens in token_lists:\n",
    "        token_buffer.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12731bb6-2493-4814-9b6f-f24f5b72010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3636"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3b0306e-dea4-48e8-9f1a-f2b2a2f6378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token_buffer.popleft() for _ in range(needed_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "797f1a83-a80f-416b-8198-0542117dc411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536, 61056, 363, 1488, 11808, 3734, 18097, 44, 4618, 44, 9575]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70c1103a-b3d5-4d80-800b-3b31d85660f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65536, 61056,   363,  1488, 11808,  3734, 18097,    44,  4618,    44,\n",
       "         9575])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scratch = torch.tensor(tokens, dtype=torch.int64); scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d02e34f-2589-404c-b7e8-ec45e1942786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65536, 61056,   363,  1488, 11808,  3734, 18097,    44,  4618,    44],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_cpu = scratch[:-1].to(dtype=torch.int32); inputs_cpu\n",
    "# I don't understand why we want in32 for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0cd728-cb3f-49e8-bc89-2e53581a92fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([61056,   363,  1488, 11808,  3734, 18097,    44,  4618,    44,  9575])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_cpu = scratch[1:]; targets_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c0319c8-197a-4a66-82be-b742521c4d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<bos>Shipment & Transport-Sea, Air,', 'Shipment & Transport-Sea, Air, Rail')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs_cpu.tolist()), tokenizer.decode(targets_cpu.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d390f8ee-0fba-4584-ad3c-499fa9ffcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65536, 61056,   363,  1488, 11808],\n",
       "        [ 3734, 18097,    44,  4618,    44]], dtype=torch.int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs_cpu.view(B, T); inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581f42d6-df2d-41f8-9cab-8c321d9fc40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61056,   363,  1488, 11808,  3734],\n",
       "        [18097,    44,  4618,    44,  9575]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = targets_cpu.view(B, T); targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2085b6a-6e8b-4fe3-a137-33891a4585db",
   "metadata": {},
   "source": [
    "ok, so `dataloader.py` does what I would expect, in a way that is scalable and compatabile with training across multiple nodes and GPUs. Similar to what I show as input/target in my [tracing the transformer blog post](https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c/) but without the \"source\" language since this is a general purpose transformer. I didn't realize though that we don't worry about each item in the batch being one \"logical unit\" such as a sentence or document. We just fill the batch with continuous tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad3237a-7dad-485d-a0e0-4871bf885651",
   "metadata": {},
   "source": [
    "### so now create `my_dataloader.py` to use going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "305420e8-7b6f-4666-a149-d2e8655ee9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "095dc2df-ba42-4f76-86a4-162bd036e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = tokenizing_distributed_data_loader(\n",
    "    B,\n",
    "    T,\n",
    "    \"train\",\n",
    "    tokenizer_threads=4,\n",
    "    tokenizer_batch_size=tokenizer_batch_size,\n",
    "    device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d65f908-8250-49e0-974c-0e76de4386f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa784175-eb05-47f8-bf9d-f23a492b9da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65536, 61056,   363,  1488, 11808],\n",
       "        [ 3734, 18097,    44,  4618,    44]], dtype=torch.int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea38858a-164a-4245-888d-6aa2b08b8a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61056,   363,  1488, 11808,  3734],\n",
       "        [18097,    44,  4618,    44,  9575]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dde90b05-0041-4908-8ca6-fa8695fd4d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(x,inputs); torch.equal(y,targets)\n",
    "# hooray, they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3459807b-eea4-4284-a7c2-ae6a07d8f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with more typical batch and sequence sizes\n",
    "dl = tokenizing_distributed_data_loader(B=32, T=2048, split=\"train\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0c3392a-e084-4b48-ab48-7e2227230b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0000ba80-513b-40c6-a15f-348b8d3a974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65536, 61056,   363,  ...,  1365,   288,   930],\n",
       "        [   46,   372,    52,  ...,   408,  3484,  3050],\n",
       "        [ 6475,   283,   261,  ...,   309,  6944,   288],\n",
       "        ...,\n",
       "        [ 5724,   257,  2239,  ..., 62468,  1855,   449],\n",
       "        [ 1480,  4135,   327,  ...,    46,  1008,   500],\n",
       "        [  519,   356,  1403,  ..., 41840,  1539, 16547]], dtype=torch.int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d20663a9-4ff1-4c47-b6dc-19e26ba917f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95be92c5-3716-4641-99c0-9347433c0938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb670359-6f82-4767-893b-2f66b190cd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(83)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many <bos> tokens do we have?\n",
    "torch.sum(x == tokenizer.get_bos_token_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b66e6006-bad8-4f5c-85d7-d70f8b6669cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(109)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many 'The' tokens do we have?\n",
    "torch.sum(x == tokenizer.encode(\"The\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2554927f-586a-47f5-a030-c13640341c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' directing the Philharmonic Choral Society and the Kansas City Symphony Orchestra.\\nBusch fell in love'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[5,30:50].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50aad935-a04b-4bae-b088-baae3a6f7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' prey for Amphiprion clarkii larviculture: effects on larval survival and growth. Aquaculture'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[10,30:50].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1367b61-d07f-4d03-adbd-ea7287ae871d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
