{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36354d0-c28f-4e9b-ba06-52233fa4d40b",
   "metadata": {},
   "source": [
    "## Understand reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b99fa-174e-4fa4-9409-ad424582ff65",
   "metadata": {},
   "source": [
    "Look at [chat_rl.py](https://github.com/karpathy/nanochat/blob/master/scripts/chat_rl.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd94a91-2693-4267-a60e-ed3ab9afb311",
   "metadata": {},
   "source": [
    "Well, his initial comment is:\n",
    "\n",
    "```\n",
    "Reinforcement learning on GSM8K via \"GRPO\".\n",
    "\n",
    "I put GRPO in quotes because we actually end up with something a lot\n",
    "simpler and more similar to just REINFORCE:\n",
    "\n",
    "1) Delete trust region, so there is no KL regularization to a reference model\n",
    "2) We are on policy, so there's no need for PPO ratio+clip.\n",
    "3) We use GAPO style normalization that is token-level, not sequence-level.\n",
    "4) Instead of z-score normalization (r - mu)/sigma, only use (r - mu) as the advantage.\n",
    "```\n",
    "\n",
    "I've never heard of GPRO, I have no idea what a trust region is, or what it means to be on policy, or GAPO.\n",
    "\n",
    "I see from skimming ahead we're going to get into the task reward stuff I copied earlier.\n",
    "\n",
    "I'll get back to looking up what some of those terms in the comment are. Start by hand copying `run_gsm8k_eval()` into this notebook to understand it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02235e3f-6d1f-4a41-af02-e94dad180952",
   "metadata": {},
   "source": [
    "### run_gsmk8k_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1665225d-de9d-44b9-96f5-b5dfad395089",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_rank = 0\n",
    "ddp_world_size = 1\n",
    "device_batch_size = 1\n",
    "def run_gsm8k_eval(task, tokenizer, engine,\n",
    "                   max_examples=None,\n",
    "                   num_samples=1,\n",
    "                   max_completion_tokens=256,\n",
    "                   temperature=0.0,\n",
    "                   top_k=50):\n",
    "    max_examples = min(max_examples, len(task)) if max_examples is not None else len(tasks)\n",
    "    for idx in range(ddp_rank, max_examples, ddp_world_size):\n",
    "        conversation = task[idx]\n",
    "        tokens = tokenizer.render_for_completion(conversation)\n",
    "        prefix_length = len(tokens)\n",
    "        assert num_samples <= device_batch_size # he comments can add loop if not, won't be true on mac\n",
    "        generated_token_sequences, masks = engine.generate_batch(tokens,\n",
    "                                                                 num_samples=num_samples,\n",
    "                                                                 max_tokens=max_completion_tokens,\n",
    "                                                                 temperature=temperature,\n",
    "                                                                 top_k=top_k)\n",
    "        outcomes = []\n",
    "        for sample_tokens in generated_token_sequences:\n",
    "            generated_tokens = sample_tokens[prefix_length:]\n",
    "            generated_text = tokenizer.decode(generated_tokens)\n",
    "            is_correct = task.evaluate(conversation, generated_text)\n",
    "            outcomes.append({\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "\n",
    "        record = {\n",
    "            'idx': idx,\n",
    "            'outcomes': outcomes,\n",
    "        }\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8c4b4-bfa9-4804-812e-fda3d2841897",
   "metadata": {},
   "source": [
    "^ ok, this part is very similar to how things work in chat_eval, but instead of seeing if any sample is correct, it returns correct or not correct for each sample\n",
    "\n",
    "as a quick test on my mac I'll have to set num_samples to 1 though (or I need to add a loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52d1731f-1602-49f1-b067-6da062f5c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d1 with step 9\n",
      "Building model with config: {'sequence_len': 256, 'vocab_size': 65536, 'n_layer': 1, 'n_head': 1, 'n_kv_head': 1, 'n_embd': 64}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_checkpoint_manager import load_model\n",
    "from my_nanochat.my_common import compute_init, autodetect_device_type\n",
    "from my_nanochat.my_engine import Engine\n",
    "from my_tasks.my_gsm8k import MyGSM8K\n",
    "device_type = autodetect_device_type() \n",
    "_, _, _, _, device = compute_init(device_type)\n",
    "model, tokenizer, meta_data = load_model('sft', model_tag='d1', device=device, phase='eval')\n",
    "engine = Engine(model, tokenizer)\n",
    "task = MyGSM8K(subset=\"main\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821c9791-8de4-437b-9dc3-5dd83ab6ade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0, 'outcomes': [{'is_correct': 0}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(run_gsm8k_eval(tasks, tokenizer, engine, max_examples=2, num_samples=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593571c-a54b-4d03-a1d2-7d224702fe2e",
   "metadata": {},
   "source": [
    "### get_batch()\n",
    "\n",
    "See what that gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe8fee4-8059-4716-8fd8-15bab5733445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2758f9c-ea1b-40ce-b86d-0e4359e9815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "1\n",
      "3\n",
      "5\n",
      "1\n",
      "3\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for n in itertools.cycle([1,3,5]):\n",
    "    print(n)\n",
    "    i += 1\n",
    "    if (i == 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5fc0c8e-0eb6-42fd-94df-472c2648d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d1 with step 9\n",
      "Building model with config: {'sequence_len': 256, 'vocab_size': 65536, 'n_layer': 1, 'n_head': 1, 'n_kv_head': 1, 'n_embd': 64}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import torch\n",
    "from my_nanochat.my_checkpoint_manager import load_model\n",
    "from my_nanochat.my_common import compute_init, autodetect_device_type\n",
    "from my_nanochat.my_engine import Engine\n",
    "from my_tasks.my_gsm8k import MyGSM8K\n",
    "device_type = autodetect_device_type() \n",
    "_, _, _, _, device = compute_init(device_type)\n",
    "model, tokenizer, meta_data = load_model('sft', model_tag='d1', device=device, phase='eval')\n",
    "engine = Engine(model, tokenizer)\n",
    "train_task = MyGSM8K(subset=\"main\", split=\"train\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch():\n",
    "    assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
    "    rank_indices = range(ddp_rank, len(train_task), ddp_world_size)\n",
    "    for example_idx in itertools.cycle(rank_indices):\n",
    "        conversation = train_task[example_idx]\n",
    "        tokens = tokenizer.render_for_completion(conversation)\n",
    "        prefix_len = len(tokens)\n",
    "\n",
    "        model.eval() # this is pretty different, we're going to use the model in generating a batch\n",
    "        generated_token_sequences = []\n",
    "        masks = []\n",
    "        num_sampling_steps = num_samples // device_batch_size\n",
    "        for sampling_step in range(num_sampling_steps):\n",
    "            seed = hash((step, example_idx, sampling_step)) & 0x7FFFFFFF\n",
    "            with autocast_ctx:\n",
    "                generated_token_sequences_batch, masks_batch = engine.generate_batch(\n",
    "                    tokens,\n",
    "                    num_samples=device_batch_size,\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_k=top_k,\n",
    "                    seed=seed,\n",
    "                )\n",
    "            generated_token_sequences.extend(generated_token_sequences_batch)\n",
    "            masks.extend(masks_batch)\n",
    "    \n",
    "        rewards = []\n",
    "        for sample_tokens in generated_token_sequences:\n",
    "            generated_tokens = sample_tokens[prefix_len:]\n",
    "            generated_text = tokenizer.decode(generated_tokens)\n",
    "            reward = train_task.reward(conversation, generated_text) # 1 or 0 right?\n",
    "            rewards.append(reward)\n",
    "\n",
    "        max_len = max(len(seq) for seq in generated_token_sequences)\n",
    "        padded_generated_token_sequences = [seq + [assistant_end] * (max_len - len(seq)) for seq in generated_token_sequences]\n",
    "        padded_masks = [mask + [0] * (max_len - len(mask)) for mask in masks]\n",
    "\n",
    "        ids = torch.tensor(padded_generated_token_sequences, dtype=torch.long, device=device)\n",
    "        mask_ids = torch.tensor(padded_masks, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs = ids[:, :-1]\n",
    "        targets = ids[:, 1:].clone()\n",
    "        targets[mask_ids[:, 1:] == 0] = -1\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float, device=device)\n",
    "\n",
    "        mu = rewards.mean()\n",
    "        advantages = rewards - mu\n",
    "\n",
    "        yield generated_token_sequences, inputs, targets, rewards, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b72e0-67f7-4940-aab7-1789b2cd215b",
   "metadata": {},
   "source": [
    "Before running it or thinking super carefully, the feel of it is that we're going to do something like judge loss by how much the model can predict whatever it predicted before but somehow weighted to prefer predictions that were correct. I can sort of see why this would work because the model already has the \"ability\" to generate that good prediction and the backprop here will reinforce weights to make it more likely. I don't understand why the inputs / targets though only have the part after the shared prefix. To take an extreme (and possibly inappropriate) example, if this was a different task where the user gave a multiple choice question and asked the assistant to respond only with A, B, C, or D, then how would this work?\n",
    "\n",
    "But back to this GSM8K case, looking at `challenge-27-understand-chat-eval/chat-eval-data-examples.ipynb` to remember exactly what they look like...\n",
    "\n",
    "oh, wait, the cutting out the prefix is only for calculating the reward for each sample, each row of the batch will contain the whole thing\n",
    "\n",
    "Suppose our initial prompt is:\n",
    "\n",
    "`<|user_start|>Mary Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.<|user_end|><|assistant_start|>`\n",
    "\n",
    "We generate 16 samples resulting in these completions:\n",
    "\n",
    "- ...a bunch of reasoning and calculating...### 109\n",
    "\n",
    "- ...some other bunch of reasoning and calculating...### 7\n",
    "\n",
    "- ...yet another bunch of reasoning and calculating...### 109\n",
    "\n",
    "- 13 other samples\n",
    "\n",
    "1 and 3 gave the right answer, 2 gave the wrong answer, and say the other 13 were wrong. There's no guarantee that 1 and 3 are right for the right reason, but the others are definitely wrong, so our goal is to adjust weights to make it more likely for the model to generate 1 and 3 and less likely to generate the other in future similar situations. We're reinforcing goodness. I think that's the idea.\n",
    "\n",
    "And just as in SFT, we're only interested in learning to predict tokens the assistant is supposed to write, so not the user part of the conversation or python output.\n",
    "\n",
    "But how will we adjust the loss? Going back to the example above, say the rewards are as follows:\n",
    "\n",
    "- 1.0\n",
    "- 0\n",
    "- 1.0\n",
    "- 0 for the 13 others\n",
    "\n",
    "I guess there are lots of ways. For example we normalize the rewards and then multiply the average cross entropy loss of the row by the corresponding reward by some constant.\n",
    "\n",
    "\"Advantages\" seems like a clue. Looking at those last two lines, for this example we'll end up with\n",
    "\n",
    "```\n",
    "rewards = [1,0,1,0...0]\n",
    "mu = 2 / 16 = 0.125\n",
    "advantages = [0.875, -0.125, 0.875, -0.125...-0.125]\n",
    "```\n",
    "\n",
    "So maybe in the actual loss calculation we use advantages and just add, or add it times a constant.\n",
    "\n",
    "Try get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "357c9b29-5797-4b4f-94a5-1ed1ffedbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "step = 1\n",
    "max_new_tokens = 128\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "from contextlib import nullcontext\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "generated_token_sequences, inputs, targets, rewards, advantages = next(get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf194000-0f4b-44cf-969e-1799ea794161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 90, 182, 109, 120)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(generated_token_sequences),\n",
    " len(generated_token_sequences[0]),\n",
    " len(generated_token_sequences[1]),\n",
    " len(generated_token_sequences[2]),\n",
    " len(generated_token_sequences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe184be6-ef9a-4b6e-aa92-f0ba29f00a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 181])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e95818e8-b0ae-49ad-984f-8144d4718f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|><|user_start|>Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|user_end|><|assistant_start|> not about of specific may,.\\n is, about isTo bayonet not of of  to more of while and about will it what which, and on of.\\n\\n\\n the,<|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a88a839e-5529-4de5-a204-bfa95be82630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|><|user_start|>Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|user_end|><|assistant_start|> also point\\n a their The andTo teller, that are, one point is can is willTo expensive will it will and as be can not and to the you.s\\n that the will to can be while can.\\n will other which in the, the each but to thatHowever, on a the the can also be, about that if can on that. to a of may. is the about\\n more of a that on also one of that but of it and each. would as while a and a the,, but also can and or as a be a\\n and\\n\\nHowever, may not.\\n\\n would with'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d55e832-8c5b-4bec-8b68-f08a4f810581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,   543,  1187,    10,   257,   472,   361,   288,\n",
       "         2240, 63906,    44,   332,   345,    44,   550,  1187,   309,   400,\n",
       "          309,   490,  2240,  5327,   490,   356,   490,   288,   343,   311,\n",
       "          400,   434,   288,   287,   261,   348,    46,   115,    10,   332,\n",
       "          261,   490,   287,   400,   311,  1095,   400,   307,   490,   534,\n",
       "          491,   283,   261,    44,   261,   961,   540,   287,   332,  4238,\n",
       "           44,   331,   257,   261,   261,   400,   543,   311,    44,   566,\n",
       "          332,   711,   400,   331,   332,    46,   287,   257,   281,   616,\n",
       "           46,   309,   261,   566,    10,   498,   281,   257,   332,   331,\n",
       "          543,   550,   281,   332,   540,   281,   356,   288,   961,    46,\n",
       "          717,   343,  1095,   257,   288,   257,   261,    44,    44,   540,\n",
       "          543,   400,   288,   355,   343,   257,   311,   257,    10,   288,\n",
       "           10,    10,  4238,    44,   616,   434,   307,    10,   717,   353,\n",
       "          287], device='mps:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d1b326f-8ece-4261-9396-d471b5e07500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expect all 0 because no way my d1 model got any right\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2150bbee-a932-4e0f-89a9-8b79b086d4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fdfce-0367-44b5-b9aa-ea3898ad5c2a",
   "metadata": {},
   "source": [
    "^ all more or less makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb33a3-6a1f-4124-b640-2f26c3160b23",
   "metadata": {},
   "source": [
    "### Loss calculation\n",
    "\n",
    "Now how is loss actually calculated? Key code:\n",
    "\n",
    "```\n",
    "logp = -model(inputs, targets, loss_reduction='none').view_as(inputs) # (B, T)\n",
    "\n",
    "# Calculate the PG objective. Note that ignore_index=-1 ensures that invalid tokens have loss 0.\n",
    "pg_obj = (logp * advantages.unsqueeze(-1)).sum()\n",
    "\n",
    "# normalize by the number of valid tokens, number of passes, and examples_per_rank\n",
    "num_valid = (targets >= 0).sum().clamp(min=1)\n",
    "pg_obj = pg_obj / (num_valid * num_passes * examples_per_rank)\n",
    "\n",
    "# Note, there is no need to add PPO ratio+clip because we are on policy\n",
    "\n",
    "# Finally, formulate the loss that we want to minimize (instead of objective we wish to maximize)\n",
    "loss = -pg_obj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45f480-3eb5-4bc6-8dee-2311665ba2f3",
   "metadata": {},
   "source": [
    "What is PG objective? Google: The objective of policy gradient (PG) in reinforcement learning is to maximize the expected cumulative reward by directly optimizing the agent's policy.\n",
    "\n",
    "ok, so the basic idea is we flip the cross entropy loss so increase is good, multiply flipped per-token loss in each row by the advantage for the row (like 0.875 and -0.125 in my example above), add it all up, and flip again so decrease is good.\n",
    "\n",
    "however, we also do some normalization in between. For example, if we processed a batch with tons of user tokens and only a few assistant tokens, the (absolute value of the) loss will probably be much smaller than a batch with tons of asssitant tokens because we're just adding everything together. This corrects for that and makes loss more like how we normally calculat it by taking the mean over the batch of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b88426-6089-454a-add6-e55768fd2acb",
   "metadata": {},
   "source": [
    "### Create my_chat_rl.py\n",
    "\n",
    "Start hand copying the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf7207-aa16-4869-933f-386386b0b422",
   "metadata": {},
   "source": [
    "What is this part of eval?\n",
    "\n",
    "```\n",
    "        for k in range(1, device_batch_size + 1):\n",
    "            passk[k - 1] = sum(any(o[\"is_correct\"] for o in r[\"outcomes\"][:k]) for r in records)\n",
    "```\n",
    "\n",
    "It's just saying pass@1 if the first one is correct, pass@2 if any of the first two are correct, etc.\n",
    "\n",
    "(This reminds me of something that either is confusing or I was confused about in earlier code: When does n-shot mean the number of examples given at the beginning of the prompt and when does n-shot mean how many chances you have to get a correct answer?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd7916bf-fb8c-4102-ae8e-757f66d67edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 4\n",
    "T = 5\n",
    "fake_logp = torch.ones((B, T))\n",
    "fake_advantages = torch.tensor([1,2,3,4])\n",
    "fake_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cff26ef3-e626-49cd-a856-d524f5096d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_advantages.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14aca734-2276-4f6b-a3bb-1db24beed5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.],\n",
       "        [4., 4., 4., 4., 4.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_logp * fake_advantages.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a57cd9-27a3-4642-a1d7-ff8b83d35775",
   "metadata": {},
   "source": [
    "What does this comment mean?\n",
    "\n",
    "`# Note, there is no need to add PPO ratio+clip because we are on policy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e7288-413f-4116-8c72-a9662f299320",
   "metadata": {},
   "source": [
    "What exactly is a \"rollout\"? Maybe it's the idea of taking one example and \"rolling it out\" into many samples and then taking those sample and their rewards and \"rolling that out\" into the model via forward and back prop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2deff-4a4b-4e40-b322-0297b7cbd66a",
   "metadata": {},
   "source": [
    "Try...might be too much of a pain to get it to run through on my mac (not even talking about it doing anything useful). Can see from things left out of his code that he didn't try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c4ce8e18-da14-469c-8c91-0cc8000cdd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f534bb4-6160-444c-8026-06d5ffe819fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d1\n",
      "overriding source = sft\n",
      "overriding device_batch_size = 1\n",
      "overriding examples_per_step = 4\n",
      "overriding num_samples = 4\n",
      "overriding max_new_tokens = 128\n",
      "overriding eval_examples = 10\n",
      "overriding eval_every = 5\n",
      "user_config: {'run': 'dummy', 'source': 'sft', 'dtype': 'bfloat16', 'device_type': '', 'device_batch_size': 1, 'examples_per_step': 4, 'num_samples': 4, 'max_new_tokens': 128, 'temperature': 1.0, 'top_k': 50, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.05, 'num_epochs': 1, 'save_every': 60, 'eval_every': 5, 'eval_examples': 10}\n",
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d1 with step 9\n",
      "Building model with config: {'sequence_len': 256, 'vocab_size': 65536, 'n_layer': 1, 'n_head': 1, 'n_kv_head': 1, 'n_embd': 64}\n",
      "Calculated number of steps: 1868\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(64/768) = 3.464101615137755\n",
      "total sequences per step: 16\n",
      "calculated examples per rank: 4\n",
      "Step 0 | Pass@1: 0.0000\n",
      "Step 0/1868 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 0 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 0 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 1 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 1 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 2 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 2 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 2 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 2 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 3 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 3 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 3 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Example step 3 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 0/1868 | Average reward: 0.0 | Average sequence length: 165.56\n",
      "W1125 20:26:45.015000 78185 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "Step 1/1868 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 0 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 0 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 1 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 1 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 2 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 2 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 2 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 2 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 3 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 3 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 3 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Example step 3 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/1868 | Average reward: 0.0 | Average sequence length: 162.94\n",
      "Step 2/1868 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 0 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 0 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 1 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 1 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 2 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 2 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 2 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 2 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 3 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 3 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 3 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Example step 3 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/1868 | Average reward: 0.0 | Average sequence length: 145.44\n",
      "Step 3/1868 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 0 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 0 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 1 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 1 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 2 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 2 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 2 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 2 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 3 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 3 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 3 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Example step 3 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/1868 | Average reward: 0.0 | Average sequence length: 149.44\n",
      "Step 4/1868 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 4/1868 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 4/1868 | Example step 0 | Pass 2 | loss: -0.000000 | average reward: 0.0\n",
      "Step 4/1868 | Example step 0 | Pass 3 | loss: -0.000000 | average reward: 0.0\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/scripts/my_chat_rl.py\", line 205, in <module>\n",
      "    sequences_all, inputs_all, targets_all, rewards_all, advantages_all = next(batch_iterator)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 59, in generator_context\n",
      "    response = gen.send(request)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/scripts/my_chat_rl.py\", line 79, in get_batch\n",
      "    generated_token_sequences_batch, masks_batch = engine.generate_batch(\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/my_nanochat/my_engine.py\", line 258, in generate_batch\n",
      "    for token_column, token_masks in self.generate(tokens, num_samples, **kwargs):\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 59, in generator_context\n",
      "    response = gen.send(request)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/my_nanochat/my_engine.py\", line 212, in generate\n",
      "    next_ids = sample_next_token(logits, rng, temperature, top_k) # (B, 1)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/my_nanochat/my_engine.py\", line 141, in sample_next_token\n",
      "    choice = torch.multinomial(probs, num_samples=1, generator=rng)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_chat_rl \\\n",
    "    --model_tag=d1 \\\n",
    "    --source=sft \\\n",
    "    --device_batch_size=1 \\\n",
    "    --examples_per_step=4 \\\n",
    "    --num_samples=4 \\\n",
    "    --max_new_tokens=128 \\\n",
    "    --eval_examples=10 \\\n",
    "    --eval_every=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9ca41-02ba-40b8-9008-3e5093f7d5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b55576-a6c3-4bb3-ae2b-3c3211a2f803",
   "metadata": {},
   "source": [
    "Code added as part of this challenge:\n",
    "\n",
    "- `my_chat_rl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd78bbe-9294-47b7-aa56-5103f5a1751e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
