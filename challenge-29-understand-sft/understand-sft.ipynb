{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68e703f-0447-4cd6-a649-26ff0e92ac70",
   "metadata": {},
   "source": [
    "### Understand SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbf9fb-5ff5-4adc-8871-e202f885a920",
   "metadata": {},
   "source": [
    "I see in [speedrun.sh](https://github.com/karpathy/nanochat/blob/master/speedrun.sh) the next thing after midtraining is to do supervised finetuning via [chat_sft.py](https://github.com/karpathy/nanochat/blob/master/scripts/chat_sft.py). See what that's about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4be59-cb4f-475e-ab23-81569efdc35b",
   "metadata": {},
   "source": [
    "The train dataset looks similar to from midtraining. Compare.\n",
    "\n",
    "Midtraining:\n",
    "\n",
    "```\n",
    "train_dataset = TaskMixture([\n",
    "    SmolTalk(split=\"train\"), # 460K rows of general conversations\n",
    "    MMLU(subset=\"auxiliary_train\", split=\"train\"), # 100K rows of multiple choice problems drawn from ARC, MC_TEST, OBQA, RACE\n",
    "    GSM8K(subset=\"main\", split=\"train\"), # 8K rows teaching simple math and (calculator) tool use\n",
    "    CustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
    "    CustomJSON(filepath=identity_conversations_filepath), # let's do 2 epochs of these\n",
    "    SimpleSpelling(size=200000, split=\"train\"), # 200K rows of Simple Spelling (e.g. spell the word 'apple')\n",
    "    SpellingBee(size=80000, split=\"train\"), # 80K rows of Spelling Bee (e.g. how many 'r' are in 'strawberry'?)\n",
    "])\n",
    "```\n",
    "\n",
    "SFT:\n",
    "\n",
    "```\n",
    "train_ds = TaskMixture([\n",
    "    ARC(subset=\"ARC-Easy\", split=\"train\"), # 2.3K rows\n",
    "    ARC(subset=\"ARC-Challenge\", split=\"train\"), # 1.1K rows\n",
    "    GSM8K(subset=\"main\", split=\"train\"), # 8K rows\n",
    "    SmolTalk(split=\"train\", stop=10_000), # 10K rows of smoltalk\n",
    "    CustomJSON(filepath=identity_conversations_filepath), # 1K rows of synthetic identity conversations\n",
    "    SimpleSpelling(size=300, split=\"train\"), # 300 rows of Simple Spelling (e.g. spell the word 'apple')\n",
    "    SpellingBee(size=300, split=\"train\"), # 300 rows of Spelling Bee (e.g. how many 'r' are in 'strawberry'?)\n",
    "])\n",
    "```\n",
    "\n",
    "```\n",
    "                            mid?            sft?\n",
    "---------------------------------------------------\n",
    "SmolTalk                    yes             no\n",
    "MMLU                        yes             no\n",
    "ARC-Easy                    no              yes\n",
    "ARC-Challenge               no              yes\n",
    "GSM8K                       yes             yes\n",
    "identity                    yes             yes\n",
    "SimpleSpelling              yes             yes\n",
    "SpellingBee                 yes             yes\n",
    "\n",
    "```\n",
    "\n",
    "Does anything jump out about SmolTalk, MMLU, and ARC about why they would be included in one and not the other?\n",
    "\n",
    "Look at challenge-26-understand-midtrain/midtrain-data-examples.ipynb. I don't have ARC in there. Make a new notebook in this challenge with that: `sft-data-examples.ipynb`\n",
    "\n",
    "Not sure. From looking at very few examples, the SmolTalk and MMLU conversations have much more text than ARC. ARC is all multiple choice but so is MMLU.\n",
    "\n",
    "Keep looking through script. Maybe it will become clear.\n",
    "\n",
    "I see now we're finally going to get into using those masks, so the model won't be learning to generate user stuff or python output. (See notes under `Tokenizer.render_conversation()` in `challenge-26-understand-midtrain/understand-midtrain.ipynb`.)\n",
    "\n",
    "And I see when we call `F.cross_entropy()` in GPT we pass `ignore_index=-1` so a -1 in our target list of tokens will mean to ignore that prediction when calculating loss. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6807b85a-1f1c-4cd5-a51e-14de6bdb0062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7444), tensor(0.7444))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# expect these two to be the same\n",
    "a = F.cross_entropy(torch.tensor([[0.1, 0.2],[0.3, 0.4]]), torch.tensor([0,-1], dtype=torch.long), ignore_index=-1)\n",
    "b = F.cross_entropy(torch.tensor([[0.1, 0.2]]), torch.tensor([0], dtype=torch.long))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe59d4-398a-4177-a17d-1777a83cc15c",
   "metadata": {},
   "source": [
    "So repeating an example just like from `challenge-26-understand-midtrain/understand-midtrain.ipynb` but now using -1 and clearer fake token numbering, this is what we want:\n",
    "\n",
    "```\n",
    "ids to train on: 3  4  1  5  6  2  7\n",
    "mask:            0  0  0  0  0  1  1\n",
    "\n",
    "say 3 = bos\n",
    "    4 = user_start\n",
    "    1 = a token from the user\n",
    "    5 = user_end\n",
    "    6 = assistant_start\n",
    "    2 = a token from assistant\n",
    "    7 = assistant_end\n",
    "\n",
    "Our \"normal\" target would be: 4  1  5  6  2  7  ?\n",
    "\n",
    "However, we're only trying to learn how to predict 2 and 7 so we don't want to count the other predictions in our loss.\n",
    "\n",
    "So our modified target is: -1 -1 -1 -1  2  7 -1 \n",
    "\n",
    "Seeing it all together:\n",
    "\n",
    "inputs:   3  4  1  5  6  2  7\n",
    "targets: -1 -1 -1 -1  2  7 -1 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c22d3-040c-4d16-ba6e-f1f0c4de7f61",
   "metadata": {},
   "source": [
    "### sft_data_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec2552-26f7-4e4b-ac70-3d597c904a10",
   "metadata": {},
   "source": [
    "Hand copy sft_data_generator() to `my_chat_sft.py` and see what else besides masking, if anything, is different than in mid train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06d60ee-f591-4ab9-9efc-f7ed0c37faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from scripts.my_chat_sft import sft_data_generator, train_ds\n",
    "from my_nanochat.my_tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1b0f03-bdb4-4fe9-8234-46c6f9f24b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(sft_data_generator(train_ds, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aaba559-77bc-431b-aefa-fb9ececd2385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 329]), torch.Size([1, 329]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd32ccb-a9a7-4a9f-9b33-155331f6622e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|><|user_start|>A woman is trying to decide whether it will be quicker to take an airplane or drive herself to a job interview. If she drives herself, the trip will take her 3 hours and 15 minutes.  If she takes an airplane, she will first need to drive 10 minutes to the airport, and then wait 20 minutes to board the plane.  After that, she will be on the airplane for one-third of the time it would have taken her to drive herself before landing in the destination city. Finally, it will take her an additional 10 minutes to get off the airplane and arrive at her interview site after the plane lands.  Given this information, how many minutes faster is it for her to take the airplane?<|user_end|><|assistant_start|>First, we must convert the driving time of 3 hours and 15 minutes to minutes. Since there are 60 minutes in an hour, driving takes a total of 3*60 + 15 = <|python_start|>3*60+15<|python_end|><|output_start|>195<|output_end|>195 minutes.\\nNext, the woman will be on the airplane for one-third of 195 minutes, or 195/3 = <|python_start|>195/3<|python_end|><|output_start|>65<|output_end|>65 minutes.\\nTherefore, in total, the airplane trip will take the woman 10 + 20 + 65 + 10 = <|python_start|>10+20+65+10<|python_end|><|output_start|>105<|output_end|>105 minutes.\\nThus, the airplane trip is 195 - 105 = <|python_start|>195-105<|python_end|><|output_start|>90<|output_end|>90 minutes faster than driving herself to the interview.\\n#### 90'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expect one conversation (document) in the input, expect to see lots of -1s in the target\n",
    "tokenizer = get_tokenizer()\n",
    "tokenizer.decode(inputs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81ae51c-c284-43bf-a657-ac3f589d91b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,  6506,    44,   384,  1197,  8312,   261,  5228,   639,   281,\n",
       "           32,    51,  2452,   288,    32,  1088,  3068,   287,  3068,    46,\n",
       "         3893,   640,   345,    32,  1942,  3068,   283,   351,  4650,    44,\n",
       "         5228,  2750,   257,  2548,   281,    32,    51,    42,  1942,  3821,\n",
       "           32,  1088,  2300,    32, 65532,    51,    42,  1942,    43,  1088,\n",
       "        65533,    -1,    -1,    -1,    -1,   549,    53,  3068,   307, 11269,\n",
       "           44,   261,  3673,   490,   311,   331,   261, 16553,   327,   550,\n",
       "        14135,   281,    32,   549,    53,  3068,    44,   355,    32,   549,\n",
       "           53,    47,    51,  2300,    32, 65532,   549,    53,    47,    51,\n",
       "        65533,    -1,    -1,    -1,  3704,  3068,   307, 14557,    44,   283,\n",
       "         2548,    44,   261, 16553,  6710,   490,  1079,   261,  3673,    32,\n",
       "          737,  3821,    32,   496,  3821,    32,  3704,  3821,    32,   737,\n",
       "         2300,    32, 65532,   737,    43,   496,    43,  3704,    43,   737,\n",
       "        65533,    -1,    -1,    -1,    -1,   737,    53,  3068,   307, 13137,\n",
       "           44,   261, 16553,  6710,   309,    32,   549,    53,   816,    32,\n",
       "          737,    53,  2300,    32, 65532,   549,    53,    45,   737,    53,\n",
       "        65533,    -1,    -1,    -1,  2206,  3068,  4774,   617,  5228,  9795,\n",
       "          287,   261,  5532,   307, 42495, 42495,    32,  2206, 65531],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c2d40-aa18-404d-8d2d-59c8f9fe69a8",
   "metadata": {},
   "source": [
    "^ Those -1s in the 2nd ~half must be for things like <|output_start|>195<|output_end|> \n",
    "\n",
    "The very last token 65531 is not -1 like the example I came up with by hand and I'm not sure why.\n",
    "\n",
    "First sanity check that the -1s are what I think. If so, 65533 should be <|python_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae24f2df-8759-41c1-979e-3b117e4eb050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|python_end|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([65533])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ae3ec-98b2-4570-bfcd-6e75694a9823",
   "metadata": {},
   "source": [
    "Now what's that last 65531?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ec5918-fdfe-4e2c-b4a9-1543018ddce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|assistant_end|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([65531])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0dfc2-ef40-4bef-acab-836e18c95204",
   "metadata": {},
   "source": [
    "I get it. In my tiny hand example above I left the final id in the input, but in reality we remove it since we're not trying to predict from it and we have nothing to check it against. So I should have done this:\n",
    "\n",
    "```\n",
    "ids to train on: 3  4  1  5  6  2  7\n",
    "mask:            0  0  0  0  0  1  1\n",
    "\n",
    "inputs:          3  4  1  5  6  2\n",
    "targets:        -1 -1 -1 -1  2  7 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b6b4d-e5fe-4c0b-8cfa-aa83cd2340f3",
   "metadata": {},
   "source": [
    "So this seems different from mid training and base training in 2 ways:\n",
    "\n",
    "- We ignore the loss on certain predictions (e.g. user tokens, python output)\n",
    "\n",
    "- Each row of the batch is a complete conversation (doc). For example, if we have a batch of size 3 corresponding to 3 conversations, and the first is 4 tokens, the second is 5 tokens, and the third is 6 tokens, we'll get inputs like this (one less token for reason explained above):\n",
    "\n",
    "```\n",
    "T T T X X\n",
    "T T T T X \n",
    "T T T T T \n",
    "```\n",
    "\n",
    "Where T is a token from the conversation and X is whatever we pad with. We're willing to waste 3 tokens of processing.\n",
    "\n",
    "But in mid training and base training, we just pack them all in and would move on to the next conversation (doc):\n",
    "\n",
    "```\n",
    "T T T T T T\n",
    "T T T T T T\n",
    "T T T T T T\n",
    "```\n",
    "\n",
    "In mid training, the batch size and sequence length is specified ahead of time and consistent. In SFT training, each batch will be the width of the longest conversation (doc). (How is the batch size picked? Not sure, maybe will see that later. Also, do we do anything to group conversations of similar lengths?)\n",
    "\n",
    "Why does it work this way? This gets back to the question I had in `challenge-09-understand-model-input/understand-model-input.ipynb` where I was surprised we **didn't** line up by \"something\" at the start of each row and instead just packed the tokens in without regard to document boundary.\n",
    "\n",
    "Maybe it just doesn't matter when we're learning more basic stuff and we don't want to waste any processing power. But now that we're fine-tuning, we need the model to see full conversations that include all of the user_start, user_end, assistant_start, assistant_end, etc. tokens. We don't want to cut any conversations off in the middle, and we don't want to train on conversations that start in the middle. Maybe (even less sure) it's also helpful to have the beginning with bos placed exactly at the start of the sequence for the positional embedding stuff to work a little better???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3177380-639d-4530-a45a-8455ad46acdf",
   "metadata": {},
   "source": [
    "Just to sanity check I understand the padding, let's also do a batch size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2103fbbd-9532-4d42-9940-6aedb755fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1107]), torch.Size([2, 1107]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = next(sft_data_generator(train_ds, batch_size=2))\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbccf75e-f970-49ae-a392-a0486cfcc713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65527, 65528,    65,  ..., 65531, 65531, 65531],\n",
       "        [65527, 65528,  1708,  ...,   309,  1058,    46]], device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d83cd-8b00-4137-807e-c16593a6beaa",
   "metadata": {},
   "source": [
    "^ Yes, you can see the 2nd conversation is longer and so the end of the first one is padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20382402-3185-4f33-b9cb-71cb48cf5341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|bos|><|user_start|>A woman is trying to decide whether it will be quicker to take an airplane or drive herself to a job interview. If she drives herself, the trip will take her 3 hours and 15 minutes.  If she takes an airplane, she will first need to drive 10 minutes to the airport, and then wait 20 minutes to board the plane.  After that, she will be on the airplane for one-third of the time it would have taken her to drive herself before landing in the destination city. Finally, it will take her an additional 10 minutes to get off the airplane and arrive at her interview site after the plane lands.  Given this information, how many minutes faster is it for her to take the airplane?<|user_end|><|assistant_start|>First, we must convert the driving time of 3 hours and 15 minutes to minutes. Since there are 60 minutes in an hour, driving takes a total of 3*60 + 15 = <|python_start|>3*60+15<|python_end|><|output_start|>195<|output_end|>195 minutes.\\nNext, the woman will be on the airplane for one-third of 195 minutes, or 195/3 = <|python_start|>195/3<|python_end|><|output_start|>65<|output_end|>65 minutes.\\nTherefore, in total, the airplane trip will take the woman 10 + 20 + 65 + 10 = <|python_start|>10+20+65+10<|python_end|><|output_start|>105<|output_end|>105 minutes.\\nThus, the airplane trip is 195 - 105 = <|python_start|>195-105<|python_end|><|output_start|>90<|output_end|>90 minutes faster than driving herself to the interview.\\n#### 90<|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|><|assistant_end|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5054887b-df26-442d-9655-008a2f944c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|bos|><|user_start|>How does standard deviation measure variability or dispersion of a data set? Can you describe a data set that would have a low standard deviation and another that would have a high standard deviation?<|user_end|><|assistant_start|>Standard deviation measures the variability or dispersion of a data set by quantifying how spread out the individual data points are from the mean value. A low standard deviation indicates that the data points tend to be close to the mean, while a high standard deviation indicates that the data points are more spread out.\\n\\nA data set with a low standard deviation would be one where the values are relatively consistent and do not deviate much from the mean. For example, consider the scores of a group of students on a math test, where most students scored between 80 and 90, with a mean score of 85. In this case, the standard deviation would be low, perhaps around 2-3 points, indicating that the scores are clustered close to the mean.\\n\\nOn the other hand, a data set with a high standard deviation would be one where the values are more spread out and varied. For instance, consider the prices of houses in a city, ranging from $200,000 to $2 million, with a mean price of $500,000. In this case, the standard deviation would be high, perhaps around $200,000 to $300,000, indicating that the prices are more dispersed and less clustered around the mean.\\n\\nIn general, a low standard deviation suggests that the data is more predictable and consistent, while a high standard deviation suggests that the data is more variable and less predictable.<|assistant_end|><|user_start|>How does variance measure variability or dispersion? Is there a specific mathematical formula to calculate variance?<|user_end|><|assistant_start|>Variance measures the variability or dispersion of a data set by calculating the average of the squared differences between each data point and the mean value. The variance gives an idea of how spread out the data is from the mean.\\n\\nThe mathematical formula to calculate variance is:\\n\\nVariance (σ²) = Σ(xi - μ)² / (n - 1)\\n\\nWhere:\\n- σ² is the variance\\n- xi is each individual data point\\n- μ is the mean of the data set\\n- n is the number of data points\\n- Σ denotes the sum of the squared differences\\n\\nIn simpler terms, the formula calculates the difference between each data point and the mean, squares each difference, and then averages the squared differences.\\n\\nTo calculate variance, you would follow these general steps:\\n\\n1. Calculate the mean of the data set.\\n2. Subtract the mean from each data point to find the deviation.\\n3. Square each deviation.\\n4. Calculate the sum of the squared deviations.\\n5. Divide the sum by the number of data points minus one (n - 1) to get the variance.\\n\\nNote that there are two types of variance: population variance (divided by n) and sample variance (divided by n - 1). The formula above calculates the sample variance, which is more commonly used in statistics.<|assistant_end|><|user_start|>Explain what the term 'statistical significance' means in the context of analyzing data. What does it mean when the results are statistically significant versus not statistically significant?<|user_end|><|assistant_start|>In the context of data analysis, statistical significance refers to the probability that the observed results are due to chance rather than a real effect. It's a measure of how likely it is that the results are a result of random variation rather than a genuine relationship or effect.\\n\\nWhen analyzing data, researchers typically formulate a null hypothesis, which states that there is no effect or relationship between variables. They then test this hypothesis against an alternative hypothesis, which states that there is an effect or relationship.\\n\\nIf the results of the analysis indicate that the observed effect or relationship is unlikely to occur by chance, the results are said to be statistically significant. This means that the probability of obtaining the observed results (or more extreme results) assuming that the null hypothesis is true is below a certain threshold, usually set at 0.05 (5%).\\n\\nIn simpler terms:\\n\\n* **Statistically significant results**: The observed effect or relationship is unlikely to be due to chance, and it's likely that there is a real effect or relationship. The null hypothesis can be rejected.\\n* **Not statistically significant results**: The observed effect or relationship could be due to chance, and it's not possible to rule out the null hypothesis.\\n\\nFor example, suppose a researcher wants to determine if a new exercise program can reduce blood pressure. They collect data from a group of participants who completed the program and a control group who didn't. If the analysis shows a significant difference in blood pressure between the two groups, and the p-value (probability of obtaining the observed results assuming no effect) is below 0.05, the results are statistically significant. This suggests that the exercise program likely has a real effect on reducing blood pressure.\\n\\nOn the other hand, if the p-value is above 0.05, the results are not statistically significant. This doesn't necessarily mean that there is no effect; it simply means that the observed effect could be due to chance, and more data or a different analysis may be needed to confirm the results.\\n\\nIt's essential to note that statistical significance does not imply practical significance. A statistically significant result may not necessarily be meaningful or practically relevant, especially if the effect size is small.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442b704-13e6-42e6-814d-79d83839d680",
   "metadata": {},
   "source": [
    "^ wow, most of the tokens in the first row of the batch are wasted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af913f-2ad5-4b8a-a287-8921b347fbf0",
   "metadata": {},
   "source": [
    "Looking ahead in the code, I see that batch_size is in fact fixed for the whole run. That could mean that each step varies quite a bit in terms of number of tokens. It also means we need to worry about a conversation being too long, but maybe no conversation in the training is? (At least for training on something like H100?)\n",
    "\n",
    "In fact now copying the rest of the code, I see he defaults device_batch_size to 4 with a comment \"max to avoid OOM.\" Not sure if the compute time to adjust batch size per step and/or organize conversations in a way to maximize GPU use is too great to make it worthwhile, or it just wasn't done in this code. One way to do it would be to maintain a small buffer of tokenized conversations and each yield would choose conversations of similar length and return a batch with an appropriate but varying number of rows. However, I wonder if to do training efficiently we want a compiled model and compiled models expect or do best with a fixed batch size. Like if the batch keeps changing shape then it need to rejigger what operations get done to what memory.\n",
    "\n",
    "Now seeing that he commented this line out: `# model = torch.compile(model, dynamic=True) # doesn't work super well because of variable lengths of inputs` makes me realize that even as currently implemented the batch sizes aren't fixed, the height is, but not the width, so maybe it will help to organize the conversations by similar length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83746f8-dcdd-4583-8447-4080faa84313",
   "metadata": {},
   "source": [
    "### Rest of code\n",
    "\n",
    "Fill out the rest of `my_chat_sft.py` with a combination of copying and pasting from `my_mid_train.py` and hand copying from his chat_sft.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74575d-cae3-4abc-94ff-cabca2b8ad97",
   "metadata": {},
   "source": [
    "From this comment `the number of \"active\" tokens of supervision seen` I'm now thinking supervised / supervision in SFT relates to masking, only certain tokens \"supervise\" the training or the \"loss\". Something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5203e9-aae7-4316-a2ce-5e2c8a5d29d0",
   "metadata": {},
   "source": [
    "While doing this noticed I left out updating the muon_momentum in mid_train. Maybe I should redo mid train before doing sft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604db9ae-e269-4032-b0d0-9818d024a151",
   "metadata": {},
   "source": [
    "added code, try, though wonder if it will OOM on my mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bee2e13c-41d9-450e-b753-4907c5f5f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4677ef-a3f9-4c5c-b7ab-f375e80bbe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d4\n",
      "overriding num_iterations = 10\n",
      "overriding device_batch_size = 1\n",
      "overriding target_examples_per_step = 4\n",
      "overriding eval_every = 5\n",
      "overriding eval_steps = 10\n",
      "overriding eval_metrics_every = 5\n",
      "overriding eval_metrics_max_problems = 2\n",
      "user_config: {'run': 'dummy', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 1, 'num_epochs': 1, 'num_iterations': 10, 'target_examples_per_step': 4, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 5, 'eval_steps': 10, 'eval_metrics_every': 5, 'eval_metrics_max_problems': 2}\n",
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4 with step 9\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65536, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n",
      "Target examples per step: 4\n",
      "Device batch size: 1\n",
      "Examples per step is device_batch_size * ddp_world_size: 1\n",
      " => grad accum steps: 4\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/scripts/my_chat_sft.py\", line 153, in <module>\n",
      "    loss = model(val_inputs, val_targets)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/my_nanochat/my_gpt.py\", line 221, in forward\n",
      "    assert T < self.cos.size(1)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_chat_sft \\\n",
    "    --model_tag=d4 \\\n",
    "    --num_iterations=10 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --target_examples_per_step=4 \\\n",
    "    --eval_every=5 \\\n",
    "    --eval_steps=10 \\\n",
    "    --eval_metrics_every=5 \\\n",
    "    --eval_metrics_max_problems=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73731bd5-150a-4b4f-8bd6-90d8398109ea",
   "metadata": {},
   "source": [
    "^ it's failing beacuse the conversations are longer than 10 times the sequence length. Maybe just to test on my mac I should make something to skip conversations above a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cca0dfa0-fd6e-449d-bcd4-fefb57cef6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d4\n",
      "overriding num_iterations = 10\n",
      "overriding device_batch_size = 1\n",
      "overriding target_examples_per_step = 4\n",
      "overriding eval_every = 5\n",
      "overriding eval_steps = 10\n",
      "overriding eval_metrics_every = 5\n",
      "overriding eval_metrics_max_problems = 2\n",
      "overriding max_data_tokens = 1280\n",
      "user_config: {'run': 'dummy', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 1, 'num_epochs': 1, 'num_iterations': 10, 'max_data_tokens': 1280, 'target_examples_per_step': 4, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 5, 'eval_steps': 10, 'eval_metrics_every': 5, 'eval_metrics_max_problems': 2}\n",
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4 with step 9\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65536, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n",
      "Target examples per step: 4\n",
      "Device batch size: 1\n",
      "Examples per step is device_batch_size * ddp_world_size: 1\n",
      " => grad accum steps: 4\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Step 00000 | Validation loss: 7.964674\n",
      "Step 00000/00010 | Training loss: 10.691246| lrm: 1.000000| num_tokens: 2,365\n",
      "Step 00001/00010 | Training loss: 8.479701| lrm: 0.900000| num_tokens: 2,195\n",
      "Step 00002/00010 | Training loss: 7.930121| lrm: 0.800000| num_tokens: 289\n",
      "Step 00003/00010 | Training loss: 8.634449| lrm: 0.700000| num_tokens: 186\n",
      "Step 00004/00010 | Training loss: 9.570097| lrm: 0.600000| num_tokens: 2,011\n",
      "Step 00005 | Validation loss: 7.943930\n",
      "final: 1/2 (50.00%)\n",
      "final: 0/2 (0.00%)\n",
      "Step 00005 | mmlu_acc: 0.500000, arc_easy_acc: 0.000000\n",
      "Step 00005/00010 | Training loss: 9.155405| lrm: 0.500000| num_tokens: 1,451\n",
      "Step 00006/00010 | Training loss: 7.308039| lrm: 0.400000| num_tokens: 520\n",
      "Step 00007/00010 | Training loss: 9.282903| lrm: 0.300000| num_tokens: 701\n",
      "Step 00008/00010 | Training loss: 10.250582| lrm: 0.200000| num_tokens: 1,347\n",
      "Step 00009 | Validation loss: 7.937372\n",
      "final: 1/2 (50.00%)\n",
      "final: 0/2 (0.00%)\n",
      "Step 00009 | mmlu_acc: 0.500000, arc_easy_acc: 0.000000\n",
      "saved model to /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d4/model_000009.pt\n",
      "saved metadata to /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d4/meta_000009.json\n",
      "Saved model checkpoint to /Users/ericsilberstein/.cache/my_nanochat/chatsft_checkpoints/d4\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_chat_sft \\\n",
    "    --model_tag=d4 \\\n",
    "    --num_iterations=10 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --target_examples_per_step=4 \\\n",
    "    --eval_every=5 \\\n",
    "    --eval_steps=10 \\\n",
    "    --eval_metrics_every=5 \\\n",
    "    --eval_metrics_max_problems=2 \\\n",
    "    --max_data_tokens=1280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054183f-06d8-4e99-915b-ec3aa8cbec0a",
   "metadata": {},
   "source": [
    "^ ok, completed. Guess none of the 2 MMLU and 2 ARC conversations were too long.\n",
    "\n",
    "Now try without num_iterations and cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b3777c-1707-49bc-bb66-3d9130678c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d4\n",
      "overriding device_batch_size = 1\n",
      "overriding target_examples_per_step = 4\n",
      "overriding eval_every = 5\n",
      "overriding eval_steps = 10\n",
      "overriding eval_metrics_every = 5\n",
      "overriding eval_metrics_max_problems = 2\n",
      "overriding max_data_tokens = 1280\n",
      "user_config: {'run': 'dummy', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 1, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': 1280, 'target_examples_per_step': 4, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 5, 'eval_steps': 10, 'eval_metrics_every': 5, 'eval_metrics_max_problems': 2}\n",
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4 with step 9\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65536, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n",
      "Target examples per step: 4\n",
      "Device batch size: 1\n",
      "Examples per step is device_batch_size * ddp_world_size: 1\n",
      " => grad accum steps: 4\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "Step 00000 | Validation loss: 7.964674\n",
      "Step 00000/05609 | Training loss: 10.691246| lrm: 1.000000| num_tokens: 2,365\n",
      "Step 00001/05609 | Training loss: 8.479702| lrm: 0.999822| num_tokens: 2,195\n",
      "Step 00002/05609 | Training loss: 7.929584| lrm: 0.999643| num_tokens: 289\n",
      "Step 00003/05609 | Training loss: 8.631446| lrm: 0.999465| num_tokens: 186\n",
      "Step 00004/05609 | Training loss: 9.531862| lrm: 0.999287| num_tokens: 2,011\n",
      "Step 00005 | Validation loss: 7.939410\n",
      "final: 1/2 (50.00%)\n",
      "final: 0/2 (0.00%)\n",
      "Step 00005 | mmlu_acc: 0.500000, arc_easy_acc: 0.000000\n",
      "Step 00005/05609 | Training loss: 9.150599| lrm: 0.999109| num_tokens: 1,451\n",
      "Step 00006/05609 | Training loss: 7.303567| lrm: 0.998930| num_tokens: 520\n",
      "Step 00007/05609 | Training loss: 9.264574| lrm: 0.998752| num_tokens: 701\n",
      "Step 00008/05609 | Training loss: 10.121643| lrm: 0.998574| num_tokens: 1,347\n",
      "Step 00009/05609 | Training loss: 7.753452| lrm: 0.998395| num_tokens: 1,717\n",
      "Step 00010 | Validation loss: 7.914809\n",
      "final: 1/2 (50.00%)\n",
      "final: 0/2 (0.00%)\n",
      "Step 00010 | mmlu_acc: 0.500000, arc_easy_acc: 0.000000\n",
      "Step 00010/05609 | Training loss: 8.409476| lrm: 0.998217| num_tokens: 1,623\n",
      "Step 00011/05609 | Training loss: 6.749553| lrm: 0.998039| num_tokens: 937\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/ericsilberstein/.local/share/uv/python/cpython-3.10.18-macos-aarch64-none/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/my_nanochat/scripts/my_chat_sft.py\", line 205, in <module>\n",
      "    loss.backward()\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 625, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 841, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_chat_sft \\\n",
    "    --model_tag=d4 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --target_examples_per_step=4 \\\n",
    "    --eval_every=5 \\\n",
    "    --eval_steps=10 \\\n",
    "    --eval_metrics_every=5 \\\n",
    "    --eval_metrics_max_problems=2 \\\n",
    "    --max_data_tokens=1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b90bc4-e618-47d7-95c3-7a5d47abb4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64795d10-baed-467b-9fa4-3eea12225fc4",
   "metadata": {},
   "source": [
    "Code added as part of this challenge:\n",
    "\n",
    "- `my_chat_sft.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1139e74-f320-4f57-9b52-48a8cbbecdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
