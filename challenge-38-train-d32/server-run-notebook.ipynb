{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4fb844-de00-4459-b225-d7fc63738640",
   "metadata": {},
   "source": [
    "## Server run notebook\n",
    "\n",
    "This is not the main notebook. See `instructions.ipynb`.\n",
    "\n",
    "I'll use this notebook to record and check things along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ade605-0222-491e-96b5-c56a8db8b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 22 15:45:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51fd9e-c493-4958-908c-d8c528219b7b",
   "metadata": {},
   "source": [
    "## hit this problem\n",
    "\n",
    "Doing the first test run. Looks like https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip is no longer accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a958c6-b861-4034-b44b-c16818dae4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank2]:     result = func(*args)\n",
      "[rank2]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank2]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank2]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank7]: Traceback (most recent call last):\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank7]:     return _run_code(code, main_globals, None,\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank7]:     exec(code, run_globals)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank7]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank7]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank7]:     with urllib.request.urlopen(url) as response:\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank7]:     return opener.open(url, data, timeout)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank7]:     response = meth(req, response)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank7]:     response = self.parent.error(\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank7]:     return self._call_chain(*args)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank7]:     result = func(*args)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank7]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank7]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank5]: Traceback (most recent call last):\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank5]:     return _run_code(code, main_globals, None,\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank5]:     exec(code, run_globals)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank5]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank5]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank5]:     with urllib.request.urlopen(url) as response:\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank5]:     return opener.open(url, data, timeout)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank5]:     response = meth(req, response)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank5]:     response = self.parent.error(\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank5]:     return self._call_chain(*args)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank5]:     result = func(*args)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank5]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank5]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank6]: Traceback (most recent call last):\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank6]:     return _run_code(code, main_globals, None,\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank6]:     exec(code, run_globals)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank6]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank6]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank6]:     with urllib.request.urlopen(url) as response:\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank6]:     return opener.open(url, data, timeout)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank6]:     response = meth(req, response)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank6]:     response = self.parent.error(\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank6]:     return self._call_chain(*args)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank6]:     result = func(*args)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank6]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank6]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank3]:     return _run_code(code, main_globals, None,\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank3]:     exec(code, run_globals)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank3]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank3]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank3]:     with urllib.request.urlopen(url) as response:\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank3]:     return opener.open(url, data, timeout)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank3]:     response = meth(req, response)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank3]:     response = self.parent.error(\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank3]:     return self._call_chain(*args)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank3]:     result = func(*args)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank3]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank3]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "    download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "    with urllib.request.urlopen(url) as response:\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "    response = self.parent.error(\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank0]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank0]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank0]:     with urllib.request.urlopen(url) as response:\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank0]:     return opener.open(url, data, timeout)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank0]:     response = meth(req, response)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank0]:     response = self.parent.error(\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank0]:     return self._call_chain(*args)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank0]:     result = func(*args)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank0]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank0]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mchallenge-38-1\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251222_154945-byeg46fu/logs\u001b[0m\n",
      "[rank1]:[W1222 15:52:01.322717052 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank4]:[W1222 15:52:01.649825257 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1222 15:52:01.719296376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank7]:[W1222 15:52:01.778273131 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank2]:[W1222 15:52:01.908061259 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank6]:[W1222 15:52:01.117893743 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank5]:[W1222 15:52:01.229616340 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1222 15:52:02.000000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21232 closing signal SIGTERM\n",
      "W1222 15:52:02.001000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21234 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21235 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21236 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21237 closing signal SIGTERM\n",
      "W1222 15:52:02.003000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21238 closing signal SIGTERM\n",
      "W1222 15:52:02.003000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21239 closing signal SIGTERM\n",
      "E1222 15:52:05.223000 21195 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 21233) of binary: /home/ubuntu/learn-nanochat/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-12-22_15:52:02\n",
      "  host      : 192-222-52-230\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 21233)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!tail -200 /home/ubuntu/mynanochat2/run_outputs/base_train_output_001.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df23bad-063e-4033-bf8c-53f31764453a",
   "metadata": {},
   "source": [
    "### repeating test run\n",
    "\n",
    "...after scp'ing base_eval from my laptop\n",
    "\n",
    "logging to `base_train_output_002.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6486ab-03a9-4ca7-bf6b-4df213dc48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb:            train/lrm ‚ñÅ\n",
      "wandb:            train/mfu ‚ñÅ\n",
      "wandb:    train/tok_per_sec ‚ñÅ\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:          core_metric -0.02267\n",
      "wandb:                 step 10\n",
      "wandb: total_training_flops 63331869759897600\n",
      "wandb:  total_training_time 0\n",
      "wandb:             train/dt 22.59089\n",
      "wandb:      train/grad_norm 0.53604\n",
      "wandb:           train/loss 11.09035\n",
      "wandb:            train/lrm 1\n",
      "wandb:            train/mfu 3.54326\n",
      "wandb:    train/tok_per_sec 23207\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-1 at: https://wandb.ai/ericsilberstein-self/my-nanochat/runs/0ie1fin2\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251222_155927-0ie1fin2/logs\n",
      "[W1222 16:01:23.002992110 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1222 16:01:23.007993651 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1222 16:01:24.529711010 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -25 /home/ubuntu/mynanochat2/run_outputs/base_train_output_002.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc1e22-97ff-46d2-aba6-8f8737475f48",
   "metadata": {},
   "source": [
    "### confirm can load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f3494a-adfc-44c3-95da-552971a662c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type, compute_init\n",
    "from my_nanochat.my_checkpoint_manager import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b30aee-9aa5-4ec4-8765-0a03e6c36b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 10\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type() \n",
    "_, _, _, _, device = compute_init(device_type)\n",
    "model, tokenizer, meta_data = load_model('base', model_tag='d32', device=device, phase='eval')\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2117a99-780d-4c60-a21f-57907e82ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1006, 345, 400, 311, 4236, 3328, 281, 261, 24142, 4398]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autocast_ctx:\n",
    "    tokens = list(model.generate(tokenizer.encode(\"First take a right on Main St.\", prepend=tokenizer.get_bos_token_id()), max_tokens=10))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e939dd49-cfe5-4155-a0e0-90f134da8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We are can be introduced examples of the utmost background'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea6f8d-3bbc-4ea8-a2af-a52546e1cf41",
   "metadata": {},
   "source": [
    "that looks ok, also check wandb\n",
    "\n",
    "look ok, do real run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f039456-ed60-433b-a537-29d0a048f674",
   "metadata": {},
   "source": [
    "### full run followed by base_loss followed by base_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06b799e-76e3-45fd-a534-73e194e86958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 71658/71680 (99.97%) | loss: 2.372278 | grad norm: 0.1515 | lrm: 0.00 | dt: 1705.46ms | tok/sec: 307,416 | mfu: 46.93 | total time: 1961.10m\n",
      "step 71659/71680 (99.97%) | loss: 2.378669 | grad norm: 0.1473 | lrm: 0.00 | dt: 2489.89ms | tok/sec: 210,566 | mfu: 32.15 | total time: 1961.14m\n",
      "step 71660/71680 (99.97%) | loss: 2.409759 | grad norm: 0.1564 | lrm: 0.00 | dt: 1513.21ms | tok/sec: 346,474 | mfu: 52.90 | total time: 1961.17m\n",
      "step 71661/71680 (99.97%) | loss: 2.400042 | grad norm: 0.1685 | lrm: 0.00 | dt: 1523.07ms | tok/sec: 344,230 | mfu: 52.56 | total time: 1961.19m\n",
      "step 71662/71680 (99.97%) | loss: 2.398359 | grad norm: 0.1515 | lrm: 0.00 | dt: 1516.79ms | tok/sec: 345,656 | mfu: 52.77 | total time: 1961.22m\n",
      "step 71663/71680 (99.98%) | loss: 2.397501 | grad norm: 0.1507 | lrm: 0.00 | dt: 1519.49ms | tok/sec: 345,041 | mfu: 52.68 | total time: 1961.24m\n",
      "step 71664/71680 (99.98%) | loss: 2.383298 | grad norm: 0.1942 | lrm: 0.00 | dt: 1520.90ms | tok/sec: 344,722 | mfu: 52.63 | total time: 1961.27m\n",
      "step 71665/71680 (99.98%) | loss: 2.390323 | grad norm: 0.1416 | lrm: 0.00 | dt: 1521.79ms | tok/sec: 344,519 | mfu: 52.60 | total time: 1961.29m\n",
      "step 71666/71680 (99.98%) | loss: 2.355329 | grad norm: 0.1843 | lrm: 0.00 | dt: 1771.04ms | tok/sec: 296,034 | mfu: 45.20 | total time: 1961.32m\n",
      "step 71667/71680 (99.98%) | loss: 2.353283 | grad norm: 0.1577 | lrm: 0.00 | dt: 2382.30ms | tok/sec: 220,075 | mfu: 33.60 | total time: 1961.36m\n",
      "step 71668/71680 (99.98%) | loss: 2.363530 | grad norm: 0.1529 | lrm: 0.00 | dt: 1513.02ms | tok/sec: 346,517 | mfu: 52.90 | total time: 1961.39m\n",
      "step 71669/71680 (99.98%) | loss: 2.366316 | grad norm: 0.1513 | lrm: 0.00 | dt: 1520.08ms | tok/sec: 344,907 | mfu: 52.66 | total time: 1961.41m\n",
      "step 71670/71680 (99.99%) | loss: 2.393173 | grad norm: 0.1577 | lrm: 0.00 | dt: 1680.22ms | tok/sec: 312,035 | mfu: 47.64 | total time: 1961.44m\n",
      "step 71671/71680 (99.99%) | loss: 2.391595 | grad norm: 0.1541 | lrm: 0.00 | dt: 2504.23ms | tok/sec: 209,360 | mfu: 31.96 | total time: 1961.48m\n",
      "step 71672/71680 (99.99%) | loss: 2.388799 | grad norm: 0.1471 | lrm: 0.00 | dt: 1674.57ms | tok/sec: 313,087 | mfu: 47.80 | total time: 1961.51m\n",
      "step 71673/71680 (99.99%) | loss: 2.400435 | grad norm: 0.1521 | lrm: 0.00 | dt: 2529.72ms | tok/sec: 207,251 | mfu: 31.64 | total time: 1961.55m\n",
      "step 71674/71680 (99.99%) | loss: 2.389731 | grad norm: 0.1486 | lrm: 0.00 | dt: 2542.54ms | tok/sec: 206,206 | mfu: 31.48 | total time: 1961.60m\n",
      "step 71675/71680 (99.99%) | loss: 2.394191 | grad norm: 0.1513 | lrm: 0.00 | dt: 1509.01ms | tok/sec: 347,438 | mfu: 53.04 | total time: 1961.62m\n",
      "step 71676/71680 (99.99%) | loss: 2.359824 | grad norm: 0.1483 | lrm: 0.00 | dt: 1518.78ms | tok/sec: 345,203 | mfu: 52.70 | total time: 1961.65m\n",
      "step 71677/71680 (100.00%) | loss: 2.362267 | grad norm: 0.1668 | lrm: 0.00 | dt: 1506.43ms | tok/sec: 348,034 | mfu: 53.14 | total time: 1961.67m\n",
      "step 71678/71680 (100.00%) | loss: 2.368727 | grad norm: 0.1709 | lrm: 0.00 | dt: 1511.36ms | tok/sec: 346,897 | mfu: 52.96 | total time: 1961.70m\n",
      "step 71679/71680 (100.00%) | loss: 2.374543 | grad norm: 0.1510 | lrm: 0.00 | dt: 1509.54ms | tok/sec: 347,317 | mfu: 53.03 | total time: 1961.72m\n",
      "step 71680 | Validation bpb: 0.7233\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.5860 | centered: 0.4480 | time: 1.22s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.3100 | centered: 0.3100 | time: 1.18s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.6280 | centered: 0.6280 | time: 1.19s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.7140 | centered: 0.6187 | time: 1.38s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.4500 | centered: 0.2667 | time: 1.51s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.7100 | centered: 0.4200 | time: 0.23s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2560 | centered: 0.0700 | time: 1.68s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7560 | centered: 0.5120 | time: 1.20s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3880 | centered: 0.1840 | time: 1.14s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.5060 | centered: 0.5060 | time: 1.15s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.5940 | centered: 0.4587 | time: 2.90s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.7473 | centered: 0.4945 | time: 0.60s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.6000 | centered: 0.2000 | time: 1.11s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0860 | centered: 0.0860 | time: 1.21s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2478 | centered: 0.0598 | time: 1.27s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3720 | centered: 0.3720 | time: 1.20s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1762 | centered: 0.1762 | time: 0.50s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0312 | centered: 0.0312 | time: 0.08s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.4360 | centered: 0.4360 | time: 2.04s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.2860 | centered: 0.2860 | time: 1.27s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.6080 | centered: -0.0316 | time: 4.00s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2520 | centered: 0.1771 | time: 4.75s\n",
      "Step 71680: CORE metric: 0.3050\n",
      "<|bos|>The capital of France is Paris. It is the largest city in the country\n",
      "<|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will\n",
      "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
      "<|bos|>My favorite color is blue. I love the color blue. I love\n",
      "<|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x\n",
      "[W1224 02:02:25.615379342 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.648592094 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.848795559 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.850615391 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:27.822987331 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat2/base_checkpoints/d32/model_071680.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/base_checkpoints/d32/meta_071680.json\n",
      "saved optimizer to /home/ubuntu/mynanochat2/base_checkpoints/d32/optim_071680_rank0.pt\n",
      "Peak memory usage: 77017.79MiB\n",
      "Total training time: 1961.72m\n",
      "Minimum validation bpb: 0.7233\n",
      "wandb: updating run metadata\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:          core_metric ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:                 step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb: total_training_flops ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "wandb:  total_training_time ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:             train/dt ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:      train/grad_norm ‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ\n",
      "wandb:           train/loss ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:            train/lrm ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
      "wandb:            train/mfu ‚ñÜ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñà‚ñà\n",
      "wandb:    train/tok_per_sec ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñá‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÇ‚ñá‚ñá‚ñá‚ñà\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:          core_metric 0.30497\n",
      "wandb:                 step 71680\n",
      "wandb: total_training_flops 4.53962842438946e+20\n",
      "wandb:  total_training_time 117703.30327\n",
      "wandb:             train/dt 3.49672\n",
      "wandb:      train/grad_norm 0.15324\n",
      "wandb:           train/loss 2.40304\n",
      "wandb:            train/lrm 0.00558\n",
      "wandb:            train/mfu 22.89155\n",
      "wandb:    train/tok_per_sec 149937\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-4 at: https://wandb.ai/ericsilberstein-self/my-nanochat/runs/oo52nxvr\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251222_161000-oo52nxvr/logs\n",
      "[W1224 02:02:45.251346943 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:46.264167514 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:46.974340210 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -100 $RUN_OUTPUTS_DIR/base_train_output_004.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59509b-62d8-4fc3-a02d-57b948091c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16aa9f9-a350-44fe-98e4-a54f6bc10eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d32\n",
      "user_config: {'device_batch_size': 32, 'split_tokens': 10485760, 'device_type': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 71680\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "train bpb: 0.7271\n",
      "val bpb: 0.7235\n",
      "<|bos|>The capital of France is Paris. It is the largest city in the country and the second largest in Europe\n",
      "<|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic number 79 and the atomic\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will be Tuesday. If today is\n",
      "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune\n",
      "<|bos|>My favorite color is blue. I love the color blue. I love the color blue. I love\n",
      "<|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x + 3 = 13\n"
     ]
    }
   ],
   "source": [
    "!cat $RUN_OUTPUTS_DIR/base_loss_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cdb06-e817-4bd4-b4f4-1b89d19798a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b26837-d3e3-45ff-841d-15c40ab617f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 71680\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.5822 | centered: 0.4429 | time: 23.57s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.2976 | centered: 0.2976 | time: 5.03s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.6202 | centered: 0.6202 | time: 47.94s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.7226 | centered: 0.6302 | time: 6.66s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.4343 | centered: 0.2457 | time: 3.54s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.7100 | centered: 0.4200 | time: 0.28s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2375 | centered: 0.0469 | time: 4.10s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7394 | centered: 0.4788 | time: 4.66s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3880 | centered: 0.1840 | time: 1.17s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.4910 | centered: 0.4910 | time: 11.64s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.5888 | centered: 0.4518 | time: 55.79s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.7473 | centered: 0.4945 | time: 0.64s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5998 | centered: 0.1997 | time: 2.88s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0780 | centered: 0.0780 | time: 2.62s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2478 | centered: 0.0598 | time: 1.27s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3720 | centered: 0.3720 | time: 3.31s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1762 | centered: 0.1762 | time: 0.65s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0312 | centered: 0.0312 | time: 0.08s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.3996 | centered: 0.3996 | time: 41.08s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.2937 | centered: 0.2937 | time: 19.63s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.6174 | centered: -0.0068 | time: 17.23s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2524 | centered: 0.1776 | time: 94.39s\n",
      "================================================================================\n",
      "Model: base_model (step 71680)\n",
      "================================================================================\n",
      "Task                               , Accuracy  , Centered  \n",
      "hellaswag_zeroshot                 , 0.582155  , 0.442873  \n",
      "jeopardy                           , 0.297591  , 0.297591  \n",
      "bigbench_qa_wikidata               , 0.620196  , 0.620196  \n",
      "arc_easy                           , 0.722643  , 0.630191  \n",
      "arc_challenge                      , 0.434300  , 0.245734  \n",
      "copa                               , 0.710000  , 0.420000  \n",
      "commonsense_qa                     , 0.237510  , 0.046888  \n",
      "piqa                               , 0.739391  , 0.478781  \n",
      "openbook_qa                        , 0.388000  , 0.184000  \n",
      "lambada_openai                     , 0.490976  , 0.490976  \n",
      "hellaswag                          , 0.588827  , 0.451769  \n",
      "winograd                           , 0.747253  , 0.494506  \n",
      "winogrande                         , 0.599842  , 0.199684  \n",
      "bigbench_dyck_languages            , 0.078000  , 0.078000  \n",
      "agi_eval_lsat_ar                   , 0.247826  , 0.059783  \n",
      "bigbench_cs_algorithms             , 0.371970  , 0.371970  \n",
      "bigbench_operators                 , 0.176190  , 0.176190  \n",
      "bigbench_repeat_copy_logic         , 0.031250  , 0.031250  \n",
      "squad                              , 0.399622  , 0.399622  \n",
      "coqa                               , 0.293749  , 0.293749  \n",
      "boolq                              , 0.617431  , -0.006760 \n",
      "bigbench_language_identification   , 0.252400  , 0.177558  \n",
      "CORE                               ,           , 0.299298  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $RUN_OUTPUTS_DIR/base_eval_output.001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b12a6-59e2-413a-8295-8175d3482d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6532c0-15f3-40bb-aa86-1185832606c5",
   "metadata": {},
   "source": [
    "### also cat reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499bba88-77d9-4185-8473-42056b9496f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nanochat training report\n",
      "\n",
      "Generated: 2025-12-22 16:07:10\n",
      "\n",
      "## Environment\n",
      "\n",
      "### Git Information\n",
      "- Branch: master\n",
      "- Commit: bc88d72 (dirty)\n",
      "- Message: getting ready to work on gpu machine for challenge 38: train d32\n",
      "\n",
      "### Hardware\n",
      "- Platform: Linux\n",
      "- CPUs: 104 cores (208 logical)\n",
      "- Memory: 1771.7 GB\n",
      "- GPUs: 8x NVIDIA H100 80GB HBM3\n",
      "- GPU Memory: 633.5 GB total\n",
      "- CUDA Version: 12.8\n",
      "\n",
      "### Software\n",
      "- Python: 3.10.12\n",
      "- PyTorch: 2.9.0+cu128\n",
      "\n",
      "Run started: 2025-12-22 16:07:10\n",
      "\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/header.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c636572a-b52b-4a3d-810d-d1693b7a90cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model training\n",
      "timestamp: 2025-12-24 02:02:38\n",
      "\n",
      "- run: challenge-38-4\n",
      "- device_type: \n",
      "- depth: 32\n",
      "- max_seq_len: 2048\n",
      "- num_iterations: -1\n",
      "- target_param_data_ratio: 20\n",
      "- device_batch_size: 8\n",
      "- total_batch_size: 524,288\n",
      "- embedding_lr: 0.2000\n",
      "- unembedding_lr: 0.0040\n",
      "- weight_decay: 0.0000\n",
      "- matrix_lr: 0.0200\n",
      "- grad_clip: 1.0000\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- eval_every: 250\n",
      "- eval_tokens: 10,485,760\n",
      "- core_metric_every: 2000\n",
      "- core_metric_max_per_task: 500\n",
      "- sample_every: 2000\n",
      "- model_tag: \n",
      "- Number of parameters: 1,879,048,192\n",
      "- Number of FLOPs per token: 1.207960e+10\n",
      "- Calculated number of iterations: 71,680\n",
      "- Number of training tokens: 37,580,963,840\n",
      "- Tokens : Params ratio: 20.0000\n",
      "- DDP world size: 8\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- Minimum validation bpb: 0.7233\n",
      "- Final validation bpb: 0.7233\n",
      "- CORE metric estimate: 0.3050\n",
      "- MFU %: 53.03%\n",
      "- Total training flops: 4.539628e+20\n",
      "- Total training time: 1961.72m\n",
      "- Peak memory usage: 77017.79MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-training.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9e42b2-a825-468e-9054-693dbfb3929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model loss\n",
      "timestamp: 2025-12-24 02:05:23\n",
      "\n",
      "- train bpb: 0.7271\n",
      "- val bpb: 0.7235\n",
      "- sample 0: <|bos|>The capital of France is Paris. It is the largest city in the country and the second largest in Europe\n",
      "- sample 1: <|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic number 79 and the atomic\n",
      "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will be Tuesday. If today is\n",
      "- sample 3: <|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.\n",
      "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune\n",
      "- sample 5: <|bos|>My favorite color is blue. I love the color blue. I love the color blue. I love\n",
      "- sample 6: <|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x + 3 = 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-loss.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569d70e6-056b-436b-81af-438ebdd96fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model evaluation\n",
      "timestamp: 2025-12-24 02:12:40\n",
      "\n",
      "- Model: base_model (step 71680)\n",
      "- CORE metric: 0.2993\n",
      "- hellaswag_zeroshot: 0.4429\n",
      "- jeopardy: 0.2976\n",
      "- bigbench_qa_wikidata: 0.6202\n",
      "- arc_easy: 0.6302\n",
      "- arc_challenge: 0.2457\n",
      "- copa: 0.4200\n",
      "- commonsense_qa: 0.0469\n",
      "- piqa: 0.4788\n",
      "- openbook_qa: 0.1840\n",
      "- lambada_openai: 0.4910\n",
      "- hellaswag: 0.4518\n",
      "- winograd: 0.4945\n",
      "- winogrande: 0.1997\n",
      "- bigbench_dyck_languages: 0.0780\n",
      "- agi_eval_lsat_ar: 0.0598\n",
      "- bigbench_cs_algorithms: 0.3720\n",
      "- bigbench_operators: 0.1762\n",
      "- bigbench_repeat_copy_logic: 0.0312\n",
      "- squad: 0.3996\n",
      "- coqa: 0.2937\n",
      "- boolq: -0.0068\n",
      "- bigbench_language_identification: 0.1776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-evaluation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada5f8e-19d3-41c2-a483-69ac65845ffa",
   "metadata": {},
   "source": [
    "### mid train and chat eval run output and reports\n",
    "\n",
    "I ran these on the 8 GPU server and then copied the run output and reports to my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf97bc2-59de-4268-aaa4-3beff97ef2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d32\n",
      "overriding device_batch_size = 8\n",
      "overriding run = challenge-38-6\n",
      "user_config: {'run': 'challenge-38-6', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 8, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "wandb: Currently logged in as: ericsilberstein (ericsilberstein-self) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.23.0\n",
      "wandb: Run data is saved locally in /home/ubuntu/learn-nanochat/challenge-38-train-d32/wandb/run-20251229_160814-0c6xkdhi\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run challenge-38-6\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ericsilberstein-self/my-nanochat-mid\n",
      "wandb: üöÄ View run at https://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/0c6xkdhi\n",
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 71680\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Tokens / micro-batch / rank: 8 x 2048 = 16,384\n",
      "Tokens / micro-batch: 131,072\n",
      "Total batch size 524,288 => gradient accumulation steps: 4\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(2048/768) = 0.6123724356957946\n",
      "Muon: Grouping 128 params of shape torch.Size([2048, 2048]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([2048, 8192]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([8192, 2048]), device cuda:0, dtype torch.float32\n",
      "downloading https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt...\n",
      "downloaded to /home/ubuntu/mynanochat2/words_alpha.txt\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14042/14042 [00:00<00:00, 570276.22 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1531/1531 [00:00<00:00, 378513.38 examples/s]\n",
      "Generating dev split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [00:00<00:00, 147723.26 examples/s]\n",
      "Generating auxiliary_train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99842/99842 [00:00<00:00, 414937.00 examples/s]\n",
      "step 00000 | Validation bpb: 0.5837\n",
      "step 00001 (0.15%) | loss: 1.227957 | lrm: 1.00 | dt: 74315.59ms | tok/sec: 7,054 | mfu: 1.08 | total time: 0.00m\n",
      "step 00002 (0.27%) | loss: 1.571874 | lrm: 1.00 | dt: 1642.85ms | tok/sec: 319,133 | mfu: 48.72 | total time: 0.00m\n",
      "step 00003 (0.40%) | loss: 1.742333 | lrm: 1.00 | dt: 2472.44ms | tok/sec: 212,053 | mfu: 32.38 | total time: 0.00m\n",
      "step 00004 (0.54%) | loss: 1.826414 | lrm: 1.00 | dt: 1571.22ms | tok/sec: 333,682 | mfu: 50.94 | total time: 0.00m\n",
      "step 00005 (0.66%) | loss: 1.844665 | lrm: 1.00 | dt: 1499.38ms | tok/sec: 349,669 | mfu: 53.39 | total time: 0.00m\n",
      "step 00006 (0.77%) | loss: 1.802518 | lrm: 1.00 | dt: 1503.22ms | tok/sec: 348,776 | mfu: 53.25 | total time: 0.00m\n",
      "step 00007 (0.90%) | loss: 1.804781 | lrm: 1.00 | dt: 1497.91ms | tok/sec: 350,012 | mfu: 53.44 | total time: 0.00m\n",
      "step 00008 (1.02%) | loss: 1.784452 | lrm: 1.00 | dt: 1501.40ms | tok/sec: 349,198 | mfu: 53.31 | total time: 0.00m\n",
      "step 00009 (1.14%) | loss: 1.755583 | lrm: 1.00 | dt: 1501.90ms | tok/sec: 349,083 | mfu: 53.30 | total time: 0.00m\n",
      "step 00010 (1.27%) | loss: 1.708544 | lrm: 1.00 | dt: 1499.24ms | tok/sec: 349,702 | mfu: 53.39 | total time: 0.00m\n",
      "step 00011 (1.39%) | loss: 1.693039 | lrm: 1.00 | dt: 1505.91ms | tok/sec: 348,153 | mfu: 53.15 | total time: 0.03m\n",
      "step 00012 (1.51%) | loss: 1.672705 | lrm: 1.00 | dt: 1508.04ms | tok/sec: 347,662 | mfu: 53.08 | total time: 0.05m\n",
      "step 00013 (1.63%) | loss: 1.686611 | lrm: 1.00 | dt: 1499.22ms | tok/sec: 349,706 | mfu: 53.39 | total time: 0.08m\n",
      "step 00014 (1.76%) | loss: 1.659394 | lrm: 1.00 | dt: 1501.88ms | tok/sec: 349,087 | mfu: 53.30 | total time: 0.10m\n",
      "step 00015 (1.88%) | loss: 1.654635 | lrm: 1.00 | dt: 1509.28ms | tok/sec: 347,376 | mfu: 53.04 | total time: 0.13m\n",
      "step 00016 (2.01%) | loss: 1.660422 | lrm: 1.00 | dt: 1660.01ms | tok/sec: 315,833 | mfu: 48.22 | total time: 0.15m\n",
      "step 00017 (2.15%) | loss: 1.622710 | lrm: 1.00 | dt: 3514.45ms | tok/sec: 149,180 | mfu: 22.78 | total time: 0.21m\n",
      "step 00018 (2.28%) | loss: 1.592935 | lrm: 1.00 | dt: 1498.75ms | tok/sec: 349,815 | mfu: 53.41 | total time: 0.24m\n",
      "step 00019 (2.40%) | loss: 1.567465 | lrm: 1.00 | dt: 1503.82ms | tok/sec: 348,638 | mfu: 53.23 | total time: 0.26m\n",
      "step 00020 (2.53%) | loss: 1.571079 | lrm: 1.00 | dt: 1506.47ms | tok/sec: 348,023 | mfu: 53.13 | total time: 0.29m\n",
      "step 00021 (2.64%) | loss: 1.577389 | lrm: 1.00 | dt: 1506.21ms | tok/sec: 348,085 | mfu: 53.14 | total time: 0.31m\n",
      "step 00022 (2.77%) | loss: 1.546467 | lrm: 1.00 | dt: 1509.86ms | tok/sec: 347,243 | mfu: 53.02 | total time: 0.34m\n",
      "step 00023 (2.90%) | loss: 1.531822 | lrm: 1.00 | dt: 1512.62ms | tok/sec: 346,610 | mfu: 52.92 | total time: 0.36m\n",
      "step 00024 (3.02%) | loss: 1.517855 | lrm: 1.00 | dt: 1512.75ms | tok/sec: 346,579 | mfu: 52.91 | total time: 0.39m\n",
      "step 00025 (3.12%) | loss: 1.503354 | lrm: 1.00 | dt: 1504.45ms | tok/sec: 348,491 | mfu: 53.21 | total time: 0.41m\n",
      "step 00026 (3.23%) | loss: 1.510062 | lrm: 1.00 | dt: 1513.39ms | tok/sec: 346,433 | mfu: 52.89 | total time: 0.44m\n",
      "step 00027 (3.37%) | loss: 1.519291 | lrm: 1.00 | dt: 1510.12ms | tok/sec: 347,183 | mfu: 53.01 | total time: 0.46m\n",
      "step 00028 (3.50%) | loss: 1.501491 | lrm: 1.00 | dt: 1512.48ms | tok/sec: 346,641 | mfu: 52.92 | total time: 0.49m\n",
      "step 00029 (3.62%) | loss: 1.486764 | lrm: 1.00 | dt: 1514.84ms | tok/sec: 346,100 | mfu: 52.84 | total time: 0.51m\n",
      "step 00030 (3.73%) | loss: 1.470861 | lrm: 1.00 | dt: 1505.60ms | tok/sec: 348,226 | mfu: 53.17 | total time: 0.54m\n",
      "step 00031 (3.86%) | loss: 1.455530 | lrm: 1.00 | dt: 1518.18ms | tok/sec: 345,338 | mfu: 52.72 | total time: 0.56m\n",
      "step 00032 (3.97%) | loss: 1.442152 | lrm: 1.00 | dt: 1507.44ms | tok/sec: 347,800 | mfu: 53.10 | total time: 0.59m\n",
      "step 00033 (4.10%) | loss: 1.448828 | lrm: 1.00 | dt: 1509.85ms | tok/sec: 347,246 | mfu: 53.02 | total time: 0.61m\n",
      "step 00034 (4.24%) | loss: 1.450560 | lrm: 1.00 | dt: 1503.82ms | tok/sec: 348,638 | mfu: 53.23 | total time: 0.64m\n",
      "step 00035 (4.36%) | loss: 1.435360 | lrm: 1.00 | dt: 1513.07ms | tok/sec: 346,505 | mfu: 52.90 | total time: 0.66m\n",
      "step 00036 (4.46%) | loss: 1.434016 | lrm: 1.00 | dt: 1514.43ms | tok/sec: 346,193 | mfu: 52.85 | total time: 0.69m\n",
      "step 00037 (4.59%) | loss: 1.426741 | lrm: 1.00 | dt: 1510.17ms | tok/sec: 347,171 | mfu: 53.00 | total time: 0.71m\n",
      "step 00038 (4.72%) | loss: 1.422921 | lrm: 1.00 | dt: 1670.72ms | tok/sec: 313,810 | mfu: 47.91 | total time: 0.74m\n",
      "step 00039 (4.85%) | loss: 1.417117 | lrm: 1.00 | dt: 2536.73ms | tok/sec: 206,679 | mfu: 31.55 | total time: 0.78m\n",
      "step 00040 (4.98%) | loss: 1.415678 | lrm: 1.00 | dt: 2561.67ms | tok/sec: 204,666 | mfu: 31.25 | total time: 0.83m\n",
      "step 00041 (5.09%) | loss: 1.409181 | lrm: 1.00 | dt: 1506.20ms | tok/sec: 348,086 | mfu: 53.14 | total time: 0.85m\n",
      "step 00042 (5.19%) | loss: 1.406257 | lrm: 1.00 | dt: 1507.28ms | tok/sec: 347,837 | mfu: 53.11 | total time: 0.88m\n",
      "step 00043 (5.33%) | loss: 1.413718 | lrm: 1.00 | dt: 1511.52ms | tok/sec: 346,861 | mfu: 52.96 | total time: 0.90m\n",
      "step 00044 (5.46%) | loss: 1.411034 | lrm: 1.00 | dt: 1510.94ms | tok/sec: 346,994 | mfu: 52.98 | total time: 0.93m\n",
      "step 00045 (5.62%) | loss: 1.416560 | lrm: 1.00 | dt: 1512.41ms | tok/sec: 346,656 | mfu: 52.93 | total time: 0.95m\n",
      "step 00046 (5.74%) | loss: 1.424937 | lrm: 1.00 | dt: 1509.14ms | tok/sec: 347,408 | mfu: 53.04 | total time: 0.98m\n",
      "step 00047 (5.86%) | loss: 1.420997 | lrm: 1.00 | dt: 1512.39ms | tok/sec: 346,661 | mfu: 52.93 | total time: 1.00m\n",
      "step 00048 (5.97%) | loss: 1.417384 | lrm: 1.00 | dt: 1516.12ms | tok/sec: 345,808 | mfu: 52.80 | total time: 1.03m\n",
      "step 00049 (6.09%) | loss: 1.409968 | lrm: 1.00 | dt: 1512.23ms | tok/sec: 346,698 | mfu: 52.93 | total time: 1.05m\n",
      "step 00050 (6.22%) | loss: 1.402135 | lrm: 1.00 | dt: 1506.55ms | tok/sec: 348,006 | mfu: 53.13 | total time: 1.08m\n",
      "step 00051 (6.35%) | loss: 1.395253 | lrm: 1.00 | dt: 1505.32ms | tok/sec: 348,288 | mfu: 53.17 | total time: 1.10m\n",
      "step 00052 (6.47%) | loss: 1.399108 | lrm: 1.00 | dt: 1510.49ms | tok/sec: 347,097 | mfu: 52.99 | total time: 1.13m\n",
      "step 00053 (6.62%) | loss: 1.388669 | lrm: 1.00 | dt: 1506.94ms | tok/sec: 347,916 | mfu: 53.12 | total time: 1.15m\n",
      "step 00054 (6.74%) | loss: 1.387732 | lrm: 1.00 | dt: 1514.22ms | tok/sec: 346,243 | mfu: 52.86 | total time: 1.18m\n",
      "step 00055 (6.87%) | loss: 1.381875 | lrm: 1.00 | dt: 1509.86ms | tok/sec: 347,242 | mfu: 53.02 | total time: 1.21m\n",
      "step 00056 (7.00%) | loss: 1.379502 | lrm: 1.00 | dt: 1512.43ms | tok/sec: 346,652 | mfu: 52.93 | total time: 1.23m\n",
      "step 00057 (7.12%) | loss: 1.351436 | lrm: 1.00 | dt: 1510.30ms | tok/sec: 347,141 | mfu: 53.00 | total time: 1.26m\n",
      "step 00058 (7.26%) | loss: 1.343187 | lrm: 1.00 | dt: 1672.57ms | tok/sec: 313,462 | mfu: 47.86 | total time: 1.28m\n",
      "step 00059 (7.37%) | loss: 1.334205 | lrm: 1.00 | dt: 2591.83ms | tok/sec: 202,284 | mfu: 30.88 | total time: 1.33m\n",
      "step 00060 (7.48%) | loss: 1.332036 | lrm: 1.00 | dt: 3572.52ms | tok/sec: 146,755 | mfu: 22.41 | total time: 1.39m\n",
      "step 00061 (7.59%) | loss: 1.335856 | lrm: 1.00 | dt: 1503.33ms | tok/sec: 348,750 | mfu: 53.25 | total time: 1.41m\n",
      "step 00062 (7.70%) | loss: 1.342045 | lrm: 1.00 | dt: 1512.15ms | tok/sec: 346,717 | mfu: 52.93 | total time: 1.44m\n",
      "step 00063 (7.83%) | loss: 1.334510 | lrm: 1.00 | dt: 1502.67ms | tok/sec: 348,903 | mfu: 53.27 | total time: 1.46m\n",
      "step 00064 (7.95%) | loss: 1.330684 | lrm: 1.00 | dt: 1510.34ms | tok/sec: 347,132 | mfu: 53.00 | total time: 1.49m\n",
      "step 00065 (8.07%) | loss: 1.334273 | lrm: 1.00 | dt: 1507.95ms | tok/sec: 347,682 | mfu: 53.08 | total time: 1.51m\n",
      "step 00066 (8.20%) | loss: 1.339202 | lrm: 1.00 | dt: 1506.53ms | tok/sec: 348,009 | mfu: 53.13 | total time: 1.54m\n",
      "step 00067 (8.32%) | loss: 1.349359 | lrm: 1.00 | dt: 1508.00ms | tok/sec: 347,671 | mfu: 53.08 | total time: 1.56m\n",
      "step 00068 (8.45%) | loss: 1.344451 | lrm: 1.00 | dt: 1502.17ms | tok/sec: 349,020 | mfu: 53.29 | total time: 1.59m\n",
      "step 00069 (8.57%) | loss: 1.354764 | lrm: 1.00 | dt: 1510.41ms | tok/sec: 347,116 | mfu: 53.00 | total time: 1.61m\n",
      "step 00070 (8.70%) | loss: 1.371802 | lrm: 1.00 | dt: 1505.69ms | tok/sec: 348,203 | mfu: 53.16 | total time: 1.64m\n",
      "step 00071 (8.83%) | loss: 1.370454 | lrm: 1.00 | dt: 1510.35ms | tok/sec: 347,130 | mfu: 53.00 | total time: 1.66m\n",
      "step 00072 (8.95%) | loss: 1.362403 | lrm: 1.00 | dt: 1505.02ms | tok/sec: 348,359 | mfu: 53.19 | total time: 1.69m\n",
      "step 00073 (9.07%) | loss: 1.344238 | lrm: 1.00 | dt: 1502.74ms | tok/sec: 348,887 | mfu: 53.27 | total time: 1.71m\n",
      "step 00074 (9.22%) | loss: 1.343131 | lrm: 1.00 | dt: 1511.40ms | tok/sec: 346,887 | mfu: 52.96 | total time: 1.74m\n",
      "step 00075 (9.34%) | loss: 1.334426 | lrm: 1.00 | dt: 1502.15ms | tok/sec: 349,025 | mfu: 53.29 | total time: 1.76m\n",
      "step 00076 (9.46%) | loss: 1.322785 | lrm: 1.00 | dt: 1506.07ms | tok/sec: 348,115 | mfu: 53.15 | total time: 1.79m\n",
      "step 00077 (9.60%) | loss: 1.306205 | lrm: 1.00 | dt: 1507.99ms | tok/sec: 347,673 | mfu: 53.08 | total time: 1.81m\n",
      "step 00078 (9.71%) | loss: 1.297228 | lrm: 1.00 | dt: 1502.06ms | tok/sec: 349,046 | mfu: 53.29 | total time: 1.84m\n",
      "step 00079 (9.84%) | loss: 1.288944 | lrm: 1.00 | dt: 1512.15ms | tok/sec: 346,717 | mfu: 52.93 | total time: 1.86m\n",
      "step 00080 (9.96%) | loss: 1.296201 | lrm: 1.00 | dt: 1503.43ms | tok/sec: 348,727 | mfu: 53.24 | total time: 1.89m\n",
      "step 00081 (10.10%) | loss: 1.309955 | lrm: 1.00 | dt: 1507.82ms | tok/sec: 347,712 | mfu: 53.09 | total time: 1.91m\n",
      "step 00082 (10.22%) | loss: 1.338317 | lrm: 1.00 | dt: 1508.66ms | tok/sec: 347,518 | mfu: 53.06 | total time: 1.94m\n",
      "step 00083 (10.35%) | loss: 1.322515 | lrm: 1.00 | dt: 1507.01ms | tok/sec: 347,898 | mfu: 53.12 | total time: 1.96m\n",
      "step 00084 (10.49%) | loss: 1.345991 | lrm: 1.00 | dt: 1509.34ms | tok/sec: 347,362 | mfu: 53.03 | total time: 1.99m\n",
      "step 00085 (10.61%) | loss: 1.337015 | lrm: 1.00 | dt: 1506.83ms | tok/sec: 347,941 | mfu: 53.12 | total time: 2.01m\n",
      "step 00086 (10.74%) | loss: 1.316222 | lrm: 1.00 | dt: 1514.61ms | tok/sec: 346,154 | mfu: 52.85 | total time: 2.04m\n",
      "step 00087 (10.85%) | loss: 1.307601 | lrm: 1.00 | dt: 1503.70ms | tok/sec: 348,664 | mfu: 53.23 | total time: 2.06m\n",
      "step 00088 (10.97%) | loss: 1.292231 | lrm: 1.00 | dt: 1510.66ms | tok/sec: 347,059 | mfu: 52.99 | total time: 2.09m\n",
      "step 00089 (11.09%) | loss: 1.319540 | lrm: 1.00 | dt: 1508.41ms | tok/sec: 347,577 | mfu: 53.07 | total time: 2.11m\n",
      "step 00090 (11.19%) | loss: 1.297647 | lrm: 1.00 | dt: 1503.51ms | tok/sec: 348,709 | mfu: 53.24 | total time: 2.14m\n",
      "step 00091 (11.32%) | loss: 1.289457 | lrm: 1.00 | dt: 1507.17ms | tok/sec: 347,863 | mfu: 53.11 | total time: 2.16m\n",
      "step 00092 (11.43%) | loss: 1.278794 | lrm: 1.00 | dt: 1506.71ms | tok/sec: 347,969 | mfu: 53.13 | total time: 2.19m\n",
      "step 00093 (11.56%) | loss: 1.283066 | lrm: 1.00 | dt: 1511.33ms | tok/sec: 346,905 | mfu: 52.96 | total time: 2.22m\n",
      "step 00094 (11.67%) | loss: 1.276401 | lrm: 1.00 | dt: 1505.44ms | tok/sec: 348,262 | mfu: 53.17 | total time: 2.24m\n",
      "step 00095 (11.80%) | loss: 1.264162 | lrm: 1.00 | dt: 1511.05ms | tok/sec: 346,968 | mfu: 52.97 | total time: 2.27m\n",
      "step 00096 (11.92%) | loss: 1.249421 | lrm: 1.00 | dt: 1501.65ms | tok/sec: 349,141 | mfu: 53.30 | total time: 2.29m\n",
      "step 00097 (12.06%) | loss: 1.258417 | lrm: 1.00 | dt: 1504.79ms | tok/sec: 348,413 | mfu: 53.19 | total time: 2.32m\n",
      "step 00098 (12.16%) | loss: 1.251572 | lrm: 1.00 | dt: 1511.86ms | tok/sec: 346,782 | mfu: 52.94 | total time: 2.34m\n",
      "step 00099 (12.29%) | loss: 1.237018 | lrm: 1.00 | dt: 1500.59ms | tok/sec: 349,387 | mfu: 53.34 | total time: 2.37m\n",
      "step 00100 (12.40%) | loss: 1.215121 | lrm: 1.00 | dt: 1502.75ms | tok/sec: 348,885 | mfu: 53.27 | total time: 2.39m\n",
      "step 00101 (12.52%) | loss: 1.216301 | lrm: 1.00 | dt: 1511.00ms | tok/sec: 346,980 | mfu: 52.98 | total time: 2.42m\n",
      "step 00102 (12.65%) | loss: 1.221688 | lrm: 1.00 | dt: 1505.25ms | tok/sec: 348,306 | mfu: 53.18 | total time: 2.44m\n",
      "step 00103 (12.79%) | loss: 1.243895 | lrm: 1.00 | dt: 1503.57ms | tok/sec: 348,696 | mfu: 53.24 | total time: 2.47m\n",
      "step 00104 (12.89%) | loss: 1.234630 | lrm: 1.00 | dt: 1508.84ms | tok/sec: 347,477 | mfu: 53.05 | total time: 2.49m\n",
      "step 00105 (13.02%) | loss: 1.237574 | lrm: 1.00 | dt: 1498.97ms | tok/sec: 349,765 | mfu: 53.40 | total time: 2.52m\n",
      "step 00106 (13.12%) | loss: 1.246958 | lrm: 1.00 | dt: 1508.56ms | tok/sec: 347,541 | mfu: 53.06 | total time: 2.54m\n",
      "step 00107 (13.23%) | loss: 1.268686 | lrm: 1.00 | dt: 1507.23ms | tok/sec: 347,848 | mfu: 53.11 | total time: 2.57m\n",
      "step 00108 (13.36%) | loss: 1.271024 | lrm: 1.00 | dt: 1504.42ms | tok/sec: 348,498 | mfu: 53.21 | total time: 2.59m\n",
      "step 00109 (13.48%) | loss: 1.275421 | lrm: 1.00 | dt: 1503.74ms | tok/sec: 348,655 | mfu: 53.23 | total time: 2.62m\n",
      "step 00110 (13.60%) | loss: 1.285808 | lrm: 1.00 | dt: 1668.42ms | tok/sec: 314,241 | mfu: 47.98 | total time: 2.64m\n",
      "step 00111 (13.72%) | loss: 1.276346 | lrm: 1.00 | dt: 2503.01ms | tok/sec: 209,462 | mfu: 31.98 | total time: 2.69m\n",
      "step 00112 (13.83%) | loss: 1.261252 | lrm: 1.00 | dt: 2699.11ms | tok/sec: 194,244 | mfu: 29.66 | total time: 2.73m\n",
      "step 00113 (13.97%) | loss: 1.264290 | lrm: 1.00 | dt: 1495.58ms | tok/sec: 350,557 | mfu: 53.52 | total time: 2.76m\n",
      "step 00114 (14.09%) | loss: 1.302058 | lrm: 1.00 | dt: 1665.26ms | tok/sec: 314,838 | mfu: 48.07 | total time: 2.78m\n",
      "step 00115 (14.22%) | loss: 1.287844 | lrm: 1.00 | dt: 2405.62ms | tok/sec: 217,942 | mfu: 33.27 | total time: 2.82m\n",
      "step 00116 (14.36%) | loss: 1.305749 | lrm: 1.00 | dt: 1498.93ms | tok/sec: 349,774 | mfu: 53.40 | total time: 2.85m\n",
      "step 00117 (14.49%) | loss: 1.322897 | lrm: 1.00 | dt: 1500.48ms | tok/sec: 349,414 | mfu: 53.35 | total time: 2.87m\n",
      "step 00118 (14.61%) | loss: 1.314826 | lrm: 1.00 | dt: 1502.65ms | tok/sec: 348,907 | mfu: 53.27 | total time: 2.90m\n",
      "step 00119 (14.73%) | loss: 1.294990 | lrm: 1.00 | dt: 1500.20ms | tok/sec: 349,477 | mfu: 53.36 | total time: 2.92m\n",
      "step 00120 (14.85%) | loss: 1.285979 | lrm: 1.00 | dt: 1511.97ms | tok/sec: 346,758 | mfu: 52.94 | total time: 2.95m\n",
      "step 00121 (14.98%) | loss: 1.287015 | lrm: 1.00 | dt: 1505.95ms | tok/sec: 348,144 | mfu: 53.15 | total time: 2.97m\n",
      "step 00122 (15.09%) | loss: 1.291227 | lrm: 1.00 | dt: 1498.54ms | tok/sec: 349,865 | mfu: 53.42 | total time: 3.00m\n",
      "step 00123 (15.23%) | loss: 1.294548 | lrm: 1.00 | dt: 1671.57ms | tok/sec: 313,649 | mfu: 47.89 | total time: 3.03m\n",
      "step 00124 (15.37%) | loss: 1.290960 | lrm: 1.00 | dt: 2526.35ms | tok/sec: 207,528 | mfu: 31.68 | total time: 3.07m\n",
      "step 00125 (15.50%) | loss: 1.285588 | lrm: 1.00 | dt: 1504.80ms | tok/sec: 348,409 | mfu: 53.19 | total time: 3.09m\n",
      "step 00126 (15.60%) | loss: 1.277111 | lrm: 1.00 | dt: 1506.31ms | tok/sec: 348,060 | mfu: 53.14 | total time: 3.12m\n",
      "step 00127 (15.72%) | loss: 1.269083 | lrm: 1.00 | dt: 1502.41ms | tok/sec: 348,965 | mfu: 53.28 | total time: 3.14m\n",
      "step 00128 (15.84%) | loss: 1.267929 | lrm: 1.00 | dt: 1511.81ms | tok/sec: 346,795 | mfu: 52.95 | total time: 3.17m\n",
      "step 00129 (15.98%) | loss: 1.260717 | lrm: 1.00 | dt: 1500.56ms | tok/sec: 349,394 | mfu: 53.34 | total time: 3.19m\n",
      "step 00130 (16.12%) | loss: 1.253123 | lrm: 1.00 | dt: 1502.19ms | tok/sec: 349,015 | mfu: 53.29 | total time: 3.22m\n",
      "step 00131 (16.24%) | loss: 1.257487 | lrm: 1.00 | dt: 1509.98ms | tok/sec: 347,215 | mfu: 53.01 | total time: 3.24m\n",
      "step 00132 (16.37%) | loss: 1.239420 | lrm: 1.00 | dt: 1505.76ms | tok/sec: 348,188 | mfu: 53.16 | total time: 3.27m\n",
      "step 00133 (16.48%) | loss: 1.244229 | lrm: 1.00 | dt: 1507.08ms | tok/sec: 347,882 | mfu: 53.11 | total time: 3.30m\n",
      "step 00134 (16.60%) | loss: 1.253939 | lrm: 1.00 | dt: 1506.39ms | tok/sec: 348,042 | mfu: 53.14 | total time: 3.32m\n",
      "step 00135 (16.71%) | loss: 1.244026 | lrm: 1.00 | dt: 1501.26ms | tok/sec: 349,231 | mfu: 53.32 | total time: 3.35m\n",
      "step 00136 (16.84%) | loss: 1.236534 | lrm: 1.00 | dt: 1509.88ms | tok/sec: 347,238 | mfu: 53.01 | total time: 3.37m\n",
      "step 00137 (16.97%) | loss: 1.241368 | lrm: 1.00 | dt: 1503.45ms | tok/sec: 348,722 | mfu: 53.24 | total time: 3.40m\n",
      "step 00138 (17.11%) | loss: 1.237959 | lrm: 1.00 | dt: 1506.02ms | tok/sec: 348,128 | mfu: 53.15 | total time: 3.42m\n",
      "step 00139 (17.22%) | loss: 1.242344 | lrm: 1.00 | dt: 1646.07ms | tok/sec: 318,508 | mfu: 48.63 | total time: 3.45m\n",
      "step 00140 (17.36%) | loss: 1.246978 | lrm: 1.00 | dt: 2531.55ms | tok/sec: 207,101 | mfu: 31.62 | total time: 3.49m\n",
      "step 00141 (17.47%) | loss: 1.240150 | lrm: 1.00 | dt: 1502.92ms | tok/sec: 348,846 | mfu: 53.26 | total time: 3.52m\n",
      "step 00142 (17.58%) | loss: 1.230074 | lrm: 1.00 | dt: 1507.77ms | tok/sec: 347,724 | mfu: 53.09 | total time: 3.54m\n",
      "step 00143 (17.70%) | loss: 1.221928 | lrm: 1.00 | dt: 1661.06ms | tok/sec: 315,634 | mfu: 48.19 | total time: 3.57m\n",
      "step 00144 (17.81%) | loss: 1.221396 | lrm: 1.00 | dt: 2537.76ms | tok/sec: 206,594 | mfu: 31.54 | total time: 3.61m\n",
      "step 00145 (17.94%) | loss: 1.221667 | lrm: 1.00 | dt: 2527.05ms | tok/sec: 207,470 | mfu: 31.68 | total time: 3.65m\n",
      "step 00146 (18.06%) | loss: 1.227730 | lrm: 1.00 | dt: 1505.56ms | tok/sec: 348,235 | mfu: 53.17 | total time: 3.68m\n",
      "step 00147 (18.19%) | loss: 1.231425 | lrm: 1.00 | dt: 1651.93ms | tok/sec: 317,378 | mfu: 48.46 | total time: 3.70m\n",
      "step 00148 (18.32%) | loss: 1.245892 | lrm: 1.00 | dt: 2378.76ms | tok/sec: 220,403 | mfu: 33.65 | total time: 3.74m\n",
      "step 00149 (18.42%) | loss: 1.236653 | lrm: 1.00 | dt: 1499.58ms | tok/sec: 349,622 | mfu: 53.38 | total time: 3.77m\n",
      "step 00150 (18.53%) | loss: 1.235428 | lrm: 1.00 | dt: 1501.62ms | tok/sec: 349,148 | mfu: 53.31 | total time: 3.79m\n",
      "step 00150 | Validation bpb: 0.3863\n",
      "step 00151 (18.66%) | loss: 1.224125 | lrm: 1.00 | dt: 1503.98ms | tok/sec: 348,601 | mfu: 53.22 | total time: 3.82m\n",
      "step 00152 (18.79%) | loss: 1.217668 | lrm: 1.00 | dt: 1498.76ms | tok/sec: 349,813 | mfu: 53.41 | total time: 3.84m\n",
      "step 00153 (18.91%) | loss: 1.222950 | lrm: 1.00 | dt: 1504.51ms | tok/sec: 348,476 | mfu: 53.20 | total time: 3.87m\n",
      "step 00154 (19.04%) | loss: 1.228336 | lrm: 1.00 | dt: 1654.86ms | tok/sec: 316,816 | mfu: 48.37 | total time: 3.90m\n",
      "step 00155 (19.15%) | loss: 1.222140 | lrm: 1.00 | dt: 2399.79ms | tok/sec: 218,472 | mfu: 33.36 | total time: 3.94m\n",
      "step 00156 (19.27%) | loss: 1.233340 | lrm: 1.00 | dt: 1502.09ms | tok/sec: 349,039 | mfu: 53.29 | total time: 3.96m\n",
      "step 00157 (19.42%) | loss: 1.228940 | lrm: 1.00 | dt: 1652.85ms | tok/sec: 317,201 | mfu: 48.43 | total time: 3.99m\n",
      "step 00158 (19.55%) | loss: 1.231252 | lrm: 1.00 | dt: 2520.09ms | tok/sec: 208,043 | mfu: 31.76 | total time: 4.03m\n",
      "step 00159 (19.68%) | loss: 1.231728 | lrm: 1.00 | dt: 2543.35ms | tok/sec: 206,140 | mfu: 31.47 | total time: 4.07m\n",
      "step 00160 (19.81%) | loss: 1.238489 | lrm: 1.00 | dt: 1496.37ms | tok/sec: 350,372 | mfu: 53.49 | total time: 4.10m\n",
      "step 00161 (19.94%) | loss: 1.240572 | lrm: 1.00 | dt: 1498.69ms | tok/sec: 349,830 | mfu: 53.41 | total time: 4.12m\n",
      "step 00162 (20.08%) | loss: 1.231103 | lrm: 1.00 | dt: 1498.81ms | tok/sec: 349,803 | mfu: 53.41 | total time: 4.15m\n",
      "step 00163 (20.21%) | loss: 1.238718 | lrm: 1.00 | dt: 1497.92ms | tok/sec: 350,010 | mfu: 53.44 | total time: 4.17m\n",
      "step 00164 (20.34%) | loss: 1.229992 | lrm: 1.00 | dt: 1509.17ms | tok/sec: 347,402 | mfu: 53.04 | total time: 4.20m\n",
      "step 00165 (20.46%) | loss: 1.218704 | lrm: 1.00 | dt: 1653.66ms | tok/sec: 317,046 | mfu: 48.40 | total time: 4.23m\n",
      "step 00166 (20.59%) | loss: 1.219815 | lrm: 1.00 | dt: 2375.57ms | tok/sec: 220,699 | mfu: 33.70 | total time: 4.27m\n",
      "step 00167 (20.71%) | loss: 1.208834 | lrm: 1.00 | dt: 1497.87ms | tok/sec: 350,021 | mfu: 53.44 | total time: 4.29m\n",
      "step 00168 (20.83%) | loss: 1.230440 | lrm: 1.00 | dt: 1499.75ms | tok/sec: 349,582 | mfu: 53.37 | total time: 4.32m\n",
      "step 00169 (20.96%) | loss: 1.230049 | lrm: 1.00 | dt: 1499.42ms | tok/sec: 349,659 | mfu: 53.38 | total time: 4.34m\n",
      "step 00170 (21.09%) | loss: 1.224314 | lrm: 1.00 | dt: 1511.20ms | tok/sec: 346,935 | mfu: 52.97 | total time: 4.37m\n",
      "step 00171 (21.20%) | loss: 1.217239 | lrm: 1.00 | dt: 1503.36ms | tok/sec: 348,743 | mfu: 53.24 | total time: 4.39m\n",
      "step 00172 (21.32%) | loss: 1.225173 | lrm: 1.00 | dt: 1502.31ms | tok/sec: 348,989 | mfu: 53.28 | total time: 4.42m\n",
      "step 00173 (21.44%) | loss: 1.212549 | lrm: 1.00 | dt: 1510.06ms | tok/sec: 347,196 | mfu: 53.01 | total time: 4.44m\n",
      "step 00174 (21.57%) | loss: 1.213559 | lrm: 1.00 | dt: 1502.73ms | tok/sec: 348,890 | mfu: 53.27 | total time: 4.47m\n",
      "step 00175 (21.70%) | loss: 1.210333 | lrm: 1.00 | dt: 1507.64ms | tok/sec: 347,754 | mfu: 53.09 | total time: 4.49m\n",
      "step 00176 (21.81%) | loss: 1.200639 | lrm: 1.00 | dt: 1509.02ms | tok/sec: 347,435 | mfu: 53.04 | total time: 4.52m\n",
      "step 00177 (21.93%) | loss: 1.208253 | lrm: 1.00 | dt: 1498.48ms | tok/sec: 349,880 | mfu: 53.42 | total time: 4.54m\n",
      "step 00178 (22.07%) | loss: 1.200111 | lrm: 1.00 | dt: 1509.22ms | tok/sec: 347,390 | mfu: 53.04 | total time: 4.57m\n",
      "step 00179 (22.18%) | loss: 1.207627 | lrm: 1.00 | dt: 1503.60ms | tok/sec: 348,689 | mfu: 53.24 | total time: 4.59m\n",
      "step 00180 (22.30%) | loss: 1.207603 | lrm: 1.00 | dt: 1507.27ms | tok/sec: 347,838 | mfu: 53.11 | total time: 4.62m\n",
      "step 00181 (22.44%) | loss: 1.209281 | lrm: 1.00 | dt: 1511.13ms | tok/sec: 346,951 | mfu: 52.97 | total time: 4.64m\n",
      "step 00182 (22.57%) | loss: 1.201296 | lrm: 1.00 | dt: 1502.25ms | tok/sec: 349,000 | mfu: 53.28 | total time: 4.67m\n",
      "step 00183 (22.71%) | loss: 1.206608 | lrm: 1.00 | dt: 1506.86ms | tok/sec: 347,934 | mfu: 53.12 | total time: 4.69m\n",
      "step 00184 (22.83%) | loss: 1.210615 | lrm: 1.00 | dt: 1502.23ms | tok/sec: 349,006 | mfu: 53.28 | total time: 4.72m\n",
      "step 00185 (22.95%) | loss: 1.199391 | lrm: 1.00 | dt: 1507.58ms | tok/sec: 347,767 | mfu: 53.10 | total time: 4.74m\n",
      "step 00186 (23.09%) | loss: 1.189848 | lrm: 1.00 | dt: 1505.26ms | tok/sec: 348,302 | mfu: 53.18 | total time: 4.77m\n",
      "step 00187 (23.21%) | loss: 1.179359 | lrm: 1.00 | dt: 1642.72ms | tok/sec: 319,157 | mfu: 48.73 | total time: 4.80m\n",
      "step 00188 (23.33%) | loss: 1.198958 | lrm: 1.00 | dt: 2490.68ms | tok/sec: 210,499 | mfu: 32.14 | total time: 4.84m\n",
      "step 00189 (23.49%) | loss: 1.209385 | lrm: 1.00 | dt: 1493.20ms | tok/sec: 351,116 | mfu: 53.61 | total time: 4.86m\n",
      "step 00190 (23.60%) | loss: 1.211561 | lrm: 1.00 | dt: 1503.17ms | tok/sec: 348,788 | mfu: 53.25 | total time: 4.89m\n",
      "step 00191 (23.72%) | loss: 1.211913 | lrm: 1.00 | dt: 1496.46ms | tok/sec: 350,353 | mfu: 53.49 | total time: 4.91m\n",
      "step 00192 (23.85%) | loss: 1.221706 | lrm: 1.00 | dt: 1510.09ms | tok/sec: 347,190 | mfu: 53.01 | total time: 4.94m\n",
      "step 00193 (23.98%) | loss: 1.214902 | lrm: 1.00 | dt: 1504.03ms | tok/sec: 348,588 | mfu: 53.22 | total time: 4.96m\n",
      "step 00194 (24.08%) | loss: 1.211834 | lrm: 1.00 | dt: 1505.95ms | tok/sec: 348,143 | mfu: 53.15 | total time: 4.99m\n",
      "step 00195 (24.23%) | loss: 1.199925 | lrm: 1.00 | dt: 1512.10ms | tok/sec: 346,728 | mfu: 52.94 | total time: 5.01m\n",
      "step 00196 (24.35%) | loss: 1.206748 | lrm: 1.00 | dt: 1644.41ms | tok/sec: 318,830 | mfu: 48.68 | total time: 5.04m\n",
      "step 00197 (24.47%) | loss: 1.208305 | lrm: 1.00 | dt: 2559.08ms | tok/sec: 204,873 | mfu: 31.28 | total time: 5.08m\n",
      "step 00198 (24.61%) | loss: 1.205431 | lrm: 1.00 | dt: 1501.57ms | tok/sec: 349,160 | mfu: 53.31 | total time: 5.11m\n",
      "step 00199 (24.71%) | loss: 1.242962 | lrm: 1.00 | dt: 1511.01ms | tok/sec: 346,979 | mfu: 52.97 | total time: 5.13m\n",
      "step 00200 (24.84%) | loss: 1.233241 | lrm: 1.00 | dt: 1496.27ms | tok/sec: 350,397 | mfu: 53.50 | total time: 5.16m\n",
      "step 00201 (24.99%) | loss: 1.244277 | lrm: 1.00 | dt: 1508.85ms | tok/sec: 347,475 | mfu: 53.05 | total time: 5.18m\n",
      "step 00202 (25.12%) | loss: 1.225162 | lrm: 1.00 | dt: 1645.42ms | tok/sec: 318,633 | mfu: 48.65 | total time: 5.21m\n",
      "step 00203 (25.25%) | loss: 1.209860 | lrm: 1.00 | dt: 2387.64ms | tok/sec: 219,584 | mfu: 33.52 | total time: 5.25m\n",
      "step 00204 (25.37%) | loss: 1.219933 | lrm: 1.00 | dt: 1500.17ms | tok/sec: 349,484 | mfu: 53.36 | total time: 5.27m\n",
      "step 00205 (25.49%) | loss: 1.217233 | lrm: 1.00 | dt: 1512.50ms | tok/sec: 346,636 | mfu: 52.92 | total time: 5.30m\n",
      "step 00206 (25.61%) | loss: 1.203400 | lrm: 1.00 | dt: 1507.20ms | tok/sec: 347,856 | mfu: 53.11 | total time: 5.33m\n",
      "step 00207 (25.73%) | loss: 1.213895 | lrm: 1.00 | dt: 1496.14ms | tok/sec: 350,426 | mfu: 53.50 | total time: 5.35m\n",
      "step 00208 (25.84%) | loss: 1.201027 | lrm: 1.00 | dt: 1509.19ms | tok/sec: 347,396 | mfu: 53.04 | total time: 5.38m\n",
      "step 00209 (25.96%) | loss: 1.197564 | lrm: 1.00 | dt: 1502.42ms | tok/sec: 348,962 | mfu: 53.28 | total time: 5.40m\n",
      "step 00210 (26.08%) | loss: 1.189404 | lrm: 1.00 | dt: 1510.89ms | tok/sec: 347,006 | mfu: 52.98 | total time: 5.43m\n",
      "step 00211 (26.20%) | loss: 1.185887 | lrm: 1.00 | dt: 1504.83ms | tok/sec: 348,403 | mfu: 53.19 | total time: 5.45m\n",
      "step 00212 (26.32%) | loss: 1.180829 | lrm: 1.00 | dt: 1501.88ms | tok/sec: 349,088 | mfu: 53.30 | total time: 5.48m\n",
      "step 00213 (26.46%) | loss: 1.180778 | lrm: 1.00 | dt: 1507.35ms | tok/sec: 347,821 | mfu: 53.10 | total time: 5.50m\n",
      "step 00214 (26.59%) | loss: 1.170031 | lrm: 1.00 | dt: 1504.23ms | tok/sec: 348,543 | mfu: 53.21 | total time: 5.53m\n",
      "step 00215 (26.71%) | loss: 1.169559 | lrm: 1.00 | dt: 1503.25ms | tok/sec: 348,768 | mfu: 53.25 | total time: 5.55m\n",
      "step 00216 (26.83%) | loss: 1.166087 | lrm: 1.00 | dt: 1674.24ms | tok/sec: 313,149 | mfu: 47.81 | total time: 5.58m\n",
      "step 00217 (26.95%) | loss: 1.178027 | lrm: 1.00 | dt: 2476.35ms | tok/sec: 211,717 | mfu: 32.32 | total time: 5.62m\n",
      "step 00218 (27.09%) | loss: 1.177422 | lrm: 1.00 | dt: 1498.76ms | tok/sec: 349,815 | mfu: 53.41 | total time: 5.65m\n",
      "step 00219 (27.23%) | loss: 1.182664 | lrm: 1.00 | dt: 1506.50ms | tok/sec: 348,016 | mfu: 53.13 | total time: 5.67m\n",
      "step 00220 (27.34%) | loss: 1.186435 | lrm: 1.00 | dt: 1506.18ms | tok/sec: 348,091 | mfu: 53.14 | total time: 5.70m\n",
      "step 00221 (27.47%) | loss: 1.193461 | lrm: 1.00 | dt: 1501.99ms | tok/sec: 349,062 | mfu: 53.29 | total time: 5.72m\n",
      "step 00222 (27.59%) | loss: 1.197167 | lrm: 1.00 | dt: 1648.14ms | tok/sec: 318,108 | mfu: 48.57 | total time: 5.75m\n",
      "step 00223 (27.71%) | loss: 1.199161 | lrm: 1.00 | dt: 2404.35ms | tok/sec: 218,057 | mfu: 33.29 | total time: 5.79m\n",
      "step 00224 (27.84%) | loss: 1.198258 | lrm: 1.00 | dt: 1645.76ms | tok/sec: 318,569 | mfu: 48.64 | total time: 5.82m\n",
      "step 00225 (27.95%) | loss: 1.201246 | lrm: 1.00 | dt: 2520.57ms | tok/sec: 208,003 | mfu: 31.76 | total time: 5.86m\n",
      "step 00226 (28.09%) | loss: 1.179326 | lrm: 1.00 | dt: 1492.82ms | tok/sec: 351,207 | mfu: 53.62 | total time: 5.88m\n",
      "step 00227 (28.23%) | loss: 1.170926 | lrm: 1.00 | dt: 1495.80ms | tok/sec: 350,506 | mfu: 53.51 | total time: 5.91m\n",
      "step 00228 (28.35%) | loss: 1.189426 | lrm: 1.00 | dt: 1498.65ms | tok/sec: 349,839 | mfu: 53.41 | total time: 5.93m\n",
      "step 00229 (28.49%) | loss: 1.207817 | lrm: 1.00 | dt: 1505.87ms | tok/sec: 348,162 | mfu: 53.16 | total time: 5.96m\n",
      "step 00230 (28.61%) | loss: 1.196689 | lrm: 1.00 | dt: 1494.45ms | tok/sec: 350,822 | mfu: 53.56 | total time: 5.98m\n",
      "step 00231 (28.73%) | loss: 1.190872 | lrm: 1.00 | dt: 1500.01ms | tok/sec: 349,524 | mfu: 53.36 | total time: 6.01m\n",
      "step 00232 (28.86%) | loss: 1.208978 | lrm: 1.00 | dt: 1511.79ms | tok/sec: 346,799 | mfu: 52.95 | total time: 6.03m\n",
      "step 00233 (28.98%) | loss: 1.207238 | lrm: 1.00 | dt: 1504.56ms | tok/sec: 348,466 | mfu: 53.20 | total time: 6.06m\n",
      "step 00234 (29.10%) | loss: 1.191028 | lrm: 1.00 | dt: 1501.61ms | tok/sec: 349,150 | mfu: 53.31 | total time: 6.08m\n",
      "step 00235 (29.23%) | loss: 1.219949 | lrm: 1.00 | dt: 1498.03ms | tok/sec: 349,984 | mfu: 53.43 | total time: 6.11m\n",
      "step 00236 (29.36%) | loss: 1.196169 | lrm: 1.00 | dt: 1500.12ms | tok/sec: 349,498 | mfu: 53.36 | total time: 6.13m\n",
      "step 00237 (29.49%) | loss: 1.190410 | lrm: 1.00 | dt: 1654.28ms | tok/sec: 316,928 | mfu: 48.39 | total time: 6.16m\n",
      "step 00238 (29.60%) | loss: 1.193005 | lrm: 1.00 | dt: 2372.27ms | tok/sec: 221,006 | mfu: 33.74 | total time: 6.20m\n",
      "step 00239 (29.71%) | loss: 1.176949 | lrm: 1.00 | dt: 1493.67ms | tok/sec: 351,006 | mfu: 53.59 | total time: 6.22m\n",
      "step 00240 (29.83%) | loss: 1.175991 | lrm: 1.00 | dt: 1502.22ms | tok/sec: 349,009 | mfu: 53.28 | total time: 6.25m\n",
      "step 00241 (29.94%) | loss: 1.181170 | lrm: 1.00 | dt: 1502.13ms | tok/sec: 349,028 | mfu: 53.29 | total time: 6.27m\n",
      "step 00242 (30.08%) | loss: 1.179951 | lrm: 1.00 | dt: 1503.72ms | tok/sec: 348,660 | mfu: 53.23 | total time: 6.30m\n",
      "step 00243 (30.20%) | loss: 1.198895 | lrm: 1.00 | dt: 1500.54ms | tok/sec: 349,399 | mfu: 53.34 | total time: 6.32m\n",
      "step 00244 (30.31%) | loss: 1.187842 | lrm: 1.00 | dt: 1496.44ms | tok/sec: 350,357 | mfu: 53.49 | total time: 6.35m\n",
      "step 00245 (30.44%) | loss: 1.186735 | lrm: 1.00 | dt: 1507.29ms | tok/sec: 347,835 | mfu: 53.11 | total time: 6.37m\n",
      "step 00246 (30.55%) | loss: 1.165177 | lrm: 1.00 | dt: 1502.66ms | tok/sec: 348,905 | mfu: 53.27 | total time: 6.40m\n",
      "step 00247 (30.67%) | loss: 1.177114 | lrm: 1.00 | dt: 1507.00ms | tok/sec: 347,901 | mfu: 53.12 | total time: 6.42m\n",
      "step 00248 (30.79%) | loss: 1.180198 | lrm: 1.00 | dt: 1497.79ms | tok/sec: 350,040 | mfu: 53.44 | total time: 6.45m\n",
      "step 00249 (30.92%) | loss: 1.177094 | lrm: 1.00 | dt: 1503.02ms | tok/sec: 348,822 | mfu: 53.26 | total time: 6.47m\n",
      "step 00250 (31.04%) | loss: 1.173679 | lrm: 1.00 | dt: 1502.43ms | tok/sec: 348,960 | mfu: 53.28 | total time: 6.50m\n",
      "step 00251 (31.17%) | loss: 1.165480 | lrm: 1.00 | dt: 1508.59ms | tok/sec: 347,535 | mfu: 53.06 | total time: 6.52m\n",
      "step 00252 (31.30%) | loss: 1.181511 | lrm: 1.00 | dt: 1505.40ms | tok/sec: 348,271 | mfu: 53.17 | total time: 6.55m\n",
      "step 00253 (31.40%) | loss: 1.226388 | lrm: 1.00 | dt: 1498.26ms | tok/sec: 349,932 | mfu: 53.43 | total time: 6.58m\n",
      "step 00254 (31.53%) | loss: 1.216383 | lrm: 1.00 | dt: 1508.04ms | tok/sec: 347,660 | mfu: 53.08 | total time: 6.60m\n",
      "step 00255 (31.65%) | loss: 1.206355 | lrm: 1.00 | dt: 1509.45ms | tok/sec: 347,337 | mfu: 53.03 | total time: 6.63m\n",
      "step 00256 (31.77%) | loss: 1.221630 | lrm: 1.00 | dt: 1503.48ms | tok/sec: 348,715 | mfu: 53.24 | total time: 6.65m\n",
      "step 00257 (31.90%) | loss: 1.212840 | lrm: 1.00 | dt: 1504.13ms | tok/sec: 348,565 | mfu: 53.22 | total time: 6.68m\n",
      "step 00258 (32.02%) | loss: 1.206637 | lrm: 1.00 | dt: 1504.47ms | tok/sec: 348,486 | mfu: 53.20 | total time: 6.70m\n",
      "step 00259 (32.14%) | loss: 1.211481 | lrm: 1.00 | dt: 1507.32ms | tok/sec: 347,827 | mfu: 53.10 | total time: 6.73m\n",
      "step 00260 (32.28%) | loss: 1.201103 | lrm: 1.00 | dt: 1510.78ms | tok/sec: 347,032 | mfu: 52.98 | total time: 6.75m\n",
      "step 00261 (32.40%) | loss: 1.233866 | lrm: 1.00 | dt: 1499.82ms | tok/sec: 349,568 | mfu: 53.37 | total time: 6.78m\n",
      "step 00262 (32.51%) | loss: 1.229512 | lrm: 1.00 | dt: 1510.42ms | tok/sec: 347,114 | mfu: 53.00 | total time: 6.80m\n",
      "step 00263 (32.63%) | loss: 1.220482 | lrm: 1.00 | dt: 1500.71ms | tok/sec: 349,358 | mfu: 53.34 | total time: 6.83m\n",
      "step 00264 (32.76%) | loss: 1.256367 | lrm: 1.00 | dt: 1503.96ms | tok/sec: 348,604 | mfu: 53.22 | total time: 6.85m\n",
      "step 00265 (32.88%) | loss: 1.244400 | lrm: 1.00 | dt: 1499.93ms | tok/sec: 349,541 | mfu: 53.37 | total time: 6.88m\n",
      "step 00266 (32.99%) | loss: 1.227854 | lrm: 1.00 | dt: 1505.54ms | tok/sec: 348,238 | mfu: 53.17 | total time: 6.90m\n",
      "step 00267 (33.11%) | loss: 1.232280 | lrm: 1.00 | dt: 1500.53ms | tok/sec: 349,402 | mfu: 53.34 | total time: 6.93m\n",
      "step 00268 (33.23%) | loss: 1.236707 | lrm: 1.00 | dt: 1514.34ms | tok/sec: 346,215 | mfu: 52.86 | total time: 6.95m\n",
      "step 00269 (33.35%) | loss: 1.233227 | lrm: 1.00 | dt: 1500.63ms | tok/sec: 349,378 | mfu: 53.34 | total time: 6.98m\n",
      "step 00270 (33.46%) | loss: 1.227992 | lrm: 1.00 | dt: 1507.17ms | tok/sec: 347,861 | mfu: 53.11 | total time: 7.00m\n",
      "step 00271 (33.59%) | loss: 1.215860 | lrm: 1.00 | dt: 1506.72ms | tok/sec: 347,966 | mfu: 53.13 | total time: 7.03m\n",
      "step 00272 (33.71%) | loss: 1.225153 | lrm: 1.00 | dt: 1508.94ms | tok/sec: 347,453 | mfu: 53.05 | total time: 7.05m\n",
      "step 00273 (33.81%) | loss: 1.208504 | lrm: 1.00 | dt: 1510.07ms | tok/sec: 347,195 | mfu: 53.01 | total time: 7.08m\n",
      "step 00274 (33.97%) | loss: 1.192644 | lrm: 1.00 | dt: 1496.33ms | tok/sec: 350,383 | mfu: 53.49 | total time: 7.10m\n",
      "step 00275 (34.08%) | loss: 1.186004 | lrm: 1.00 | dt: 1501.73ms | tok/sec: 349,123 | mfu: 53.30 | total time: 7.13m\n",
      "step 00276 (34.20%) | loss: 1.197679 | lrm: 1.00 | dt: 1499.76ms | tok/sec: 349,581 | mfu: 53.37 | total time: 7.15m\n",
      "step 00277 (34.32%) | loss: 1.237837 | lrm: 1.00 | dt: 1503.64ms | tok/sec: 348,678 | mfu: 53.23 | total time: 7.18m\n",
      "step 00278 (34.44%) | loss: 1.225257 | lrm: 1.00 | dt: 1505.48ms | tok/sec: 348,252 | mfu: 53.17 | total time: 7.20m\n",
      "step 00279 (34.55%) | loss: 1.220205 | lrm: 1.00 | dt: 1499.59ms | tok/sec: 349,620 | mfu: 53.38 | total time: 7.23m\n",
      "step 00280 (34.66%) | loss: 1.231676 | lrm: 1.00 | dt: 1659.06ms | tok/sec: 316,015 | mfu: 48.25 | total time: 7.25m\n",
      "step 00281 (34.78%) | loss: 1.228723 | lrm: 1.00 | dt: 2574.83ms | tok/sec: 203,620 | mfu: 31.09 | total time: 7.30m\n",
      "step 00282 (34.91%) | loss: 1.218066 | lrm: 1.00 | dt: 2373.94ms | tok/sec: 220,851 | mfu: 33.72 | total time: 7.34m\n",
      "step 00283 (35.04%) | loss: 1.239080 | lrm: 1.00 | dt: 1497.81ms | tok/sec: 350,035 | mfu: 53.44 | total time: 7.36m\n",
      "step 00284 (35.16%) | loss: 1.233698 | lrm: 1.00 | dt: 1507.56ms | tok/sec: 347,772 | mfu: 53.10 | total time: 7.39m\n",
      "step 00285 (35.28%) | loss: 1.237322 | lrm: 1.00 | dt: 1656.87ms | tok/sec: 316,431 | mfu: 48.31 | total time: 7.41m\n",
      "step 00286 (35.38%) | loss: 1.209078 | lrm: 1.00 | dt: 2531.54ms | tok/sec: 207,102 | mfu: 31.62 | total time: 7.46m\n",
      "step 00287 (35.49%) | loss: 1.210605 | lrm: 1.00 | dt: 1503.07ms | tok/sec: 348,812 | mfu: 53.25 | total time: 7.48m\n",
      "step 00288 (35.60%) | loss: 1.200741 | lrm: 1.00 | dt: 1493.42ms | tok/sec: 351,064 | mfu: 53.60 | total time: 7.51m\n",
      "step 00289 (35.73%) | loss: 1.197321 | lrm: 1.00 | dt: 1502.48ms | tok/sec: 348,948 | mfu: 53.28 | total time: 7.53m\n",
      "step 00290 (35.85%) | loss: 1.180820 | lrm: 1.00 | dt: 1504.20ms | tok/sec: 348,548 | mfu: 53.21 | total time: 7.56m\n",
      "step 00291 (35.98%) | loss: 1.226637 | lrm: 1.00 | dt: 1508.26ms | tok/sec: 347,611 | mfu: 53.07 | total time: 7.58m\n",
      "step 00292 (36.11%) | loss: 1.220266 | lrm: 1.00 | dt: 1507.05ms | tok/sec: 347,890 | mfu: 53.11 | total time: 7.61m\n",
      "step 00293 (36.23%) | loss: 1.228705 | lrm: 1.00 | dt: 1507.84ms | tok/sec: 347,708 | mfu: 53.09 | total time: 7.63m\n",
      "step 00294 (36.36%) | loss: 1.239173 | lrm: 1.00 | dt: 1508.90ms | tok/sec: 347,464 | mfu: 53.05 | total time: 7.66m\n",
      "step 00295 (36.48%) | loss: 1.243846 | lrm: 1.00 | dt: 1500.83ms | tok/sec: 349,331 | mfu: 53.33 | total time: 7.68m\n",
      "step 00296 (36.61%) | loss: 1.256560 | lrm: 1.00 | dt: 1507.66ms | tok/sec: 347,749 | mfu: 53.09 | total time: 7.71m\n",
      "step 00297 (36.74%) | loss: 1.245585 | lrm: 1.00 | dt: 1506.77ms | tok/sec: 347,954 | mfu: 53.12 | total time: 7.73m\n",
      "step 00298 (36.88%) | loss: 1.225616 | lrm: 1.00 | dt: 1504.83ms | tok/sec: 348,402 | mfu: 53.19 | total time: 7.76m\n",
      "step 00299 (37.00%) | loss: 1.213549 | lrm: 1.00 | dt: 1507.59ms | tok/sec: 347,764 | mfu: 53.09 | total time: 7.78m\n",
      "step 00300 (37.11%) | loss: 1.209480 | lrm: 1.00 | dt: 1503.24ms | tok/sec: 348,771 | mfu: 53.25 | total time: 7.81m\n",
      "step 00300 | Validation bpb: 0.3688\n",
      "step 00301 (37.24%) | loss: 1.215937 | lrm: 1.00 | dt: 1503.96ms | tok/sec: 348,605 | mfu: 53.22 | total time: 7.83m\n",
      "step 00302 (37.35%) | loss: 1.180953 | lrm: 1.00 | dt: 1658.44ms | tok/sec: 316,132 | mfu: 48.27 | total time: 7.86m\n",
      "step 00303 (37.47%) | loss: 1.183307 | lrm: 1.00 | dt: 2369.59ms | tok/sec: 221,256 | mfu: 33.78 | total time: 7.90m\n",
      "step 00304 (37.59%) | loss: 1.185793 | lrm: 1.00 | dt: 1635.33ms | tok/sec: 320,600 | mfu: 48.95 | total time: 7.93m\n",
      "step 00305 (37.72%) | loss: 1.174017 | lrm: 1.00 | dt: 2369.75ms | tok/sec: 221,241 | mfu: 33.78 | total time: 7.97m\n",
      "step 00306 (37.86%) | loss: 1.184254 | lrm: 1.00 | dt: 1494.14ms | tok/sec: 350,895 | mfu: 53.57 | total time: 7.99m\n",
      "step 00307 (37.98%) | loss: 1.181275 | lrm: 1.00 | dt: 1505.05ms | tok/sec: 348,352 | mfu: 53.18 | total time: 8.02m\n",
      "step 00308 (38.11%) | loss: 1.185446 | lrm: 1.00 | dt: 1504.42ms | tok/sec: 348,497 | mfu: 53.21 | total time: 8.04m\n",
      "step 00309 (38.21%) | loss: 1.193198 | lrm: 1.00 | dt: 1508.60ms | tok/sec: 347,533 | mfu: 53.06 | total time: 8.07m\n",
      "step 00310 (38.32%) | loss: 1.204925 | lrm: 1.00 | dt: 1496.02ms | tok/sec: 350,454 | mfu: 53.51 | total time: 8.09m\n",
      "step 00311 (38.45%) | loss: 1.191521 | lrm: 1.00 | dt: 1664.72ms | tok/sec: 314,941 | mfu: 48.08 | total time: 8.12m\n",
      "step 00312 (38.55%) | loss: 1.183280 | lrm: 1.00 | dt: 2532.50ms | tok/sec: 207,023 | mfu: 31.61 | total time: 8.16m\n",
      "step 00313 (38.68%) | loss: 1.176733 | lrm: 1.00 | dt: 1502.05ms | tok/sec: 349,048 | mfu: 53.29 | total time: 8.19m\n",
      "step 00314 (38.80%) | loss: 1.186628 | lrm: 1.00 | dt: 1501.45ms | tok/sec: 349,188 | mfu: 53.31 | total time: 8.21m\n",
      "step 00315 (38.92%) | loss: 1.162016 | lrm: 1.00 | dt: 1500.20ms | tok/sec: 349,478 | mfu: 53.36 | total time: 8.24m\n",
      "step 00316 (39.02%) | loss: 1.161828 | lrm: 1.00 | dt: 1499.51ms | tok/sec: 349,640 | mfu: 53.38 | total time: 8.26m\n",
      "step 00317 (39.14%) | loss: 1.162316 | lrm: 1.00 | dt: 1510.74ms | tok/sec: 347,041 | mfu: 52.98 | total time: 8.29m\n",
      "step 00318 (39.26%) | loss: 1.193236 | lrm: 1.00 | dt: 1499.43ms | tok/sec: 349,657 | mfu: 53.38 | total time: 8.31m\n",
      "step 00319 (39.36%) | loss: 1.186812 | lrm: 1.00 | dt: 1504.12ms | tok/sec: 348,566 | mfu: 53.22 | total time: 8.34m\n",
      "step 00320 (39.48%) | loss: 1.196133 | lrm: 1.00 | dt: 2211.76ms | tok/sec: 237,045 | mfu: 36.19 | total time: 8.37m\n",
      "step 00321 (39.60%) | loss: 1.192616 | lrm: 1.00 | dt: 1501.84ms | tok/sec: 349,097 | mfu: 53.30 | total time: 8.40m\n",
      "step 00322 (39.73%) | loss: 1.200001 | lrm: 1.00 | dt: 1505.71ms | tok/sec: 348,199 | mfu: 53.16 | total time: 8.42m\n",
      "step 00323 (39.84%) | loss: 1.191645 | lrm: 1.00 | dt: 1510.10ms | tok/sec: 347,188 | mfu: 53.01 | total time: 8.45m\n",
      "step 00324 (39.95%) | loss: 1.173625 | lrm: 1.00 | dt: 1503.29ms | tok/sec: 348,760 | mfu: 53.25 | total time: 8.47m\n",
      "step 00325 (40.07%) | loss: 1.161669 | lrm: 1.00 | dt: 1494.78ms | tok/sec: 350,746 | mfu: 53.55 | total time: 8.50m\n",
      "step 00326 (40.20%) | loss: 1.156676 | lrm: 1.00 | dt: 1502.87ms | tok/sec: 348,858 | mfu: 53.26 | total time: 8.52m\n",
      "step 00327 (40.32%) | loss: 1.180283 | lrm: 1.00 | dt: 1499.93ms | tok/sec: 349,541 | mfu: 53.37 | total time: 8.55m\n",
      "step 00328 (40.45%) | loss: 1.176492 | lrm: 1.00 | dt: 1503.17ms | tok/sec: 348,787 | mfu: 53.25 | total time: 8.57m\n",
      "step 00329 (40.57%) | loss: 1.167650 | lrm: 1.00 | dt: 1493.97ms | tok/sec: 350,936 | mfu: 53.58 | total time: 8.60m\n",
      "step 00330 (40.69%) | loss: 1.145271 | lrm: 1.00 | dt: 1506.46ms | tok/sec: 348,026 | mfu: 53.13 | total time: 8.62m\n",
      "step 00331 (40.82%) | loss: 1.131182 | lrm: 1.00 | dt: 1506.50ms | tok/sec: 348,016 | mfu: 53.13 | total time: 8.65m\n",
      "step 00332 (40.92%) | loss: 1.126384 | lrm: 1.00 | dt: 1500.64ms | tok/sec: 349,377 | mfu: 53.34 | total time: 8.67m\n",
      "step 00333 (41.04%) | loss: 1.128420 | lrm: 1.00 | dt: 1500.41ms | tok/sec: 349,429 | mfu: 53.35 | total time: 8.70m\n",
      "step 00334 (41.16%) | loss: 1.125054 | lrm: 1.00 | dt: 1659.80ms | tok/sec: 315,874 | mfu: 48.23 | total time: 8.73m\n",
      "step 00335 (41.29%) | loss: 1.133591 | lrm: 1.00 | dt: 2366.13ms | tok/sec: 221,580 | mfu: 33.83 | total time: 8.77m\n",
      "step 00336 (41.41%) | loss: 1.131996 | lrm: 1.00 | dt: 1651.13ms | tok/sec: 317,532 | mfu: 48.48 | total time: 8.79m\n",
      "step 00337 (41.56%) | loss: 1.151751 | lrm: 1.00 | dt: 2491.73ms | tok/sec: 210,410 | mfu: 32.12 | total time: 8.84m\n",
      "step 00338 (41.69%) | loss: 1.158155 | lrm: 1.00 | dt: 2560.17ms | tok/sec: 204,786 | mfu: 31.27 | total time: 8.88m\n",
      "step 00339 (41.80%) | loss: 1.154270 | lrm: 1.00 | dt: 1656.92ms | tok/sec: 316,423 | mfu: 48.31 | total time: 8.91m\n",
      "step 00340 (41.94%) | loss: 1.144488 | lrm: 1.00 | dt: 2394.24ms | tok/sec: 218,978 | mfu: 33.43 | total time: 8.95m\n",
      "step 00341 (42.07%) | loss: 1.142160 | lrm: 1.00 | dt: 1652.92ms | tok/sec: 317,188 | mfu: 48.43 | total time: 8.97m\n",
      "step 00342 (42.20%) | loss: 1.165755 | lrm: 1.00 | dt: 2346.22ms | tok/sec: 223,460 | mfu: 34.12 | total time: 9.01m\n",
      "step 00343 (42.31%) | loss: 1.146935 | lrm: 1.00 | dt: 1495.61ms | tok/sec: 350,550 | mfu: 53.52 | total time: 9.04m\n",
      "step 00344 (42.45%) | loss: 1.143998 | lrm: 1.00 | dt: 1506.71ms | tok/sec: 347,969 | mfu: 53.13 | total time: 9.06m\n",
      "step 00345 (42.57%) | loss: 1.138435 | lrm: 1.00 | dt: 1501.94ms | tok/sec: 349,074 | mfu: 53.29 | total time: 9.09m\n",
      "step 00346 (42.71%) | loss: 1.132212 | lrm: 1.00 | dt: 1509.24ms | tok/sec: 347,385 | mfu: 53.04 | total time: 9.11m\n",
      "step 00347 (42.84%) | loss: 1.129428 | lrm: 1.00 | dt: 1659.71ms | tok/sec: 315,891 | mfu: 48.23 | total time: 9.14m\n",
      "step 00348 (42.96%) | loss: 1.132402 | lrm: 1.00 | dt: 2530.22ms | tok/sec: 207,210 | mfu: 31.64 | total time: 9.18m\n",
      "step 00349 (43.08%) | loss: 1.128349 | lrm: 1.00 | dt: 1500.21ms | tok/sec: 349,475 | mfu: 53.36 | total time: 9.21m\n",
      "step 00350 (43.20%) | loss: 1.131039 | lrm: 1.00 | dt: 1505.26ms | tok/sec: 348,304 | mfu: 53.18 | total time: 9.23m\n",
      "step 00351 (43.32%) | loss: 1.143905 | lrm: 1.00 | dt: 1503.19ms | tok/sec: 348,782 | mfu: 53.25 | total time: 9.26m\n",
      "step 00352 (43.45%) | loss: 1.144473 | lrm: 1.00 | dt: 2099.31ms | tok/sec: 249,742 | mfu: 38.13 | total time: 9.29m\n",
      "step 00353 (43.58%) | loss: 1.143559 | lrm: 1.00 | dt: 1500.93ms | tok/sec: 349,309 | mfu: 53.33 | total time: 9.32m\n",
      "step 00354 (43.70%) | loss: 1.150143 | lrm: 1.00 | dt: 1504.98ms | tok/sec: 348,367 | mfu: 53.19 | total time: 9.34m\n",
      "step 00355 (43.82%) | loss: 1.145697 | lrm: 1.00 | dt: 1507.48ms | tok/sec: 347,790 | mfu: 53.10 | total time: 9.37m\n",
      "step 00356 (43.94%) | loss: 1.135495 | lrm: 1.00 | dt: 1503.34ms | tok/sec: 348,748 | mfu: 53.24 | total time: 9.39m\n",
      "step 00357 (44.05%) | loss: 1.122539 | lrm: 1.00 | dt: 1504.41ms | tok/sec: 348,500 | mfu: 53.21 | total time: 9.42m\n",
      "step 00358 (44.16%) | loss: 1.124258 | lrm: 1.00 | dt: 1663.23ms | tok/sec: 315,222 | mfu: 48.13 | total time: 9.45m\n",
      "step 00359 (44.30%) | loss: 1.117585 | lrm: 1.00 | dt: 2469.35ms | tok/sec: 212,318 | mfu: 32.42 | total time: 9.49m\n",
      "step 00360 (44.43%) | loss: 1.135535 | lrm: 1.00 | dt: 1508.67ms | tok/sec: 347,517 | mfu: 53.06 | total time: 9.51m\n",
      "step 00361 (44.56%) | loss: 1.123366 | lrm: 1.00 | dt: 1505.72ms | tok/sec: 348,198 | mfu: 53.16 | total time: 9.54m\n",
      "step 00362 (44.68%) | loss: 1.138685 | lrm: 1.00 | dt: 1504.98ms | tok/sec: 348,369 | mfu: 53.19 | total time: 9.56m\n",
      "step 00363 (44.83%) | loss: 1.151078 | lrm: 1.00 | dt: 1502.99ms | tok/sec: 348,829 | mfu: 53.26 | total time: 9.59m\n",
      "step 00364 (44.96%) | loss: 1.157132 | lrm: 1.00 | dt: 1507.17ms | tok/sec: 347,861 | mfu: 53.11 | total time: 9.61m\n",
      "step 00365 (45.10%) | loss: 1.147404 | lrm: 1.00 | dt: 1500.94ms | tok/sec: 349,305 | mfu: 53.33 | total time: 9.64m\n",
      "step 00366 (45.21%) | loss: 1.148394 | lrm: 1.00 | dt: 2116.09ms | tok/sec: 247,762 | mfu: 37.83 | total time: 9.67m\n",
      "step 00367 (45.33%) | loss: 1.140155 | lrm: 1.00 | dt: 2099.09ms | tok/sec: 249,769 | mfu: 38.13 | total time: 9.71m\n",
      "step 00368 (45.46%) | loss: 1.161688 | lrm: 1.00 | dt: 1497.38ms | tok/sec: 350,137 | mfu: 53.46 | total time: 9.73m\n",
      "step 00369 (45.60%) | loss: 1.167473 | lrm: 1.00 | dt: 1502.26ms | tok/sec: 349,000 | mfu: 53.28 | total time: 9.76m\n",
      "step 00370 (45.72%) | loss: 1.160603 | lrm: 1.00 | dt: 1505.61ms | tok/sec: 348,221 | mfu: 53.16 | total time: 9.78m\n",
      "step 00371 (45.84%) | loss: 1.161320 | lrm: 1.00 | dt: 1505.28ms | tok/sec: 348,300 | mfu: 53.18 | total time: 9.81m\n",
      "step 00372 (45.99%) | loss: 1.164648 | lrm: 1.00 | dt: 1506.23ms | tok/sec: 348,079 | mfu: 53.14 | total time: 9.83m\n",
      "step 00373 (46.12%) | loss: 1.174519 | lrm: 1.00 | dt: 1506.52ms | tok/sec: 348,012 | mfu: 53.13 | total time: 9.86m\n",
      "step 00374 (46.25%) | loss: 1.167093 | lrm: 1.00 | dt: 1501.26ms | tok/sec: 349,231 | mfu: 53.32 | total time: 9.88m\n",
      "step 00375 (46.38%) | loss: 1.174212 | lrm: 1.00 | dt: 1498.17ms | tok/sec: 349,952 | mfu: 53.43 | total time: 9.91m\n",
      "step 00376 (46.50%) | loss: 1.179258 | lrm: 1.00 | dt: 1498.85ms | tok/sec: 349,793 | mfu: 53.40 | total time: 9.93m\n",
      "step 00377 (46.61%) | loss: 1.187537 | lrm: 1.00 | dt: 1659.55ms | tok/sec: 315,921 | mfu: 48.23 | total time: 9.96m\n",
      "step 00378 (46.74%) | loss: 1.174810 | lrm: 1.00 | dt: 2379.05ms | tok/sec: 220,376 | mfu: 33.65 | total time: 10.00m\n",
      "step 00379 (46.88%) | loss: 1.181854 | lrm: 1.00 | dt: 1499.89ms | tok/sec: 349,550 | mfu: 53.37 | total time: 10.03m\n",
      "step 00380 (47.02%) | loss: 1.183823 | lrm: 1.00 | dt: 1497.65ms | tok/sec: 350,073 | mfu: 53.45 | total time: 10.05m\n",
      "step 00381 (47.14%) | loss: 1.196334 | lrm: 1.00 | dt: 1499.50ms | tok/sec: 349,641 | mfu: 53.38 | total time: 10.08m\n",
      "step 00382 (47.27%) | loss: 1.173105 | lrm: 1.00 | dt: 1506.48ms | tok/sec: 348,022 | mfu: 53.13 | total time: 10.10m\n",
      "step 00383 (47.40%) | loss: 1.171190 | lrm: 1.00 | dt: 1504.01ms | tok/sec: 348,593 | mfu: 53.22 | total time: 10.13m\n",
      "step 00384 (47.52%) | loss: 1.170305 | lrm: 1.00 | dt: 2117.54ms | tok/sec: 247,592 | mfu: 37.80 | total time: 10.16m\n",
      "step 00385 (47.65%) | loss: 1.207027 | lrm: 1.00 | dt: 1500.17ms | tok/sec: 349,484 | mfu: 53.36 | total time: 10.19m\n",
      "step 00386 (47.79%) | loss: 1.186046 | lrm: 1.00 | dt: 1502.77ms | tok/sec: 348,880 | mfu: 53.27 | total time: 10.21m\n",
      "step 00387 (47.93%) | loss: 1.179962 | lrm: 1.00 | dt: 1499.62ms | tok/sec: 349,614 | mfu: 53.38 | total time: 10.24m\n",
      "step 00388 (48.07%) | loss: 1.166197 | lrm: 1.00 | dt: 1503.37ms | tok/sec: 348,742 | mfu: 53.24 | total time: 10.26m\n",
      "step 00389 (48.17%) | loss: 1.206195 | lrm: 1.00 | dt: 1504.00ms | tok/sec: 348,595 | mfu: 53.22 | total time: 10.29m\n",
      "step 00390 (48.30%) | loss: 1.207603 | lrm: 1.00 | dt: 1508.58ms | tok/sec: 347,536 | mfu: 53.06 | total time: 10.31m\n",
      "step 00391 (48.44%) | loss: 1.182056 | lrm: 1.00 | dt: 1494.60ms | tok/sec: 350,787 | mfu: 53.56 | total time: 10.34m\n",
      "step 00392 (48.55%) | loss: 1.180138 | lrm: 1.00 | dt: 1503.17ms | tok/sec: 348,788 | mfu: 53.25 | total time: 10.36m\n",
      "step 00393 (48.66%) | loss: 1.173248 | lrm: 1.00 | dt: 1499.35ms | tok/sec: 349,677 | mfu: 53.39 | total time: 10.39m\n",
      "step 00394 (48.79%) | loss: 1.165619 | lrm: 1.00 | dt: 1501.62ms | tok/sec: 349,148 | mfu: 53.31 | total time: 10.41m\n",
      "step 00395 (48.91%) | loss: 1.161625 | lrm: 1.00 | dt: 1503.57ms | tok/sec: 348,694 | mfu: 53.24 | total time: 10.44m\n",
      "step 00396 (49.04%) | loss: 1.154535 | lrm: 1.00 | dt: 1509.73ms | tok/sec: 347,273 | mfu: 53.02 | total time: 10.46m\n",
      "step 00397 (49.18%) | loss: 1.165396 | lrm: 1.00 | dt: 1505.01ms | tok/sec: 348,362 | mfu: 53.19 | total time: 10.49m\n",
      "step 00398 (49.31%) | loss: 1.164886 | lrm: 1.00 | dt: 2216.45ms | tok/sec: 236,544 | mfu: 36.11 | total time: 10.52m\n",
      "step 00399 (49.45%) | loss: 1.163461 | lrm: 1.00 | dt: 1650.90ms | tok/sec: 317,576 | mfu: 48.49 | total time: 10.55m\n",
      "step 00400 (49.56%) | loss: 1.173569 | lrm: 1.00 | dt: 2424.75ms | tok/sec: 216,223 | mfu: 33.01 | total time: 10.59m\n",
      "step 00401 (49.68%) | loss: 1.156247 | lrm: 1.00 | dt: 1499.37ms | tok/sec: 349,672 | mfu: 53.39 | total time: 10.62m\n",
      "step 00402 (49.82%) | loss: 1.142034 | lrm: 1.00 | dt: 1512.43ms | tok/sec: 346,651 | mfu: 52.92 | total time: 10.64m\n",
      "step 00403 (49.94%) | loss: 1.139153 | lrm: 1.00 | dt: 1648.54ms | tok/sec: 318,032 | mfu: 48.56 | total time: 10.67m\n",
      "step 00404 (50.07%) | loss: 1.137261 | lrm: 1.00 | dt: 2366.88ms | tok/sec: 221,509 | mfu: 33.82 | total time: 10.71m\n",
      "step 00405 (50.19%) | loss: 1.135608 | lrm: 1.00 | dt: 1498.34ms | tok/sec: 349,912 | mfu: 53.42 | total time: 10.73m\n",
      "step 00406 (50.30%) | loss: 1.125689 | lrm: 1.00 | dt: 1509.99ms | tok/sec: 347,212 | mfu: 53.01 | total time: 10.76m\n",
      "step 00407 (50.42%) | loss: 1.116974 | lrm: 1.00 | dt: 1657.08ms | tok/sec: 316,392 | mfu: 48.31 | total time: 10.79m\n",
      "step 00408 (50.54%) | loss: 1.130953 | lrm: 1.00 | dt: 2533.68ms | tok/sec: 206,927 | mfu: 31.59 | total time: 10.83m\n",
      "step 00409 (50.66%) | loss: 1.131185 | lrm: 1.00 | dt: 1495.17ms | tok/sec: 350,654 | mfu: 53.54 | total time: 10.85m\n",
      "step 00410 (50.78%) | loss: 1.137952 | lrm: 1.00 | dt: 1502.15ms | tok/sec: 349,025 | mfu: 53.29 | total time: 10.88m\n",
      "step 00411 (50.91%) | loss: 1.153037 | lrm: 1.00 | dt: 1505.14ms | tok/sec: 348,332 | mfu: 53.18 | total time: 10.90m\n",
      "step 00412 (51.02%) | loss: 1.160908 | lrm: 1.00 | dt: 2214.38ms | tok/sec: 236,765 | mfu: 36.15 | total time: 10.94m\n",
      "step 00413 (51.13%) | loss: 1.166181 | lrm: 1.00 | dt: 1660.82ms | tok/sec: 315,680 | mfu: 48.20 | total time: 10.97m\n",
      "step 00414 (51.26%) | loss: 1.139288 | lrm: 1.00 | dt: 2398.04ms | tok/sec: 218,631 | mfu: 33.38 | total time: 11.01m\n",
      "step 00415 (51.38%) | loss: 1.125250 | lrm: 1.00 | dt: 1500.11ms | tok/sec: 349,498 | mfu: 53.36 | total time: 11.03m\n",
      "step 00416 (51.50%) | loss: 1.121610 | lrm: 1.00 | dt: 1505.70ms | tok/sec: 348,202 | mfu: 53.16 | total time: 11.06m\n",
      "step 00417 (51.61%) | loss: 1.127097 | lrm: 1.00 | dt: 1502.13ms | tok/sec: 349,029 | mfu: 53.29 | total time: 11.08m\n",
      "step 00418 (51.72%) | loss: 1.140118 | lrm: 1.00 | dt: 1497.87ms | tok/sec: 350,022 | mfu: 53.44 | total time: 11.11m\n",
      "step 00419 (51.84%) | loss: 1.137132 | lrm: 1.00 | dt: 1509.91ms | tok/sec: 347,232 | mfu: 53.01 | total time: 11.13m\n",
      "step 00420 (51.97%) | loss: 1.123143 | lrm: 1.00 | dt: 1505.91ms | tok/sec: 348,152 | mfu: 53.15 | total time: 11.16m\n",
      "step 00421 (52.09%) | loss: 1.113143 | lrm: 1.00 | dt: 1501.64ms | tok/sec: 349,143 | mfu: 53.31 | total time: 11.18m\n",
      "step 00422 (52.20%) | loss: 1.120087 | lrm: 1.00 | dt: 1496.81ms | tok/sec: 350,270 | mfu: 53.48 | total time: 11.21m\n",
      "step 00423 (52.34%) | loss: 1.127183 | lrm: 1.00 | dt: 1501.18ms | tok/sec: 349,250 | mfu: 53.32 | total time: 11.23m\n",
      "step 00424 (52.47%) | loss: 1.133702 | lrm: 1.00 | dt: 1660.40ms | tok/sec: 315,759 | mfu: 48.21 | total time: 11.26m\n",
      "step 00425 (52.59%) | loss: 1.124591 | lrm: 1.00 | dt: 2532.54ms | tok/sec: 207,020 | mfu: 31.61 | total time: 11.30m\n",
      "step 00426 (52.71%) | loss: 1.118110 | lrm: 1.00 | dt: 1498.07ms | tok/sec: 349,975 | mfu: 53.43 | total time: 11.33m\n",
      "step 00427 (52.81%) | loss: 1.116115 | lrm: 1.00 | dt: 1499.02ms | tok/sec: 349,753 | mfu: 53.40 | total time: 11.35m\n",
      "step 00428 (52.92%) | loss: 1.096894 | lrm: 1.00 | dt: 1505.09ms | tok/sec: 348,342 | mfu: 53.18 | total time: 11.38m\n",
      "step 00429 (53.04%) | loss: 1.106709 | lrm: 1.00 | dt: 1500.94ms | tok/sec: 349,305 | mfu: 53.33 | total time: 11.40m\n",
      "step 00430 (53.15%) | loss: 1.114676 | lrm: 1.00 | dt: 1510.38ms | tok/sec: 347,123 | mfu: 53.00 | total time: 11.43m\n",
      "step 00431 (53.27%) | loss: 1.101673 | lrm: 1.00 | dt: 1656.92ms | tok/sec: 316,424 | mfu: 48.31 | total time: 11.46m\n",
      "step 00432 (53.39%) | loss: 1.107660 | lrm: 1.00 | dt: 2371.69ms | tok/sec: 221,061 | mfu: 33.75 | total time: 11.50m\n",
      "step 00433 (53.49%) | loss: 1.103359 | lrm: 1.00 | dt: 1650.19ms | tok/sec: 317,714 | mfu: 48.51 | total time: 11.52m\n",
      "step 00434 (53.61%) | loss: 1.107593 | lrm: 1.00 | dt: 3528.03ms | tok/sec: 148,606 | mfu: 22.69 | total time: 11.58m\n",
      "step 00435 (53.73%) | loss: 1.104894 | lrm: 1.00 | dt: 1501.49ms | tok/sec: 349,179 | mfu: 53.31 | total time: 11.61m\n",
      "step 00436 (53.86%) | loss: 1.119717 | lrm: 1.00 | dt: 1498.64ms | tok/sec: 349,843 | mfu: 53.41 | total time: 11.63m\n",
      "step 00437 (53.97%) | loss: 1.115417 | lrm: 1.00 | dt: 1499.91ms | tok/sec: 349,547 | mfu: 53.37 | total time: 11.66m\n",
      "step 00438 (54.11%) | loss: 1.115206 | lrm: 1.00 | dt: 1498.47ms | tok/sec: 349,882 | mfu: 53.42 | total time: 11.68m\n",
      "step 00439 (54.21%) | loss: 1.102276 | lrm: 1.00 | dt: 1501.50ms | tok/sec: 349,175 | mfu: 53.31 | total time: 11.71m\n",
      "step 00440 (54.33%) | loss: 1.117856 | lrm: 1.00 | dt: 1493.77ms | tok/sec: 350,983 | mfu: 53.59 | total time: 11.73m\n",
      "step 00441 (54.45%) | loss: 1.105636 | lrm: 1.00 | dt: 1500.10ms | tok/sec: 349,502 | mfu: 53.36 | total time: 11.76m\n",
      "step 00442 (54.58%) | loss: 1.114839 | lrm: 1.00 | dt: 1495.89ms | tok/sec: 350,486 | mfu: 53.51 | total time: 11.78m\n",
      "step 00443 (54.69%) | loss: 1.099225 | lrm: 1.00 | dt: 1503.60ms | tok/sec: 348,687 | mfu: 53.24 | total time: 11.81m\n",
      "step 00444 (54.78%) | loss: 1.095168 | lrm: 1.00 | dt: 1497.58ms | tok/sec: 350,090 | mfu: 53.45 | total time: 11.83m\n",
      "step 00445 (54.89%) | loss: 1.087302 | lrm: 1.00 | dt: 1505.37ms | tok/sec: 348,278 | mfu: 53.17 | total time: 11.86m\n",
      "step 00446 (55.02%) | loss: 1.107868 | lrm: 1.00 | dt: 1508.50ms | tok/sec: 347,555 | mfu: 53.06 | total time: 11.88m\n",
      "step 00447 (55.14%) | loss: 1.081527 | lrm: 1.00 | dt: 1500.77ms | tok/sec: 349,345 | mfu: 53.34 | total time: 11.91m\n",
      "step 00448 (55.26%) | loss: 1.091151 | lrm: 1.00 | dt: 1503.24ms | tok/sec: 348,772 | mfu: 53.25 | total time: 11.93m\n",
      "step 00449 (55.37%) | loss: 1.091875 | lrm: 1.00 | dt: 1496.60ms | tok/sec: 350,318 | mfu: 53.48 | total time: 11.96m\n",
      "step 00450 (55.49%) | loss: 1.075820 | lrm: 1.00 | dt: 1502.55ms | tok/sec: 348,931 | mfu: 53.27 | total time: 11.98m\n",
      "step 00450 | Validation bpb: 0.3579\n",
      "step 00451 (55.60%) | loss: 1.093943 | lrm: 1.00 | dt: 1643.20ms | tok/sec: 319,065 | mfu: 48.71 | total time: 12.01m\n",
      "step 00452 (55.72%) | loss: 1.095754 | lrm: 1.00 | dt: 2397.38ms | tok/sec: 218,692 | mfu: 33.39 | total time: 12.05m\n",
      "step 00453 (55.85%) | loss: 1.100757 | lrm: 1.00 | dt: 1657.54ms | tok/sec: 316,304 | mfu: 48.29 | total time: 12.08m\n",
      "step 00454 (55.96%) | loss: 1.111886 | lrm: 1.00 | dt: 2378.22ms | tok/sec: 220,453 | mfu: 33.66 | total time: 12.12m\n",
      "step 00455 (56.08%) | loss: 1.092820 | lrm: 1.00 | dt: 1656.51ms | tok/sec: 316,502 | mfu: 48.32 | total time: 12.14m\n",
      "step 00456 (56.21%) | loss: 1.097462 | lrm: 1.00 | dt: 2527.95ms | tok/sec: 207,396 | mfu: 31.66 | total time: 12.19m\n",
      "step 00457 (56.33%) | loss: 1.093555 | lrm: 1.00 | dt: 1493.85ms | tok/sec: 350,965 | mfu: 53.58 | total time: 12.21m\n",
      "step 00458 (56.47%) | loss: 1.097115 | lrm: 1.00 | dt: 1495.94ms | tok/sec: 350,473 | mfu: 53.51 | total time: 12.24m\n",
      "step 00459 (56.58%) | loss: 1.103065 | lrm: 1.00 | dt: 1656.78ms | tok/sec: 316,450 | mfu: 48.31 | total time: 12.26m\n",
      "step 00460 (56.70%) | loss: 1.105334 | lrm: 1.00 | dt: 2380.62ms | tok/sec: 220,231 | mfu: 33.62 | total time: 12.30m\n",
      "step 00461 (56.82%) | loss: 1.116616 | lrm: 1.00 | dt: 1664.60ms | tok/sec: 314,962 | mfu: 48.09 | total time: 12.33m\n",
      "step 00462 (56.94%) | loss: 1.104869 | lrm: 1.00 | dt: 2342.16ms | tok/sec: 223,848 | mfu: 34.18 | total time: 12.37m\n",
      "step 00463 (57.06%) | loss: 1.114808 | lrm: 1.00 | dt: 1650.80ms | tok/sec: 317,597 | mfu: 48.49 | total time: 12.40m\n",
      "step 00464 (57.18%) | loss: 1.117739 | lrm: 1.00 | dt: 2349.69ms | tok/sec: 223,130 | mfu: 34.07 | total time: 12.44m\n",
      "step 00465 (57.30%) | loss: 1.113952 | lrm: 1.00 | dt: 1499.38ms | tok/sec: 349,669 | mfu: 53.39 | total time: 12.46m\n",
      "step 00466 (57.45%) | loss: 1.110318 | lrm: 1.00 | dt: 1504.80ms | tok/sec: 348,409 | mfu: 53.19 | total time: 12.49m\n",
      "step 00467 (57.58%) | loss: 1.115427 | lrm: 1.00 | dt: 1507.70ms | tok/sec: 347,739 | mfu: 53.09 | total time: 12.51m\n",
      "step 00468 (57.71%) | loss: 1.106829 | lrm: 1.00 | dt: 1503.33ms | tok/sec: 348,752 | mfu: 53.25 | total time: 12.54m\n",
      "step 00469 (57.83%) | loss: 1.095778 | lrm: 1.00 | dt: 1501.31ms | tok/sec: 349,221 | mfu: 53.32 | total time: 12.56m\n",
      "step 00470 (57.93%) | loss: 1.100011 | lrm: 1.00 | dt: 1505.42ms | tok/sec: 348,267 | mfu: 53.17 | total time: 12.59m\n",
      "step 00471 (58.04%) | loss: 1.116221 | lrm: 1.00 | dt: 1664.64ms | tok/sec: 314,956 | mfu: 48.09 | total time: 12.62m\n",
      "step 00472 (58.15%) | loss: 1.101909 | lrm: 1.00 | dt: 2527.56ms | tok/sec: 207,428 | mfu: 31.67 | total time: 12.66m\n",
      "step 00473 (58.27%) | loss: 1.096922 | lrm: 1.00 | dt: 2417.57ms | tok/sec: 216,865 | mfu: 33.11 | total time: 12.70m\n",
      "step 00474 (58.39%) | loss: 1.087859 | lrm: 1.00 | dt: 1496.53ms | tok/sec: 350,336 | mfu: 53.49 | total time: 12.72m\n",
      "step 00475 (58.51%) | loss: 1.073051 | lrm: 1.00 | dt: 1667.22ms | tok/sec: 314,468 | mfu: 48.01 | total time: 12.75m\n",
      "step 00476 (58.65%) | loss: 1.126939 | lrm: 1.00 | dt: 2428.67ms | tok/sec: 215,874 | mfu: 32.96 | total time: 12.79m\n",
      "step 00477 (58.78%) | loss: 1.123282 | lrm: 1.00 | dt: 1495.20ms | tok/sec: 350,646 | mfu: 53.53 | total time: 12.82m\n",
      "step 00478 (58.89%) | loss: 1.120800 | lrm: 1.00 | dt: 1504.25ms | tok/sec: 348,538 | mfu: 53.21 | total time: 12.84m\n",
      "step 00479 (59.02%) | loss: 1.121986 | lrm: 1.00 | dt: 1646.06ms | tok/sec: 318,511 | mfu: 48.63 | total time: 12.87m\n",
      "step 00480 (59.14%) | loss: 1.132498 | lrm: 1.00 | dt: 2385.27ms | tok/sec: 219,802 | mfu: 33.56 | total time: 12.91m\n",
      "step 00481 (59.28%) | loss: 1.130936 | lrm: 1.00 | dt: 1500.58ms | tok/sec: 349,389 | mfu: 53.34 | total time: 12.93m\n",
      "step 00482 (59.39%) | loss: 1.126479 | lrm: 1.00 | dt: 1503.08ms | tok/sec: 348,808 | mfu: 53.25 | total time: 12.96m\n",
      "step 00483 (59.51%) | loss: 1.150143 | lrm: 1.00 | dt: 1511.11ms | tok/sec: 346,954 | mfu: 52.97 | total time: 12.98m\n",
      "step 00484 (59.63%) | loss: 1.139933 | lrm: 1.00 | dt: 1656.06ms | tok/sec: 316,588 | mfu: 48.33 | total time: 13.01m\n",
      "step 00485 (59.75%) | loss: 1.132533 | lrm: 1.00 | dt: 2523.99ms | tok/sec: 207,722 | mfu: 31.71 | total time: 13.05m\n",
      "step 00486 (59.89%) | loss: 1.124726 | lrm: 1.00 | dt: 1654.10ms | tok/sec: 316,963 | mfu: 48.39 | total time: 13.08m\n",
      "step 00487 (60.01%) | loss: 1.124560 | lrm: 1.00 | dt: 2377.58ms | tok/sec: 220,512 | mfu: 33.67 | total time: 13.12m\n",
      "step 00488 (60.13%) | loss: 1.116653 | lrm: 1.00 | dt: 1491.68ms | tok/sec: 351,474 | mfu: 53.66 | total time: 13.15m\n",
      "step 00489 (60.25%) | loss: 1.097900 | lrm: 1.00 | dt: 1500.82ms | tok/sec: 349,334 | mfu: 53.33 | total time: 13.17m\n",
      "step 00490 (60.41%) | loss: 1.101598 | lrm: 1.00 | dt: 1496.02ms | tok/sec: 350,455 | mfu: 53.51 | total time: 13.20m\n",
      "step 00491 (60.52%) | loss: 1.095725 | lrm: 1.00 | dt: 1506.54ms | tok/sec: 348,007 | mfu: 53.13 | total time: 13.22m\n",
      "step 00492 (60.65%) | loss: 1.119105 | lrm: 1.00 | dt: 1498.66ms | tok/sec: 349,837 | mfu: 53.41 | total time: 13.25m\n",
      "step 00493 (60.78%) | loss: 1.114290 | lrm: 1.00 | dt: 1499.64ms | tok/sec: 349,609 | mfu: 53.38 | total time: 13.27m\n",
      "step 00494 (60.90%) | loss: 1.108744 | lrm: 1.00 | dt: 1497.34ms | tok/sec: 350,145 | mfu: 53.46 | total time: 13.30m\n",
      "step 00495 (61.04%) | loss: 1.096560 | lrm: 1.00 | dt: 1502.40ms | tok/sec: 348,966 | mfu: 53.28 | total time: 13.32m\n",
      "step 00496 (61.18%) | loss: 1.101891 | lrm: 1.00 | dt: 1671.76ms | tok/sec: 313,614 | mfu: 47.88 | total time: 13.35m\n",
      "step 00497 (61.32%) | loss: 1.102149 | lrm: 1.00 | dt: 2374.39ms | tok/sec: 220,809 | mfu: 33.71 | total time: 13.39m\n",
      "step 00498 (61.45%) | loss: 1.107814 | lrm: 1.00 | dt: 1503.36ms | tok/sec: 348,743 | mfu: 53.24 | total time: 13.41m\n",
      "step 00499 (61.56%) | loss: 1.110568 | lrm: 1.00 | dt: 1502.66ms | tok/sec: 348,907 | mfu: 53.27 | total time: 13.44m\n",
      "step 00500 (61.70%) | loss: 1.136089 | lrm: 1.00 | dt: 1506.75ms | tok/sec: 347,959 | mfu: 53.12 | total time: 13.46m\n",
      "step 00501 (61.81%) | loss: 1.121299 | lrm: 1.00 | dt: 1504.83ms | tok/sec: 348,403 | mfu: 53.19 | total time: 13.49m\n",
      "step 00502 (61.95%) | loss: 1.109150 | lrm: 1.00 | dt: 1498.46ms | tok/sec: 349,883 | mfu: 53.42 | total time: 13.51m\n",
      "step 00503 (62.07%) | loss: 1.113089 | lrm: 1.00 | dt: 1498.89ms | tok/sec: 349,784 | mfu: 53.40 | total time: 13.54m\n",
      "step 00504 (62.19%) | loss: 1.123924 | lrm: 1.00 | dt: 1503.32ms | tok/sec: 348,752 | mfu: 53.25 | total time: 13.56m\n",
      "step 00505 (62.32%) | loss: 1.100416 | lrm: 1.00 | dt: 1498.78ms | tok/sec: 349,809 | mfu: 53.41 | total time: 13.59m\n",
      "step 00506 (62.45%) | loss: 1.099984 | lrm: 1.00 | dt: 1507.53ms | tok/sec: 347,779 | mfu: 53.10 | total time: 13.61m\n",
      "step 00507 (62.59%) | loss: 1.122937 | lrm: 1.00 | dt: 1509.75ms | tok/sec: 347,267 | mfu: 53.02 | total time: 13.64m\n",
      "step 00508 (62.71%) | loss: 1.114787 | lrm: 1.00 | dt: 1504.25ms | tok/sec: 348,536 | mfu: 53.21 | total time: 13.66m\n",
      "step 00509 (62.83%) | loss: 1.126101 | lrm: 1.00 | dt: 1507.56ms | tok/sec: 347,771 | mfu: 53.10 | total time: 13.69m\n",
      "step 00510 (62.95%) | loss: 1.107666 | lrm: 1.00 | dt: 1639.59ms | tok/sec: 319,767 | mfu: 48.82 | total time: 13.72m\n",
      "step 00511 (63.07%) | loss: 1.120880 | lrm: 1.00 | dt: 2488.16ms | tok/sec: 210,712 | mfu: 32.17 | total time: 13.76m\n",
      "step 00512 (63.20%) | loss: 1.130369 | lrm: 1.00 | dt: 1501.78ms | tok/sec: 349,110 | mfu: 53.30 | total time: 13.78m\n",
      "step 00513 (63.32%) | loss: 1.122218 | lrm: 1.00 | dt: 1501.08ms | tok/sec: 349,272 | mfu: 53.33 | total time: 13.81m\n",
      "step 00514 (63.44%) | loss: 1.118571 | lrm: 1.00 | dt: 1498.19ms | tok/sec: 349,946 | mfu: 53.43 | total time: 13.83m\n",
      "step 00515 (63.55%) | loss: 1.146734 | lrm: 1.00 | dt: 1495.35ms | tok/sec: 350,611 | mfu: 53.53 | total time: 13.86m\n",
      "step 00516 (63.69%) | loss: 1.133144 | lrm: 1.00 | dt: 1504.88ms | tok/sec: 348,392 | mfu: 53.19 | total time: 13.88m\n",
      "step 00517 (63.82%) | loss: 1.134014 | lrm: 1.00 | dt: 1657.45ms | tok/sec: 316,321 | mfu: 48.29 | total time: 13.91m\n",
      "step 00518 (63.94%) | loss: 1.120845 | lrm: 1.00 | dt: 2527.18ms | tok/sec: 207,459 | mfu: 31.67 | total time: 13.95m\n",
      "step 00519 (64.06%) | loss: 1.102602 | lrm: 1.00 | dt: 1505.03ms | tok/sec: 348,358 | mfu: 53.19 | total time: 13.98m\n",
      "step 00520 (64.18%) | loss: 1.103300 | lrm: 1.00 | dt: 1500.74ms | tok/sec: 349,353 | mfu: 53.34 | total time: 14.00m\n",
      "step 00521 (64.29%) | loss: 1.098562 | lrm: 1.00 | dt: 2704.26ms | tok/sec: 193,874 | mfu: 29.60 | total time: 14.05m\n",
      "step 00522 (64.41%) | loss: 1.090815 | lrm: 1.00 | dt: 1649.46ms | tok/sec: 317,853 | mfu: 48.53 | total time: 14.07m\n",
      "step 00523 (64.52%) | loss: 1.106712 | lrm: 1.00 | dt: 2408.91ms | tok/sec: 217,645 | mfu: 33.23 | total time: 14.11m\n",
      "step 00524 (64.64%) | loss: 1.110223 | lrm: 1.00 | dt: 1665.17ms | tok/sec: 314,856 | mfu: 48.07 | total time: 14.14m\n",
      "step 00525 (64.77%) | loss: 1.120540 | lrm: 1.00 | dt: 2343.26ms | tok/sec: 223,742 | mfu: 34.16 | total time: 14.18m\n",
      "step 00526 (64.91%) | loss: 1.123678 | lrm: 1.00 | dt: 1498.67ms | tok/sec: 349,835 | mfu: 53.41 | total time: 14.21m\n",
      "step 00527 (65.03%) | loss: 1.113371 | lrm: 1.00 | dt: 1512.85ms | tok/sec: 346,555 | mfu: 52.91 | total time: 14.23m\n",
      "step 00528 (65.17%) | loss: 1.096089 | lrm: 1.00 | dt: 1660.04ms | tok/sec: 315,828 | mfu: 48.22 | total time: 14.26m\n",
      "step 00529 (65.29%) | loss: 1.092662 | lrm: 1.00 | dt: 2539.95ms | tok/sec: 206,416 | mfu: 31.51 | total time: 14.30m\n",
      "step 00530 (65.42%) | loss: 1.098144 | lrm: 1.00 | dt: 1502.80ms | tok/sec: 348,875 | mfu: 53.26 | total time: 14.33m\n",
      "step 00531 (65.54%) | loss: 1.120224 | lrm: 1.00 | dt: 1499.50ms | tok/sec: 349,641 | mfu: 53.38 | total time: 14.35m\n",
      "step 00532 (65.65%) | loss: 1.109114 | lrm: 1.00 | dt: 1506.04ms | tok/sec: 348,123 | mfu: 53.15 | total time: 14.38m\n",
      "step 00533 (65.78%) | loss: 1.088074 | lrm: 1.00 | dt: 1506.81ms | tok/sec: 347,945 | mfu: 53.12 | total time: 14.40m\n",
      "step 00534 (65.89%) | loss: 1.080304 | lrm: 1.00 | dt: 1669.16ms | tok/sec: 314,102 | mfu: 47.96 | total time: 14.43m\n",
      "step 00535 (66.04%) | loss: 1.073540 | lrm: 1.00 | dt: 2367.25ms | tok/sec: 221,475 | mfu: 33.81 | total time: 14.47m\n",
      "step 00536 (66.17%) | loss: 1.070466 | lrm: 1.00 | dt: 1495.61ms | tok/sec: 350,552 | mfu: 53.52 | total time: 14.49m\n",
      "step 00537 (66.28%) | loss: 1.068378 | lrm: 1.00 | dt: 1497.85ms | tok/sec: 350,027 | mfu: 53.44 | total time: 14.52m\n",
      "step 00538 (66.40%) | loss: 1.074209 | lrm: 1.00 | dt: 1504.36ms | tok/sec: 348,512 | mfu: 53.21 | total time: 14.54m\n",
      "step 00539 (66.54%) | loss: 1.072029 | lrm: 1.00 | dt: 1501.35ms | tok/sec: 349,211 | mfu: 53.32 | total time: 14.57m\n",
      "step 00540 (66.67%) | loss: 1.067126 | lrm: 1.00 | dt: 1512.38ms | tok/sec: 346,663 | mfu: 52.93 | total time: 14.59m\n",
      "step 00541 (66.80%) | loss: 1.077262 | lrm: 1.00 | dt: 1499.84ms | tok/sec: 349,562 | mfu: 53.37 | total time: 14.62m\n",
      "step 00542 (66.93%) | loss: 1.062502 | lrm: 1.00 | dt: 1656.57ms | tok/sec: 316,489 | mfu: 48.32 | total time: 14.65m\n",
      "step 00543 (67.06%) | loss: 1.063946 | lrm: 1.00 | dt: 2520.77ms | tok/sec: 207,986 | mfu: 31.75 | total time: 14.69m\n",
      "step 00544 (67.16%) | loss: 1.058991 | lrm: 1.00 | dt: 1646.62ms | tok/sec: 318,401 | mfu: 48.61 | total time: 14.72m\n",
      "step 00545 (67.28%) | loss: 1.050823 | lrm: 1.00 | dt: 2385.71ms | tok/sec: 219,761 | mfu: 33.55 | total time: 14.76m\n",
      "step 00546 (67.42%) | loss: 1.026989 | lrm: 1.00 | dt: 1651.48ms | tok/sec: 317,465 | mfu: 48.47 | total time: 14.78m\n",
      "step 00547 (67.53%) | loss: 1.038556 | lrm: 1.00 | dt: 2353.15ms | tok/sec: 222,802 | mfu: 34.02 | total time: 14.82m\n",
      "step 00548 (67.67%) | loss: 1.049086 | lrm: 1.00 | dt: 1501.54ms | tok/sec: 349,167 | mfu: 53.31 | total time: 14.85m\n",
      "step 00549 (67.80%) | loss: 1.051696 | lrm: 1.00 | dt: 1510.92ms | tok/sec: 346,999 | mfu: 52.98 | total time: 14.87m\n",
      "step 00550 (67.94%) | loss: 1.064569 | lrm: 1.00 | dt: 1509.95ms | tok/sec: 347,222 | mfu: 53.01 | total time: 14.90m\n",
      "step 00551 (68.08%) | loss: 1.055316 | lrm: 1.00 | dt: 1503.63ms | tok/sec: 348,682 | mfu: 53.23 | total time: 14.92m\n",
      "step 00552 (68.21%) | loss: 1.061078 | lrm: 1.00 | dt: 1665.12ms | tok/sec: 314,864 | mfu: 48.07 | total time: 14.95m\n",
      "step 00553 (68.34%) | loss: 1.069627 | lrm: 1.00 | dt: 2519.45ms | tok/sec: 208,096 | mfu: 31.77 | total time: 14.99m\n",
      "step 00554 (68.46%) | loss: 1.060306 | lrm: 1.00 | dt: 1499.04ms | tok/sec: 349,748 | mfu: 53.40 | total time: 15.02m\n",
      "step 00555 (68.59%) | loss: 1.089023 | lrm: 1.00 | dt: 1512.21ms | tok/sec: 346,702 | mfu: 52.93 | total time: 15.04m\n",
      "step 00556 (68.72%) | loss: 1.097738 | lrm: 1.00 | dt: 1643.47ms | tok/sec: 319,012 | mfu: 48.71 | total time: 15.07m\n",
      "step 00557 (68.85%) | loss: 1.112597 | lrm: 1.00 | dt: 2374.96ms | tok/sec: 220,756 | mfu: 33.70 | total time: 15.11m\n",
      "step 00558 (68.98%) | loss: 1.126743 | lrm: 1.00 | dt: 1493.22ms | tok/sec: 351,112 | mfu: 53.61 | total time: 15.14m\n",
      "step 00559 (69.11%) | loss: 1.138655 | lrm: 1.00 | dt: 1504.62ms | tok/sec: 348,453 | mfu: 53.20 | total time: 15.16m\n",
      "step 00560 (69.22%) | loss: 1.141713 | lrm: 1.00 | dt: 1502.37ms | tok/sec: 348,974 | mfu: 53.28 | total time: 15.19m\n",
      "step 00561 (69.34%) | loss: 1.136191 | lrm: 1.00 | dt: 1498.75ms | tok/sec: 349,817 | mfu: 53.41 | total time: 15.21m\n",
      "step 00562 (69.48%) | loss: 1.127964 | lrm: 1.00 | dt: 1503.22ms | tok/sec: 348,776 | mfu: 53.25 | total time: 15.24m\n",
      "step 00563 (69.62%) | loss: 1.139904 | lrm: 1.00 | dt: 1500.98ms | tok/sec: 349,297 | mfu: 53.33 | total time: 15.26m\n",
      "step 00564 (69.73%) | loss: 1.116944 | lrm: 1.00 | dt: 1504.99ms | tok/sec: 348,366 | mfu: 53.19 | total time: 15.29m\n",
      "step 00565 (69.85%) | loss: 1.112342 | lrm: 1.00 | dt: 1498.91ms | tok/sec: 349,778 | mfu: 53.40 | total time: 15.31m\n",
      "step 00566 (69.98%) | loss: 1.125957 | lrm: 1.00 | dt: 1501.65ms | tok/sec: 349,141 | mfu: 53.30 | total time: 15.34m\n",
      "step 00567 (70.09%) | loss: 1.130821 | lrm: 1.00 | dt: 1671.30ms | tok/sec: 313,700 | mfu: 47.89 | total time: 15.36m\n",
      "step 00568 (70.22%) | loss: 1.136495 | lrm: 1.00 | dt: 3355.31ms | tok/sec: 156,256 | mfu: 23.86 | total time: 15.42m\n",
      "step 00569 (70.36%) | loss: 1.137416 | lrm: 1.00 | dt: 1667.96ms | tok/sec: 314,328 | mfu: 47.99 | total time: 15.45m\n",
      "step 00570 (70.50%) | loss: 1.136839 | lrm: 1.00 | dt: 2513.40ms | tok/sec: 208,597 | mfu: 31.85 | total time: 15.49m\n",
      "step 00571 (70.62%) | loss: 1.144242 | lrm: 1.00 | dt: 1492.77ms | tok/sec: 351,218 | mfu: 53.62 | total time: 15.51m\n",
      "step 00572 (70.71%) | loss: 1.142324 | lrm: 1.00 | dt: 1500.49ms | tok/sec: 349,410 | mfu: 53.35 | total time: 15.54m\n",
      "step 00573 (70.84%) | loss: 1.145587 | lrm: 1.00 | dt: 1503.11ms | tok/sec: 348,801 | mfu: 53.25 | total time: 15.56m\n",
      "step 00574 (70.97%) | loss: 1.128368 | lrm: 1.00 | dt: 1501.20ms | tok/sec: 349,245 | mfu: 53.32 | total time: 15.59m\n",
      "step 00575 (71.08%) | loss: 1.116049 | lrm: 1.00 | dt: 1510.01ms | tok/sec: 347,209 | mfu: 53.01 | total time: 15.61m\n",
      "step 00576 (71.19%) | loss: 1.122281 | lrm: 1.00 | dt: 1503.04ms | tok/sec: 348,818 | mfu: 53.26 | total time: 15.64m\n",
      "step 00577 (71.30%) | loss: 1.118013 | lrm: 1.00 | dt: 1505.34ms | tok/sec: 348,285 | mfu: 53.17 | total time: 15.66m\n",
      "step 00578 (71.41%) | loss: 1.099386 | lrm: 1.00 | dt: 1509.44ms | tok/sec: 347,339 | mfu: 53.03 | total time: 15.69m\n",
      "step 00579 (71.55%) | loss: 1.100425 | lrm: 1.00 | dt: 1501.82ms | tok/sec: 349,101 | mfu: 53.30 | total time: 15.71m\n",
      "step 00580 (71.68%) | loss: 1.120850 | lrm: 1.00 | dt: 1506.62ms | tok/sec: 347,990 | mfu: 53.13 | total time: 15.74m\n",
      "step 00581 (71.81%) | loss: 1.117446 | lrm: 1.00 | dt: 1508.05ms | tok/sec: 347,660 | mfu: 53.08 | total time: 15.76m\n",
      "step 00582 (71.94%) | loss: 1.126275 | lrm: 1.00 | dt: 1502.49ms | tok/sec: 348,945 | mfu: 53.28 | total time: 15.79m\n",
      "step 00583 (72.07%) | loss: 1.120026 | lrm: 1.00 | dt: 1658.88ms | tok/sec: 316,048 | mfu: 48.25 | total time: 15.82m\n",
      "step 00584 (72.18%) | loss: 1.118741 | lrm: 1.00 | dt: 2540.86ms | tok/sec: 206,342 | mfu: 31.50 | total time: 15.86m\n",
      "step 00585 (72.28%) | loss: 1.115780 | lrm: 1.00 | dt: 1498.10ms | tok/sec: 349,968 | mfu: 53.43 | total time: 15.88m\n",
      "step 00586 (72.42%) | loss: 1.121109 | lrm: 1.00 | dt: 1501.55ms | tok/sec: 349,164 | mfu: 53.31 | total time: 15.91m\n",
      "step 00587 (72.55%) | loss: 1.106862 | lrm: 1.00 | dt: 1506.76ms | tok/sec: 347,956 | mfu: 53.12 | total time: 15.93m\n",
      "step 00588 (72.67%) | loss: 1.116849 | lrm: 1.00 | dt: 1501.75ms | tok/sec: 349,117 | mfu: 53.30 | total time: 15.96m\n",
      "step 00589 (72.80%) | loss: 1.135102 | lrm: 1.00 | dt: 1500.43ms | tok/sec: 349,425 | mfu: 53.35 | total time: 15.98m\n",
      "step 00590 (72.92%) | loss: 1.141044 | lrm: 1.00 | dt: 1503.16ms | tok/sec: 348,791 | mfu: 53.25 | total time: 16.01m\n",
      "step 00591 (73.04%) | loss: 1.148382 | lrm: 1.00 | dt: 1507.48ms | tok/sec: 347,790 | mfu: 53.10 | total time: 16.04m\n",
      "step 00592 (73.19%) | loss: 1.124577 | lrm: 1.00 | dt: 1504.43ms | tok/sec: 348,495 | mfu: 53.21 | total time: 16.06m\n",
      "step 00593 (73.31%) | loss: 1.107940 | lrm: 1.00 | dt: 1504.68ms | tok/sec: 348,439 | mfu: 53.20 | total time: 16.09m\n",
      "step 00594 (73.42%) | loss: 1.090895 | lrm: 1.00 | dt: 1510.30ms | tok/sec: 347,141 | mfu: 53.00 | total time: 16.11m\n",
      "step 00595 (73.54%) | loss: 1.097259 | lrm: 1.00 | dt: 1503.91ms | tok/sec: 348,616 | mfu: 53.22 | total time: 16.14m\n",
      "step 00596 (73.66%) | loss: 1.089122 | lrm: 1.00 | dt: 1501.15ms | tok/sec: 349,256 | mfu: 53.32 | total time: 16.16m\n",
      "step 00597 (73.77%) | loss: 1.091404 | lrm: 1.00 | dt: 1510.49ms | tok/sec: 347,097 | mfu: 52.99 | total time: 16.19m\n",
      "step 00598 (73.92%) | loss: 1.119137 | lrm: 1.00 | dt: 1501.27ms | tok/sec: 349,228 | mfu: 53.32 | total time: 16.21m\n",
      "step 00599 (74.05%) | loss: 1.125548 | lrm: 1.00 | dt: 1505.62ms | tok/sec: 348,220 | mfu: 53.16 | total time: 16.24m\n",
      "step 00600 (74.16%) | loss: 1.124510 | lrm: 1.00 | dt: 1502.50ms | tok/sec: 348,943 | mfu: 53.27 | total time: 16.26m\n",
      "step 00600 | Validation bpb: 0.3509\n",
      "step 00601 (74.26%) | loss: 1.118453 | lrm: 1.00 | dt: 1501.64ms | tok/sec: 349,142 | mfu: 53.31 | total time: 16.29m\n",
      "step 00602 (74.38%) | loss: 1.111627 | lrm: 1.00 | dt: 1504.01ms | tok/sec: 348,593 | mfu: 53.22 | total time: 16.31m\n",
      "step 00603 (74.49%) | loss: 1.124617 | lrm: 1.00 | dt: 1499.87ms | tok/sec: 349,554 | mfu: 53.37 | total time: 16.34m\n",
      "step 00604 (74.61%) | loss: 1.097606 | lrm: 1.00 | dt: 1509.37ms | tok/sec: 347,354 | mfu: 53.03 | total time: 16.36m\n",
      "step 00605 (74.72%) | loss: 1.106599 | lrm: 1.00 | dt: 1506.79ms | tok/sec: 347,949 | mfu: 53.12 | total time: 16.39m\n",
      "step 00606 (74.84%) | loss: 1.098549 | lrm: 1.00 | dt: 1502.63ms | tok/sec: 348,913 | mfu: 53.27 | total time: 16.41m\n",
      "step 00607 (74.98%) | loss: 1.085331 | lrm: 1.00 | dt: 1500.06ms | tok/sec: 349,510 | mfu: 53.36 | total time: 16.44m\n",
      "step 00608 (75.12%) | loss: 1.071109 | lrm: 1.00 | dt: 1505.30ms | tok/sec: 348,294 | mfu: 53.18 | total time: 16.46m\n",
      "step 00609 (75.22%) | loss: 1.077889 | lrm: 1.00 | dt: 1686.45ms | tok/sec: 310,883 | mfu: 47.46 | total time: 16.49m\n",
      "step 00610 (75.33%) | loss: 1.078992 | lrm: 1.00 | dt: 2464.44ms | tok/sec: 212,741 | mfu: 32.48 | total time: 16.53m\n",
      "step 00611 (75.45%) | loss: 1.088091 | lrm: 1.00 | dt: 1507.43ms | tok/sec: 347,803 | mfu: 53.10 | total time: 16.56m\n",
      "step 00612 (75.57%) | loss: 1.088712 | lrm: 1.00 | dt: 1501.89ms | tok/sec: 349,086 | mfu: 53.30 | total time: 16.58m\n",
      "step 00613 (75.69%) | loss: 1.071799 | lrm: 1.00 | dt: 1499.72ms | tok/sec: 349,590 | mfu: 53.37 | total time: 16.61m\n",
      "step 00614 (75.82%) | loss: 1.097687 | lrm: 1.00 | dt: 1501.10ms | tok/sec: 349,269 | mfu: 53.32 | total time: 16.63m\n",
      "step 00615 (75.93%) | loss: 1.095927 | lrm: 1.00 | dt: 1498.10ms | tok/sec: 349,968 | mfu: 53.43 | total time: 16.66m\n",
      "step 00616 (76.07%) | loss: 1.101331 | lrm: 1.00 | dt: 1502.79ms | tok/sec: 348,876 | mfu: 53.26 | total time: 16.68m\n",
      "step 00617 (76.18%) | loss: 1.104813 | lrm: 1.00 | dt: 1653.96ms | tok/sec: 316,989 | mfu: 48.40 | total time: 16.71m\n",
      "step 00618 (76.31%) | loss: 1.111379 | lrm: 1.00 | dt: 2403.62ms | tok/sec: 218,124 | mfu: 33.30 | total time: 16.75m\n",
      "step 00619 (76.44%) | loss: 1.125196 | lrm: 1.00 | dt: 1504.72ms | tok/sec: 348,428 | mfu: 53.20 | total time: 16.77m\n",
      "step 00620 (76.54%) | loss: 1.118626 | lrm: 1.00 | dt: 1513.42ms | tok/sec: 346,426 | mfu: 52.89 | total time: 16.80m\n",
      "step 00621 (76.68%) | loss: 1.115254 | lrm: 1.00 | dt: 1507.55ms | tok/sec: 347,775 | mfu: 53.10 | total time: 16.82m\n",
      "step 00622 (76.78%) | loss: 1.108818 | lrm: 1.00 | dt: 1501.32ms | tok/sec: 349,217 | mfu: 53.32 | total time: 16.85m\n",
      "step 00623 (76.91%) | loss: 1.121038 | lrm: 1.00 | dt: 1500.90ms | tok/sec: 349,314 | mfu: 53.33 | total time: 16.87m\n",
      "step 00624 (77.02%) | loss: 1.116348 | lrm: 1.00 | dt: 1499.74ms | tok/sec: 349,584 | mfu: 53.37 | total time: 16.90m\n",
      "step 00625 (77.16%) | loss: 1.126094 | lrm: 1.00 | dt: 1503.07ms | tok/sec: 348,810 | mfu: 53.25 | total time: 16.92m\n",
      "step 00626 (77.27%) | loss: 1.120361 | lrm: 1.00 | dt: 1504.95ms | tok/sec: 348,376 | mfu: 53.19 | total time: 16.95m\n",
      "step 00627 (77.38%) | loss: 1.106941 | lrm: 1.00 | dt: 1508.01ms | tok/sec: 347,668 | mfu: 53.08 | total time: 16.97m\n",
      "step 00628 (77.49%) | loss: 1.099735 | lrm: 1.00 | dt: 1507.53ms | tok/sec: 347,780 | mfu: 53.10 | total time: 17.00m\n",
      "step 00629 (77.63%) | loss: 1.121575 | lrm: 1.00 | dt: 1665.57ms | tok/sec: 314,780 | mfu: 48.06 | total time: 17.03m\n",
      "step 00630 (77.77%) | loss: 1.123248 | lrm: 1.00 | dt: 2462.15ms | tok/sec: 212,939 | mfu: 32.51 | total time: 17.07m\n",
      "step 00631 (77.89%) | loss: 1.111230 | lrm: 1.00 | dt: 1499.45ms | tok/sec: 349,652 | mfu: 53.38 | total time: 17.09m\n",
      "step 00632 (78.01%) | loss: 1.120515 | lrm: 1.00 | dt: 1668.86ms | tok/sec: 314,158 | mfu: 47.96 | total time: 17.12m\n",
      "step 00633 (78.12%) | loss: 1.102479 | lrm: 1.00 | dt: 2358.71ms | tok/sec: 222,277 | mfu: 33.94 | total time: 17.16m\n",
      "step 00634 (78.24%) | loss: 1.090890 | lrm: 1.00 | dt: 1499.16ms | tok/sec: 349,722 | mfu: 53.39 | total time: 17.19m\n",
      "step 00635 (78.36%) | loss: 1.087023 | lrm: 1.00 | dt: 1498.31ms | tok/sec: 349,918 | mfu: 53.42 | total time: 17.21m\n",
      "step 00636 (78.48%) | loss: 1.080291 | lrm: 1.00 | dt: 1503.63ms | tok/sec: 348,682 | mfu: 53.23 | total time: 17.24m\n",
      "step 00637 (78.61%) | loss: 1.088004 | lrm: 1.00 | dt: 1495.81ms | tok/sec: 350,504 | mfu: 53.51 | total time: 17.26m\n",
      "step 00638 (78.72%) | loss: 1.085341 | lrm: 1.00 | dt: 1498.57ms | tok/sec: 349,858 | mfu: 53.41 | total time: 17.28m\n",
      "step 00639 (78.84%) | loss: 1.097933 | lrm: 1.00 | dt: 1501.16ms | tok/sec: 349,256 | mfu: 53.32 | total time: 17.31m\n",
      "step 00640 (78.94%) | loss: 1.104858 | lrm: 1.00 | dt: 1498.51ms | tok/sec: 349,872 | mfu: 53.42 | total time: 17.33m\n",
      "step 00641 (79.07%) | loss: 1.108430 | lrm: 1.00 | dt: 1498.77ms | tok/sec: 349,811 | mfu: 53.41 | total time: 17.36m\n",
      "step 00642 (79.19%) | loss: 1.097250 | lrm: 1.00 | dt: 1504.80ms | tok/sec: 348,410 | mfu: 53.19 | total time: 17.39m\n",
      "step 00643 (79.31%) | loss: 1.093407 | lrm: 1.00 | dt: 1504.22ms | tok/sec: 348,543 | mfu: 53.21 | total time: 17.41m\n",
      "step 00644 (79.46%) | loss: 1.087289 | lrm: 1.00 | dt: 1503.66ms | tok/sec: 348,675 | mfu: 53.23 | total time: 17.44m\n",
      "step 00645 (79.58%) | loss: 1.098079 | lrm: 1.00 | dt: 1504.19ms | tok/sec: 348,552 | mfu: 53.22 | total time: 17.46m\n",
      "step 00646 (79.68%) | loss: 1.092696 | lrm: 1.00 | dt: 1501.56ms | tok/sec: 349,161 | mfu: 53.31 | total time: 17.49m\n",
      "step 00647 (79.83%) | loss: 1.100535 | lrm: 1.00 | dt: 1513.09ms | tok/sec: 346,501 | mfu: 52.90 | total time: 17.51m\n",
      "step 00648 (79.95%) | loss: 1.088322 | lrm: 1.00 | dt: 1506.59ms | tok/sec: 347,996 | mfu: 53.13 | total time: 17.54m\n",
      "step 00649 (80.07%) | loss: 1.090698 | lrm: 1.00 | dt: 1510.47ms | tok/sec: 347,102 | mfu: 52.99 | total time: 17.56m\n",
      "step 00650 (80.17%) | loss: 1.086985 | lrm: 0.99 | dt: 1499.41ms | tok/sec: 349,663 | mfu: 53.38 | total time: 17.59m\n",
      "step 00651 (80.29%) | loss: 1.075565 | lrm: 0.99 | dt: 1501.91ms | tok/sec: 349,079 | mfu: 53.30 | total time: 17.61m\n",
      "step 00652 (80.40%) | loss: 1.048395 | lrm: 0.98 | dt: 1507.10ms | tok/sec: 347,878 | mfu: 53.11 | total time: 17.64m\n",
      "step 00653 (80.53%) | loss: 1.061468 | lrm: 0.97 | dt: 1498.87ms | tok/sec: 349,787 | mfu: 53.40 | total time: 17.66m\n",
      "step 00654 (80.66%) | loss: 1.064486 | lrm: 0.97 | dt: 1506.03ms | tok/sec: 348,126 | mfu: 53.15 | total time: 17.69m\n",
      "step 00655 (80.78%) | loss: 1.058615 | lrm: 0.96 | dt: 1500.57ms | tok/sec: 349,391 | mfu: 53.34 | total time: 17.71m\n",
      "step 00656 (80.90%) | loss: 1.042718 | lrm: 0.96 | dt: 1499.90ms | tok/sec: 349,549 | mfu: 53.37 | total time: 17.74m\n",
      "step 00657 (81.02%) | loss: 1.028278 | lrm: 0.95 | dt: 1503.47ms | tok/sec: 348,718 | mfu: 53.24 | total time: 17.76m\n",
      "step 00658 (81.13%) | loss: 1.038323 | lrm: 0.94 | dt: 1510.22ms | tok/sec: 347,160 | mfu: 53.00 | total time: 17.79m\n",
      "step 00659 (81.26%) | loss: 1.038930 | lrm: 0.94 | dt: 1500.27ms | tok/sec: 349,461 | mfu: 53.35 | total time: 17.81m\n",
      "step 00660 (81.38%) | loss: 1.033488 | lrm: 0.93 | dt: 1503.78ms | tok/sec: 348,647 | mfu: 53.23 | total time: 17.84m\n",
      "step 00661 (81.51%) | loss: 1.036228 | lrm: 0.92 | dt: 1497.26ms | tok/sec: 350,164 | mfu: 53.46 | total time: 17.86m\n",
      "step 00662 (81.63%) | loss: 1.048497 | lrm: 0.92 | dt: 1499.92ms | tok/sec: 349,544 | mfu: 53.37 | total time: 17.89m\n",
      "step 00663 (81.76%) | loss: 1.065670 | lrm: 0.91 | dt: 1513.39ms | tok/sec: 346,431 | mfu: 52.89 | total time: 17.91m\n",
      "step 00664 (81.89%) | loss: 1.081541 | lrm: 0.91 | dt: 1504.70ms | tok/sec: 348,433 | mfu: 53.20 | total time: 17.94m\n",
      "step 00665 (82.02%) | loss: 1.093189 | lrm: 0.90 | dt: 1503.37ms | tok/sec: 348,742 | mfu: 53.24 | total time: 17.96m\n",
      "step 00666 (82.13%) | loss: 1.092904 | lrm: 0.89 | dt: 1507.81ms | tok/sec: 347,713 | mfu: 53.09 | total time: 17.99m\n",
      "step 00667 (82.26%) | loss: 1.079966 | lrm: 0.89 | dt: 1508.11ms | tok/sec: 347,645 | mfu: 53.08 | total time: 18.01m\n",
      "step 00668 (82.39%) | loss: 1.069499 | lrm: 0.88 | dt: 1506.68ms | tok/sec: 347,976 | mfu: 53.13 | total time: 18.04m\n",
      "step 00669 (82.51%) | loss: 1.089680 | lrm: 0.87 | dt: 1505.27ms | tok/sec: 348,301 | mfu: 53.18 | total time: 18.06m\n",
      "step 00670 (82.62%) | loss: 1.079401 | lrm: 0.87 | dt: 1676.09ms | tok/sec: 312,804 | mfu: 47.76 | total time: 18.09m\n",
      "step 00671 (82.74%) | loss: 1.112704 | lrm: 0.86 | dt: 2576.56ms | tok/sec: 203,483 | mfu: 31.07 | total time: 18.13m\n",
      "step 00672 (82.87%) | loss: 1.127171 | lrm: 0.86 | dt: 2524.42ms | tok/sec: 207,686 | mfu: 31.71 | total time: 18.18m\n",
      "step 00673 (82.98%) | loss: 1.100337 | lrm: 0.85 | dt: 2540.23ms | tok/sec: 206,394 | mfu: 31.51 | total time: 18.22m\n",
      "step 00674 (83.10%) | loss: 1.075419 | lrm: 0.85 | dt: 1501.96ms | tok/sec: 349,069 | mfu: 53.29 | total time: 18.24m\n",
      "step 00675 (83.21%) | loss: 1.054768 | lrm: 0.84 | dt: 1652.47ms | tok/sec: 317,275 | mfu: 48.44 | total time: 18.27m\n",
      "step 00676 (83.33%) | loss: 1.060065 | lrm: 0.83 | dt: 2378.61ms | tok/sec: 220,418 | mfu: 33.65 | total time: 18.31m\n",
      "step 00677 (83.45%) | loss: 1.077019 | lrm: 0.83 | dt: 1501.80ms | tok/sec: 349,105 | mfu: 53.30 | total time: 18.33m\n",
      "step 00678 (83.58%) | loss: 1.076132 | lrm: 0.82 | dt: 1504.53ms | tok/sec: 348,473 | mfu: 53.20 | total time: 18.36m\n",
      "step 00679 (83.70%) | loss: 1.074116 | lrm: 0.81 | dt: 1499.41ms | tok/sec: 349,663 | mfu: 53.38 | total time: 18.38m\n",
      "step 00680 (83.83%) | loss: 1.071911 | lrm: 0.81 | dt: 1613.34ms | tok/sec: 324,970 | mfu: 49.61 | total time: 18.41m\n",
      "step 00681 (83.94%) | loss: 1.076270 | lrm: 0.80 | dt: 2587.00ms | tok/sec: 202,662 | mfu: 30.94 | total time: 18.45m\n",
      "step 00682 (84.06%) | loss: 1.089068 | lrm: 0.80 | dt: 1500.24ms | tok/sec: 349,468 | mfu: 53.35 | total time: 18.48m\n",
      "step 00683 (84.19%) | loss: 1.076523 | lrm: 0.79 | dt: 1507.77ms | tok/sec: 347,723 | mfu: 53.09 | total time: 18.50m\n",
      "step 00684 (84.30%) | loss: 1.081246 | lrm: 0.78 | dt: 1499.44ms | tok/sec: 349,656 | mfu: 53.38 | total time: 18.53m\n",
      "step 00685 (84.42%) | loss: 1.073417 | lrm: 0.78 | dt: 1501.36ms | tok/sec: 349,209 | mfu: 53.32 | total time: 18.55m\n",
      "step 00686 (84.53%) | loss: 1.070564 | lrm: 0.77 | dt: 1499.54ms | tok/sec: 349,633 | mfu: 53.38 | total time: 18.58m\n",
      "step 00687 (84.65%) | loss: 1.093638 | lrm: 0.77 | dt: 1651.66ms | tok/sec: 317,431 | mfu: 48.46 | total time: 18.61m\n",
      "step 00688 (84.76%) | loss: 1.090892 | lrm: 0.76 | dt: 2383.98ms | tok/sec: 219,920 | mfu: 33.58 | total time: 18.65m\n",
      "step 00689 (84.87%) | loss: 1.076774 | lrm: 0.76 | dt: 1493.52ms | tok/sec: 351,042 | mfu: 53.60 | total time: 18.67m\n",
      "step 00690 (85.00%) | loss: 1.085518 | lrm: 0.75 | dt: 1504.30ms | tok/sec: 348,526 | mfu: 53.21 | total time: 18.70m\n",
      "step 00691 (85.14%) | loss: 1.095279 | lrm: 0.74 | dt: 1503.73ms | tok/sec: 348,657 | mfu: 53.23 | total time: 18.72m\n",
      "step 00692 (85.25%) | loss: 1.111322 | lrm: 0.74 | dt: 1508.15ms | tok/sec: 347,635 | mfu: 53.08 | total time: 18.75m\n",
      "step 00693 (85.36%) | loss: 1.113329 | lrm: 0.73 | dt: 1497.94ms | tok/sec: 350,006 | mfu: 53.44 | total time: 18.77m\n",
      "step 00694 (85.50%) | loss: 1.143383 | lrm: 0.73 | dt: 1659.32ms | tok/sec: 315,965 | mfu: 48.24 | total time: 18.80m\n",
      "step 00695 (85.61%) | loss: 1.145703 | lrm: 0.72 | dt: 2541.58ms | tok/sec: 206,283 | mfu: 31.49 | total time: 18.84m\n",
      "step 00696 (85.76%) | loss: 1.136517 | lrm: 0.71 | dt: 1654.63ms | tok/sec: 316,861 | mfu: 48.38 | total time: 18.87m\n",
      "step 00697 (85.87%) | loss: 1.129472 | lrm: 0.71 | dt: 3395.27ms | tok/sec: 154,417 | mfu: 23.58 | total time: 18.93m\n",
      "step 00698 (86.01%) | loss: 1.136303 | lrm: 0.70 | dt: 1496.42ms | tok/sec: 350,362 | mfu: 53.49 | total time: 18.95m\n",
      "step 00699 (86.15%) | loss: 1.145336 | lrm: 0.69 | dt: 1500.78ms | tok/sec: 349,343 | mfu: 53.34 | total time: 18.98m\n",
      "step 00700 (86.28%) | loss: 1.143461 | lrm: 0.69 | dt: 1498.54ms | tok/sec: 349,864 | mfu: 53.42 | total time: 19.00m\n",
      "step 00701 (86.39%) | loss: 1.128501 | lrm: 0.68 | dt: 1498.40ms | tok/sec: 349,898 | mfu: 53.42 | total time: 19.03m\n",
      "step 00702 (86.50%) | loss: 1.131637 | lrm: 0.67 | dt: 1656.03ms | tok/sec: 316,594 | mfu: 48.34 | total time: 19.05m\n",
      "step 00703 (86.63%) | loss: 1.124142 | lrm: 0.67 | dt: 2540.20ms | tok/sec: 206,396 | mfu: 31.51 | total time: 19.10m\n",
      "step 00704 (86.77%) | loss: 1.116997 | lrm: 0.66 | dt: 1495.71ms | tok/sec: 350,528 | mfu: 53.52 | total time: 19.12m\n",
      "step 00705 (86.89%) | loss: 1.125689 | lrm: 0.66 | dt: 1502.23ms | tok/sec: 349,006 | mfu: 53.28 | total time: 19.15m\n",
      "step 00706 (87.02%) | loss: 1.120085 | lrm: 0.65 | dt: 1653.92ms | tok/sec: 316,997 | mfu: 48.40 | total time: 19.17m\n",
      "step 00707 (87.14%) | loss: 1.112510 | lrm: 0.64 | dt: 2382.72ms | tok/sec: 220,037 | mfu: 33.59 | total time: 19.21m\n",
      "step 00708 (87.26%) | loss: 1.104254 | lrm: 0.64 | dt: 1654.34ms | tok/sec: 316,916 | mfu: 48.39 | total time: 19.24m\n",
      "step 00709 (87.38%) | loss: 1.099654 | lrm: 0.63 | dt: 2349.44ms | tok/sec: 223,154 | mfu: 34.07 | total time: 19.28m\n",
      "step 00710 (87.48%) | loss: 1.093558 | lrm: 0.63 | dt: 1504.47ms | tok/sec: 348,487 | mfu: 53.21 | total time: 19.31m\n",
      "step 00711 (87.60%) | loss: 1.075489 | lrm: 0.62 | dt: 1503.20ms | tok/sec: 348,780 | mfu: 53.25 | total time: 19.33m\n",
      "step 00712 (87.73%) | loss: 1.077636 | lrm: 0.61 | dt: 1641.80ms | tok/sec: 319,336 | mfu: 48.75 | total time: 19.36m\n",
      "step 00713 (87.85%) | loss: 1.091433 | lrm: 0.61 | dt: 2561.53ms | tok/sec: 204,677 | mfu: 31.25 | total time: 19.40m\n",
      "step 00714 (87.96%) | loss: 1.076843 | lrm: 0.60 | dt: 1662.55ms | tok/sec: 315,351 | mfu: 48.15 | total time: 19.43m\n",
      "step 00715 (88.09%) | loss: 1.090613 | lrm: 0.60 | dt: 2375.54ms | tok/sec: 220,703 | mfu: 33.70 | total time: 19.47m\n",
      "step 00716 (88.21%) | loss: 1.081037 | lrm: 0.59 | dt: 1664.45ms | tok/sec: 314,990 | mfu: 48.09 | total time: 19.50m\n",
      "step 00717 (88.34%) | loss: 1.077678 | lrm: 0.58 | dt: 2362.58ms | tok/sec: 221,913 | mfu: 33.88 | total time: 19.53m\n",
      "step 00718 (88.48%) | loss: 1.093394 | lrm: 0.58 | dt: 1497.49ms | tok/sec: 350,110 | mfu: 53.45 | total time: 19.56m\n",
      "step 00719 (88.61%) | loss: 1.093466 | lrm: 0.57 | dt: 1501.09ms | tok/sec: 349,270 | mfu: 53.32 | total time: 19.58m\n",
      "step 00720 (88.72%) | loss: 1.092288 | lrm: 0.56 | dt: 1510.41ms | tok/sec: 347,116 | mfu: 53.00 | total time: 19.61m\n",
      "step 00721 (88.84%) | loss: 1.076630 | lrm: 0.56 | dt: 1496.02ms | tok/sec: 350,454 | mfu: 53.51 | total time: 19.63m\n",
      "step 00722 (88.95%) | loss: 1.079747 | lrm: 0.55 | dt: 1502.53ms | tok/sec: 348,937 | mfu: 53.27 | total time: 19.66m\n",
      "step 00723 (89.08%) | loss: 1.078552 | lrm: 0.55 | dt: 1503.19ms | tok/sec: 348,783 | mfu: 53.25 | total time: 19.68m\n",
      "step 00724 (89.20%) | loss: 1.078324 | lrm: 0.54 | dt: 1513.00ms | tok/sec: 346,523 | mfu: 52.91 | total time: 19.71m\n",
      "step 00725 (89.33%) | loss: 1.072581 | lrm: 0.53 | dt: 1499.45ms | tok/sec: 349,652 | mfu: 53.38 | total time: 19.74m\n",
      "step 00726 (89.44%) | loss: 1.065887 | lrm: 0.53 | dt: 1501.68ms | tok/sec: 349,133 | mfu: 53.30 | total time: 19.76m\n",
      "step 00727 (89.54%) | loss: 1.074052 | lrm: 0.52 | dt: 1503.69ms | tok/sec: 348,668 | mfu: 53.23 | total time: 19.79m\n",
      "step 00728 (89.66%) | loss: 1.090751 | lrm: 0.52 | dt: 1498.43ms | tok/sec: 349,891 | mfu: 53.42 | total time: 19.81m\n",
      "step 00729 (89.79%) | loss: 1.076670 | lrm: 0.51 | dt: 1500.73ms | tok/sec: 349,354 | mfu: 53.34 | total time: 19.84m\n",
      "step 00730 (89.91%) | loss: 1.099346 | lrm: 0.50 | dt: 1511.96ms | tok/sec: 346,759 | mfu: 52.94 | total time: 19.86m\n",
      "step 00731 (90.04%) | loss: 1.118384 | lrm: 0.50 | dt: 1504.23ms | tok/sec: 348,542 | mfu: 53.21 | total time: 19.89m\n",
      "step 00732 (90.17%) | loss: 1.112130 | lrm: 0.49 | dt: 1510.60ms | tok/sec: 347,072 | mfu: 52.99 | total time: 19.91m\n",
      "step 00733 (90.30%) | loss: 1.127304 | lrm: 0.48 | dt: 1503.08ms | tok/sec: 348,808 | mfu: 53.25 | total time: 19.94m\n",
      "step 00734 (90.43%) | loss: 1.106116 | lrm: 0.48 | dt: 1506.49ms | tok/sec: 348,020 | mfu: 53.13 | total time: 19.96m\n",
      "step 00735 (90.55%) | loss: 1.140817 | lrm: 0.47 | dt: 1511.24ms | tok/sec: 346,925 | mfu: 52.97 | total time: 19.99m\n",
      "step 00736 (90.68%) | loss: 1.136536 | lrm: 0.47 | dt: 1502.47ms | tok/sec: 348,950 | mfu: 53.28 | total time: 20.01m\n",
      "step 00737 (90.82%) | loss: 1.131296 | lrm: 0.46 | dt: 1655.60ms | tok/sec: 316,675 | mfu: 48.35 | total time: 20.04m\n",
      "step 00738 (90.95%) | loss: 1.129635 | lrm: 0.45 | dt: 3474.59ms | tok/sec: 150,891 | mfu: 23.04 | total time: 20.10m\n",
      "step 00739 (91.06%) | loss: 1.119991 | lrm: 0.45 | dt: 1497.08ms | tok/sec: 350,205 | mfu: 53.47 | total time: 20.12m\n",
      "step 00740 (91.18%) | loss: 1.113917 | lrm: 0.44 | dt: 1498.81ms | tok/sec: 349,801 | mfu: 53.41 | total time: 20.15m\n",
      "step 00741 (91.30%) | loss: 1.092517 | lrm: 0.43 | dt: 1498.18ms | tok/sec: 349,950 | mfu: 53.43 | total time: 20.17m\n",
      "step 00742 (91.41%) | loss: 1.084678 | lrm: 0.43 | dt: 1499.08ms | tok/sec: 349,739 | mfu: 53.40 | total time: 20.20m\n",
      "step 00743 (91.53%) | loss: 1.097296 | lrm: 0.42 | dt: 1501.22ms | tok/sec: 349,240 | mfu: 53.32 | total time: 20.22m\n",
      "step 00744 (91.68%) | loss: 1.089271 | lrm: 0.42 | dt: 1502.39ms | tok/sec: 348,968 | mfu: 53.28 | total time: 20.25m\n",
      "step 00745 (91.79%) | loss: 1.088032 | lrm: 0.41 | dt: 1500.07ms | tok/sec: 349,508 | mfu: 53.36 | total time: 20.27m\n",
      "step 00746 (91.91%) | loss: 1.076527 | lrm: 0.40 | dt: 1498.09ms | tok/sec: 349,970 | mfu: 53.43 | total time: 20.30m\n",
      "step 00747 (92.04%) | loss: 1.079302 | lrm: 0.40 | dt: 1498.21ms | tok/sec: 349,943 | mfu: 53.43 | total time: 20.32m\n",
      "step 00748 (92.15%) | loss: 1.073528 | lrm: 0.39 | dt: 1667.33ms | tok/sec: 314,447 | mfu: 48.01 | total time: 20.35m\n",
      "step 00749 (92.27%) | loss: 1.074870 | lrm: 0.39 | dt: 2498.60ms | tok/sec: 209,832 | mfu: 32.04 | total time: 20.39m\n",
      "step 00750 (92.38%) | loss: 1.067753 | lrm: 0.38 | dt: 2547.74ms | tok/sec: 205,785 | mfu: 31.42 | total time: 20.43m\n",
      "step 00750 | Validation bpb: 0.3419\n",
      "step 00751 (92.50%) | loss: 1.069659 | lrm: 0.37 | dt: 1496.33ms | tok/sec: 350,383 | mfu: 53.49 | total time: 20.46m\n",
      "step 00752 (92.62%) | loss: 1.107473 | lrm: 0.37 | dt: 1501.59ms | tok/sec: 349,155 | mfu: 53.31 | total time: 20.48m\n",
      "step 00753 (92.75%) | loss: 1.104738 | lrm: 0.36 | dt: 1504.51ms | tok/sec: 348,476 | mfu: 53.20 | total time: 20.51m\n",
      "step 00754 (92.87%) | loss: 1.110667 | lrm: 0.36 | dt: 1498.33ms | tok/sec: 349,914 | mfu: 53.42 | total time: 20.53m\n",
      "step 00755 (92.99%) | loss: 1.090833 | lrm: 0.35 | dt: 1496.78ms | tok/sec: 350,277 | mfu: 53.48 | total time: 20.56m\n",
      "step 00756 (93.10%) | loss: 1.068221 | lrm: 0.35 | dt: 1505.63ms | tok/sec: 348,217 | mfu: 53.16 | total time: 20.58m\n",
      "step 00757 (93.23%) | loss: 1.058864 | lrm: 0.34 | dt: 1659.24ms | tok/sec: 315,981 | mfu: 48.24 | total time: 20.61m\n",
      "step 00758 (93.36%) | loss: 1.046968 | lrm: 0.33 | dt: 2495.87ms | tok/sec: 210,062 | mfu: 32.07 | total time: 20.65m\n",
      "step 00759 (93.46%) | loss: 1.046018 | lrm: 0.33 | dt: 2348.59ms | tok/sec: 223,235 | mfu: 34.08 | total time: 20.69m\n",
      "step 00760 (93.58%) | loss: 1.068708 | lrm: 0.32 | dt: 1656.05ms | tok/sec: 316,589 | mfu: 48.34 | total time: 20.72m\n",
      "step 00761 (93.71%) | loss: 1.075266 | lrm: 0.31 | dt: 2391.90ms | tok/sec: 219,192 | mfu: 33.47 | total time: 20.76m\n",
      "step 00762 (93.84%) | loss: 1.065497 | lrm: 0.31 | dt: 1497.59ms | tok/sec: 350,087 | mfu: 53.45 | total time: 20.78m\n",
      "step 00763 (93.95%) | loss: 1.062821 | lrm: 0.30 | dt: 1504.99ms | tok/sec: 348,365 | mfu: 53.19 | total time: 20.81m\n",
      "step 00764 (94.05%) | loss: 1.072966 | lrm: 0.30 | dt: 1492.08ms | tok/sec: 351,381 | mfu: 53.65 | total time: 20.83m\n",
      "step 00765 (94.18%) | loss: 1.076566 | lrm: 0.29 | dt: 1501.93ms | tok/sec: 349,075 | mfu: 53.29 | total time: 20.86m\n",
      "step 00766 (94.29%) | loss: 1.068065 | lrm: 0.29 | dt: 1644.40ms | tok/sec: 318,831 | mfu: 48.68 | total time: 20.89m\n",
      "step 00767 (94.42%) | loss: 1.066291 | lrm: 0.28 | dt: 2523.38ms | tok/sec: 207,772 | mfu: 31.72 | total time: 20.93m\n",
      "step 00768 (94.55%) | loss: 1.064695 | lrm: 0.27 | dt: 1648.05ms | tok/sec: 318,126 | mfu: 48.57 | total time: 20.96m\n",
      "step 00769 (94.68%) | loss: 1.054676 | lrm: 0.27 | dt: 3595.38ms | tok/sec: 145,822 | mfu: 22.26 | total time: 21.02m\n",
      "step 00770 (94.80%) | loss: 1.056159 | lrm: 0.26 | dt: 1506.86ms | tok/sec: 347,935 | mfu: 53.12 | total time: 21.04m\n",
      "step 00771 (94.92%) | loss: 1.042943 | lrm: 0.25 | dt: 1503.59ms | tok/sec: 348,690 | mfu: 53.24 | total time: 21.07m\n",
      "step 00772 (95.04%) | loss: 1.040188 | lrm: 0.25 | dt: 1504.33ms | tok/sec: 348,518 | mfu: 53.21 | total time: 21.09m\n",
      "step 00773 (95.16%) | loss: 1.044713 | lrm: 0.24 | dt: 1500.39ms | tok/sec: 349,433 | mfu: 53.35 | total time: 21.12m\n",
      "step 00774 (95.29%) | loss: 1.035041 | lrm: 0.24 | dt: 1498.41ms | tok/sec: 349,897 | mfu: 53.42 | total time: 21.14m\n",
      "step 00775 (95.41%) | loss: 1.024099 | lrm: 0.23 | dt: 1503.39ms | tok/sec: 348,736 | mfu: 53.24 | total time: 21.17m\n",
      "step 00776 (95.54%) | loss: 1.021545 | lrm: 0.22 | dt: 1501.28ms | tok/sec: 349,226 | mfu: 53.32 | total time: 21.19m\n",
      "step 00777 (95.67%) | loss: 1.008609 | lrm: 0.22 | dt: 1493.01ms | tok/sec: 351,162 | mfu: 53.61 | total time: 21.22m\n",
      "step 00778 (95.78%) | loss: 1.008890 | lrm: 0.21 | dt: 1503.85ms | tok/sec: 348,631 | mfu: 53.23 | total time: 21.24m\n",
      "step 00779 (95.90%) | loss: 1.003972 | lrm: 0.20 | dt: 1503.21ms | tok/sec: 348,779 | mfu: 53.25 | total time: 21.27m\n",
      "step 00780 (96.02%) | loss: 1.001590 | lrm: 0.20 | dt: 1507.76ms | tok/sec: 347,725 | mfu: 53.09 | total time: 21.29m\n",
      "step 00781 (96.13%) | loss: 1.054900 | lrm: 0.19 | dt: 1669.71ms | tok/sec: 313,999 | mfu: 47.94 | total time: 21.32m\n",
      "step 00782 (96.26%) | loss: 1.056014 | lrm: 0.19 | dt: 2537.44ms | tok/sec: 206,621 | mfu: 31.55 | total time: 21.36m\n",
      "step 00783 (96.38%) | loss: 1.048507 | lrm: 0.18 | dt: 2516.96ms | tok/sec: 208,302 | mfu: 31.80 | total time: 21.40m\n",
      "step 00784 (96.50%) | loss: 1.058203 | lrm: 0.17 | dt: 1501.24ms | tok/sec: 349,236 | mfu: 53.32 | total time: 21.43m\n",
      "step 00785 (96.63%) | loss: 1.057865 | lrm: 0.17 | dt: 1508.80ms | tok/sec: 347,486 | mfu: 53.05 | total time: 21.45m\n",
      "step 00786 (96.76%) | loss: 1.061396 | lrm: 0.16 | dt: 1501.40ms | tok/sec: 349,199 | mfu: 53.31 | total time: 21.48m\n",
      "step 00787 (96.88%) | loss: 1.046872 | lrm: 0.16 | dt: 1504.37ms | tok/sec: 348,510 | mfu: 53.21 | total time: 21.50m\n",
      "step 00788 (97.01%) | loss: 1.040529 | lrm: 0.15 | dt: 1505.22ms | tok/sec: 348,313 | mfu: 53.18 | total time: 21.53m\n",
      "step 00789 (97.13%) | loss: 1.021900 | lrm: 0.14 | dt: 1500.06ms | tok/sec: 349,511 | mfu: 53.36 | total time: 21.55m\n",
      "step 00790 (97.27%) | loss: 1.026555 | lrm: 0.14 | dt: 1503.98ms | tok/sec: 348,600 | mfu: 53.22 | total time: 21.58m\n",
      "step 00791 (97.40%) | loss: 1.024294 | lrm: 0.13 | dt: 1509.97ms | tok/sec: 347,216 | mfu: 53.01 | total time: 21.60m\n",
      "step 00792 (97.54%) | loss: 1.039754 | lrm: 0.12 | dt: 1503.10ms | tok/sec: 348,803 | mfu: 53.25 | total time: 21.63m\n",
      "step 00793 (97.66%) | loss: 1.036369 | lrm: 0.12 | dt: 1499.20ms | tok/sec: 349,712 | mfu: 53.39 | total time: 21.65m\n",
      "step 00794 (97.77%) | loss: 1.026910 | lrm: 0.11 | dt: 1496.86ms | tok/sec: 350,257 | mfu: 53.48 | total time: 21.68m\n",
      "step 00795 (97.91%) | loss: 1.021873 | lrm: 0.10 | dt: 1499.83ms | tok/sec: 349,565 | mfu: 53.37 | total time: 21.70m\n",
      "step 00796 (98.02%) | loss: 1.024236 | lrm: 0.10 | dt: 1504.27ms | tok/sec: 348,532 | mfu: 53.21 | total time: 21.73m\n",
      "step 00797 (98.14%) | loss: 1.019529 | lrm: 0.09 | dt: 1507.60ms | tok/sec: 347,763 | mfu: 53.09 | total time: 21.75m\n",
      "step 00798 (98.28%) | loss: 1.037649 | lrm: 0.09 | dt: 1670.47ms | tok/sec: 313,857 | mfu: 47.92 | total time: 21.78m\n",
      "step 00799 (98.39%) | loss: 1.035972 | lrm: 0.08 | dt: 2529.09ms | tok/sec: 207,303 | mfu: 31.65 | total time: 21.82m\n",
      "step 00800 (98.53%) | loss: 1.046756 | lrm: 0.07 | dt: 1499.25ms | tok/sec: 349,699 | mfu: 53.39 | total time: 21.85m\n",
      "step 00801 (98.63%) | loss: 1.035855 | lrm: 0.07 | dt: 1507.39ms | tok/sec: 347,812 | mfu: 53.10 | total time: 21.87m\n",
      "step 00802 (98.76%) | loss: 1.026633 | lrm: 0.06 | dt: 1501.71ms | tok/sec: 349,128 | mfu: 53.30 | total time: 21.90m\n",
      "step 00803 (98.90%) | loss: 1.013708 | lrm: 0.06 | dt: 1507.36ms | tok/sec: 347,818 | mfu: 53.10 | total time: 21.92m\n",
      "step 00804 (99.03%) | loss: 1.001264 | lrm: 0.05 | dt: 1503.61ms | tok/sec: 348,687 | mfu: 53.24 | total time: 21.95m\n",
      "step 00805 (99.16%) | loss: 1.015193 | lrm: 0.04 | dt: 1655.91ms | tok/sec: 316,616 | mfu: 48.34 | total time: 21.98m\n",
      "step 00806 (99.29%) | loss: 1.015151 | lrm: 0.04 | dt: 2399.51ms | tok/sec: 218,498 | mfu: 33.36 | total time: 22.02m\n",
      "step 00807 (99.43%) | loss: 1.007939 | lrm: 0.03 | dt: 1499.13ms | tok/sec: 349,727 | mfu: 53.39 | total time: 22.04m\n",
      "step 00808 (99.56%) | loss: 1.015002 | lrm: 0.02 | dt: 1507.46ms | tok/sec: 347,795 | mfu: 53.10 | total time: 22.07m\n",
      "step 00809 (99.69%) | loss: 1.010904 | lrm: 0.02 | dt: 1660.86ms | tok/sec: 315,671 | mfu: 48.20 | total time: 22.10m\n",
      "step 00810 (99.83%) | loss: 1.015021 | lrm: 0.01 | dt: 2506.89ms | tok/sec: 209,138 | mfu: 31.93 | total time: 22.14m\n",
      "step 00810 | Validation bpb: 0.3392\n",
      "[W1229 16:34:07.664043096 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:07.666352204 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:07.967605721 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:07.977061413 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:07.061404989 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat2/mid_checkpoints/d32/model_000810.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/mid_checkpoints/d32/meta_000810.json\n",
      "saved optimizer to /home/ubuntu/mynanochat2/mid_checkpoints/d32/optim_000810_rank0.pt\n",
      "Peak memory usage: 77017.79MiB\n",
      "Total training time: 22.14m\n",
      "Minimum validation bpb: 0.3392\n",
      "wandb: updating run metadata\n",
      "wandb: uploading summary, console lines 826-831\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                 step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "wandb: total_training_flops ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "wandb:  total_training_time ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:             train/dt ‚ñÖ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:           train/loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "wandb:            train/lrm ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ\n",
      "wandb:            train/mfu ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ\n",
      "wandb:    train/tok_per_sec ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñà‚ñÉ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñà\n",
      "wandb:              val/bpb ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                 step 810\n",
      "wandb: total_training_flops 5129881450551705600\n",
      "wandb:  total_training_time 1328.21058\n",
      "wandb:             train/dt 2.50689\n",
      "wandb:           train/loss 1.01502\n",
      "wandb:            train/lrm 0.00847\n",
      "wandb:            train/mfu 31.93009\n",
      "wandb:    train/tok_per_sec 209138\n",
      "wandb:              val/bpb 0.33921\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-6 at: https://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/0c6xkdhi\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat-mid\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251229_160814-0c6xkdhi/logs\n",
      "[W1229 16:34:24.583899394 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:24.821414332 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:34:24.067253382 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/mid_train_output_002.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e84aa5-6e2f-41d8-93c3-8eec245e804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/mid_checkpoints/d32 with step 810\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2251/2251 [00:00<00:00, 368070.57 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2376/2376 [00:00<00:00, 527590.99 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 249348.49 examples/s]\n",
      "final: 1546/2376 (65.07%)\n",
      "ARC-Easy accuracy: 65.07%\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1119/1119 [00:00<00:00, 224662.59 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [00:00<00:00, 316184.75 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 140420.66 examples/s]\n",
      "final: 545/1172 (46.50%)\n",
      "ARC-Challenge accuracy: 46.50%\n",
      "final: 5489/14042 (39.09%)\n",
      "MMLU accuracy: 39.09%\n",
      "\u001b[KRank 4 | 14/165 (8.48%)]]\n",
      "\u001b[KRank 0 | 12/165 (7.27%)]]\n",
      "\u001b[KRank 5 | 19/165 (11.52%)]\n",
      "\u001b[KRank 6 | 17/165 (10.30%)]\n",
      "\u001b[KRank 7 | 18/164 (10.98%)]\n",
      "\u001b[KRank 1 | 21/165 (12.73%)]\n",
      "\u001b[KRank 3 | 24/165 (14.55%)]\n",
      "\u001b[KRank 2 | 17/165 (10.30%)]\n",
      "==================================================\n",
      "final: 142/1319 (10.77%)\n",
      "GSM8K accuracy: 10.77%\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:00<00:00, 40306.21 examples/s]\n",
      "\u001b[KRank 5 | 4/20 (20.00%)]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]]\n",
      "\u001b[KRank 4 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 0 | 0/21 (0.00%)]]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]\n",
      "\u001b[KRank 1 | 3/21 (14.29%)]\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "==================================================\n",
      "final: 14/164 (8.54%)\n",
      "HumanEval accuracy: 8.54%\n",
      "\u001b[KRank 0 | 32/32 (100.00%)]\n",
      "\u001b[KRank 7 | 31/32 (96.88%)]\n",
      "\u001b[KRank 5 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 3 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 254/256 (99.22%)\n",
      "SpellingBee accuracy: 99.22%\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/mid_chat_eval_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197bff7a-623a-4a24-8fd8-d9bdcd484bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Midtraining\n",
      "timestamp: 2025-12-29 16:34:21\n",
      "\n",
      "- run: challenge-38-6\n",
      "- device_type: \n",
      "- dtype: bfloat16\n",
      "- num_iterations: -1\n",
      "- max_seq_len: 2048\n",
      "- device_batch_size: 8\n",
      "- unembedding_lr: 0.0040\n",
      "- embedding_lr: 0.2000\n",
      "- matrix_lr: 0.0200\n",
      "- init_lr_frac: 1.0000\n",
      "- weight_decay: 0.0000\n",
      "- eval_every: 150\n",
      "- eval_tokens: 10,485,760\n",
      "- total_batch_size: 524,288\n",
      "- dry_run: 0\n",
      "- Number of iterations: 810\n",
      "- DDP world size: 8\n",
      "- Minimum validation bpb: 0.3392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/midtraining.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aba55de-d203-41d0-83a4-981af163a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chat evaluation mid\n",
      "timestamp: 2025-12-29 16:44:49\n",
      "\n",
      "- source: mid\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d32\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.6507\n",
      "- ARC-Challenge: 0.4650\n",
      "- MMLU: 0.3909\n",
      "- GSM8K: 0.1077\n",
      "- HumanEval: 0.0854\n",
      "- SpellingBee: 0.9922\n",
      "- ChatCORE metric: 0.3657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/chat-evaluation-mid.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64216e-085c-4111-8e6d-dc1c25c79250",
   "metadata": {},
   "source": [
    "### sft train and chat eval run output and reports\n",
    "\n",
    "I ran these on the 8 GPU server and then copied the run output and reports to my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc261a1-1127-42ad-a7ac-29fc5b66dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d32\n",
      "overriding run = challenge-38-7\n",
      "user_config: {'run': 'challenge-38-7', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 100, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "wandb: Currently logged in as: ericsilberstein (ericsilberstein-self) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.23.0\n",
      "wandb: Run data is saved locally in /home/ubuntu/learn-nanochat/challenge-38-train-d32/wandb/run-20251229_164703-svzf7cfx\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run challenge-38-7\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ericsilberstein-self/my-nanochat-sft\n",
      "wandb: üöÄ View run at https://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/svzf7cfx\n",
      "loading the model from /home/ubuntu/mynanochat2/mid_checkpoints/d32 with step 810\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(2048/768) = 0.6123724356957946\n",
      "Muon: Grouping 128 params of shape torch.Size([2048, 2048]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([2048, 8192]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([8192, 2048]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 0.833262\n",
      "Step 00000/00701 | Training loss: 0.626952| lrm: 1.000000| num_tokens: 9,725\n",
      "Step 00001/00701 | Training loss: 0.537030| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 0.909322| lrm: 0.997147| num_tokens: 11,282\n",
      "Step 00003/00701 | Training loss: 0.956894| lrm: 0.995720| num_tokens: 12,684\n",
      "Step 00004/00701 | Training loss: 0.668391| lrm: 0.994294| num_tokens: 9,517\n",
      "Step 00005/00701 | Training loss: 0.653749| lrm: 0.992867| num_tokens: 8,343\n",
      "Step 00006/00701 | Training loss: 0.554590| lrm: 0.991441| num_tokens: 8,762\n",
      "Step 00007/00701 | Training loss: 0.348492| lrm: 0.990014| num_tokens: 10,672\n",
      "Step 00008/00701 | Training loss: 0.791417| lrm: 0.988588| num_tokens: 11,586\n",
      "Step 00009/00701 | Training loss: 0.459942| lrm: 0.987161| num_tokens: 9,917\n",
      "Step 00010/00701 | Training loss: 0.340549| lrm: 0.985735| num_tokens: 15,006\n",
      "Step 00011/00701 | Training loss: 0.843882| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.650689| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.789095| lrm: 0.981455| num_tokens: 10,807\n",
      "Step 00014/00701 | Training loss: 0.661955| lrm: 0.980029| num_tokens: 7,545\n",
      "Step 00015/00701 | Training loss: 0.752023| lrm: 0.978602| num_tokens: 10,517\n",
      "Step 00016/00701 | Training loss: 0.837091| lrm: 0.977175| num_tokens: 14,474\n",
      "Step 00017/00701 | Training loss: 0.462057| lrm: 0.975749| num_tokens: 7,716\n",
      "Step 00018/00701 | Training loss: 0.980688| lrm: 0.974322| num_tokens: 11,670\n",
      "Step 00019/00701 | Training loss: 0.487150| lrm: 0.972896| num_tokens: 11,229\n",
      "Step 00020/00701 | Training loss: 0.382518| lrm: 0.971469| num_tokens: 10,680\n",
      "Step 00021/00701 | Training loss: 0.600039| lrm: 0.970043| num_tokens: 12,327\n",
      "Step 00022/00701 | Training loss: 0.874820| lrm: 0.968616| num_tokens: 9,325\n",
      "Step 00023/00701 | Training loss: 0.578078| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.689032| lrm: 0.965763| num_tokens: 10,722\n",
      "Step 00025/00701 | Training loss: 0.527527| lrm: 0.964337| num_tokens: 12,385\n",
      "Step 00026/00701 | Training loss: 0.784482| lrm: 0.962910| num_tokens: 12,580\n",
      "Step 00027/00701 | Training loss: 0.791684| lrm: 0.961484| num_tokens: 8,361\n",
      "Step 00028/00701 | Training loss: 0.718131| lrm: 0.960057| num_tokens: 10,177\n",
      "Step 00029/00701 | Training loss: 0.820760| lrm: 0.958631| num_tokens: 10,635\n",
      "Step 00030/00701 | Training loss: 0.550615| lrm: 0.957204| num_tokens: 9,134\n",
      "Step 00031/00701 | Training loss: 0.926318| lrm: 0.955777| num_tokens: 7,477\n",
      "Step 00032/00701 | Training loss: 0.740240| lrm: 0.954351| num_tokens: 9,975\n",
      "Step 00033/00701 | Training loss: 0.682844| lrm: 0.952924| num_tokens: 11,263\n",
      "Step 00034/00701 | Training loss: 0.830849| lrm: 0.951498| num_tokens: 11,184\n",
      "Step 00035/00701 | Training loss: 0.728564| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.821157| lrm: 0.948645| num_tokens: 9,342\n",
      "Step 00037/00701 | Training loss: 0.865314| lrm: 0.947218| num_tokens: 10,965\n",
      "Step 00038/00701 | Training loss: 0.575153| lrm: 0.945792| num_tokens: 6,795\n",
      "Step 00039/00701 | Training loss: 0.555237| lrm: 0.944365| num_tokens: 5,853\n",
      "Step 00040/00701 | Training loss: 0.796019| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.426730| lrm: 0.941512| num_tokens: 14,347\n",
      "Step 00042/00701 | Training loss: 0.936800| lrm: 0.940086| num_tokens: 11,230\n",
      "Step 00043/00701 | Training loss: 0.826683| lrm: 0.938659| num_tokens: 11,059\n",
      "Step 00044/00701 | Training loss: 0.869838| lrm: 0.937233| num_tokens: 12,665\n",
      "Step 00045/00701 | Training loss: 0.851986| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.539792| lrm: 0.934379| num_tokens: 9,628\n",
      "Step 00047/00701 | Training loss: 0.826756| lrm: 0.932953| num_tokens: 12,896\n",
      "Step 00048/00701 | Training loss: 0.547853| lrm: 0.931526| num_tokens: 10,056\n",
      "Step 00049/00701 | Training loss: 0.434610| lrm: 0.930100| num_tokens: 11,238\n",
      "Step 00050/00701 | Training loss: 0.631922| lrm: 0.928673| num_tokens: 9,081\n",
      "Step 00051/00701 | Training loss: 0.232881| lrm: 0.927247| num_tokens: 9,594\n",
      "Step 00052/00701 | Training loss: 0.728564| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 0.790792| lrm: 0.924394| num_tokens: 7,927\n",
      "Step 00054/00701 | Training loss: 0.306182| lrm: 0.922967| num_tokens: 9,625\n",
      "Step 00055/00701 | Training loss: 0.819428| lrm: 0.921541| num_tokens: 6,511\n",
      "Step 00056/00701 | Training loss: 1.089125| lrm: 0.920114| num_tokens: 10,512\n",
      "Step 00057/00701 | Training loss: 0.882493| lrm: 0.918688| num_tokens: 9,778\n",
      "Step 00058/00701 | Training loss: 0.977666| lrm: 0.917261| num_tokens: 13,367\n",
      "Step 00059/00701 | Training loss: 0.825935| lrm: 0.915835| num_tokens: 7,950\n",
      "Step 00060/00701 | Training loss: 0.858754| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.008095| lrm: 0.912981| num_tokens: 13,727\n",
      "Step 00062/00701 | Training loss: 0.679831| lrm: 0.911555| num_tokens: 12,176\n",
      "Step 00063/00701 | Training loss: 0.502374| lrm: 0.910128| num_tokens: 10,612\n",
      "Step 00064/00701 | Training loss: 0.565331| lrm: 0.908702| num_tokens: 8,113\n",
      "Step 00065/00701 | Training loss: 0.434020| lrm: 0.907275| num_tokens: 11,038\n",
      "Step 00066/00701 | Training loss: 0.812475| lrm: 0.905849| num_tokens: 9,528\n",
      "Step 00067/00701 | Training loss: 0.469998| lrm: 0.904422| num_tokens: 11,574\n",
      "Step 00068/00701 | Training loss: 0.967038| lrm: 0.902996| num_tokens: 12,927\n",
      "Step 00069/00701 | Training loss: 0.660187| lrm: 0.901569| num_tokens: 7,941\n",
      "Step 00070/00701 | Training loss: 0.390004| lrm: 0.900143| num_tokens: 10,774\n",
      "Step 00071/00701 | Training loss: 0.579041| lrm: 0.898716| num_tokens: 9,197\n",
      "Step 00072/00701 | Training loss: 0.779326| lrm: 0.897290| num_tokens: 9,388\n",
      "Step 00073/00701 | Training loss: 0.993763| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.525233| lrm: 0.894437| num_tokens: 7,686\n",
      "Step 00075/00701 | Training loss: 0.558395| lrm: 0.893010| num_tokens: 6,802\n",
      "Step 00076/00701 | Training loss: 0.631583| lrm: 0.891583| num_tokens: 7,531\n",
      "Step 00077/00701 | Training loss: 0.723261| lrm: 0.890157| num_tokens: 9,332\n",
      "Step 00078/00701 | Training loss: 0.499203| lrm: 0.888730| num_tokens: 11,561\n",
      "Step 00079/00701 | Training loss: 0.493316| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080/00701 | Training loss: 0.866826| lrm: 0.885877| num_tokens: 11,672\n",
      "Step 00081/00701 | Training loss: 0.511677| lrm: 0.884451| num_tokens: 7,782\n",
      "Step 00082/00701 | Training loss: 0.525807| lrm: 0.883024| num_tokens: 14,538\n",
      "Step 00083/00701 | Training loss: 0.481019| lrm: 0.881598| num_tokens: 11,071\n",
      "Step 00084/00701 | Training loss: 0.609188| lrm: 0.880171| num_tokens: 13,571\n",
      "Step 00085/00701 | Training loss: 0.876958| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.805355| lrm: 0.877318| num_tokens: 9,907\n",
      "Step 00087/00701 | Training loss: 0.819889| lrm: 0.875892| num_tokens: 9,615\n",
      "Step 00088/00701 | Training loss: 0.664618| lrm: 0.874465| num_tokens: 11,072\n",
      "Step 00089/00701 | Training loss: 1.178611| lrm: 0.873039| num_tokens: 9,518\n",
      "Step 00090/00701 | Training loss: 0.770178| lrm: 0.871612| num_tokens: 10,575\n",
      "Step 00091/00701 | Training loss: 0.590252| lrm: 0.870185| num_tokens: 10,764\n",
      "Step 00092/00701 | Training loss: 0.667155| lrm: 0.868759| num_tokens: 10,412\n",
      "Step 00093/00701 | Training loss: 0.759154| lrm: 0.867332| num_tokens: 9,586\n",
      "Step 00094/00701 | Training loss: 0.570157| lrm: 0.865906| num_tokens: 9,469\n",
      "Step 00095/00701 | Training loss: 0.726843| lrm: 0.864479| num_tokens: 9,398\n",
      "Step 00096/00701 | Training loss: 0.830937| lrm: 0.863053| num_tokens: 13,246\n",
      "Step 00097/00701 | Training loss: 0.729490| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.610641| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 0.903598| lrm: 0.858773| num_tokens: 8,985\n",
      "Step 00100 | Validation loss: 0.838915\n",
      "Step 00100/00701 | Training loss: 1.745143| lrm: 0.857347| num_tokens: 10,534\n",
      "Step 00101/00701 | Training loss: 0.386328| lrm: 0.855920| num_tokens: 9,832\n",
      "Step 00102/00701 | Training loss: 0.497568| lrm: 0.854494| num_tokens: 10,664\n",
      "Step 00103/00701 | Training loss: 0.604642| lrm: 0.853067| num_tokens: 11,262\n",
      "Step 00104/00701 | Training loss: 0.567865| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.588479| lrm: 0.850214| num_tokens: 10,154\n",
      "Step 00106/00701 | Training loss: 0.647287| lrm: 0.848787| num_tokens: 11,163\n",
      "Step 00107/00701 | Training loss: 0.858380| lrm: 0.847361| num_tokens: 10,163\n",
      "Step 00108/00701 | Training loss: 0.709499| lrm: 0.845934| num_tokens: 8,476\n",
      "Step 00109/00701 | Training loss: 0.549267| lrm: 0.844508| num_tokens: 8,729\n",
      "Step 00110/00701 | Training loss: 0.641955| lrm: 0.843081| num_tokens: 12,238\n",
      "Step 00111/00701 | Training loss: 0.554728| lrm: 0.841655| num_tokens: 10,739\n",
      "Step 00112/00701 | Training loss: 1.271504| lrm: 0.840228| num_tokens: 14,859\n",
      "Step 00113/00701 | Training loss: 0.559405| lrm: 0.838802| num_tokens: 16,104\n",
      "Step 00114/00701 | Training loss: 0.925242| lrm: 0.837375| num_tokens: 14,297\n",
      "Step 00115/00701 | Training loss: 0.246816| lrm: 0.835949| num_tokens: 11,081\n",
      "Step 00116/00701 | Training loss: 0.630876| lrm: 0.834522| num_tokens: 9,136\n",
      "Step 00117/00701 | Training loss: 0.763943| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.549567| lrm: 0.831669| num_tokens: 9,695\n",
      "Step 00119/00701 | Training loss: 0.509206| lrm: 0.830243| num_tokens: 14,020\n",
      "Step 00120/00701 | Training loss: 0.785729| lrm: 0.828816| num_tokens: 12,992\n",
      "Step 00121/00701 | Training loss: 0.883300| lrm: 0.827389| num_tokens: 16,516\n",
      "Step 00122/00701 | Training loss: 0.673819| lrm: 0.825963| num_tokens: 7,270\n",
      "Step 00123/00701 | Training loss: 0.765598| lrm: 0.824536| num_tokens: 6,635\n",
      "Step 00124/00701 | Training loss: 0.576269| lrm: 0.823110| num_tokens: 11,606\n",
      "Step 00125/00701 | Training loss: 0.885867| lrm: 0.821683| num_tokens: 5,772\n",
      "Step 00126/00701 | Training loss: 0.545132| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 0.876221| lrm: 0.818830| num_tokens: 10,156\n",
      "Step 00128/00701 | Training loss: 0.382460| lrm: 0.817404| num_tokens: 10,643\n",
      "Step 00129/00701 | Training loss: 0.768392| lrm: 0.815977| num_tokens: 8,876\n",
      "Step 00130/00701 | Training loss: 0.699813| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.704746| lrm: 0.813124| num_tokens: 11,226\n",
      "Step 00132/00701 | Training loss: 0.644621| lrm: 0.811698| num_tokens: 12,016\n",
      "Step 00133/00701 | Training loss: 0.748538| lrm: 0.810271| num_tokens: 16,441\n",
      "Step 00134/00701 | Training loss: 0.584824| lrm: 0.808845| num_tokens: 10,743\n",
      "Step 00135/00701 | Training loss: 1.126933| lrm: 0.807418| num_tokens: 13,495\n",
      "Step 00136/00701 | Training loss: 0.473752| lrm: 0.805991| num_tokens: 10,198\n",
      "Step 00137/00701 | Training loss: 0.765167| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.558042| lrm: 0.803138| num_tokens: 11,018\n",
      "Step 00139/00701 | Training loss: 0.733914| lrm: 0.801712| num_tokens: 12,149\n",
      "Step 00140/00701 | Training loss: 0.802555| lrm: 0.800285| num_tokens: 13,091\n",
      "Step 00141/00701 | Training loss: 0.590301| lrm: 0.798859| num_tokens: 13,652\n",
      "Step 00142/00701 | Training loss: 0.384707| lrm: 0.797432| num_tokens: 10,282\n",
      "Step 00143/00701 | Training loss: 1.006879| lrm: 0.796006| num_tokens: 8,951\n",
      "Step 00144/00701 | Training loss: 1.032173| lrm: 0.794579| num_tokens: 14,842\n",
      "Step 00145/00701 | Training loss: 0.746091| lrm: 0.793153| num_tokens: 14,652\n",
      "Step 00146/00701 | Training loss: 0.476447| lrm: 0.791726| num_tokens: 11,997\n",
      "Step 00147/00701 | Training loss: 0.550756| lrm: 0.790300| num_tokens: 8,029\n",
      "Step 00148/00701 | Training loss: 0.872957| lrm: 0.788873| num_tokens: 11,349\n",
      "Step 00149/00701 | Training loss: 0.833083| lrm: 0.787447| num_tokens: 14,184\n",
      "Step 00150/00701 | Training loss: 0.516434| lrm: 0.786020| num_tokens: 10,785\n",
      "Step 00151/00701 | Training loss: 0.570719| lrm: 0.784593| num_tokens: 14,041\n",
      "Step 00152/00701 | Training loss: 0.643658| lrm: 0.783167| num_tokens: 8,531\n",
      "Step 00153/00701 | Training loss: 0.638181| lrm: 0.781740| num_tokens: 12,788\n",
      "Step 00154/00701 | Training loss: 0.561450| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.745639| lrm: 0.778887| num_tokens: 10,620\n",
      "Step 00156/00701 | Training loss: 0.445471| lrm: 0.777461| num_tokens: 13,567\n",
      "Step 00157/00701 | Training loss: 0.867266| lrm: 0.776034| num_tokens: 13,326\n",
      "Step 00158/00701 | Training loss: 0.417564| lrm: 0.774608| num_tokens: 7,499\n",
      "Step 00159/00701 | Training loss: 0.587353| lrm: 0.773181| num_tokens: 9,271\n",
      "Step 00160/00701 | Training loss: 0.528330| lrm: 0.771755| num_tokens: 9,402\n",
      "Step 00161/00701 | Training loss: 0.786510| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.790705| lrm: 0.768902| num_tokens: 11,781\n",
      "Step 00163/00701 | Training loss: 0.405253| lrm: 0.767475| num_tokens: 7,482\n",
      "Step 00164/00701 | Training loss: 0.355728| lrm: 0.766049| num_tokens: 9,083\n",
      "Step 00165/00701 | Training loss: 0.597696| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.786423| lrm: 0.763195| num_tokens: 12,310\n",
      "Step 00167/00701 | Training loss: 0.712158| lrm: 0.761769| num_tokens: 10,548\n",
      "Step 00168/00701 | Training loss: 0.526060| lrm: 0.760342| num_tokens: 9,718\n",
      "Step 00169/00701 | Training loss: 0.760805| lrm: 0.758916| num_tokens: 9,549\n",
      "Step 00170/00701 | Training loss: 1.096917| lrm: 0.757489| num_tokens: 13,463\n",
      "Step 00171/00701 | Training loss: 0.546691| lrm: 0.756063| num_tokens: 10,157\n",
      "Step 00172/00701 | Training loss: 0.754797| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.783212| lrm: 0.753210| num_tokens: 13,928\n",
      "Step 00174/00701 | Training loss: 0.705287| lrm: 0.751783| num_tokens: 9,131\n",
      "Step 00175/00701 | Training loss: 0.739889| lrm: 0.750357| num_tokens: 9,838\n",
      "Step 00176/00701 | Training loss: 0.801486| lrm: 0.748930| num_tokens: 11,248\n",
      "Step 00177/00701 | Training loss: 0.781551| lrm: 0.747504| num_tokens: 14,671\n",
      "Step 00178/00701 | Training loss: 0.470546| lrm: 0.746077| num_tokens: 9,617\n",
      "Step 00179/00701 | Training loss: 0.681256| lrm: 0.744650| num_tokens: 11,083\n",
      "Step 00180/00701 | Training loss: 0.473964| lrm: 0.743224| num_tokens: 11,992\n",
      "Step 00181/00701 | Training loss: 0.720267| lrm: 0.741797| num_tokens: 15,747\n",
      "Step 00182/00701 | Training loss: 0.939566| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.400254| lrm: 0.738944| num_tokens: 12,479\n",
      "Step 00184/00701 | Training loss: 0.794235| lrm: 0.737518| num_tokens: 13,571\n",
      "Step 00185/00701 | Training loss: 0.495905| lrm: 0.736091| num_tokens: 11,272\n",
      "Step 00186/00701 | Training loss: 0.830540| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.390460| lrm: 0.733238| num_tokens: 13,611\n",
      "Step 00188/00701 | Training loss: 0.790278| lrm: 0.731812| num_tokens: 9,422\n",
      "Step 00189/00701 | Training loss: 0.822254| lrm: 0.730385| num_tokens: 9,302\n",
      "Step 00190/00701 | Training loss: 0.379803| lrm: 0.728959| num_tokens: 15,822\n",
      "Step 00191/00701 | Training loss: 0.675783| lrm: 0.727532| num_tokens: 12,337\n",
      "Step 00192/00701 | Training loss: 0.421710| lrm: 0.726106| num_tokens: 9,968\n",
      "Step 00193/00701 | Training loss: 0.897917| lrm: 0.724679| num_tokens: 10,652\n",
      "Step 00194/00701 | Training loss: 1.027825| lrm: 0.723252| num_tokens: 8,194\n",
      "Step 00195/00701 | Training loss: 0.427989| lrm: 0.721826| num_tokens: 14,330\n",
      "Step 00196/00701 | Training loss: 0.621100| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 0.797050| lrm: 0.718973| num_tokens: 10,274\n",
      "Step 00198/00701 | Training loss: 0.457178| lrm: 0.717546| num_tokens: 9,713\n",
      "Step 00199/00701 | Training loss: 0.491974| lrm: 0.716120| num_tokens: 10,344\n",
      "Step 00200 | Validation loss: 0.838494\n",
      "final: 363/1024 (35.45%)\n",
      "final: 645/1024 (62.99%)\n",
      "Step 00200 | mmlu_acc: 0.354492, arc_easy_acc: 0.629883\n",
      "Step 00200/00701 | Training loss: 0.695050| lrm: 0.714693| num_tokens: 17,785\n",
      "Step 00201/00701 | Training loss: 0.692560| lrm: 0.713267| num_tokens: 9,677\n",
      "Step 00202/00701 | Training loss: 1.532188| lrm: 0.711840| num_tokens: 15,700\n",
      "Step 00203/00701 | Training loss: 0.953896| lrm: 0.710414| num_tokens: 8,339\n",
      "Step 00204/00701 | Training loss: 0.808744| lrm: 0.708987| num_tokens: 11,402\n",
      "Step 00205/00701 | Training loss: 0.577518| lrm: 0.707561| num_tokens: 11,167\n",
      "Step 00206/00701 | Training loss: 0.740910| lrm: 0.706134| num_tokens: 10,519\n",
      "Step 00207/00701 | Training loss: 0.532970| lrm: 0.704708| num_tokens: 11,287\n",
      "Step 00208/00701 | Training loss: 1.033324| lrm: 0.703281| num_tokens: 13,203\n",
      "Step 00209/00701 | Training loss: 0.501841| lrm: 0.701854| num_tokens: 10,618\n",
      "Step 00210/00701 | Training loss: 0.614038| lrm: 0.700428| num_tokens: 11,860\n",
      "Step 00211/00701 | Training loss: 0.836714| lrm: 0.699001| num_tokens: 11,444\n",
      "Step 00212/00701 | Training loss: 0.815663| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.608975| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 0.856681| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.532371| lrm: 0.693295| num_tokens: 10,047\n",
      "Step 00216/00701 | Training loss: 0.603113| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 0.750604| lrm: 0.690442| num_tokens: 8,205\n",
      "Step 00218/00701 | Training loss: 0.417155| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.713532| lrm: 0.687589| num_tokens: 14,710\n",
      "Step 00220/00701 | Training loss: 0.709574| lrm: 0.686163| num_tokens: 8,475\n",
      "Step 00221/00701 | Training loss: 0.731709| lrm: 0.684736| num_tokens: 7,852\n",
      "Step 00222/00701 | Training loss: 0.888908| lrm: 0.683310| num_tokens: 7,405\n",
      "Step 00223/00701 | Training loss: 0.903018| lrm: 0.681883| num_tokens: 11,904\n",
      "Step 00224/00701 | Training loss: 0.594100| lrm: 0.680456| num_tokens: 10,062\n",
      "Step 00225/00701 | Training loss: 0.877047| lrm: 0.679030| num_tokens: 8,889\n",
      "Step 00226/00701 | Training loss: 0.509641| lrm: 0.677603| num_tokens: 10,746\n",
      "Step 00227/00701 | Training loss: 0.594882| lrm: 0.676177| num_tokens: 10,028\n",
      "Step 00228/00701 | Training loss: 0.538093| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 0.915781| lrm: 0.673324| num_tokens: 9,396\n",
      "Step 00230/00701 | Training loss: 0.877120| lrm: 0.671897| num_tokens: 7,843\n",
      "Step 00231/00701 | Training loss: 0.935110| lrm: 0.670471| num_tokens: 11,681\n",
      "Step 00232/00701 | Training loss: 1.402160| lrm: 0.669044| num_tokens: 10,778\n",
      "Step 00233/00701 | Training loss: 0.617487| lrm: 0.667618| num_tokens: 10,202\n",
      "Step 00234/00701 | Training loss: 0.804279| lrm: 0.666191| num_tokens: 14,221\n",
      "Step 00235/00701 | Training loss: 0.843757| lrm: 0.664765| num_tokens: 9,745\n",
      "Step 00236/00701 | Training loss: 0.822919| lrm: 0.663338| num_tokens: 7,455\n",
      "Step 00237/00701 | Training loss: 0.656145| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 0.790750| lrm: 0.660485| num_tokens: 13,487\n",
      "Step 00239/00701 | Training loss: 1.100365| lrm: 0.659058| num_tokens: 12,522\n",
      "Step 00240/00701 | Training loss: 0.487775| lrm: 0.657632| num_tokens: 12,894\n",
      "Step 00241/00701 | Training loss: 0.878147| lrm: 0.656205| num_tokens: 11,278\n",
      "Step 00242/00701 | Training loss: 0.704406| lrm: 0.654779| num_tokens: 8,153\n",
      "Step 00243/00701 | Training loss: 0.533584| lrm: 0.653352| num_tokens: 10,580\n",
      "Step 00244/00701 | Training loss: 0.384653| lrm: 0.651926| num_tokens: 6,550\n",
      "Step 00245/00701 | Training loss: 0.824377| lrm: 0.650499| num_tokens: 12,908\n",
      "Step 00246/00701 | Training loss: 0.275077| lrm: 0.649073| num_tokens: 8,773\n",
      "Step 00247/00701 | Training loss: 0.445922| lrm: 0.647646| num_tokens: 10,634\n",
      "Step 00248/00701 | Training loss: 0.656515| lrm: 0.646220| num_tokens: 14,879\n",
      "Step 00249/00701 | Training loss: 0.624191| lrm: 0.644793| num_tokens: 8,756\n",
      "Step 00250/00701 | Training loss: 0.746663| lrm: 0.643367| num_tokens: 11,349\n",
      "Step 00251/00701 | Training loss: 0.922707| lrm: 0.641940| num_tokens: 13,772\n",
      "Step 00252/00701 | Training loss: 0.554271| lrm: 0.640514| num_tokens: 10,829\n",
      "Step 00253/00701 | Training loss: 0.692060| lrm: 0.639087| num_tokens: 14,176\n",
      "Step 00254/00701 | Training loss: 0.714901| lrm: 0.637660| num_tokens: 8,625\n",
      "Step 00255/00701 | Training loss: 0.560129| lrm: 0.636234| num_tokens: 10,644\n",
      "Step 00256/00701 | Training loss: 1.138375| lrm: 0.634807| num_tokens: 10,569\n",
      "Step 00257/00701 | Training loss: 0.480731| lrm: 0.633381| num_tokens: 8,376\n",
      "Step 00258/00701 | Training loss: 0.810005| lrm: 0.631954| num_tokens: 11,896\n",
      "Step 00259/00701 | Training loss: 0.737741| lrm: 0.630528| num_tokens: 6,293\n",
      "Step 00260/00701 | Training loss: 0.369683| lrm: 0.629101| num_tokens: 12,742\n",
      "Step 00261/00701 | Training loss: 0.534542| lrm: 0.627675| num_tokens: 11,080\n",
      "Step 00262/00701 | Training loss: 0.578805| lrm: 0.626248| num_tokens: 10,284\n",
      "Step 00263/00701 | Training loss: 0.501817| lrm: 0.624822| num_tokens: 6,969\n",
      "Step 00264/00701 | Training loss: 0.884094| lrm: 0.623395| num_tokens: 9,508\n",
      "Step 00265/00701 | Training loss: 0.626963| lrm: 0.621969| num_tokens: 11,234\n",
      "Step 00266/00701 | Training loss: 0.647884| lrm: 0.620542| num_tokens: 11,271\n",
      "Step 00267/00701 | Training loss: 0.432214| lrm: 0.619116| num_tokens: 8,963\n",
      "Step 00268/00701 | Training loss: 0.757047| lrm: 0.617689| num_tokens: 12,257\n",
      "Step 00269/00701 | Training loss: 0.609898| lrm: 0.616262| num_tokens: 9,577\n",
      "Step 00270/00701 | Training loss: 0.892005| lrm: 0.614836| num_tokens: 12,114\n",
      "Step 00271/00701 | Training loss: 1.061539| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.719252| lrm: 0.611983| num_tokens: 13,059\n",
      "Step 00273/00701 | Training loss: 0.476544| lrm: 0.610556| num_tokens: 9,788\n",
      "Step 00274/00701 | Training loss: 0.552449| lrm: 0.609130| num_tokens: 12,235\n",
      "Step 00275/00701 | Training loss: 0.488813| lrm: 0.607703| num_tokens: 10,850\n",
      "Step 00276/00701 | Training loss: 0.602733| lrm: 0.606277| num_tokens: 10,711\n",
      "Step 00277/00701 | Training loss: 0.498134| lrm: 0.604850| num_tokens: 7,564\n",
      "Step 00278/00701 | Training loss: 0.660712| lrm: 0.603424| num_tokens: 11,620\n",
      "Step 00279/00701 | Training loss: 0.712236| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280/00701 | Training loss: 0.656444| lrm: 0.600571| num_tokens: 8,962\n",
      "Step 00281/00701 | Training loss: 0.797371| lrm: 0.599144| num_tokens: 14,103\n",
      "Step 00282/00701 | Training loss: 0.789902| lrm: 0.597718| num_tokens: 10,946\n",
      "Step 00283/00701 | Training loss: 0.518271| lrm: 0.596291| num_tokens: 8,908\n",
      "Step 00284/00701 | Training loss: 0.334117| lrm: 0.594864| num_tokens: 12,622\n",
      "Step 00285/00701 | Training loss: 0.661795| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.493139| lrm: 0.592011| num_tokens: 11,909\n",
      "Step 00287/00701 | Training loss: 0.202922| lrm: 0.590585| num_tokens: 9,544\n",
      "Step 00288/00701 | Training loss: 0.573739| lrm: 0.589158| num_tokens: 5,017\n",
      "Step 00289/00701 | Training loss: 0.542194| lrm: 0.587732| num_tokens: 9,428\n",
      "Step 00290/00701 | Training loss: 0.820480| lrm: 0.586305| num_tokens: 11,031\n",
      "Step 00291/00701 | Training loss: 0.819917| lrm: 0.584879| num_tokens: 10,854\n",
      "Step 00292/00701 | Training loss: 0.756754| lrm: 0.583452| num_tokens: 11,658\n",
      "Step 00293/00701 | Training loss: 0.687862| lrm: 0.582026| num_tokens: 15,051\n",
      "Step 00294/00701 | Training loss: 0.575917| lrm: 0.580599| num_tokens: 9,435\n",
      "Step 00295/00701 | Training loss: 0.894490| lrm: 0.579173| num_tokens: 7,003\n",
      "Step 00296/00701 | Training loss: 0.914561| lrm: 0.577746| num_tokens: 12,269\n",
      "Step 00297/00701 | Training loss: 0.443130| lrm: 0.576320| num_tokens: 17,564\n",
      "Step 00298/00701 | Training loss: 0.800566| lrm: 0.574893| num_tokens: 10,108\n",
      "Step 00299/00701 | Training loss: 0.529975| lrm: 0.573466| num_tokens: 7,734\n",
      "Step 00300 | Validation loss: 0.838447\n",
      "Step 00300/00701 | Training loss: 0.602957| lrm: 0.572040| num_tokens: 8,909\n",
      "Step 00301/00701 | Training loss: 0.681407| lrm: 0.570613| num_tokens: 12,834\n",
      "Step 00302/00701 | Training loss: 0.653598| lrm: 0.569187| num_tokens: 12,202\n",
      "Step 00303/00701 | Training loss: 0.791117| lrm: 0.567760| num_tokens: 11,591\n",
      "Step 00304/00701 | Training loss: 0.492604| lrm: 0.566334| num_tokens: 8,218\n",
      "Step 00305/00701 | Training loss: 0.686426| lrm: 0.564907| num_tokens: 14,648\n",
      "Step 00306/00701 | Training loss: 0.703712| lrm: 0.563481| num_tokens: 11,571\n",
      "Step 00307/00701 | Training loss: 0.855033| lrm: 0.562054| num_tokens: 12,886\n",
      "Step 00308/00701 | Training loss: 0.593580| lrm: 0.560628| num_tokens: 6,840\n",
      "Step 00309/00701 | Training loss: 0.745213| lrm: 0.559201| num_tokens: 11,014\n",
      "Step 00310/00701 | Training loss: 0.468926| lrm: 0.557775| num_tokens: 12,124\n",
      "Step 00311/00701 | Training loss: 1.030250| lrm: 0.556348| num_tokens: 10,304\n",
      "Step 00312/00701 | Training loss: 0.591734| lrm: 0.554922| num_tokens: 11,424\n",
      "Step 00313/00701 | Training loss: 0.605797| lrm: 0.553495| num_tokens: 8,440\n",
      "Step 00314/00701 | Training loss: 0.871991| lrm: 0.552068| num_tokens: 11,861\n",
      "Step 00315/00701 | Training loss: 0.414871| lrm: 0.550642| num_tokens: 14,156\n",
      "Step 00316/00701 | Training loss: 1.121581| lrm: 0.549215| num_tokens: 6,452\n",
      "Step 00317/00701 | Training loss: 0.328863| lrm: 0.547789| num_tokens: 11,652\n",
      "Step 00318/00701 | Training loss: 0.600513| lrm: 0.546362| num_tokens: 10,041\n",
      "Step 00319/00701 | Training loss: 0.962416| lrm: 0.544936| num_tokens: 11,372\n",
      "Step 00320/00701 | Training loss: 0.507313| lrm: 0.543509| num_tokens: 6,439\n",
      "Step 00321/00701 | Training loss: 0.877498| lrm: 0.542083| num_tokens: 8,881\n",
      "Step 00322/00701 | Training loss: 0.602855| lrm: 0.540656| num_tokens: 4,150\n",
      "Step 00323/00701 | Training loss: 0.616989| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.765231| lrm: 0.537803| num_tokens: 11,809\n",
      "Step 00325/00701 | Training loss: 0.491817| lrm: 0.536377| num_tokens: 9,006\n",
      "Step 00326/00701 | Training loss: 0.683790| lrm: 0.534950| num_tokens: 7,342\n",
      "Step 00327/00701 | Training loss: 0.546069| lrm: 0.533524| num_tokens: 12,013\n",
      "Step 00328/00701 | Training loss: 0.841145| lrm: 0.532097| num_tokens: 12,712\n",
      "Step 00329/00701 | Training loss: 0.520608| lrm: 0.530670| num_tokens: 10,358\n",
      "Step 00330/00701 | Training loss: 0.814138| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.677175| lrm: 0.527817| num_tokens: 7,790\n",
      "Step 00332/00701 | Training loss: 0.647784| lrm: 0.526391| num_tokens: 10,631\n",
      "Step 00333/00701 | Training loss: 0.986419| lrm: 0.524964| num_tokens: 9,781\n",
      "Step 00334/00701 | Training loss: 0.322461| lrm: 0.523538| num_tokens: 12,076\n",
      "Step 00335/00701 | Training loss: 0.347068| lrm: 0.522111| num_tokens: 7,316\n",
      "Step 00336/00701 | Training loss: 0.934140| lrm: 0.520685| num_tokens: 12,909\n",
      "Step 00337/00701 | Training loss: 0.625790| lrm: 0.519258| num_tokens: 11,904\n",
      "Step 00338/00701 | Training loss: 0.314126| lrm: 0.517832| num_tokens: 10,207\n",
      "Step 00339/00701 | Training loss: 0.701137| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340/00701 | Training loss: 0.611415| lrm: 0.514979| num_tokens: 11,161\n",
      "Step 00341/00701 | Training loss: 0.972250| lrm: 0.513552| num_tokens: 8,985\n",
      "Step 00342/00701 | Training loss: 0.553719| lrm: 0.512126| num_tokens: 6,243\n",
      "Step 00343/00701 | Training loss: 0.911699| lrm: 0.510699| num_tokens: 11,620\n",
      "Step 00344/00701 | Training loss: 0.439559| lrm: 0.509272| num_tokens: 7,703\n",
      "Step 00345/00701 | Training loss: 0.582708| lrm: 0.507846| num_tokens: 12,527\n",
      "Step 00346/00701 | Training loss: 0.744769| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.538334| lrm: 0.504993| num_tokens: 11,177\n",
      "Step 00348/00701 | Training loss: 1.076281| lrm: 0.503566| num_tokens: 9,511\n",
      "Step 00349/00701 | Training loss: 0.558182| lrm: 0.502140| num_tokens: 10,290\n",
      "Step 00350/00701 | Training loss: 0.689770| lrm: 0.500713| num_tokens: 9,021\n",
      "Step 00351/00701 | Training loss: 0.574709| lrm: 0.499287| num_tokens: 11,741\n",
      "Step 00352/00701 | Training loss: 0.792116| lrm: 0.497860| num_tokens: 12,672\n",
      "Step 00353/00701 | Training loss: 0.688480| lrm: 0.496434| num_tokens: 8,386\n",
      "Step 00354/00701 | Training loss: 0.637182| lrm: 0.495007| num_tokens: 10,060\n",
      "Step 00355/00701 | Training loss: 0.698913| lrm: 0.493581| num_tokens: 10,086\n",
      "Step 00356/00701 | Training loss: 0.537388| lrm: 0.492154| num_tokens: 8,332\n",
      "Step 00357/00701 | Training loss: 0.561653| lrm: 0.490728| num_tokens: 8,267\n",
      "Step 00358/00701 | Training loss: 0.581862| lrm: 0.489301| num_tokens: 9,977\n",
      "Step 00359/00701 | Training loss: 0.621485| lrm: 0.487874| num_tokens: 9,009\n",
      "Step 00360/00701 | Training loss: 0.479153| lrm: 0.486448| num_tokens: 8,622\n",
      "Step 00361/00701 | Training loss: 0.582433| lrm: 0.485021| num_tokens: 11,844\n",
      "Step 00362/00701 | Training loss: 0.963783| lrm: 0.483595| num_tokens: 11,201\n",
      "Step 00363/00701 | Training loss: 0.359138| lrm: 0.482168| num_tokens: 11,216\n",
      "Step 00364/00701 | Training loss: 0.587768| lrm: 0.480742| num_tokens: 13,302\n",
      "Step 00365/00701 | Training loss: 0.426611| lrm: 0.479315| num_tokens: 7,757\n",
      "Step 00366/00701 | Training loss: 0.776398| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.663259| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 0.909254| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.655208| lrm: 0.473609| num_tokens: 13,350\n",
      "Step 00370/00701 | Training loss: 0.612902| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.782901| lrm: 0.470756| num_tokens: 11,946\n",
      "Step 00372/00701 | Training loss: 0.776343| lrm: 0.469330| num_tokens: 11,405\n",
      "Step 00373/00701 | Training loss: 0.519140| lrm: 0.467903| num_tokens: 8,818\n",
      "Step 00374/00701 | Training loss: 0.654802| lrm: 0.466476| num_tokens: 13,105\n",
      "Step 00375/00701 | Training loss: 0.938132| lrm: 0.465050| num_tokens: 13,717\n",
      "Step 00376/00701 | Training loss: 0.966523| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.574878| lrm: 0.462197| num_tokens: 13,884\n",
      "Step 00378/00701 | Training loss: 0.531305| lrm: 0.460770| num_tokens: 11,851\n",
      "Step 00379/00701 | Training loss: 0.614182| lrm: 0.459344| num_tokens: 7,958\n",
      "Step 00380/00701 | Training loss: 0.999031| lrm: 0.457917| num_tokens: 8,518\n",
      "Step 00381/00701 | Training loss: 0.718710| lrm: 0.456491| num_tokens: 14,256\n",
      "Step 00382/00701 | Training loss: 0.502417| lrm: 0.455064| num_tokens: 13,357\n",
      "Step 00383/00701 | Training loss: 0.438342| lrm: 0.453638| num_tokens: 15,145\n",
      "Step 00384/00701 | Training loss: 0.515705| lrm: 0.452211| num_tokens: 12,583\n",
      "Step 00385/00701 | Training loss: 0.381992| lrm: 0.450785| num_tokens: 10,100\n",
      "Step 00386/00701 | Training loss: 0.792682| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 0.807941| lrm: 0.447932| num_tokens: 9,868\n",
      "Step 00388/00701 | Training loss: 0.725868| lrm: 0.446505| num_tokens: 11,295\n",
      "Step 00389/00701 | Training loss: 0.685013| lrm: 0.445078| num_tokens: 15,859\n",
      "Step 00390/00701 | Training loss: 0.412017| lrm: 0.443652| num_tokens: 12,774\n",
      "Step 00391/00701 | Training loss: 0.562268| lrm: 0.442225| num_tokens: 13,896\n",
      "Step 00392/00701 | Training loss: 0.281880| lrm: 0.440799| num_tokens: 10,502\n",
      "Step 00393/00701 | Training loss: 1.033660| lrm: 0.439372| num_tokens: 13,465\n",
      "Step 00394/00701 | Training loss: 0.809846| lrm: 0.437946| num_tokens: 13,513\n",
      "Step 00395/00701 | Training loss: 0.689846| lrm: 0.436519| num_tokens: 10,028\n",
      "Step 00396/00701 | Training loss: 0.808276| lrm: 0.435093| num_tokens: 11,008\n",
      "Step 00397/00701 | Training loss: 0.878148| lrm: 0.433666| num_tokens: 8,181\n",
      "Step 00398/00701 | Training loss: 0.803130| lrm: 0.432240| num_tokens: 8,069\n",
      "Step 00399/00701 | Training loss: 0.708762| lrm: 0.430813| num_tokens: 8,561\n",
      "Step 00400 | Validation loss: 0.838309\n",
      "final: 402/1024 (39.26%)\n",
      "final: 678/1024 (66.21%)\n",
      "Step 00400 | mmlu_acc: 0.392578, arc_easy_acc: 0.662109\n",
      "Step 00400/00701 | Training loss: 0.358556| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.606244| lrm: 0.427960| num_tokens: 6,685\n",
      "Step 00402/00701 | Training loss: 0.542407| lrm: 0.426534| num_tokens: 7,154\n",
      "Step 00403/00701 | Training loss: 1.042550| lrm: 0.425107| num_tokens: 15,327\n",
      "Step 00404/00701 | Training loss: 0.738531| lrm: 0.423680| num_tokens: 8,836\n",
      "Step 00405/00701 | Training loss: 0.484945| lrm: 0.422254| num_tokens: 10,884\n",
      "Step 00406/00701 | Training loss: 0.549255| lrm: 0.420827| num_tokens: 9,682\n",
      "Step 00407/00701 | Training loss: 0.502466| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.498452| lrm: 0.417974| num_tokens: 12,825\n",
      "Step 00409/00701 | Training loss: 0.525126| lrm: 0.416548| num_tokens: 8,561\n",
      "Step 00410/00701 | Training loss: 0.540220| lrm: 0.415121| num_tokens: 13,381\n",
      "Step 00411/00701 | Training loss: 0.672638| lrm: 0.413695| num_tokens: 12,908\n",
      "Step 00412/00701 | Training loss: 0.556915| lrm: 0.412268| num_tokens: 11,920\n",
      "Step 00413/00701 | Training loss: 0.764381| lrm: 0.410842| num_tokens: 7,318\n",
      "Step 00414/00701 | Training loss: 0.784515| lrm: 0.409415| num_tokens: 14,703\n",
      "Step 00415/00701 | Training loss: 0.621869| lrm: 0.407989| num_tokens: 13,803\n",
      "Step 00416/00701 | Training loss: 0.848757| lrm: 0.406562| num_tokens: 11,950\n",
      "Step 00417/00701 | Training loss: 0.576378| lrm: 0.405136| num_tokens: 12,015\n",
      "Step 00418/00701 | Training loss: 0.343718| lrm: 0.403709| num_tokens: 3,822\n",
      "Step 00419/00701 | Training loss: 0.849203| lrm: 0.402282| num_tokens: 9,954\n",
      "Step 00420/00701 | Training loss: 1.078566| lrm: 0.400856| num_tokens: 10,389\n",
      "Step 00421/00701 | Training loss: 0.661342| lrm: 0.399429| num_tokens: 12,794\n",
      "Step 00422/00701 | Training loss: 0.476593| lrm: 0.398003| num_tokens: 13,678\n",
      "Step 00423/00701 | Training loss: 0.467539| lrm: 0.396576| num_tokens: 15,301\n",
      "Step 00424/00701 | Training loss: 0.545202| lrm: 0.395150| num_tokens: 14,936\n",
      "Step 00425/00701 | Training loss: 0.775753| lrm: 0.393723| num_tokens: 7,674\n",
      "Step 00426/00701 | Training loss: 0.670309| lrm: 0.392297| num_tokens: 6,525\n",
      "Step 00427/00701 | Training loss: 0.610168| lrm: 0.390870| num_tokens: 8,750\n",
      "Step 00428/00701 | Training loss: 0.619577| lrm: 0.389444| num_tokens: 9,854\n",
      "Step 00429/00701 | Training loss: 0.578559| lrm: 0.388017| num_tokens: 10,685\n",
      "Step 00430/00701 | Training loss: 0.646812| lrm: 0.386591| num_tokens: 11,203\n",
      "Step 00431/00701 | Training loss: 1.465271| lrm: 0.385164| num_tokens: 8,915\n",
      "Step 00432/00701 | Training loss: 0.791376| lrm: 0.383738| num_tokens: 13,410\n",
      "Step 00433/00701 | Training loss: 0.471707| lrm: 0.382311| num_tokens: 10,482\n",
      "Step 00434/00701 | Training loss: 0.703845| lrm: 0.380884| num_tokens: 11,900\n",
      "Step 00435/00701 | Training loss: 0.555724| lrm: 0.379458| num_tokens: 12,163\n",
      "Step 00436/00701 | Training loss: 0.583573| lrm: 0.378031| num_tokens: 13,110\n",
      "Step 00437/00701 | Training loss: 0.739474| lrm: 0.376605| num_tokens: 12,842\n",
      "Step 00438/00701 | Training loss: 1.021697| lrm: 0.375178| num_tokens: 10,418\n",
      "Step 00439/00701 | Training loss: 0.660454| lrm: 0.373752| num_tokens: 11,834\n",
      "Step 00440/00701 | Training loss: 0.554809| lrm: 0.372325| num_tokens: 10,470\n",
      "Step 00441/00701 | Training loss: 0.634389| lrm: 0.370899| num_tokens: 11,811\n",
      "Step 00442/00701 | Training loss: 0.678502| lrm: 0.369472| num_tokens: 12,068\n",
      "Step 00443/00701 | Training loss: 0.591110| lrm: 0.368046| num_tokens: 10,748\n",
      "Step 00444/00701 | Training loss: 0.563902| lrm: 0.366619| num_tokens: 13,115\n",
      "Step 00445/00701 | Training loss: 0.632529| lrm: 0.365193| num_tokens: 9,908\n",
      "Step 00446/00701 | Training loss: 0.383065| lrm: 0.363766| num_tokens: 13,292\n",
      "Step 00447/00701 | Training loss: 0.880913| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.779500| lrm: 0.360913| num_tokens: 10,543\n",
      "Step 00449/00701 | Training loss: 0.656256| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450/00701 | Training loss: 0.857146| lrm: 0.358060| num_tokens: 15,859\n",
      "Step 00451/00701 | Training loss: 0.615360| lrm: 0.356633| num_tokens: 10,661\n",
      "Step 00452/00701 | Training loss: 0.617424| lrm: 0.355207| num_tokens: 9,309\n",
      "Step 00453/00701 | Training loss: 0.616582| lrm: 0.353780| num_tokens: 12,534\n",
      "Step 00454/00701 | Training loss: 0.732812| lrm: 0.352354| num_tokens: 7,954\n",
      "Step 00455/00701 | Training loss: 0.747389| lrm: 0.350927| num_tokens: 7,756\n",
      "Step 00456/00701 | Training loss: 0.795297| lrm: 0.349501| num_tokens: 10,553\n",
      "Step 00457/00701 | Training loss: 0.508480| lrm: 0.348074| num_tokens: 10,102\n",
      "Step 00458/00701 | Training loss: 0.653916| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.553973| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460/00701 | Training loss: 0.612337| lrm: 0.343795| num_tokens: 8,423\n",
      "Step 00461/00701 | Training loss: 0.676053| lrm: 0.342368| num_tokens: 12,619\n",
      "Step 00462/00701 | Training loss: 0.515601| lrm: 0.340942| num_tokens: 9,897\n",
      "Step 00463/00701 | Training loss: 0.887015| lrm: 0.339515| num_tokens: 7,315\n",
      "Step 00464/00701 | Training loss: 0.509728| lrm: 0.338088| num_tokens: 9,796\n",
      "Step 00465/00701 | Training loss: 0.771618| lrm: 0.336662| num_tokens: 8,337\n",
      "Step 00466/00701 | Training loss: 0.733140| lrm: 0.335235| num_tokens: 11,437\n",
      "Step 00467/00701 | Training loss: 0.549579| lrm: 0.333809| num_tokens: 10,836\n",
      "Step 00468/00701 | Training loss: 0.602202| lrm: 0.332382| num_tokens: 9,669\n",
      "Step 00469/00701 | Training loss: 0.400333| lrm: 0.330956| num_tokens: 8,334\n",
      "Step 00470/00701 | Training loss: 0.873316| lrm: 0.329529| num_tokens: 8,696\n",
      "Step 00471/00701 | Training loss: 0.773230| lrm: 0.328103| num_tokens: 15,225\n",
      "Step 00472/00701 | Training loss: 1.390581| lrm: 0.326676| num_tokens: 13,449\n",
      "Step 00473/00701 | Training loss: 0.530597| lrm: 0.325250| num_tokens: 9,588\n",
      "Step 00474/00701 | Training loss: 0.900329| lrm: 0.323823| num_tokens: 14,735\n",
      "Step 00475/00701 | Training loss: 0.698999| lrm: 0.322397| num_tokens: 12,695\n",
      "Step 00476/00701 | Training loss: 0.964238| lrm: 0.320970| num_tokens: 11,269\n",
      "Step 00477/00701 | Training loss: 0.773819| lrm: 0.319544| num_tokens: 13,479\n",
      "Step 00478/00701 | Training loss: 1.065739| lrm: 0.318117| num_tokens: 6,844\n",
      "Step 00479/00701 | Training loss: 0.680897| lrm: 0.316690| num_tokens: 13,524\n",
      "Step 00480/00701 | Training loss: 0.347454| lrm: 0.315264| num_tokens: 13,579\n",
      "Step 00481/00701 | Training loss: 0.842713| lrm: 0.313837| num_tokens: 7,030\n",
      "Step 00482/00701 | Training loss: 0.749617| lrm: 0.312411| num_tokens: 9,860\n",
      "Step 00483/00701 | Training loss: 0.603225| lrm: 0.310984| num_tokens: 11,052\n",
      "Step 00484/00701 | Training loss: 0.386250| lrm: 0.309558| num_tokens: 10,658\n",
      "Step 00485/00701 | Training loss: 0.923354| lrm: 0.308131| num_tokens: 10,626\n",
      "Step 00486/00701 | Training loss: 0.355420| lrm: 0.306705| num_tokens: 9,659\n",
      "Step 00487/00701 | Training loss: 0.556594| lrm: 0.305278| num_tokens: 8,589\n",
      "Step 00488/00701 | Training loss: 0.354309| lrm: 0.303852| num_tokens: 11,262\n",
      "Step 00489/00701 | Training loss: 0.764723| lrm: 0.302425| num_tokens: 12,386\n",
      "Step 00490/00701 | Training loss: 0.751845| lrm: 0.300999| num_tokens: 8,470\n",
      "Step 00491/00701 | Training loss: 0.802475| lrm: 0.299572| num_tokens: 8,823\n",
      "Step 00492/00701 | Training loss: 1.185403| lrm: 0.298146| num_tokens: 14,328\n",
      "Step 00493/00701 | Training loss: 0.478703| lrm: 0.296719| num_tokens: 7,280\n",
      "Step 00494/00701 | Training loss: 0.452476| lrm: 0.295292| num_tokens: 8,602\n",
      "Step 00495/00701 | Training loss: 0.669844| lrm: 0.293866| num_tokens: 10,853\n",
      "Step 00496/00701 | Training loss: 0.615499| lrm: 0.292439| num_tokens: 10,978\n",
      "Step 00497/00701 | Training loss: 0.573461| lrm: 0.291013| num_tokens: 11,917\n",
      "Step 00498/00701 | Training loss: 0.747741| lrm: 0.289586| num_tokens: 11,750\n",
      "Step 00499/00701 | Training loss: 0.240420| lrm: 0.288160| num_tokens: 14,268\n",
      "Step 00500 | Validation loss: 0.838459\n",
      "Step 00500/00701 | Training loss: 0.641377| lrm: 0.286733| num_tokens: 13,674\n",
      "Step 00501/00701 | Training loss: 0.585713| lrm: 0.285307| num_tokens: 9,956\n",
      "Step 00502/00701 | Training loss: 0.320142| lrm: 0.283880| num_tokens: 9,790\n",
      "Step 00503/00701 | Training loss: 1.166208| lrm: 0.282454| num_tokens: 13,073\n",
      "Step 00504/00701 | Training loss: 1.306847| lrm: 0.281027| num_tokens: 9,770\n",
      "Step 00505/00701 | Training loss: 0.649235| lrm: 0.279601| num_tokens: 8,896\n",
      "Step 00506/00701 | Training loss: 1.054149| lrm: 0.278174| num_tokens: 9,653\n",
      "Step 00507/00701 | Training loss: 0.486554| lrm: 0.276748| num_tokens: 6,766\n",
      "Step 00508/00701 | Training loss: 1.063532| lrm: 0.275321| num_tokens: 18,631\n",
      "Step 00509/00701 | Training loss: 0.630547| lrm: 0.273894| num_tokens: 9,677\n",
      "Step 00510/00701 | Training loss: 0.489247| lrm: 0.272468| num_tokens: 9,945\n",
      "Step 00511/00701 | Training loss: 0.451476| lrm: 0.271041| num_tokens: 5,589\n",
      "Step 00512/00701 | Training loss: 0.807082| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.714291| lrm: 0.268188| num_tokens: 8,748\n",
      "Step 00514/00701 | Training loss: 0.690907| lrm: 0.266762| num_tokens: 9,906\n",
      "Step 00515/00701 | Training loss: 0.924400| lrm: 0.265335| num_tokens: 10,737\n",
      "Step 00516/00701 | Training loss: 0.959553| lrm: 0.263909| num_tokens: 12,028\n",
      "Step 00517/00701 | Training loss: 0.468500| lrm: 0.262482| num_tokens: 10,743\n",
      "Step 00518/00701 | Training loss: 0.855585| lrm: 0.261056| num_tokens: 10,295\n",
      "Step 00519/00701 | Training loss: 0.744247| lrm: 0.259629| num_tokens: 8,455\n",
      "Step 00520/00701 | Training loss: 0.728762| lrm: 0.258203| num_tokens: 9,734\n",
      "Step 00521/00701 | Training loss: 0.798012| lrm: 0.256776| num_tokens: 14,754\n",
      "Step 00522/00701 | Training loss: 0.893556| lrm: 0.255350| num_tokens: 9,474\n",
      "Step 00523/00701 | Training loss: 0.551819| lrm: 0.253923| num_tokens: 11,748\n",
      "Step 00524/00701 | Training loss: 0.721186| lrm: 0.252496| num_tokens: 7,307\n",
      "Step 00525/00701 | Training loss: 1.343570| lrm: 0.251070| num_tokens: 10,440\n",
      "Step 00526/00701 | Training loss: 0.918978| lrm: 0.249643| num_tokens: 9,488\n",
      "Step 00527/00701 | Training loss: 0.882192| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 0.973257| lrm: 0.246790| num_tokens: 14,133\n",
      "Step 00529/00701 | Training loss: 0.873341| lrm: 0.245364| num_tokens: 14,955\n",
      "Step 00530/00701 | Training loss: 0.380924| lrm: 0.243937| num_tokens: 8,918\n",
      "Step 00531/00701 | Training loss: 0.490819| lrm: 0.242511| num_tokens: 10,504\n",
      "Step 00532/00701 | Training loss: 0.958311| lrm: 0.241084| num_tokens: 8,050\n",
      "Step 00533/00701 | Training loss: 0.848293| lrm: 0.239658| num_tokens: 11,660\n",
      "Step 00534/00701 | Training loss: 0.935520| lrm: 0.238231| num_tokens: 12,670\n",
      "Step 00535/00701 | Training loss: 0.513641| lrm: 0.236805| num_tokens: 9,335\n",
      "Step 00536/00701 | Training loss: 0.413200| lrm: 0.235378| num_tokens: 10,272\n",
      "Step 00537/00701 | Training loss: 0.581989| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.537581| lrm: 0.232525| num_tokens: 10,262\n",
      "Step 00539/00701 | Training loss: 0.654072| lrm: 0.231098| num_tokens: 13,039\n",
      "Step 00540/00701 | Training loss: 0.672221| lrm: 0.229672| num_tokens: 7,602\n",
      "Step 00541/00701 | Training loss: 0.857668| lrm: 0.228245| num_tokens: 15,928\n",
      "Step 00542/00701 | Training loss: 0.982991| lrm: 0.226819| num_tokens: 10,868\n",
      "Step 00543/00701 | Training loss: 0.313316| lrm: 0.225392| num_tokens: 11,889\n",
      "Step 00544/00701 | Training loss: 0.775673| lrm: 0.223966| num_tokens: 11,869\n",
      "Step 00545/00701 | Training loss: 0.617418| lrm: 0.222539| num_tokens: 13,439\n",
      "Step 00546/00701 | Training loss: 0.636224| lrm: 0.221113| num_tokens: 13,832\n",
      "Step 00547/00701 | Training loss: 0.982623| lrm: 0.219686| num_tokens: 6,025\n",
      "Step 00548/00701 | Training loss: 0.623296| lrm: 0.218260| num_tokens: 13,320\n",
      "Step 00549/00701 | Training loss: 0.863742| lrm: 0.216833| num_tokens: 8,365\n",
      "Step 00550/00701 | Training loss: 0.600349| lrm: 0.215407| num_tokens: 13,486\n",
      "Step 00551/00701 | Training loss: 0.622710| lrm: 0.213980| num_tokens: 11,187\n",
      "Step 00552/00701 | Training loss: 0.591207| lrm: 0.212553| num_tokens: 8,527\n",
      "Step 00553/00701 | Training loss: 0.208982| lrm: 0.211127| num_tokens: 10,575\n",
      "Step 00554/00701 | Training loss: 0.876818| lrm: 0.209700| num_tokens: 13,010\n",
      "Step 00555/00701 | Training loss: 0.748187| lrm: 0.208274| num_tokens: 11,591\n",
      "Step 00556/00701 | Training loss: 0.725119| lrm: 0.206847| num_tokens: 7,430\n",
      "Step 00557/00701 | Training loss: 0.595235| lrm: 0.205421| num_tokens: 9,357\n",
      "Step 00558/00701 | Training loss: 0.715941| lrm: 0.203994| num_tokens: 10,195\n",
      "Step 00559/00701 | Training loss: 0.795669| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560/00701 | Training loss: 0.371122| lrm: 0.201141| num_tokens: 13,607\n",
      "Step 00561/00701 | Training loss: 0.984448| lrm: 0.199715| num_tokens: 6,692\n",
      "Step 00562/00701 | Training loss: 0.215164| lrm: 0.198288| num_tokens: 8,684\n",
      "Step 00563/00701 | Training loss: 0.779475| lrm: 0.196862| num_tokens: 12,884\n",
      "Step 00564/00701 | Training loss: 0.706231| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 0.824729| lrm: 0.194009| num_tokens: 13,881\n",
      "Step 00566/00701 | Training loss: 0.812887| lrm: 0.192582| num_tokens: 12,218\n",
      "Step 00567/00701 | Training loss: 0.701183| lrm: 0.191155| num_tokens: 18,530\n",
      "Step 00568/00701 | Training loss: 0.619690| lrm: 0.189729| num_tokens: 10,006\n",
      "Step 00569/00701 | Training loss: 0.739754| lrm: 0.188302| num_tokens: 9,470\n",
      "Step 00570/00701 | Training loss: 1.028446| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.601505| lrm: 0.185449| num_tokens: 13,107\n",
      "Step 00572/00701 | Training loss: 0.585724| lrm: 0.184023| num_tokens: 7,263\n",
      "Step 00573/00701 | Training loss: 0.895014| lrm: 0.182596| num_tokens: 10,394\n",
      "Step 00574/00701 | Training loss: 0.606565| lrm: 0.181170| num_tokens: 14,548\n",
      "Step 00575/00701 | Training loss: 0.310901| lrm: 0.179743| num_tokens: 7,573\n",
      "Step 00576/00701 | Training loss: 1.407811| lrm: 0.178317| num_tokens: 8,840\n",
      "Step 00577/00701 | Training loss: 0.571059| lrm: 0.176890| num_tokens: 13,431\n",
      "Step 00578/00701 | Training loss: 0.475303| lrm: 0.175464| num_tokens: 8,194\n",
      "Step 00579/00701 | Training loss: 0.398530| lrm: 0.174037| num_tokens: 10,225\n",
      "Step 00580/00701 | Training loss: 0.830318| lrm: 0.172611| num_tokens: 9,080\n",
      "Step 00581/00701 | Training loss: 0.835681| lrm: 0.171184| num_tokens: 13,479\n",
      "Step 00582/00701 | Training loss: 0.868037| lrm: 0.169757| num_tokens: 8,855\n",
      "Step 00583/00701 | Training loss: 0.512010| lrm: 0.168331| num_tokens: 12,739\n",
      "Step 00584/00701 | Training loss: 0.643892| lrm: 0.166904| num_tokens: 13,828\n",
      "Step 00585/00701 | Training loss: 0.488158| lrm: 0.165478| num_tokens: 14,931\n",
      "Step 00586/00701 | Training loss: 0.735968| lrm: 0.164051| num_tokens: 9,932\n",
      "Step 00587/00701 | Training loss: 0.589827| lrm: 0.162625| num_tokens: 7,822\n",
      "Step 00588/00701 | Training loss: 0.768748| lrm: 0.161198| num_tokens: 12,327\n",
      "Step 00589/00701 | Training loss: 0.412357| lrm: 0.159772| num_tokens: 9,059\n",
      "Step 00590/00701 | Training loss: 0.456057| lrm: 0.158345| num_tokens: 10,346\n",
      "Step 00591/00701 | Training loss: 0.641057| lrm: 0.156919| num_tokens: 8,572\n",
      "Step 00592/00701 | Training loss: 0.820233| lrm: 0.155492| num_tokens: 11,747\n",
      "Step 00593/00701 | Training loss: 0.839082| lrm: 0.154066| num_tokens: 11,792\n",
      "Step 00594/00701 | Training loss: 0.181506| lrm: 0.152639| num_tokens: 11,223\n",
      "Step 00595/00701 | Training loss: 0.428453| lrm: 0.151213| num_tokens: 8,404\n",
      "Step 00596/00701 | Training loss: 0.637910| lrm: 0.149786| num_tokens: 5,659\n",
      "Step 00597/00701 | Training loss: 0.980509| lrm: 0.148359| num_tokens: 10,837\n",
      "Step 00598/00701 | Training loss: 0.622088| lrm: 0.146933| num_tokens: 9,773\n",
      "Step 00599/00701 | Training loss: 1.330005| lrm: 0.145506| num_tokens: 8,271\n",
      "Step 00600 | Validation loss: 0.838267\n",
      "final: 412/1024 (40.23%)\n",
      "final: 685/1024 (66.89%)\n",
      "Step 00600 | mmlu_acc: 0.402344, arc_easy_acc: 0.668945\n",
      "Step 00600/00701 | Training loss: 0.618573| lrm: 0.144080| num_tokens: 10,440\n",
      "Step 00601/00701 | Training loss: 0.854795| lrm: 0.142653| num_tokens: 11,013\n",
      "Step 00602/00701 | Training loss: 0.584015| lrm: 0.141227| num_tokens: 8,231\n",
      "Step 00603/00701 | Training loss: 0.869883| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.778230| lrm: 0.138374| num_tokens: 13,521\n",
      "Step 00605/00701 | Training loss: 0.649862| lrm: 0.136947| num_tokens: 10,454\n",
      "Step 00606/00701 | Training loss: 0.702967| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.559560| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.490092| lrm: 0.132668| num_tokens: 12,844\n",
      "Step 00609/00701 | Training loss: 0.924995| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610/00701 | Training loss: 0.401428| lrm: 0.129815| num_tokens: 14,180\n",
      "Step 00611/00701 | Training loss: 0.763083| lrm: 0.128388| num_tokens: 9,318\n",
      "Step 00612/00701 | Training loss: 0.692121| lrm: 0.126961| num_tokens: 9,526\n",
      "Step 00613/00701 | Training loss: 0.843200| lrm: 0.125535| num_tokens: 16,556\n",
      "Step 00614/00701 | Training loss: 0.460230| lrm: 0.124108| num_tokens: 15,344\n",
      "Step 00615/00701 | Training loss: 0.735136| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.474265| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.573072| lrm: 0.119829| num_tokens: 11,178\n",
      "Step 00618/00701 | Training loss: 0.473020| lrm: 0.118402| num_tokens: 8,839\n",
      "Step 00619/00701 | Training loss: 0.562298| lrm: 0.116976| num_tokens: 9,198\n",
      "Step 00620/00701 | Training loss: 0.509403| lrm: 0.115549| num_tokens: 12,335\n",
      "Step 00621/00701 | Training loss: 0.436846| lrm: 0.114123| num_tokens: 7,717\n",
      "Step 00622/00701 | Training loss: 0.734720| lrm: 0.112696| num_tokens: 7,352\n",
      "Step 00623/00701 | Training loss: 0.705701| lrm: 0.111270| num_tokens: 14,524\n",
      "Step 00624/00701 | Training loss: 1.119468| lrm: 0.109843| num_tokens: 12,340\n",
      "Step 00625/00701 | Training loss: 0.508640| lrm: 0.108417| num_tokens: 6,846\n",
      "Step 00626/00701 | Training loss: 0.609664| lrm: 0.106990| num_tokens: 11,578\n",
      "Step 00627/00701 | Training loss: 0.447369| lrm: 0.105563| num_tokens: 8,516\n",
      "Step 00628/00701 | Training loss: 0.691901| lrm: 0.104137| num_tokens: 6,970\n",
      "Step 00629/00701 | Training loss: 0.503871| lrm: 0.102710| num_tokens: 12,393\n",
      "Step 00630/00701 | Training loss: 0.472569| lrm: 0.101284| num_tokens: 10,247\n",
      "Step 00631/00701 | Training loss: 0.597468| lrm: 0.099857| num_tokens: 12,196\n",
      "Step 00632/00701 | Training loss: 0.535187| lrm: 0.098431| num_tokens: 10,965\n",
      "Step 00633/00701 | Training loss: 0.762045| lrm: 0.097004| num_tokens: 11,202\n",
      "Step 00634/00701 | Training loss: 0.909387| lrm: 0.095578| num_tokens: 7,816\n",
      "Step 00635/00701 | Training loss: 0.624073| lrm: 0.094151| num_tokens: 12,055\n",
      "Step 00636/00701 | Training loss: 0.636832| lrm: 0.092725| num_tokens: 8,142\n",
      "Step 00637/00701 | Training loss: 0.676470| lrm: 0.091298| num_tokens: 13,111\n",
      "Step 00638/00701 | Training loss: 0.435646| lrm: 0.089872| num_tokens: 8,344\n",
      "Step 00639/00701 | Training loss: 0.562770| lrm: 0.088445| num_tokens: 13,046\n",
      "Step 00640/00701 | Training loss: 0.769012| lrm: 0.087019| num_tokens: 12,705\n",
      "Step 00641/00701 | Training loss: 0.751552| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.293954| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.712248| lrm: 0.082739| num_tokens: 8,870\n",
      "Step 00644/00701 | Training loss: 0.693224| lrm: 0.081312| num_tokens: 8,860\n",
      "Step 00645/00701 | Training loss: 1.007270| lrm: 0.079886| num_tokens: 13,445\n",
      "Step 00646/00701 | Training loss: 0.614739| lrm: 0.078459| num_tokens: 12,791\n",
      "Step 00647/00701 | Training loss: 0.848109| lrm: 0.077033| num_tokens: 8,244\n",
      "Step 00648/00701 | Training loss: 0.681618| lrm: 0.075606| num_tokens: 10,328\n",
      "Step 00649/00701 | Training loss: 0.521793| lrm: 0.074180| num_tokens: 12,274\n",
      "Step 00650/00701 | Training loss: 1.159785| lrm: 0.072753| num_tokens: 7,238\n",
      "Step 00651/00701 | Training loss: 1.071152| lrm: 0.071327| num_tokens: 13,995\n",
      "Step 00652/00701 | Training loss: 0.951002| lrm: 0.069900| num_tokens: 11,527\n",
      "Step 00653/00701 | Training loss: 0.545041| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.687800| lrm: 0.067047| num_tokens: 8,706\n",
      "Step 00655/00701 | Training loss: 0.705683| lrm: 0.065621| num_tokens: 11,506\n",
      "Step 00656/00701 | Training loss: 0.898924| lrm: 0.064194| num_tokens: 11,147\n",
      "Step 00657/00701 | Training loss: 0.271694| lrm: 0.062767| num_tokens: 10,337\n",
      "Step 00658/00701 | Training loss: 0.826303| lrm: 0.061341| num_tokens: 9,782\n",
      "Step 00659/00701 | Training loss: 0.606934| lrm: 0.059914| num_tokens: 8,106\n",
      "Step 00660/00701 | Training loss: 0.649960| lrm: 0.058488| num_tokens: 10,482\n",
      "Step 00661/00701 | Training loss: 0.910826| lrm: 0.057061| num_tokens: 11,312\n",
      "Step 00662/00701 | Training loss: 0.550343| lrm: 0.055635| num_tokens: 7,873\n",
      "Step 00663/00701 | Training loss: 0.552655| lrm: 0.054208| num_tokens: 9,862\n",
      "Step 00664/00701 | Training loss: 0.586206| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.363571| lrm: 0.051355| num_tokens: 9,695\n",
      "Step 00666/00701 | Training loss: 0.735613| lrm: 0.049929| num_tokens: 10,706\n",
      "Step 00667/00701 | Training loss: 0.659400| lrm: 0.048502| num_tokens: 13,505\n",
      "Step 00668/00701 | Training loss: 0.514684| lrm: 0.047076| num_tokens: 11,914\n",
      "Step 00669/00701 | Training loss: 0.898354| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670/00701 | Training loss: 0.643805| lrm: 0.044223| num_tokens: 9,518\n",
      "Step 00671/00701 | Training loss: 0.611323| lrm: 0.042796| num_tokens: 10,909\n",
      "Step 00672/00701 | Training loss: 0.560672| lrm: 0.041369| num_tokens: 10,538\n",
      "Step 00673/00701 | Training loss: 0.607424| lrm: 0.039943| num_tokens: 9,858\n",
      "Step 00674/00701 | Training loss: 0.511082| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675/00701 | Training loss: 0.561546| lrm: 0.037090| num_tokens: 10,396\n",
      "Step 00676/00701 | Training loss: 0.666294| lrm: 0.035663| num_tokens: 10,102\n",
      "Step 00677/00701 | Training loss: 0.807462| lrm: 0.034237| num_tokens: 16,341\n",
      "Step 00678/00701 | Training loss: 0.294584| lrm: 0.032810| num_tokens: 12,043\n",
      "Step 00679/00701 | Training loss: 0.710408| lrm: 0.031384| num_tokens: 8,347\n",
      "Step 00680/00701 | Training loss: 0.955309| lrm: 0.029957| num_tokens: 12,051\n",
      "Step 00681/00701 | Training loss: 0.537868| lrm: 0.028531| num_tokens: 9,419\n",
      "Step 00682/00701 | Training loss: 0.588857| lrm: 0.027104| num_tokens: 14,514\n",
      "Step 00683/00701 | Training loss: 0.726063| lrm: 0.025678| num_tokens: 9,744\n",
      "Step 00684/00701 | Training loss: 1.328259| lrm: 0.024251| num_tokens: 12,753\n",
      "Step 00685/00701 | Training loss: 0.429536| lrm: 0.022825| num_tokens: 9,278\n",
      "Step 00686/00701 | Training loss: 0.486602| lrm: 0.021398| num_tokens: 10,169\n",
      "Step 00687/00701 | Training loss: 0.595416| lrm: 0.019971| num_tokens: 13,347\n",
      "Step 00688/00701 | Training loss: 0.582650| lrm: 0.018545| num_tokens: 10,466\n",
      "Step 00689/00701 | Training loss: 0.436261| lrm: 0.017118| num_tokens: 10,657\n",
      "Step 00690/00701 | Training loss: 0.375175| lrm: 0.015692| num_tokens: 8,626\n",
      "Step 00691/00701 | Training loss: 0.474071| lrm: 0.014265| num_tokens: 10,636\n",
      "Step 00692/00701 | Training loss: 0.678132| lrm: 0.012839| num_tokens: 11,938\n",
      "Step 00693/00701 | Training loss: 0.949519| lrm: 0.011412| num_tokens: 9,694\n",
      "Step 00694/00701 | Training loss: 0.982275| lrm: 0.009986| num_tokens: 10,112\n",
      "Step 00695/00701 | Training loss: 0.836974| lrm: 0.008559| num_tokens: 8,530\n",
      "Step 00696/00701 | Training loss: 0.425500| lrm: 0.007133| num_tokens: 12,552\n",
      "Step 00697/00701 | Training loss: 1.007003| lrm: 0.005706| num_tokens: 10,318\n",
      "Step 00698/00701 | Training loss: 0.463417| lrm: 0.004280| num_tokens: 8,863\n",
      "Step 00699/00701 | Training loss: 0.447066| lrm: 0.002853| num_tokens: 11,151\n",
      "Step 00700 | Validation loss: 0.838492\n",
      "final: 416/1024 (40.62%)\n",
      "final: 686/1024 (66.99%)\n",
      "Step 00700 | mmlu_acc: 0.406250, arc_easy_acc: 0.669922\n",
      "[W1229 16:51:41.706466353 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:41.777146118 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:41.782925809 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:41.802573376 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:41.923803813 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat2/chatsft_checkpoints/d32/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatsft_checkpoints/d32/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatsft_checkpoints/d32\n",
      "wandb: updating run metadata\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb: arc_easy_acc ‚ñÅ‚ñá‚ñà‚ñà\n",
      "wandb:          lrm ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:     mmlu_acc ‚ñÅ‚ñÜ‚ñá‚ñà\n",
      "wandb:   num_tokens ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ\n",
      "wandb:         step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:   train_loss ‚ñÑ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ\n",
      "wandb:     val_loss ‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb: arc_easy_acc 0.66992\n",
      "wandb:          lrm 0.00285\n",
      "wandb:     mmlu_acc 0.40625\n",
      "wandb:   num_tokens 11151\n",
      "wandb:         step 700\n",
      "wandb:   train_loss 0.44707\n",
      "wandb:     val_loss 0.83849\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-7 at: https://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/svzf7cfx\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat-sft\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251229_164703-svzf7cfx/logs\n",
      "[W1229 16:51:54.462503523 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:54.612196876 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1229 16:51:54.618948060 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/sft_train_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f97f61e-a1ae-4dd8-a9cc-f0a58443b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/chatsft_checkpoints/d32 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "final: 1580/2376 (66.50%)\n",
      "ARC-Easy accuracy: 66.50%\n",
      "final: 541/1172 (46.16%)\n",
      "ARC-Challenge accuracy: 46.16%\n",
      "final: 5576/14042 (39.71%)\n",
      "MMLU accuracy: 39.71%\n",
      "\u001b[KRank 5 | 15/165 (9.09%)]]\n",
      "\u001b[KRank 0 | 16/165 (9.70%)]]\n",
      "\u001b[KRank 7 | 27/164 (16.46%)]\n",
      "\u001b[KRank 6 | 26/165 (15.76%)]\n",
      "\u001b[KRank 1 | 22/165 (13.33%)]\n",
      "\u001b[KRank 4 | 20/165 (12.12%)]\n",
      "\u001b[KRank 2 | 19/165 (11.52%)]\n",
      "\u001b[KRank 3 | 31/165 (18.79%)]\n",
      "==================================================\n",
      "final: 176/1319 (13.34%)\n",
      "GSM8K accuracy: 13.34%\n",
      "\u001b[KRank 5 | 4/20 (20.00%)]\n",
      "\u001b[KRank 1 | 3/21 (14.29%)]\n",
      "\u001b[KRank 7 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 0 | 0/21 (0.00%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "==================================================\n",
      "final: 13/164 (7.93%)\n",
      "HumanEval accuracy: 7.93%\n",
      "\u001b[KRank 7 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 5 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 0 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 32/32 (100.00%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 3 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 254/256 (99.22%)\n",
      "SpellingBee accuracy: 99.22%\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/sft_chat_eval_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c05ddf-99ba-47be-8fd2-d3ee0d419696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chat SFT\n",
      "timestamp: 2025-12-29 16:51:52\n",
      "\n",
      "- run: challenge-38-7\n",
      "- source: mid\n",
      "- device_type: \n",
      "- dtype: bfloat16\n",
      "- device_batch_size: 4\n",
      "- num_epochs: 1\n",
      "- num_iterations: -1\n",
      "- max_data_tokens: -1\n",
      "- target_examples_per_step: 32\n",
      "- unembedding_lr: 0.0040\n",
      "- embedding_lr: 0.2000\n",
      "- matrix_lr: 0.0200\n",
      "- weight_decay: 0.0000\n",
      "- init_lr_frac: 0.0200\n",
      "- eval_every: 100\n",
      "- eval_steps: 100\n",
      "- eval_metrics_every: 200\n",
      "- eval_metrics_max_problems: 1024\n",
      "- Training rows: 22,439\n",
      "- Number of iterations: 701\n",
      "- Training loss: 0.4471\n",
      "- Validation loss: 0.8385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/chat-sft.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f31878-8ffe-4cc3-a624-e22241fa2f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chat evaluation sft\n",
      "timestamp: 2025-12-29 17:02:10\n",
      "\n",
      "- source: sft\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d32\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.6650\n",
      "- ARC-Challenge: 0.4616\n",
      "- MMLU: 0.3971\n",
      "- GSM8K: 0.1334\n",
      "- HumanEval: 0.0793\n",
      "- SpellingBee: 0.9922\n",
      "- ChatCORE metric: 0.3727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/chat-evaluation-sft.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7ee07-343d-4511-822c-7903e3590c5a",
   "metadata": {},
   "source": [
    "### \"final\" report (excludes reinforcement learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1300856f-7fb2-47b3-afdc-4bcc5696f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nanochat training report\n",
      "\n",
      "Generated: 2025-12-22 16:07:10\n",
      "\n",
      "## Environment\n",
      "\n",
      "### Git Information\n",
      "- Branch: master\n",
      "- Commit: bc88d72 (dirty)\n",
      "- Message: getting ready to work on gpu machine for challenge 38: train d32\n",
      "\n",
      "### Hardware\n",
      "- Platform: Linux\n",
      "- CPUs: 104 cores (208 logical)\n",
      "- Memory: 1771.7 GB\n",
      "- GPUs: 8x NVIDIA H100 80GB HBM3\n",
      "- GPU Memory: 633.5 GB total\n",
      "- CUDA Version: 12.8\n",
      "\n",
      "### Software\n",
      "- Python: 3.10.12\n",
      "- PyTorch: 2.9.0+cu128\n",
      "\n",
      "Run started: 2025-12-22 16:07:10\n",
      "\n",
      "--\n",
      "\n",
      "## Base model training\n",
      "timestamp: 2025-12-24 02:02:38\n",
      "\n",
      "- run: challenge-38-4\n",
      "- device_type: \n",
      "- depth: 32\n",
      "- max_seq_len: 2048\n",
      "- num_iterations: -1\n",
      "- target_param_data_ratio: 20\n",
      "- device_batch_size: 8\n",
      "- total_batch_size: 524,288\n",
      "- embedding_lr: 0.2000\n",
      "- unembedding_lr: 0.0040\n",
      "- weight_decay: 0.0000\n",
      "- matrix_lr: 0.0200\n",
      "- grad_clip: 1.0000\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- eval_every: 250\n",
      "- eval_tokens: 10,485,760\n",
      "- core_metric_every: 2000\n",
      "- core_metric_max_per_task: 500\n",
      "- sample_every: 2000\n",
      "- model_tag: \n",
      "- Number of parameters: 1,879,048,192\n",
      "- Number of FLOPs per token: 1.207960e+10\n",
      "- Calculated number of iterations: 71,680\n",
      "- Number of training tokens: 37,580,963,840\n",
      "- Tokens : Params ratio: 20.0000\n",
      "- DDP world size: 8\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- Minimum validation bpb: 0.7233\n",
      "- Final validation bpb: 0.7233\n",
      "- CORE metric estimate: 0.3050\n",
      "- MFU %: 53.03%\n",
      "- Total training flops: 4.539628e+20\n",
      "- Total training time: 1961.72m\n",
      "- Peak memory usage: 77017.79MiB\n",
      "\n",
      "\n",
      "## Base model loss\n",
      "timestamp: 2025-12-24 02:05:23\n",
      "\n",
      "- train bpb: 0.7271\n",
      "- val bpb: 0.7235\n",
      "- sample 0: <|bos|>The capital of France is Paris. It is the largest city in the country and the second largest in Europe\n",
      "- sample 1: <|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic number 79 and the atomic\n",
      "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will be Tuesday. If today is\n",
      "- sample 3: <|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.\n",
      "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune\n",
      "- sample 5: <|bos|>My favorite color is blue. I love the color blue. I love the color blue. I love\n",
      "- sample 6: <|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x + 3 = 13\n",
      "\n",
      "\n",
      "## Base model evaluation\n",
      "timestamp: 2025-12-24 02:12:40\n",
      "\n",
      "- Model: base_model (step 71680)\n",
      "- CORE metric: 0.2993\n",
      "- hellaswag_zeroshot: 0.4429\n",
      "- jeopardy: 0.2976\n",
      "- bigbench_qa_wikidata: 0.6202\n",
      "- arc_easy: 0.6302\n",
      "- arc_challenge: 0.2457\n",
      "- copa: 0.4200\n",
      "- commonsense_qa: 0.0469\n",
      "- piqa: 0.4788\n",
      "- openbook_qa: 0.1840\n",
      "- lambada_openai: 0.4910\n",
      "- hellaswag: 0.4518\n",
      "- winograd: 0.4945\n",
      "- winogrande: 0.1997\n",
      "- bigbench_dyck_languages: 0.0780\n",
      "- agi_eval_lsat_ar: 0.0598\n",
      "- bigbench_cs_algorithms: 0.3720\n",
      "- bigbench_operators: 0.1762\n",
      "- bigbench_repeat_copy_logic: 0.0312\n",
      "- squad: 0.3996\n",
      "- coqa: 0.2937\n",
      "- boolq: -0.0068\n",
      "- bigbench_language_identification: 0.1776\n",
      "\n",
      "\n",
      "## Midtraining\n",
      "timestamp: 2025-12-29 16:34:21\n",
      "\n",
      "- run: challenge-38-6\n",
      "- device_type: \n",
      "- dtype: bfloat16\n",
      "- num_iterations: -1\n",
      "- max_seq_len: 2048\n",
      "- device_batch_size: 8\n",
      "- unembedding_lr: 0.0040\n",
      "- embedding_lr: 0.2000\n",
      "- matrix_lr: 0.0200\n",
      "- init_lr_frac: 1.0000\n",
      "- weight_decay: 0.0000\n",
      "- eval_every: 150\n",
      "- eval_tokens: 10,485,760\n",
      "- total_batch_size: 524,288\n",
      "- dry_run: 0\n",
      "- Number of iterations: 810\n",
      "- DDP world size: 8\n",
      "- Minimum validation bpb: 0.3392\n",
      "\n",
      "\n",
      "## Chat evaluation mid\n",
      "timestamp: 2025-12-29 16:44:49\n",
      "\n",
      "- source: mid\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d32\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.6507\n",
      "- ARC-Challenge: 0.4650\n",
      "- MMLU: 0.3909\n",
      "- GSM8K: 0.1077\n",
      "- HumanEval: 0.0854\n",
      "- SpellingBee: 0.9922\n",
      "- ChatCORE metric: 0.3657\n",
      "\n",
      "\n",
      "## Chat SFT\n",
      "timestamp: 2025-12-29 16:51:52\n",
      "\n",
      "- run: challenge-38-7\n",
      "- source: mid\n",
      "- device_type: \n",
      "- dtype: bfloat16\n",
      "- device_batch_size: 4\n",
      "- num_epochs: 1\n",
      "- num_iterations: -1\n",
      "- max_data_tokens: -1\n",
      "- target_examples_per_step: 32\n",
      "- unembedding_lr: 0.0040\n",
      "- embedding_lr: 0.2000\n",
      "- matrix_lr: 0.0200\n",
      "- weight_decay: 0.0000\n",
      "- init_lr_frac: 0.0200\n",
      "- eval_every: 100\n",
      "- eval_steps: 100\n",
      "- eval_metrics_every: 200\n",
      "- eval_metrics_max_problems: 1024\n",
      "- Training rows: 22,439\n",
      "- Number of iterations: 701\n",
      "- Training loss: 0.4471\n",
      "- Validation loss: 0.8385\n",
      "\n",
      "\n",
      "## Chat evaluation sft\n",
      "timestamp: 2025-12-29 17:02:10\n",
      "\n",
      "- source: sft\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d32\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.6650\n",
      "- ARC-Challenge: 0.4616\n",
      "- MMLU: 0.3971\n",
      "- GSM8K: 0.1334\n",
      "- HumanEval: 0.0793\n",
      "- SpellingBee: 0.9922\n",
      "- ChatCORE metric: 0.3727\n",
      "\n",
      "\n",
      "## Summary\n",
      "\n",
      "| Metric          | BASE     | MID      | SFT      | RL       |\n",
      "|-----------------|----------|----------|----------|----------|\n",
      "| CORE            | 0.2993   | -        | -        | -        |\n",
      "| ARC-Challenge   | -        | 0.4650   | 0.4616   | -        |\n",
      "| ARC-Easy        | -        | 0.6507   | 0.6650   | -        |\n",
      "| GSM8K           | -        | 0.1077   | 0.1334   | -        |\n",
      "| HumanEval       | -        | 0.0854   | 0.0793   | -        |\n",
      "| MMLU            | -        | 0.3909   | 0.3971   | -        |\n",
      "| ChatCORE        | -        | 0.3657   | 0.3727   | -        |\n",
      "\n",
      "Total wall clock time: 168h55m\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3b9c8-ba28-4790-a413-5dd3b2f989fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a13004-3db6-40e2-ba15-e9bd88913d26",
   "metadata": {},
   "source": [
    "### rl train and chat eval run output and reports\n",
    "\n",
    "I ran these on the 8 GPU server and then copied the run output and reports to my laptop.\n",
    "\n",
    "The chat eval is based on the step 360 checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c427577-7a83-4597-b365-ac4c375fccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding source = sftAutodetected device type: cudaAutodetected device type: cudaAutodetected device type: cuda\n",
      "Autodetected device type: cudaAutodetected device type: cudaAutodetected device type: cuda\n",
      "\n",
      "\n",
      "overriding model_tag = d32\n",
      "\n",
      "\n",
      "\n",
      "Autodetected device type: cudaoverriding run = challenge-38-8\n",
      "\n",
      "user_config: {'run': 'challenge-38-8', 'source': 'sft', 'dtype': 'bfloat16', 'device_type': '', 'num_steps': -1, 'device_batch_size': 8, 'examples_per_step': 16, 'num_samples': 16, 'max_new_tokens': 256, 'temperature': 1.0, 'top_k': 50, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.05, 'num_epochs': 1, 'save_every': 60, 'eval_every': 60, 'eval_examples': 400}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "wandb: Currently logged in as: ericsilberstein (ericsilberstein-self) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.23.0\n",
      "wandb: Run data is saved locally in /home/ubuntu/learn-nanochat/challenge-38-train-d32/wandb/run-20251231_150432-1dhrcxg0\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run challenge-38-8\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ericsilberstein-self/my-nanochat-rl\n",
      "wandb: üöÄ View run at https://wandb.ai/ericsilberstein-self/my-nanochat-rl/runs/1dhrcxg0\n",
      "loading the model from /home/ubuntu/mynanochat2/chatsft_checkpoints/d32 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7473/7473 [00:00<00:00, 475493.16 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1319/1319 [00:00<00:00, 356255.20 examples/s]\n",
      "Calculated number of steps: 467\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(2048/768) = 0.6123724356957946\n",
      "Muon: Grouping 128 params of shape torch.Size([2048, 2048]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([2048, 8192]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 32 params of shape torch.Size([8192, 2048]), device cuda:0, dtype torch.float32\n",
      "total sequences per step: 256\n",
      "calculated examples per rank: 2\n",
      "Step 0 | Pass@1: 0.0675, Pass@2: 0.1125, Pass@3: 0.1525, Pass@4: 0.1800, Pass@5: 0.2100, Pass@6: 0.2400, Pass@7: 0.2575, Pass@8: 0.2750\n",
      "Step 0/467 | Example step 0 | Pass 0 | loss: -0.019835 | average reward: 0.0\n",
      "Step 0/467 | Example step 0 | Pass 1 | loss: 0.006328 | average reward: 0.25\n",
      "Step 0/467 | Example step 1 | Pass 0 | loss: 0.015659 | average reward: 0.125\n",
      "Step 0/467 | Example step 1 | Pass 1 | loss: -0.011587 | average reward: 0.0\n",
      "Step 0/467 | Average reward: 0.0390625 | Average sequence length: 207.74\n",
      "Step 1/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 1/467 | Example step 1 | Pass 0 | loss: 0.008279 | average reward: 0.125\n",
      "Step 1/467 | Example step 1 | Pass 1 | loss: -0.013587 | average reward: 0.0\n",
      "Step 1/467 | Average reward: 0.02734375 | Average sequence length: 200.82\n",
      "Step 2/467 | Example step 0 | Pass 0 | loss: -0.008574 | average reward: 0.25\n",
      "Step 2/467 | Example step 0 | Pass 1 | loss: -0.024815 | average reward: 0.0\n",
      "Step 2/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 2/467 | Average reward: 0.0625 | Average sequence length: 175.84\n",
      "Step 3/467 | Example step 0 | Pass 0 | loss: 0.012287 | average reward: 0.125\n",
      "Step 3/467 | Example step 0 | Pass 1 | loss: -0.010355 | average reward: 0.125\n",
      "Step 3/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 3/467 | Average reward: 0.0703125 | Average sequence length: 182.22\n",
      "Step 4/467 | Example step 0 | Pass 0 | loss: -0.007463 | average reward: 0.125\n",
      "Step 4/467 | Example step 0 | Pass 1 | loss: -0.002869 | average reward: 0.125\n",
      "Step 4/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 4/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 4/467 | Average reward: 0.0390625 | Average sequence length: 182.56\n",
      "Step 5/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 5/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 5/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 5/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 5/467 | Average reward: 0.0234375 | Average sequence length: 175.84\n",
      "Step 6/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 6/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 6/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 6/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 6/467 | Average reward: 0.078125 | Average sequence length: 152.02\n",
      "Step 7/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 7/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 7/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 7/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 7/467 | Average reward: 0.0390625 | Average sequence length: 184.17\n",
      "Step 8/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 8/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 8/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 8/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 8/467 | Average reward: 0.03125 | Average sequence length: 183.59\n",
      "Step 9/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 9/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 9/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 9/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 9/467 | Average reward: 0.0625 | Average sequence length: 171.54\n",
      "Step 10/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 10/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 10/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 10/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 10/467 | Average reward: 0.08984375 | Average sequence length: 170.60\n",
      "Step 11/467 | Example step 0 | Pass 0 | loss: 0.010362 | average reward: 0.25\n",
      "Step 11/467 | Example step 0 | Pass 1 | loss: 0.000249 | average reward: 0.125\n",
      "Step 11/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 11/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 11/467 | Average reward: 0.04296875 | Average sequence length: 198.48\n",
      "Step 12/467 | Example step 0 | Pass 0 | loss: 0.013821 | average reward: 0.25\n",
      "Step 12/467 | Example step 0 | Pass 1 | loss: -0.020202 | average reward: 0.0\n",
      "Step 12/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 12/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 12/467 | Average reward: 0.08203125 | Average sequence length: 172.13\n",
      "Step 13/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 13/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 13/467 | Example step 1 | Pass 0 | loss: -0.013634 | average reward: 0.0\n",
      "Step 13/467 | Example step 1 | Pass 1 | loss: 0.055398 | average reward: 0.125\n",
      "Step 13/467 | Average reward: 0.046875 | Average sequence length: 197.46\n",
      "Step 14/467 | Example step 0 | Pass 0 | loss: -0.010649 | average reward: 0.0\n",
      "Step 14/467 | Example step 0 | Pass 1 | loss: 0.003073 | average reward: 0.125\n",
      "Step 14/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 14/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 14/467 | Average reward: 0.06640625 | Average sequence length: 181.39\n",
      "Step 15/467 | Example step 0 | Pass 0 | loss: -0.015127 | average reward: 0.125\n",
      "Step 15/467 | Example step 0 | Pass 1 | loss: 0.003940 | average reward: 0.25\n",
      "Step 15/467 | Example step 1 | Pass 0 | loss: 0.001223 | average reward: 0.125\n",
      "Step 15/467 | Example step 1 | Pass 1 | loss: -0.005883 | average reward: 0.0\n",
      "Step 15/467 | Average reward: 0.0546875 | Average sequence length: 183.05\n",
      "Step 16/467 | Example step 0 | Pass 0 | loss: -0.009743 | average reward: 0.125\n",
      "Step 16/467 | Example step 0 | Pass 1 | loss: -0.010506 | average reward: 0.125\n",
      "Step 16/467 | Example step 1 | Pass 0 | loss: -0.008605 | average reward: 0.125\n",
      "Step 16/467 | Example step 1 | Pass 1 | loss: -0.007154 | average reward: 0.125\n",
      "Step 16/467 | Average reward: 0.06640625 | Average sequence length: 190.87\n",
      "Step 17/467 | Example step 0 | Pass 0 | loss: 0.048359 | average reward: 0.25\n",
      "Step 17/467 | Example step 0 | Pass 1 | loss: -0.025788 | average reward: 0.125\n",
      "Step 17/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 17/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 17/467 | Average reward: 0.08203125 | Average sequence length: 179.95\n",
      "Step 18/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 18/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 18/467 | Example step 1 | Pass 0 | loss: 0.012897 | average reward: 0.125\n",
      "Step 18/467 | Example step 1 | Pass 1 | loss: -0.010250 | average reward: 0.0\n",
      "Step 18/467 | Average reward: 0.06640625 | Average sequence length: 162.10\n",
      "Step 19/467 | Example step 0 | Pass 0 | loss: 0.007328 | average reward: 0.375\n",
      "Step 19/467 | Example step 0 | Pass 1 | loss: -0.026894 | average reward: 0.125\n",
      "Step 19/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 19/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 19/467 | Average reward: 0.0859375 | Average sequence length: 148.16\n",
      "Step 20/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 20/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 20/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 20/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 20/467 | Average reward: 0.0234375 | Average sequence length: 194.37\n",
      "Step 21/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 21/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 21/467 | Example step 1 | Pass 0 | loss: -0.012158 | average reward: 0.0\n",
      "Step 21/467 | Example step 1 | Pass 1 | loss: 0.013356 | average reward: 0.125\n",
      "Step 21/467 | Average reward: 0.07421875 | Average sequence length: 196.69\n",
      "Step 22/467 | Example step 0 | Pass 0 | loss: 0.008676 | average reward: 0.25\n",
      "Step 22/467 | Example step 0 | Pass 1 | loss: -0.013560 | average reward: 0.0\n",
      "Step 22/467 | Example step 1 | Pass 0 | loss: 0.034740 | average reward: 0.5\n",
      "Step 22/467 | Example step 1 | Pass 1 | loss: -0.045196 | average reward: 0.0\n",
      "Step 22/467 | Average reward: 0.11328125 | Average sequence length: 159.40\n",
      "Step 23/467 | Example step 0 | Pass 0 | loss: -0.010011 | average reward: 0.0\n",
      "Step 23/467 | Example step 0 | Pass 1 | loss: 0.001157 | average reward: 0.125\n",
      "Step 23/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 23/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 23/467 | Average reward: 0.05078125 | Average sequence length: 185.22\n",
      "Step 24/467 | Example step 0 | Pass 0 | loss: 0.006507 | average reward: 0.125\n",
      "Step 24/467 | Example step 0 | Pass 1 | loss: -0.005993 | average reward: 0.0\n",
      "Step 24/467 | Example step 1 | Pass 0 | loss: 0.003863 | average reward: 0.125\n",
      "Step 24/467 | Example step 1 | Pass 1 | loss: -0.009536 | average reward: 0.0\n",
      "Step 24/467 | Average reward: 0.0859375 | Average sequence length: 180.59\n",
      "Step 25/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 25/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 25/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 25/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 25/467 | Average reward: 0.08203125 | Average sequence length: 175.30\n",
      "Step 26/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 26/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 26/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 26/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 26/467 | Average reward: 0.03515625 | Average sequence length: 154.02\n",
      "Step 27/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 27/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 27/467 | Example step 1 | Pass 0 | loss: -0.017674 | average reward: 0.125\n",
      "Step 27/467 | Example step 1 | Pass 1 | loss: -0.007235 | average reward: 0.25\n",
      "Step 27/467 | Average reward: 0.08203125 | Average sequence length: 158.71\n",
      "Step 28/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 28/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 28/467 | Example step 1 | Pass 0 | loss: 0.003829 | average reward: 0.5\n",
      "Step 28/467 | Example step 1 | Pass 1 | loss: -0.017592 | average reward: 0.25\n",
      "Step 28/467 | Average reward: 0.1171875 | Average sequence length: 189.53\n",
      "Step 29/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 29/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 29/467 | Example step 1 | Pass 0 | loss: -0.012181 | average reward: 0.0\n",
      "Step 29/467 | Example step 1 | Pass 1 | loss: 0.008505 | average reward: 0.125\n",
      "Step 29/467 | Average reward: 0.109375 | Average sequence length: 172.37\n",
      "Step 30/467 | Example step 0 | Pass 0 | loss: -0.011939 | average reward: 0.0\n",
      "Step 30/467 | Example step 0 | Pass 1 | loss: 0.023971 | average reward: 0.125\n",
      "Step 30/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 30/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 30/467 | Average reward: 0.0625 | Average sequence length: 177.80\n",
      "Step 31/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 31/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 31/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 31/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 31/467 | Average reward: 0.0703125 | Average sequence length: 161.31\n",
      "Step 32/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 32/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 32/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 32/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 32/467 | Average reward: 0.08984375 | Average sequence length: 176.46\n",
      "Step 33/467 | Example step 0 | Pass 0 | loss: -0.005955 | average reward: 0.25\n",
      "Step 33/467 | Example step 0 | Pass 1 | loss: -0.018559 | average reward: 0.25\n",
      "Step 33/467 | Example step 1 | Pass 0 | loss: 0.006870 | average reward: 0.125\n",
      "Step 33/467 | Example step 1 | Pass 1 | loss: -0.009470 | average reward: 0.0\n",
      "Step 33/467 | Average reward: 0.0625 | Average sequence length: 168.50\n",
      "Step 34/467 | Example step 0 | Pass 0 | loss: -0.024355 | average reward: 0.375\n",
      "Step 34/467 | Example step 0 | Pass 1 | loss: -0.003893 | average reward: 0.5\n",
      "Step 34/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 34/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 34/467 | Average reward: 0.0546875 | Average sequence length: 158.64\n",
      "Step 35/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 35/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 35/467 | Example step 1 | Pass 0 | loss: 0.002747 | average reward: 0.125\n",
      "Step 35/467 | Example step 1 | Pass 1 | loss: -0.011474 | average reward: 0.0\n",
      "Step 35/467 | Average reward: 0.015625 | Average sequence length: 160.47\n",
      "Step 36/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 36/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 36/467 | Example step 1 | Pass 0 | loss: -0.007037 | average reward: 0.0\n",
      "Step 36/467 | Example step 1 | Pass 1 | loss: 0.006230 | average reward: 0.125\n",
      "Step 36/467 | Average reward: 0.0390625 | Average sequence length: 177.15\n",
      "Step 37/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 37/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 37/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 37/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 37/467 | Average reward: 0.0625 | Average sequence length: 148.75\n",
      "Step 38/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 38/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 38/467 | Example step 1 | Pass 0 | loss: 0.006102 | average reward: 0.5\n",
      "Step 38/467 | Example step 1 | Pass 1 | loss: 0.004440 | average reward: 0.625\n",
      "Step 38/467 | Average reward: 0.078125 | Average sequence length: 158.79\n",
      "Step 39/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 39/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 39/467 | Example step 1 | Pass 0 | loss: -0.001598 | average reward: 0.25\n",
      "Step 39/467 | Example step 1 | Pass 1 | loss: -0.010868 | average reward: 0.0\n",
      "Step 39/467 | Average reward: 0.09375 | Average sequence length: 159.92\n",
      "Step 40/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 40/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 40/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 40/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 40/467 | Average reward: 0.09765625 | Average sequence length: 156.12\n",
      "Step 41/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 41/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 41/467 | Example step 1 | Pass 0 | loss: -0.010341 | average reward: 0.375\n",
      "Step 41/467 | Example step 1 | Pass 1 | loss: 0.000733 | average reward: 0.625\n",
      "Step 41/467 | Average reward: 0.1328125 | Average sequence length: 155.11\n",
      "Step 42/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 42/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 42/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 42/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 42/467 | Average reward: 0.11328125 | Average sequence length: 163.56\n",
      "Step 43/467 | Example step 0 | Pass 0 | loss: 0.004731 | average reward: 0.125\n",
      "Step 43/467 | Example step 0 | Pass 1 | loss: -0.008261 | average reward: 0.125\n",
      "Step 43/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 43/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 43/467 | Average reward: 0.15625 | Average sequence length: 174.57\n",
      "Step 44/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 44/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 44/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 44/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 44/467 | Average reward: 0.07421875 | Average sequence length: 161.48\n",
      "Step 45/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 45/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 45/467 | Example step 1 | Pass 0 | loss: -0.018168 | average reward: 0.5\n",
      "Step 45/467 | Example step 1 | Pass 1 | loss: 0.001373 | average reward: 0.375\n",
      "Step 45/467 | Average reward: 0.1015625 | Average sequence length: 171.34\n",
      "Step 46/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 46/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 46/467 | Example step 1 | Pass 0 | loss: -0.012683 | average reward: 0.0\n",
      "Step 46/467 | Example step 1 | Pass 1 | loss: 0.014192 | average reward: 0.125\n",
      "Step 46/467 | Average reward: 0.0859375 | Average sequence length: 184.53\n",
      "Step 47/467 | Example step 0 | Pass 0 | loss: -0.011681 | average reward: 0.0\n",
      "Step 47/467 | Example step 0 | Pass 1 | loss: 0.002429 | average reward: 0.125\n",
      "Step 47/467 | Example step 1 | Pass 0 | loss: -0.003277 | average reward: 0.0\n",
      "Step 47/467 | Example step 1 | Pass 1 | loss: 0.005803 | average reward: 0.125\n",
      "Step 47/467 | Average reward: 0.04296875 | Average sequence length: 169.50\n",
      "Step 48/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 48/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 48/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 48/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 48/467 | Average reward: 0.0546875 | Average sequence length: 163.92\n",
      "Step 49/467 | Example step 0 | Pass 0 | loss: -0.022474 | average reward: 0.25\n",
      "Step 49/467 | Example step 0 | Pass 1 | loss: 0.003847 | average reward: 0.375\n",
      "Step 49/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 49/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 49/467 | Average reward: 0.1328125 | Average sequence length: 136.07\n",
      "Step 50/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 50/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 50/467 | Example step 1 | Pass 0 | loss: 0.004211 | average reward: 0.125\n",
      "Step 50/467 | Example step 1 | Pass 1 | loss: 0.001768 | average reward: 0.125\n",
      "Step 50/467 | Average reward: 0.16796875 | Average sequence length: 156.73\n",
      "Step 51/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 51/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 51/467 | Example step 1 | Pass 0 | loss: -0.014664 | average reward: 0.0\n",
      "Step 51/467 | Example step 1 | Pass 1 | loss: -0.003041 | average reward: 0.25\n",
      "Step 51/467 | Average reward: 0.05859375 | Average sequence length: 151.43\n",
      "Step 52/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 52/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 52/467 | Example step 1 | Pass 0 | loss: -0.019848 | average reward: 0.375\n",
      "Step 52/467 | Example step 1 | Pass 1 | loss: -0.018997 | average reward: 0.375\n",
      "Step 52/467 | Average reward: 0.203125 | Average sequence length: 136.24\n",
      "Step 53/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 53/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 53/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 53/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 53/467 | Average reward: 0.09375 | Average sequence length: 132.79\n",
      "Step 54/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 54/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 54/467 | Example step 1 | Pass 0 | loss: 0.008859 | average reward: 0.125\n",
      "Step 54/467 | Example step 1 | Pass 1 | loss: -0.004750 | average reward: 0.0\n",
      "Step 54/467 | Average reward: 0.0390625 | Average sequence length: 142.93\n",
      "Step 55/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 55/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 55/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 55/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 55/467 | Average reward: 0.0625 | Average sequence length: 131.36\n",
      "Step 56/467 | Example step 0 | Pass 0 | loss: -0.008093 | average reward: 0.0\n",
      "Step 56/467 | Example step 0 | Pass 1 | loss: 0.006628 | average reward: 0.125\n",
      "Step 56/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 56/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 56/467 | Average reward: 0.08203125 | Average sequence length: 149.38\n",
      "Step 57/467 | Example step 0 | Pass 0 | loss: -0.000346 | average reward: 0.125\n",
      "Step 57/467 | Example step 0 | Pass 1 | loss: -0.004828 | average reward: 0.0\n",
      "Step 57/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 57/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 57/467 | Average reward: 0.1640625 | Average sequence length: 136.65\n",
      "Step 58/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 58/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 58/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 58/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 58/467 | Average reward: 0.0625 | Average sequence length: 158.16\n",
      "Step 59/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 59/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 59/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 59/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 59/467 | Average reward: 0.05859375 | Average sequence length: 155.30\n",
      "Step 60 | Pass@1: 0.1050, Pass@2: 0.1600, Pass@3: 0.1925, Pass@4: 0.2350, Pass@5: 0.2725, Pass@6: 0.2900, Pass@7: 0.3150, Pass@8: 0.3425\n",
      "Step 60/467 | Example step 0 | Pass 0 | loss: -0.005746 | average reward: 0.0\n",
      "Step 60/467 | Example step 0 | Pass 1 | loss: 0.002107 | average reward: 0.125\n",
      "Step 60/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 60/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 60/467 | Average reward: 0.02734375 | Average sequence length: 161.45\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000060.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000060.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 61/467 | Example step 0 | Pass 0 | loss: 0.003689 | average reward: 0.25\n",
      "Step 61/467 | Example step 0 | Pass 1 | loss: -0.012080 | average reward: 0.125\n",
      "Step 61/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 61/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 61/467 | Average reward: 0.13671875 | Average sequence length: 129.14\n",
      "Step 62/467 | Example step 0 | Pass 0 | loss: -0.005032 | average reward: 0.125\n",
      "Step 62/467 | Example step 0 | Pass 1 | loss: 0.007664 | average reward: 0.25\n",
      "Step 62/467 | Example step 1 | Pass 0 | loss: -0.002971 | average reward: 0.125\n",
      "Step 62/467 | Example step 1 | Pass 1 | loss: 0.002807 | average reward: 0.125\n",
      "Step 62/467 | Average reward: 0.0625 | Average sequence length: 164.54\n",
      "Step 63/467 | Example step 0 | Pass 0 | loss: -0.008289 | average reward: 0.0\n",
      "Step 63/467 | Example step 0 | Pass 1 | loss: -0.001615 | average reward: 0.125\n",
      "Step 63/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 63/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 63/467 | Average reward: 0.13671875 | Average sequence length: 135.79\n",
      "Step 64/467 | Example step 0 | Pass 0 | loss: 0.003324 | average reward: 0.125\n",
      "Step 64/467 | Example step 0 | Pass 1 | loss: -0.004735 | average reward: 0.0\n",
      "Step 64/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 64/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 64/467 | Average reward: 0.07421875 | Average sequence length: 157.75\n",
      "Step 65/467 | Example step 0 | Pass 0 | loss: 0.027582 | average reward: 0.5\n",
      "Step 65/467 | Example step 0 | Pass 1 | loss: -0.021549 | average reward: 0.125\n",
      "Step 65/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 65/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 65/467 | Average reward: 0.05859375 | Average sequence length: 165.00\n",
      "Step 66/467 | Example step 0 | Pass 0 | loss: 0.010165 | average reward: 0.25\n",
      "Step 66/467 | Example step 0 | Pass 1 | loss: -0.001592 | average reward: 0.125\n",
      "Step 66/467 | Example step 1 | Pass 0 | loss: 0.003144 | average reward: 0.125\n",
      "Step 66/467 | Example step 1 | Pass 1 | loss: -0.003038 | average reward: 0.0\n",
      "Step 66/467 | Average reward: 0.18359375 | Average sequence length: 149.10\n",
      "Step 67/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 67/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 67/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 67/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 67/467 | Average reward: 0.18359375 | Average sequence length: 168.76\n",
      "Step 68/467 | Example step 0 | Pass 0 | loss: -0.025499 | average reward: 0.25\n",
      "Step 68/467 | Example step 0 | Pass 1 | loss: 0.015686 | average reward: 0.75\n",
      "Step 68/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 68/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 68/467 | Average reward: 0.0859375 | Average sequence length: 155.64\n",
      "Step 69/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 69/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 69/467 | Example step 1 | Pass 0 | loss: 0.026859 | average reward: 0.75\n",
      "Step 69/467 | Example step 1 | Pass 1 | loss: -0.018714 | average reward: 0.375\n",
      "Step 69/467 | Average reward: 0.1015625 | Average sequence length: 150.16\n",
      "Step 70/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 70/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 70/467 | Example step 1 | Pass 0 | loss: -0.000917 | average reward: 0.125\n",
      "Step 70/467 | Example step 1 | Pass 1 | loss: -0.006784 | average reward: 0.125\n",
      "Step 70/467 | Average reward: 0.09375 | Average sequence length: 151.20\n",
      "Step 71/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 71/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 71/467 | Example step 1 | Pass 0 | loss: 0.000965 | average reward: 0.125\n",
      "Step 71/467 | Example step 1 | Pass 1 | loss: 0.015657 | average reward: 0.375\n",
      "Step 71/467 | Average reward: 0.1015625 | Average sequence length: 156.72\n",
      "Step 72/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 72/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 72/467 | Example step 1 | Pass 0 | loss: -0.000578 | average reward: 0.125\n",
      "Step 72/467 | Example step 1 | Pass 1 | loss: -0.004991 | average reward: 0.0\n",
      "Step 72/467 | Average reward: 0.15234375 | Average sequence length: 153.30\n",
      "Step 73/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 73/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 73/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 73/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 73/467 | Average reward: 0.08984375 | Average sequence length: 175.97\n",
      "Step 74/467 | Example step 0 | Pass 0 | loss: -0.003957 | average reward: 0.0\n",
      "Step 74/467 | Example step 0 | Pass 1 | loss: 0.011133 | average reward: 0.125\n",
      "Step 74/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 74/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 74/467 | Average reward: 0.08984375 | Average sequence length: 166.18\n",
      "Step 75/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 75/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 75/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 75/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 75/467 | Average reward: 0.19140625 | Average sequence length: 131.27\n",
      "Step 76/467 | Example step 0 | Pass 0 | loss: -0.005972 | average reward: 0.5\n",
      "Step 76/467 | Example step 0 | Pass 1 | loss: -0.004704 | average reward: 0.5\n",
      "Step 76/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 76/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 76/467 | Average reward: 0.125 | Average sequence length: 148.38\n",
      "Step 77/467 | Example step 0 | Pass 0 | loss: 0.006088 | average reward: 0.125\n",
      "Step 77/467 | Example step 0 | Pass 1 | loss: -0.007653 | average reward: 0.0\n",
      "Step 77/467 | Example step 1 | Pass 0 | loss: 0.011986 | average reward: 0.125\n",
      "Step 77/467 | Example step 1 | Pass 1 | loss: -0.006169 | average reward: 0.0\n",
      "Step 77/467 | Average reward: 0.16015625 | Average sequence length: 138.33\n",
      "Step 78/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 78/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 78/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 78/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 78/467 | Average reward: 0.078125 | Average sequence length: 163.46\n",
      "Step 79/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 79/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 79/467 | Example step 1 | Pass 0 | loss: -0.000195 | average reward: 0.125\n",
      "Step 79/467 | Example step 1 | Pass 1 | loss: -0.006902 | average reward: 0.0\n",
      "Step 79/467 | Average reward: 0.0546875 | Average sequence length: 163.11\n",
      "Step 80/467 | Example step 0 | Pass 0 | loss: 0.002863 | average reward: 0.25\n",
      "Step 80/467 | Example step 0 | Pass 1 | loss: 0.006173 | average reward: 0.125\n",
      "Step 80/467 | Example step 1 | Pass 0 | loss: 0.012023 | average reward: 0.375\n",
      "Step 80/467 | Example step 1 | Pass 1 | loss: -0.004318 | average reward: 0.25\n",
      "Step 80/467 | Average reward: 0.18359375 | Average sequence length: 169.53\n",
      "Step 81/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 81/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 81/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 81/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 81/467 | Average reward: 0.05859375 | Average sequence length: 167.80\n",
      "Step 82/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 82/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 82/467 | Example step 1 | Pass 0 | loss: -0.012460 | average reward: 0.75\n",
      "Step 82/467 | Example step 1 | Pass 1 | loss: -0.006060 | average reward: 0.875\n",
      "Step 82/467 | Average reward: 0.12109375 | Average sequence length: 153.36\n",
      "Step 83/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 83/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 83/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 83/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 83/467 | Average reward: 0.1015625 | Average sequence length: 182.09\n",
      "Step 84/467 | Example step 0 | Pass 0 | loss: -0.015497 | average reward: 0.375\n",
      "Step 84/467 | Example step 0 | Pass 1 | loss: 0.021032 | average reward: 0.75\n",
      "Step 84/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 84/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 84/467 | Average reward: 0.14453125 | Average sequence length: 174.97\n",
      "Step 85/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 85/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 85/467 | Example step 1 | Pass 0 | loss: -0.001455 | average reward: 0.125\n",
      "Step 85/467 | Example step 1 | Pass 1 | loss: 0.007526 | average reward: 0.125\n",
      "Step 85/467 | Average reward: 0.1796875 | Average sequence length: 149.28\n",
      "Step 86/467 | Example step 0 | Pass 0 | loss: -0.005969 | average reward: 0.0\n",
      "Step 86/467 | Example step 0 | Pass 1 | loss: 0.005592 | average reward: 0.125\n",
      "Step 86/467 | Example step 1 | Pass 0 | loss: 0.007572 | average reward: 0.125\n",
      "Step 86/467 | Example step 1 | Pass 1 | loss: -0.011464 | average reward: 0.0\n",
      "Step 86/467 | Average reward: 0.078125 | Average sequence length: 156.99\n",
      "Step 87/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 87/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 87/467 | Example step 1 | Pass 0 | loss: -0.036129 | average reward: 0.125\n",
      "Step 87/467 | Example step 1 | Pass 1 | loss: 0.007882 | average reward: 0.625\n",
      "Step 87/467 | Average reward: 0.0703125 | Average sequence length: 151.68\n",
      "Step 88/467 | Example step 0 | Pass 0 | loss: -0.029729 | average reward: 0.5\n",
      "Step 88/467 | Example step 0 | Pass 1 | loss: 0.010018 | average reward: 1.0\n",
      "Step 88/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 88/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 88/467 | Average reward: 0.15234375 | Average sequence length: 160.68\n",
      "Step 89/467 | Example step 0 | Pass 0 | loss: -0.002523 | average reward: 0.75\n",
      "Step 89/467 | Example step 0 | Pass 1 | loss: -0.028869 | average reward: 0.5\n",
      "Step 89/467 | Example step 1 | Pass 0 | loss: 0.000895 | average reward: 0.125\n",
      "Step 89/467 | Example step 1 | Pass 1 | loss: -0.005187 | average reward: 0.0\n",
      "Step 89/467 | Average reward: 0.1875 | Average sequence length: 178.03\n",
      "Step 90/467 | Example step 0 | Pass 0 | loss: -0.007927 | average reward: 0.0\n",
      "Step 90/467 | Example step 0 | Pass 1 | loss: 0.011945 | average reward: 0.25\n",
      "Step 90/467 | Example step 1 | Pass 0 | loss: 0.001218 | average reward: 0.625\n",
      "Step 90/467 | Example step 1 | Pass 1 | loss: -0.024501 | average reward: 0.5\n",
      "Step 90/467 | Average reward: 0.24609375 | Average sequence length: 147.46\n",
      "Step 91/467 | Example step 0 | Pass 0 | loss: -0.007050 | average reward: 0.125\n",
      "Step 91/467 | Example step 0 | Pass 1 | loss: 0.002864 | average reward: 0.25\n",
      "Step 91/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 91/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 91/467 | Average reward: 0.0859375 | Average sequence length: 154.36\n",
      "Step 92/467 | Example step 0 | Pass 0 | loss: -0.014885 | average reward: 0.0\n",
      "Step 92/467 | Example step 0 | Pass 1 | loss: 0.005614 | average reward: 0.5\n",
      "Step 92/467 | Example step 1 | Pass 0 | loss: -0.004233 | average reward: 0.0\n",
      "Step 92/467 | Example step 1 | Pass 1 | loss: 0.008284 | average reward: 0.125\n",
      "Step 92/467 | Average reward: 0.18359375 | Average sequence length: 165.00\n",
      "Step 93/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 93/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 93/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 93/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 93/467 | Average reward: 0.12109375 | Average sequence length: 159.02\n",
      "Step 94/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 94/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 94/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 94/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 94/467 | Average reward: 0.09765625 | Average sequence length: 172.92\n",
      "Step 95/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 95/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 95/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 95/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 95/467 | Average reward: 0.15625 | Average sequence length: 157.57\n",
      "Step 96/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 96/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 96/467 | Example step 1 | Pass 0 | loss: -0.008591 | average reward: 0.375\n",
      "Step 96/467 | Example step 1 | Pass 1 | loss: 0.002580 | average reward: 0.375\n",
      "Step 96/467 | Average reward: 0.2109375 | Average sequence length: 177.31\n",
      "Step 97/467 | Example step 0 | Pass 0 | loss: -0.004676 | average reward: 0.0\n",
      "Step 97/467 | Example step 0 | Pass 1 | loss: 0.006246 | average reward: 0.125\n",
      "Step 97/467 | Example step 1 | Pass 0 | loss: 0.008144 | average reward: 0.125\n",
      "Step 97/467 | Example step 1 | Pass 1 | loss: -0.008082 | average reward: 0.0\n",
      "Step 97/467 | Average reward: 0.1484375 | Average sequence length: 159.03\n",
      "Step 98/467 | Example step 0 | Pass 0 | loss: -0.014715 | average reward: 0.5\n",
      "Step 98/467 | Example step 0 | Pass 1 | loss: 0.001460 | average reward: 0.75\n",
      "Step 98/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 98/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 98/467 | Average reward: 0.265625 | Average sequence length: 160.82\n",
      "Step 99/467 | Example step 0 | Pass 0 | loss: 0.014777 | average reward: 0.125\n",
      "Step 99/467 | Example step 0 | Pass 1 | loss: -0.008603 | average reward: 0.0\n",
      "Step 99/467 | Example step 1 | Pass 0 | loss: 0.000265 | average reward: 0.125\n",
      "Step 99/467 | Example step 1 | Pass 1 | loss: 0.008352 | average reward: 0.125\n",
      "Step 99/467 | Average reward: 0.15234375 | Average sequence length: 169.80\n",
      "Step 100/467 | Example step 0 | Pass 0 | loss: -0.010013 | average reward: 0.375\n",
      "Step 100/467 | Example step 0 | Pass 1 | loss: 0.006947 | average reward: 0.625\n",
      "Step 100/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 100/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 100/467 | Average reward: 0.140625 | Average sequence length: 168.38\n",
      "Step 101/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 101/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 101/467 | Example step 1 | Pass 0 | loss: -0.000349 | average reward: 0.375\n",
      "Step 101/467 | Example step 1 | Pass 1 | loss: -0.002430 | average reward: 0.375\n",
      "Step 101/467 | Average reward: 0.1328125 | Average sequence length: 167.30\n",
      "Step 102/467 | Example step 0 | Pass 0 | loss: -0.006672 | average reward: 0.0\n",
      "Step 102/467 | Example step 0 | Pass 1 | loss: 0.005882 | average reward: 0.125\n",
      "Step 102/467 | Example step 1 | Pass 0 | loss: 0.001942 | average reward: 0.125\n",
      "Step 102/467 | Example step 1 | Pass 1 | loss: -0.003032 | average reward: 0.0\n",
      "Step 102/467 | Average reward: 0.0546875 | Average sequence length: 171.05\n",
      "Step 103/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 103/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 103/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 103/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 103/467 | Average reward: 0.08203125 | Average sequence length: 178.98\n",
      "Step 104/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 104/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 104/467 | Example step 1 | Pass 0 | loss: -0.000285 | average reward: 0.125\n",
      "Step 104/467 | Example step 1 | Pass 1 | loss: -0.008946 | average reward: 0.0\n",
      "Step 104/467 | Average reward: 0.13671875 | Average sequence length: 164.86\n",
      "Step 105/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 105/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 105/467 | Example step 1 | Pass 0 | loss: -0.007276 | average reward: 0.0\n",
      "Step 105/467 | Example step 1 | Pass 1 | loss: 0.013422 | average reward: 0.125\n",
      "Step 105/467 | Average reward: 0.2265625 | Average sequence length: 135.29\n",
      "Step 106/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 106/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 106/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 106/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 106/467 | Average reward: 0.04296875 | Average sequence length: 150.60\n",
      "Step 107/467 | Example step 0 | Pass 0 | loss: -0.005735 | average reward: 0.0\n",
      "Step 107/467 | Example step 0 | Pass 1 | loss: 0.019066 | average reward: 0.25\n",
      "Step 107/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 107/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 107/467 | Average reward: 0.17578125 | Average sequence length: 160.39\n",
      "Step 108/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 108/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 108/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 108/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 108/467 | Average reward: 0.06640625 | Average sequence length: 168.08\n",
      "Step 109/467 | Example step 0 | Pass 0 | loss: 0.012971 | average reward: 1.0\n",
      "Step 109/467 | Example step 0 | Pass 1 | loss: -0.020661 | average reward: 0.0\n",
      "Step 109/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 109/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 109/467 | Average reward: 0.09375 | Average sequence length: 179.18\n",
      "Step 110/467 | Example step 0 | Pass 0 | loss: -0.004780 | average reward: 0.0\n",
      "Step 110/467 | Example step 0 | Pass 1 | loss: 0.001192 | average reward: 0.125\n",
      "Step 110/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 110/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 110/467 | Average reward: 0.1484375 | Average sequence length: 161.31\n",
      "Step 111/467 | Example step 0 | Pass 0 | loss: -0.001587 | average reward: 0.625\n",
      "Step 111/467 | Example step 0 | Pass 1 | loss: -0.001540 | average reward: 0.75\n",
      "Step 111/467 | Example step 1 | Pass 0 | loss: -0.018113 | average reward: 0.375\n",
      "Step 111/467 | Example step 1 | Pass 1 | loss: 0.006855 | average reward: 0.75\n",
      "Step 111/467 | Average reward: 0.1953125 | Average sequence length: 160.23\n",
      "Step 112/467 | Example step 0 | Pass 0 | loss: -0.003402 | average reward: 0.0\n",
      "Step 112/467 | Example step 0 | Pass 1 | loss: 0.006329 | average reward: 0.125\n",
      "Step 112/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 112/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 112/467 | Average reward: 0.10546875 | Average sequence length: 170.85\n",
      "Step 113/467 | Example step 0 | Pass 0 | loss: -0.002996 | average reward: 0.0\n",
      "Step 113/467 | Example step 0 | Pass 1 | loss: 0.003608 | average reward: 0.125\n",
      "Step 113/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 113/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 113/467 | Average reward: 0.14453125 | Average sequence length: 173.42\n",
      "Step 114/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 114/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 114/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 114/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 114/467 | Average reward: 0.109375 | Average sequence length: 194.47\n",
      "Step 115/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 115/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 115/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 115/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 115/467 | Average reward: 0.18359375 | Average sequence length: 151.94\n",
      "Step 116/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 116/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 116/467 | Example step 1 | Pass 0 | loss: 0.003457 | average reward: 0.125\n",
      "Step 116/467 | Example step 1 | Pass 1 | loss: -0.006627 | average reward: 0.0\n",
      "Step 116/467 | Average reward: 0.01953125 | Average sequence length: 168.27\n",
      "Step 117/467 | Example step 0 | Pass 0 | loss: 0.005149 | average reward: 0.25\n",
      "Step 117/467 | Example step 0 | Pass 1 | loss: -0.003537 | average reward: 0.125\n",
      "Step 117/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 117/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 117/467 | Average reward: 0.171875 | Average sequence length: 165.82\n",
      "Step 118/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 118/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 118/467 | Example step 1 | Pass 0 | loss: 0.002627 | average reward: 0.625\n",
      "Step 118/467 | Example step 1 | Pass 1 | loss: -0.008346 | average reward: 0.5\n",
      "Step 118/467 | Average reward: 0.3515625 | Average sequence length: 164.34\n",
      "Step 119/467 | Example step 0 | Pass 0 | loss: 0.003929 | average reward: 0.5\n",
      "Step 119/467 | Example step 0 | Pass 1 | loss: -0.021061 | average reward: 0.375\n",
      "Step 119/467 | Example step 1 | Pass 0 | loss: 0.008960 | average reward: 0.25\n",
      "Step 119/467 | Example step 1 | Pass 1 | loss: -0.009353 | average reward: 0.0\n",
      "Step 119/467 | Average reward: 0.18359375 | Average sequence length: 150.57\n",
      "Step 120 | Pass@1: 0.1400, Pass@2: 0.2050, Pass@3: 0.2550, Pass@4: 0.2825, Pass@5: 0.3150, Pass@6: 0.3325, Pass@7: 0.3650, Pass@8: 0.3800\n",
      "Step 120/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 120/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 120/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 120/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 120/467 | Average reward: 0.171875 | Average sequence length: 157.48\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000120.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000120.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 121/467 | Example step 0 | Pass 0 | loss: 0.012544 | average reward: 0.25\n",
      "Step 121/467 | Example step 0 | Pass 1 | loss: -0.004349 | average reward: 0.0\n",
      "Step 121/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 121/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 121/467 | Average reward: 0.07421875 | Average sequence length: 181.79\n",
      "Step 122/467 | Example step 0 | Pass 0 | loss: -0.003134 | average reward: 0.125\n",
      "Step 122/467 | Example step 0 | Pass 1 | loss: 0.003942 | average reward: 0.125\n",
      "Step 122/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 122/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 122/467 | Average reward: 0.15234375 | Average sequence length: 163.04\n",
      "Step 123/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 123/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 123/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 123/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 123/467 | Average reward: 0.1484375 | Average sequence length: 141.73\n",
      "Step 124/467 | Example step 0 | Pass 0 | loss: 0.009576 | average reward: 0.75\n",
      "Step 124/467 | Example step 0 | Pass 1 | loss: -0.021677 | average reward: 0.375\n",
      "Step 124/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 124/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 124/467 | Average reward: 0.05859375 | Average sequence length: 169.73\n",
      "Step 125/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 125/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 125/467 | Example step 1 | Pass 0 | loss: -0.000415 | average reward: 0.125\n",
      "Step 125/467 | Example step 1 | Pass 1 | loss: -0.006273 | average reward: 0.125\n",
      "Step 125/467 | Average reward: 0.1640625 | Average sequence length: 144.46\n",
      "Step 126/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 126/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 126/467 | Example step 1 | Pass 0 | loss: 0.009187 | average reward: 0.75\n",
      "Step 126/467 | Example step 1 | Pass 1 | loss: -0.018972 | average reward: 0.0\n",
      "Step 126/467 | Average reward: 0.09765625 | Average sequence length: 196.39\n",
      "Step 127/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 127/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 127/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 127/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 127/467 | Average reward: 0.17578125 | Average sequence length: 165.74\n",
      "Step 128/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 128/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 128/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 128/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 128/467 | Average reward: 0.078125 | Average sequence length: 156.91\n",
      "Step 129/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 129/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 129/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 129/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 129/467 | Average reward: 0.08203125 | Average sequence length: 160.87\n",
      "Step 130/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 130/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 130/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 130/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 130/467 | Average reward: 0.0703125 | Average sequence length: 174.20\n",
      "Step 131/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 131/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 131/467 | Example step 1 | Pass 0 | loss: -0.001744 | average reward: 0.25\n",
      "Step 131/467 | Example step 1 | Pass 1 | loss: -0.005797 | average reward: 0.125\n",
      "Step 131/467 | Average reward: 0.125 | Average sequence length: 164.12\n",
      "Step 132/467 | Example step 0 | Pass 0 | loss: 0.007750 | average reward: 0.125\n",
      "Step 132/467 | Example step 0 | Pass 1 | loss: -0.002047 | average reward: 0.0\n",
      "Step 132/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 132/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 132/467 | Average reward: 0.09765625 | Average sequence length: 158.61\n",
      "Step 133/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 133/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 133/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 133/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 133/467 | Average reward: 0.12109375 | Average sequence length: 163.23\n",
      "Step 134/467 | Example step 0 | Pass 0 | loss: 0.006451 | average reward: 1.0\n",
      "Step 134/467 | Example step 0 | Pass 1 | loss: -0.007515 | average reward: 0.625\n",
      "Step 134/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 134/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 134/467 | Average reward: 0.1796875 | Average sequence length: 137.20\n",
      "Step 135/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 135/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 135/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 135/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 135/467 | Average reward: 0.1171875 | Average sequence length: 150.45\n",
      "Step 136/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 136/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 136/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 136/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 136/467 | Average reward: 0.08984375 | Average sequence length: 146.45\n",
      "Step 137/467 | Example step 0 | Pass 0 | loss: -0.004538 | average reward: 0.0\n",
      "Step 137/467 | Example step 0 | Pass 1 | loss: 0.002105 | average reward: 0.125\n",
      "Step 137/467 | Example step 1 | Pass 0 | loss: -0.004009 | average reward: 0.0\n",
      "Step 137/467 | Example step 1 | Pass 1 | loss: 0.000600 | average reward: 0.125\n",
      "Step 137/467 | Average reward: 0.1796875 | Average sequence length: 159.07\n",
      "Step 138/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 138/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 138/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 138/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 138/467 | Average reward: 0.05859375 | Average sequence length: 161.11\n",
      "Step 139/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 139/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 139/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 139/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 139/467 | Average reward: 0.10546875 | Average sequence length: 167.64\n",
      "Step 140/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 140/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 140/467 | Example step 1 | Pass 0 | loss: -0.000362 | average reward: 0.125\n",
      "Step 140/467 | Example step 1 | Pass 1 | loss: -0.002393 | average reward: 0.125\n",
      "Step 140/467 | Average reward: 0.11328125 | Average sequence length: 169.18\n",
      "Step 141/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 141/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 141/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 141/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 141/467 | Average reward: 0.14453125 | Average sequence length: 159.34\n",
      "Step 142/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 142/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 142/467 | Example step 1 | Pass 0 | loss: 0.001784 | average reward: 0.125\n",
      "Step 142/467 | Example step 1 | Pass 1 | loss: -0.004144 | average reward: 0.0\n",
      "Step 142/467 | Average reward: 0.21484375 | Average sequence length: 149.61\n",
      "Step 143/467 | Example step 0 | Pass 0 | loss: 0.014439 | average reward: 0.125\n",
      "Step 143/467 | Example step 0 | Pass 1 | loss: -0.005613 | average reward: 0.0\n",
      "Step 143/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 143/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 143/467 | Average reward: 0.12109375 | Average sequence length: 188.25\n",
      "Step 144/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 144/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 144/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 144/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 144/467 | Average reward: 0.23046875 | Average sequence length: 149.62\n",
      "Step 145/467 | Example step 0 | Pass 0 | loss: -0.002548 | average reward: 0.125\n",
      "Step 145/467 | Example step 0 | Pass 1 | loss: -0.004079 | average reward: 0.125\n",
      "Step 145/467 | Example step 1 | Pass 0 | loss: 0.002219 | average reward: 0.25\n",
      "Step 145/467 | Example step 1 | Pass 1 | loss: -0.004328 | average reward: 0.125\n",
      "Step 145/467 | Average reward: 0.14453125 | Average sequence length: 142.62\n",
      "Step 146/467 | Example step 0 | Pass 0 | loss: -0.006968 | average reward: 0.0\n",
      "Step 146/467 | Example step 0 | Pass 1 | loss: 0.002196 | average reward: 0.25\n",
      "Step 146/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 146/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 146/467 | Average reward: 0.1875 | Average sequence length: 178.59\n",
      "Step 147/467 | Example step 0 | Pass 0 | loss: 0.000954 | average reward: 0.5\n",
      "Step 147/467 | Example step 0 | Pass 1 | loss: -0.014431 | average reward: 0.25\n",
      "Step 147/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 147/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 147/467 | Average reward: 0.10546875 | Average sequence length: 168.64\n",
      "Step 148/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 148/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 148/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 148/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 148/467 | Average reward: 0.125 | Average sequence length: 160.23\n",
      "Step 149/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 149/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 149/467 | Example step 1 | Pass 0 | loss: -0.003468 | average reward: 0.0\n",
      "Step 149/467 | Example step 1 | Pass 1 | loss: 0.001619 | average reward: 0.125\n",
      "Step 149/467 | Average reward: 0.08984375 | Average sequence length: 159.20\n",
      "Step 150/467 | Example step 0 | Pass 0 | loss: -0.006348 | average reward: 0.25\n",
      "Step 150/467 | Example step 0 | Pass 1 | loss: 0.007088 | average reward: 0.75\n",
      "Step 150/467 | Example step 1 | Pass 0 | loss: 0.001140 | average reward: 0.25\n",
      "Step 150/467 | Example step 1 | Pass 1 | loss: -0.004965 | average reward: 0.125\n",
      "Step 150/467 | Average reward: 0.2421875 | Average sequence length: 161.29\n",
      "Step 151/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 151/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 151/467 | Example step 1 | Pass 0 | loss: -0.001149 | average reward: 0.5\n",
      "Step 151/467 | Example step 1 | Pass 1 | loss: -0.002093 | average reward: 0.625\n",
      "Step 151/467 | Average reward: 0.2421875 | Average sequence length: 151.02\n",
      "Step 152/467 | Example step 0 | Pass 0 | loss: -0.002383 | average reward: 0.75\n",
      "Step 152/467 | Example step 0 | Pass 1 | loss: -0.001533 | average reward: 0.875\n",
      "Step 152/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 152/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 152/467 | Average reward: 0.15625 | Average sequence length: 157.33\n",
      "Step 153/467 | Example step 0 | Pass 0 | loss: 0.009830 | average reward: 0.125\n",
      "Step 153/467 | Example step 0 | Pass 1 | loss: -0.004292 | average reward: 0.0\n",
      "Step 153/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 153/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 153/467 | Average reward: 0.1796875 | Average sequence length: 160.21\n",
      "Step 154/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 154/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 154/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 154/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 154/467 | Average reward: 0.0703125 | Average sequence length: 175.86\n",
      "Step 155/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 155/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 155/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 155/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 155/467 | Average reward: 0.10546875 | Average sequence length: 146.11\n",
      "Step 156/467 | Example step 0 | Pass 0 | loss: -0.007044 | average reward: 0.125\n",
      "Step 156/467 | Example step 0 | Pass 1 | loss: 0.003118 | average reward: 0.25\n",
      "Step 156/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 156/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 156/467 | Average reward: 0.05078125 | Average sequence length: 185.75\n",
      "Step 157/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 157/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 157/467 | Example step 1 | Pass 0 | loss: 0.002569 | average reward: 1.0\n",
      "Step 157/467 | Example step 1 | Pass 1 | loss: -0.007749 | average reward: 0.875\n",
      "Step 157/467 | Average reward: 0.09375 | Average sequence length: 160.39\n",
      "Step 158/467 | Example step 0 | Pass 0 | loss: 0.001012 | average reward: 0.125\n",
      "Step 158/467 | Example step 0 | Pass 1 | loss: -0.003296 | average reward: 0.0\n",
      "Step 158/467 | Example step 1 | Pass 0 | loss: -0.005870 | average reward: 0.0\n",
      "Step 158/467 | Example step 1 | Pass 1 | loss: 0.010799 | average reward: 0.125\n",
      "Step 158/467 | Average reward: 0.09375 | Average sequence length: 167.96\n",
      "Step 159/467 | Example step 0 | Pass 0 | loss: 0.009967 | average reward: 0.25\n",
      "Step 159/467 | Example step 0 | Pass 1 | loss: -0.005458 | average reward: 0.0\n",
      "Step 159/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 159/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 159/467 | Average reward: 0.13671875 | Average sequence length: 179.05\n",
      "Step 160/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 160/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 160/467 | Example step 1 | Pass 0 | loss: 0.002402 | average reward: 0.375\n",
      "Step 160/467 | Example step 1 | Pass 1 | loss: -0.005916 | average reward: 0.125\n",
      "Step 160/467 | Average reward: 0.21484375 | Average sequence length: 148.25\n",
      "Step 161/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 161/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 161/467 | Example step 1 | Pass 0 | loss: -0.007533 | average reward: 0.0\n",
      "Step 161/467 | Example step 1 | Pass 1 | loss: 0.009920 | average reward: 0.375\n",
      "Step 161/467 | Average reward: 0.1015625 | Average sequence length: 144.71\n",
      "Step 162/467 | Example step 0 | Pass 0 | loss: -0.011177 | average reward: 0.5\n",
      "Step 162/467 | Example step 0 | Pass 1 | loss: -0.000994 | average reward: 0.75\n",
      "Step 162/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 162/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 162/467 | Average reward: 0.28125 | Average sequence length: 166.10\n",
      "Step 163/467 | Example step 0 | Pass 0 | loss: 0.008432 | average reward: 0.375\n",
      "Step 163/467 | Example step 0 | Pass 1 | loss: -0.005952 | average reward: 0.125\n",
      "Step 163/467 | Example step 1 | Pass 0 | loss: -0.000262 | average reward: 0.25\n",
      "Step 163/467 | Example step 1 | Pass 1 | loss: 0.007282 | average reward: 0.375\n",
      "Step 163/467 | Average reward: 0.1640625 | Average sequence length: 174.73\n",
      "Step 164/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 164/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 164/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 164/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 164/467 | Average reward: 0.16796875 | Average sequence length: 152.60\n",
      "Step 165/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 165/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 165/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 165/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 165/467 | Average reward: 0.2109375 | Average sequence length: 173.91\n",
      "Step 166/467 | Example step 0 | Pass 0 | loss: 0.000836 | average reward: 0.5\n",
      "Step 166/467 | Example step 0 | Pass 1 | loss: -0.003460 | average reward: 0.375\n",
      "Step 166/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 166/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 166/467 | Average reward: 0.2109375 | Average sequence length: 166.05\n",
      "Step 167/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 167/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 167/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 167/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 167/467 | Average reward: 0.1640625 | Average sequence length: 173.05\n",
      "Step 168/467 | Example step 0 | Pass 0 | loss: 0.000075 | average reward: 0.25\n",
      "Step 168/467 | Example step 0 | Pass 1 | loss: -0.004088 | average reward: 0.0\n",
      "Step 168/467 | Example step 1 | Pass 0 | loss: 0.016809 | average reward: 0.875\n",
      "Step 168/467 | Example step 1 | Pass 1 | loss: -0.010911 | average reward: 0.125\n",
      "Step 168/467 | Average reward: 0.140625 | Average sequence length: 174.67\n",
      "Step 169/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 169/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 169/467 | Example step 1 | Pass 0 | loss: -0.007052 | average reward: 0.25\n",
      "Step 169/467 | Example step 1 | Pass 1 | loss: 0.006637 | average reward: 0.25\n",
      "Step 169/467 | Average reward: 0.21875 | Average sequence length: 172.94\n",
      "Step 170/467 | Example step 0 | Pass 0 | loss: 0.002473 | average reward: 0.125\n",
      "Step 170/467 | Example step 0 | Pass 1 | loss: -0.002541 | average reward: 0.0\n",
      "Step 170/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 170/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 170/467 | Average reward: 0.09375 | Average sequence length: 164.24\n",
      "Step 171/467 | Example step 0 | Pass 0 | loss: -0.003055 | average reward: 0.5\n",
      "Step 171/467 | Example step 0 | Pass 1 | loss: 0.001158 | average reward: 0.375\n",
      "Step 171/467 | Example step 1 | Pass 0 | loss: -0.003294 | average reward: 0.125\n",
      "Step 171/467 | Example step 1 | Pass 1 | loss: 0.006973 | average reward: 0.25\n",
      "Step 171/467 | Average reward: 0.23046875 | Average sequence length: 173.96\n",
      "Step 172/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 172/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 172/467 | Example step 1 | Pass 0 | loss: -0.005123 | average reward: 0.375\n",
      "Step 172/467 | Example step 1 | Pass 1 | loss: -0.004625 | average reward: 0.375\n",
      "Step 172/467 | Average reward: 0.0859375 | Average sequence length: 172.09\n",
      "Step 173/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 173/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 173/467 | Example step 1 | Pass 0 | loss: -0.009880 | average reward: 0.0\n",
      "Step 173/467 | Example step 1 | Pass 1 | loss: 0.016558 | average reward: 0.375\n",
      "Step 173/467 | Average reward: 0.15625 | Average sequence length: 170.68\n",
      "Step 174/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 174/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 174/467 | Example step 1 | Pass 0 | loss: 0.013029 | average reward: 1.0\n",
      "Step 174/467 | Example step 1 | Pass 1 | loss: -0.027985 | average reward: 0.125\n",
      "Step 174/467 | Average reward: 0.2890625 | Average sequence length: 174.75\n",
      "Step 175/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 175/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 175/467 | Example step 1 | Pass 0 | loss: 0.005733 | average reward: 0.125\n",
      "Step 175/467 | Example step 1 | Pass 1 | loss: -0.005314 | average reward: 0.0\n",
      "Step 175/467 | Average reward: 0.28515625 | Average sequence length: 183.57\n",
      "Step 176/467 | Example step 0 | Pass 0 | loss: 0.006946 | average reward: 0.125\n",
      "Step 176/467 | Example step 0 | Pass 1 | loss: -0.003078 | average reward: 0.0\n",
      "Step 176/467 | Example step 1 | Pass 0 | loss: 0.002575 | average reward: 0.375\n",
      "Step 176/467 | Example step 1 | Pass 1 | loss: -0.007064 | average reward: 0.375\n",
      "Step 176/467 | Average reward: 0.19140625 | Average sequence length: 160.64\n",
      "Step 177/467 | Example step 0 | Pass 0 | loss: -0.006284 | average reward: 0.75\n",
      "Step 177/467 | Example step 0 | Pass 1 | loss: -0.012387 | average reward: 0.625\n",
      "Step 177/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 177/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 177/467 | Average reward: 0.1875 | Average sequence length: 154.69\n",
      "Step 178/467 | Example step 0 | Pass 0 | loss: 0.003580 | average reward: 0.125\n",
      "Step 178/467 | Example step 0 | Pass 1 | loss: -0.003584 | average reward: 0.0\n",
      "Step 178/467 | Example step 1 | Pass 0 | loss: -0.001437 | average reward: 0.875\n",
      "Step 178/467 | Example step 1 | Pass 1 | loss: -0.000524 | average reward: 0.875\n",
      "Step 178/467 | Average reward: 0.11328125 | Average sequence length: 173.39\n",
      "Step 179/467 | Example step 0 | Pass 0 | loss: 0.000737 | average reward: 0.875\n",
      "Step 179/467 | Example step 0 | Pass 1 | loss: -0.002351 | average reward: 0.875\n",
      "Step 179/467 | Example step 1 | Pass 0 | loss: -0.002786 | average reward: 0.25\n",
      "Step 179/467 | Example step 1 | Pass 1 | loss: -0.001490 | average reward: 0.25\n",
      "Step 179/467 | Average reward: 0.34765625 | Average sequence length: 149.83\n",
      "Step 180 | Pass@1: 0.1625, Pass@2: 0.2175, Pass@3: 0.2650, Pass@4: 0.3100, Pass@5: 0.3375, Pass@6: 0.3600, Pass@7: 0.3875, Pass@8: 0.3975\n",
      "Step 180/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 180/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 180/467 | Example step 1 | Pass 0 | loss: 0.003404 | average reward: 0.125\n",
      "Step 180/467 | Example step 1 | Pass 1 | loss: -0.005919 | average reward: 0.0\n",
      "Step 180/467 | Average reward: 0.17578125 | Average sequence length: 188.62\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000180.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000180.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 181/467 | Example step 0 | Pass 0 | loss: -0.008287 | average reward: 0.0\n",
      "Step 181/467 | Example step 0 | Pass 1 | loss: 0.007370 | average reward: 0.25\n",
      "Step 181/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 181/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 181/467 | Average reward: 0.1015625 | Average sequence length: 158.44\n",
      "Step 182/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 182/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 182/467 | Example step 1 | Pass 0 | loss: 0.013389 | average reward: 0.25\n",
      "Step 182/467 | Example step 1 | Pass 1 | loss: -0.005821 | average reward: 0.0\n",
      "Step 182/467 | Average reward: 0.24609375 | Average sequence length: 173.67\n",
      "Step 183/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 183/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 183/467 | Example step 1 | Pass 0 | loss: -0.008537 | average reward: 0.125\n",
      "Step 183/467 | Example step 1 | Pass 1 | loss: 0.008031 | average reward: 0.25\n",
      "Step 183/467 | Average reward: 0.3671875 | Average sequence length: 166.17\n",
      "Step 184/467 | Example step 0 | Pass 0 | loss: 0.005965 | average reward: 0.125\n",
      "Step 184/467 | Example step 0 | Pass 1 | loss: -0.004317 | average reward: 0.0\n",
      "Step 184/467 | Example step 1 | Pass 0 | loss: -0.002327 | average reward: 0.0\n",
      "Step 184/467 | Example step 1 | Pass 1 | loss: 0.000543 | average reward: 0.125\n",
      "Step 184/467 | Average reward: 0.2578125 | Average sequence length: 173.35\n",
      "Step 185/467 | Example step 0 | Pass 0 | loss: 0.003036 | average reward: 0.125\n",
      "Step 185/467 | Example step 0 | Pass 1 | loss: -0.003417 | average reward: 0.0\n",
      "Step 185/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 185/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 185/467 | Average reward: 0.109375 | Average sequence length: 165.87\n",
      "Step 186/467 | Example step 0 | Pass 0 | loss: -0.008691 | average reward: 0.0\n",
      "Step 186/467 | Example step 0 | Pass 1 | loss: 0.019242 | average reward: 0.25\n",
      "Step 186/467 | Example step 1 | Pass 0 | loss: -0.010707 | average reward: 0.0\n",
      "Step 186/467 | Example step 1 | Pass 1 | loss: 0.001903 | average reward: 0.5\n",
      "Step 186/467 | Average reward: 0.30859375 | Average sequence length: 177.91\n",
      "Step 187/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 187/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 187/467 | Example step 1 | Pass 0 | loss: -0.004202 | average reward: 0.0\n",
      "Step 187/467 | Example step 1 | Pass 1 | loss: 0.000706 | average reward: 0.125\n",
      "Step 187/467 | Average reward: 0.09765625 | Average sequence length: 168.54\n",
      "Step 188/467 | Example step 0 | Pass 0 | loss: -0.003822 | average reward: 0.25\n",
      "Step 188/467 | Example step 0 | Pass 1 | loss: -0.004324 | average reward: 0.25\n",
      "Step 188/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 188/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 188/467 | Average reward: 0.16015625 | Average sequence length: 171.52\n",
      "Step 189/467 | Example step 0 | Pass 0 | loss: -0.003818 | average reward: 0.0\n",
      "Step 189/467 | Example step 0 | Pass 1 | loss: 0.008814 | average reward: 0.25\n",
      "Step 189/467 | Example step 1 | Pass 0 | loss: -0.002826 | average reward: 0.5\n",
      "Step 189/467 | Example step 1 | Pass 1 | loss: -0.007053 | average reward: 0.625\n",
      "Step 189/467 | Average reward: 0.2890625 | Average sequence length: 163.45\n",
      "Step 190/467 | Example step 0 | Pass 0 | loss: -0.003974 | average reward: 0.625\n",
      "Step 190/467 | Example step 0 | Pass 1 | loss: -0.001509 | average reward: 0.875\n",
      "Step 190/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 190/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 190/467 | Average reward: 0.25 | Average sequence length: 159.17\n",
      "Step 191/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 191/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 191/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 191/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 191/467 | Average reward: 0.1640625 | Average sequence length: 179.70\n",
      "Step 192/467 | Example step 0 | Pass 0 | loss: 0.010204 | average reward: 1.0\n",
      "Step 192/467 | Example step 0 | Pass 1 | loss: -0.018465 | average reward: 0.25\n",
      "Step 192/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 192/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 192/467 | Average reward: 0.11328125 | Average sequence length: 177.82\n",
      "Step 193/467 | Example step 0 | Pass 0 | loss: -0.004521 | average reward: 0.25\n",
      "Step 193/467 | Example step 0 | Pass 1 | loss: -0.004594 | average reward: 0.25\n",
      "Step 193/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 193/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 193/467 | Average reward: 0.140625 | Average sequence length: 186.77\n",
      "Step 194/467 | Example step 0 | Pass 0 | loss: -0.028175 | average reward: 0.0\n",
      "Step 194/467 | Example step 0 | Pass 1 | loss: 0.018471 | average reward: 0.75\n",
      "Step 194/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 194/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 194/467 | Average reward: 0.15625 | Average sequence length: 180.52\n",
      "Step 195/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 195/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 195/467 | Example step 1 | Pass 0 | loss: -0.007618 | average reward: 0.0\n",
      "Step 195/467 | Example step 1 | Pass 1 | loss: 0.001526 | average reward: 0.125\n",
      "Step 195/467 | Average reward: 0.203125 | Average sequence length: 162.62\n",
      "Step 196/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 196/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 196/467 | Example step 1 | Pass 0 | loss: -0.001421 | average reward: 0.625\n",
      "Step 196/467 | Example step 1 | Pass 1 | loss: -0.000508 | average reward: 0.75\n",
      "Step 196/467 | Average reward: 0.15234375 | Average sequence length: 168.55\n",
      "Step 197/467 | Example step 0 | Pass 0 | loss: -0.001821 | average reward: 0.0\n",
      "Step 197/467 | Example step 0 | Pass 1 | loss: 0.003392 | average reward: 0.125\n",
      "Step 197/467 | Example step 1 | Pass 0 | loss: -0.003671 | average reward: 0.0\n",
      "Step 197/467 | Example step 1 | Pass 1 | loss: 0.006428 | average reward: 0.125\n",
      "Step 197/467 | Average reward: 0.22265625 | Average sequence length: 136.80\n",
      "Step 198/467 | Example step 0 | Pass 0 | loss: 0.006044 | average reward: 0.875\n",
      "Step 198/467 | Example step 0 | Pass 1 | loss: -0.011864 | average reward: 0.5\n",
      "Step 198/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 198/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 198/467 | Average reward: 0.171875 | Average sequence length: 190.73\n",
      "Step 199/467 | Example step 0 | Pass 0 | loss: 0.004569 | average reward: 1.0\n",
      "Step 199/467 | Example step 0 | Pass 1 | loss: -0.008725 | average reward: 0.75\n",
      "Step 199/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 199/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 199/467 | Average reward: 0.14453125 | Average sequence length: 154.20\n",
      "Step 200/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 200/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 200/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 200/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 200/467 | Average reward: 0.18359375 | Average sequence length: 161.09\n",
      "Step 201/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 201/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 201/467 | Example step 1 | Pass 0 | loss: -0.011916 | average reward: 0.25\n",
      "Step 201/467 | Example step 1 | Pass 1 | loss: 0.012710 | average reward: 0.75\n",
      "Step 201/467 | Average reward: 0.21875 | Average sequence length: 153.30\n",
      "Step 202/467 | Example step 0 | Pass 0 | loss: -0.003474 | average reward: 0.0\n",
      "Step 202/467 | Example step 0 | Pass 1 | loss: 0.003537 | average reward: 0.125\n",
      "Step 202/467 | Example step 1 | Pass 0 | loss: -0.003689 | average reward: 0.0\n",
      "Step 202/467 | Example step 1 | Pass 1 | loss: 0.003112 | average reward: 0.25\n",
      "Step 202/467 | Average reward: 0.16796875 | Average sequence length: 148.69\n",
      "Step 203/467 | Example step 0 | Pass 0 | loss: 0.010513 | average reward: 0.125\n",
      "Step 203/467 | Example step 0 | Pass 1 | loss: -0.003622 | average reward: 0.0\n",
      "Step 203/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 203/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 203/467 | Average reward: 0.2734375 | Average sequence length: 152.63\n",
      "Step 204/467 | Example step 0 | Pass 0 | loss: 0.008911 | average reward: 0.125\n",
      "Step 204/467 | Example step 0 | Pass 1 | loss: -0.004063 | average reward: 0.0\n",
      "Step 204/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 204/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 204/467 | Average reward: 0.0390625 | Average sequence length: 180.77\n",
      "Step 205/467 | Example step 0 | Pass 0 | loss: 0.004210 | average reward: 0.25\n",
      "Step 205/467 | Example step 0 | Pass 1 | loss: -0.005594 | average reward: 0.0\n",
      "Step 205/467 | Example step 1 | Pass 0 | loss: 0.006565 | average reward: 0.125\n",
      "Step 205/467 | Example step 1 | Pass 1 | loss: -0.004603 | average reward: 0.0\n",
      "Step 205/467 | Average reward: 0.30859375 | Average sequence length: 147.84\n",
      "Step 206/467 | Example step 0 | Pass 0 | loss: -0.003450 | average reward: 0.0\n",
      "Step 206/467 | Example step 0 | Pass 1 | loss: 0.001152 | average reward: 0.125\n",
      "Step 206/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 206/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 206/467 | Average reward: 0.13671875 | Average sequence length: 168.16\n",
      "Step 207/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 207/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 207/467 | Example step 1 | Pass 0 | loss: 0.003060 | average reward: 1.0\n",
      "Step 207/467 | Example step 1 | Pass 1 | loss: -0.005724 | average reward: 0.75\n",
      "Step 207/467 | Average reward: 0.26953125 | Average sequence length: 147.46\n",
      "Step 208/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 208/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 208/467 | Example step 1 | Pass 0 | loss: -0.001889 | average reward: 0.25\n",
      "Step 208/467 | Example step 1 | Pass 1 | loss: -0.005875 | average reward: 0.25\n",
      "Step 208/467 | Average reward: 0.296875 | Average sequence length: 164.52\n",
      "Step 209/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 209/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 209/467 | Example step 1 | Pass 0 | loss: -0.012028 | average reward: 0.375\n",
      "Step 209/467 | Example step 1 | Pass 1 | loss: 0.003801 | average reward: 0.625\n",
      "Step 209/467 | Average reward: 0.20703125 | Average sequence length: 160.15\n",
      "Step 210/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 210/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 210/467 | Example step 1 | Pass 0 | loss: -0.007806 | average reward: 0.25\n",
      "Step 210/467 | Example step 1 | Pass 1 | loss: 0.004832 | average reward: 0.5\n",
      "Step 210/467 | Average reward: 0.14453125 | Average sequence length: 143.79\n",
      "Step 211/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 211/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 211/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 211/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 211/467 | Average reward: 0.1796875 | Average sequence length: 155.63\n",
      "Step 212/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 212/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 212/467 | Example step 1 | Pass 0 | loss: -0.007661 | average reward: 0.5\n",
      "Step 212/467 | Example step 1 | Pass 1 | loss: -0.010279 | average reward: 0.5\n",
      "Step 212/467 | Average reward: 0.2109375 | Average sequence length: 178.95\n",
      "Step 213/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 213/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 213/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 213/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 213/467 | Average reward: 0.1171875 | Average sequence length: 147.27\n",
      "Step 214/467 | Example step 0 | Pass 0 | loss: -0.011658 | average reward: 0.125\n",
      "Step 214/467 | Example step 0 | Pass 1 | loss: 0.000329 | average reward: 0.5\n",
      "Step 214/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 214/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 214/467 | Average reward: 0.27734375 | Average sequence length: 157.25\n",
      "Step 215/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 215/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 215/467 | Example step 1 | Pass 0 | loss: 0.000400 | average reward: 0.875\n",
      "Step 215/467 | Example step 1 | Pass 1 | loss: 0.002212 | average reward: 1.0\n",
      "Step 215/467 | Average reward: 0.1796875 | Average sequence length: 150.26\n",
      "Step 216/467 | Example step 0 | Pass 0 | loss: -0.004964 | average reward: 0.375\n",
      "Step 216/467 | Example step 0 | Pass 1 | loss: 0.004835 | average reward: 0.625\n",
      "Step 216/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 216/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 216/467 | Average reward: 0.23828125 | Average sequence length: 144.18\n",
      "Step 217/467 | Example step 0 | Pass 0 | loss: 0.005998 | average reward: 0.75\n",
      "Step 217/467 | Example step 0 | Pass 1 | loss: -0.001654 | average reward: 0.5\n",
      "Step 217/467 | Example step 1 | Pass 0 | loss: 0.005168 | average reward: 0.125\n",
      "Step 217/467 | Example step 1 | Pass 1 | loss: -0.006844 | average reward: 0.0\n",
      "Step 217/467 | Average reward: 0.234375 | Average sequence length: 141.20\n",
      "Step 218/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 218/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 218/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 218/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 218/467 | Average reward: 0.25390625 | Average sequence length: 155.20\n",
      "Step 219/467 | Example step 0 | Pass 0 | loss: -0.001369 | average reward: 0.25\n",
      "Step 219/467 | Example step 0 | Pass 1 | loss: -0.001120 | average reward: 0.25\n",
      "Step 219/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 219/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 219/467 | Average reward: 0.1328125 | Average sequence length: 151.67\n",
      "Step 220/467 | Example step 0 | Pass 0 | loss: -0.004588 | average reward: 0.0\n",
      "Step 220/467 | Example step 0 | Pass 1 | loss: 0.006870 | average reward: 0.125\n",
      "Step 220/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 220/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 220/467 | Average reward: 0.1171875 | Average sequence length: 169.68\n",
      "Step 221/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 221/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 221/467 | Example step 1 | Pass 0 | loss: -0.003219 | average reward: 0.125\n",
      "Step 221/467 | Example step 1 | Pass 1 | loss: -0.000860 | average reward: 0.25\n",
      "Step 221/467 | Average reward: 0.17578125 | Average sequence length: 156.48\n",
      "Step 222/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 222/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 222/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 222/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 222/467 | Average reward: 0.1953125 | Average sequence length: 151.99\n",
      "Step 223/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 223/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 223/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 223/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 223/467 | Average reward: 0.11328125 | Average sequence length: 172.93\n",
      "Step 224/467 | Example step 0 | Pass 0 | loss: 0.003046 | average reward: 0.875\n",
      "Step 224/467 | Example step 0 | Pass 1 | loss: -0.002226 | average reward: 0.625\n",
      "Step 224/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 224/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 224/467 | Average reward: 0.1484375 | Average sequence length: 185.58\n",
      "Step 225/467 | Example step 0 | Pass 0 | loss: 0.001985 | average reward: 0.125\n",
      "Step 225/467 | Example step 0 | Pass 1 | loss: -0.001837 | average reward: 0.0\n",
      "Step 225/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 225/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 225/467 | Average reward: 0.203125 | Average sequence length: 155.43\n",
      "Step 226/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 226/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 226/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 226/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 226/467 | Average reward: 0.07421875 | Average sequence length: 156.34\n",
      "Step 227/467 | Example step 0 | Pass 0 | loss: 0.003345 | average reward: 0.125\n",
      "Step 227/467 | Example step 0 | Pass 1 | loss: -0.003652 | average reward: 0.0\n",
      "Step 227/467 | Example step 1 | Pass 0 | loss: -0.001197 | average reward: 0.25\n",
      "Step 227/467 | Example step 1 | Pass 1 | loss: -0.004798 | average reward: 0.25\n",
      "Step 227/467 | Average reward: 0.22265625 | Average sequence length: 178.87\n",
      "Step 228/467 | Example step 0 | Pass 0 | loss: -0.005994 | average reward: 0.375\n",
      "Step 228/467 | Example step 0 | Pass 1 | loss: 0.002719 | average reward: 0.5\n",
      "Step 228/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 228/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 228/467 | Average reward: 0.18359375 | Average sequence length: 147.71\n",
      "Step 229/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 229/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 229/467 | Example step 1 | Pass 0 | loss: 0.008945 | average reward: 0.25\n",
      "Step 229/467 | Example step 1 | Pass 1 | loss: -0.008265 | average reward: 0.0\n",
      "Step 229/467 | Average reward: 0.17578125 | Average sequence length: 166.53\n",
      "Step 230/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 230/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 230/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 230/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 230/467 | Average reward: 0.23828125 | Average sequence length: 159.46\n",
      "Step 231/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 231/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 231/467 | Example step 1 | Pass 0 | loss: -0.005613 | average reward: 0.375\n",
      "Step 231/467 | Example step 1 | Pass 1 | loss: 0.002447 | average reward: 0.375\n",
      "Step 231/467 | Average reward: 0.23046875 | Average sequence length: 151.79\n",
      "Step 232/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 232/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 232/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 232/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 232/467 | Average reward: 0.05859375 | Average sequence length: 190.71\n",
      "Step 233/467 | Example step 0 | Pass 0 | loss: 0.001563 | average reward: 1.0\n",
      "Step 233/467 | Example step 0 | Pass 1 | loss: -0.004602 | average reward: 0.875\n",
      "Step 233/467 | Example step 1 | Pass 0 | loss: -0.005055 | average reward: 0.0\n",
      "Step 233/467 | Example step 1 | Pass 1 | loss: 0.006269 | average reward: 0.125\n",
      "Step 233/467 | Average reward: 0.1484375 | Average sequence length: 175.09\n",
      "Step 234/467 | Example step 0 | Pass 0 | loss: 0.001459 | average reward: 0.125\n",
      "Step 234/467 | Example step 0 | Pass 1 | loss: -0.004336 | average reward: 0.0\n",
      "Step 234/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 234/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 234/467 | Average reward: 0.140625 | Average sequence length: 162.32\n",
      "Step 235/467 | Example step 0 | Pass 0 | loss: 0.001527 | average reward: 0.375\n",
      "Step 235/467 | Example step 0 | Pass 1 | loss: -0.001727 | average reward: 0.25\n",
      "Step 235/467 | Example step 1 | Pass 0 | loss: 0.010727 | average reward: 0.125\n",
      "Step 235/467 | Example step 1 | Pass 1 | loss: -0.005056 | average reward: 0.0\n",
      "Step 235/467 | Average reward: 0.30078125 | Average sequence length: 168.99\n",
      "Step 236/467 | Example step 0 | Pass 0 | loss: 0.012023 | average reward: 0.625\n",
      "Step 236/467 | Example step 0 | Pass 1 | loss: -0.014306 | average reward: 0.25\n",
      "Step 236/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 236/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 236/467 | Average reward: 0.1171875 | Average sequence length: 162.36\n",
      "Step 237/467 | Example step 0 | Pass 0 | loss: -0.007297 | average reward: 0.0\n",
      "Step 237/467 | Example step 0 | Pass 1 | loss: 0.009638 | average reward: 0.375\n",
      "Step 237/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 237/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 237/467 | Average reward: 0.32421875 | Average sequence length: 150.47\n",
      "Step 238/467 | Example step 0 | Pass 0 | loss: -0.004457 | average reward: 0.0\n",
      "Step 238/467 | Example step 0 | Pass 1 | loss: 0.002669 | average reward: 0.25\n",
      "Step 238/467 | Example step 1 | Pass 0 | loss: 0.003185 | average reward: 0.125\n",
      "Step 238/467 | Example step 1 | Pass 1 | loss: -0.001299 | average reward: 0.0\n",
      "Step 238/467 | Average reward: 0.234375 | Average sequence length: 180.12\n",
      "Step 239/467 | Example step 0 | Pass 0 | loss: -0.021225 | average reward: 0.375\n",
      "Step 239/467 | Example step 0 | Pass 1 | loss: 0.005746 | average reward: 0.75\n",
      "Step 239/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 239/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 239/467 | Average reward: 0.1875 | Average sequence length: 171.58\n",
      "Step 240 | Pass@1: 0.1900, Pass@2: 0.2225, Pass@3: 0.2625, Pass@4: 0.3050, Pass@5: 0.3250, Pass@6: 0.3450, Pass@7: 0.3650, Pass@8: 0.3775\n",
      "Step 240/467 | Example step 0 | Pass 0 | loss: -0.002489 | average reward: 0.25\n",
      "Step 240/467 | Example step 0 | Pass 1 | loss: 0.006602 | average reward: 0.375\n",
      "Step 240/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 240/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 240/467 | Average reward: 0.2109375 | Average sequence length: 145.11\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000240.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000240.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 241/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 241/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 241/467 | Example step 1 | Pass 0 | loss: 0.002283 | average reward: 1.0\n",
      "Step 241/467 | Example step 1 | Pass 1 | loss: -0.000848 | average reward: 0.875\n",
      "Step 241/467 | Average reward: 0.1953125 | Average sequence length: 153.85\n",
      "Step 242/467 | Example step 0 | Pass 0 | loss: -0.005925 | average reward: 0.375\n",
      "Step 242/467 | Example step 0 | Pass 1 | loss: -0.003722 | average reward: 0.375\n",
      "Step 242/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 242/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 242/467 | Average reward: 0.13671875 | Average sequence length: 167.16\n",
      "Step 243/467 | Example step 0 | Pass 0 | loss: 0.004302 | average reward: 0.25\n",
      "Step 243/467 | Example step 0 | Pass 1 | loss: -0.006174 | average reward: 0.0\n",
      "Step 243/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 243/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 243/467 | Average reward: 0.046875 | Average sequence length: 172.33\n",
      "Step 244/467 | Example step 0 | Pass 0 | loss: 0.008580 | average reward: 0.125\n",
      "Step 244/467 | Example step 0 | Pass 1 | loss: -0.005284 | average reward: 0.0\n",
      "Step 244/467 | Example step 1 | Pass 0 | loss: -0.000580 | average reward: 0.625\n",
      "Step 244/467 | Example step 1 | Pass 1 | loss: -0.004528 | average reward: 0.5\n",
      "Step 244/467 | Average reward: 0.0859375 | Average sequence length: 177.48\n",
      "Step 245/467 | Example step 0 | Pass 0 | loss: -0.004858 | average reward: 0.125\n",
      "Step 245/467 | Example step 0 | Pass 1 | loss: 0.003828 | average reward: 0.25\n",
      "Step 245/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 245/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 245/467 | Average reward: 0.19140625 | Average sequence length: 175.65\n",
      "Step 246/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 246/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 246/467 | Example step 1 | Pass 0 | loss: -0.001818 | average reward: 0.0\n",
      "Step 246/467 | Example step 1 | Pass 1 | loss: 0.002268 | average reward: 0.125\n",
      "Step 246/467 | Average reward: 0.30859375 | Average sequence length: 161.29\n",
      "Step 247/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 247/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 247/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 247/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 247/467 | Average reward: 0.0625 | Average sequence length: 159.59\n",
      "Step 248/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 248/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 248/467 | Example step 1 | Pass 0 | loss: -0.009128 | average reward: 0.375\n",
      "Step 248/467 | Example step 1 | Pass 1 | loss: -0.004423 | average reward: 0.375\n",
      "Step 248/467 | Average reward: 0.1875 | Average sequence length: 179.14\n",
      "Step 249/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 249/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 249/467 | Example step 1 | Pass 0 | loss: -0.001568 | average reward: 0.0\n",
      "Step 249/467 | Example step 1 | Pass 1 | loss: 0.004062 | average reward: 0.125\n",
      "Step 249/467 | Average reward: 0.16015625 | Average sequence length: 184.12\n",
      "Step 250/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 250/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 250/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 250/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 250/467 | Average reward: 0.1328125 | Average sequence length: 191.33\n",
      "Step 251/467 | Example step 0 | Pass 0 | loss: -0.008974 | average reward: 0.75\n",
      "Step 251/467 | Example step 0 | Pass 1 | loss: -0.004304 | average reward: 0.75\n",
      "Step 251/467 | Example step 1 | Pass 0 | loss: -0.000403 | average reward: 0.5\n",
      "Step 251/467 | Example step 1 | Pass 1 | loss: -0.012857 | average reward: 0.25\n",
      "Step 251/467 | Average reward: 0.2421875 | Average sequence length: 165.43\n",
      "Step 252/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 252/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 252/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 252/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 252/467 | Average reward: 0.1953125 | Average sequence length: 180.91\n",
      "Step 253/467 | Example step 0 | Pass 0 | loss: -0.000091 | average reward: 0.25\n",
      "Step 253/467 | Example step 0 | Pass 1 | loss: -0.000253 | average reward: 0.125\n",
      "Step 253/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 253/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 253/467 | Average reward: 0.21875 | Average sequence length: 157.38\n",
      "Step 254/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 254/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 254/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 254/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 254/467 | Average reward: 0.34765625 | Average sequence length: 153.62\n",
      "Step 255/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 255/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 255/467 | Example step 1 | Pass 0 | loss: -0.002685 | average reward: 0.75\n",
      "Step 255/467 | Example step 1 | Pass 1 | loss: 0.004619 | average reward: 1.0\n",
      "Step 255/467 | Average reward: 0.19140625 | Average sequence length: 158.39\n",
      "Step 256/467 | Example step 0 | Pass 0 | loss: -0.005055 | average reward: 0.0\n",
      "Step 256/467 | Example step 0 | Pass 1 | loss: 0.007293 | average reward: 0.125\n",
      "Step 256/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 256/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 256/467 | Average reward: 0.1953125 | Average sequence length: 156.96\n",
      "Step 257/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 257/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 257/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 257/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 257/467 | Average reward: 0.12109375 | Average sequence length: 167.52\n",
      "Step 258/467 | Example step 0 | Pass 0 | loss: 0.004096 | average reward: 0.125\n",
      "Step 258/467 | Example step 0 | Pass 1 | loss: -0.001765 | average reward: 0.0\n",
      "Step 258/467 | Example step 1 | Pass 0 | loss: 0.000747 | average reward: 0.875\n",
      "Step 258/467 | Example step 1 | Pass 1 | loss: -0.004488 | average reward: 0.75\n",
      "Step 258/467 | Average reward: 0.2421875 | Average sequence length: 171.67\n",
      "Step 259/467 | Example step 0 | Pass 0 | loss: 0.008695 | average reward: 0.25\n",
      "Step 259/467 | Example step 0 | Pass 1 | loss: -0.006613 | average reward: 0.0\n",
      "Step 259/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 259/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 259/467 | Average reward: 0.171875 | Average sequence length: 179.79\n",
      "Step 260/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 260/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 260/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 260/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 260/467 | Average reward: 0.2421875 | Average sequence length: 154.50\n",
      "Step 261/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 261/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 261/467 | Example step 1 | Pass 0 | loss: 0.003227 | average reward: 0.125\n",
      "Step 261/467 | Example step 1 | Pass 1 | loss: -0.002967 | average reward: 0.0\n",
      "Step 261/467 | Average reward: 0.23828125 | Average sequence length: 180.17\n",
      "Step 262/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 262/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 262/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 262/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 262/467 | Average reward: 0.140625 | Average sequence length: 192.65\n",
      "Step 263/467 | Example step 0 | Pass 0 | loss: -0.006458 | average reward: 0.625\n",
      "Step 263/467 | Example step 0 | Pass 1 | loss: 0.008742 | average reward: 0.875\n",
      "Step 263/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 263/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 263/467 | Average reward: 0.2109375 | Average sequence length: 165.60\n",
      "Step 264/467 | Example step 0 | Pass 0 | loss: 0.000885 | average reward: 0.25\n",
      "Step 264/467 | Example step 0 | Pass 1 | loss: 0.006165 | average reward: 0.25\n",
      "Step 264/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 264/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 264/467 | Average reward: 0.0625 | Average sequence length: 167.98\n",
      "Step 265/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 265/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 265/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 265/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 265/467 | Average reward: 0.1484375 | Average sequence length: 149.31\n",
      "Step 266/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 266/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 266/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 266/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 266/467 | Average reward: 0.06640625 | Average sequence length: 192.20\n",
      "Step 267/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 267/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 267/467 | Example step 1 | Pass 0 | loss: -0.004177 | average reward: 0.0\n",
      "Step 267/467 | Example step 1 | Pass 1 | loss: 0.015096 | average reward: 0.25\n",
      "Step 267/467 | Average reward: 0.22265625 | Average sequence length: 177.11\n",
      "Step 268/467 | Example step 0 | Pass 0 | loss: -0.002496 | average reward: 0.375\n",
      "Step 268/467 | Example step 0 | Pass 1 | loss: -0.008524 | average reward: 0.25\n",
      "Step 268/467 | Example step 1 | Pass 0 | loss: -0.003552 | average reward: 0.0\n",
      "Step 268/467 | Example step 1 | Pass 1 | loss: 0.004999 | average reward: 0.125\n",
      "Step 268/467 | Average reward: 0.1953125 | Average sequence length: 178.36\n",
      "Step 269/467 | Example step 0 | Pass 0 | loss: -0.002266 | average reward: 0.0\n",
      "Step 269/467 | Example step 0 | Pass 1 | loss: 0.001226 | average reward: 0.125\n",
      "Step 269/467 | Example step 1 | Pass 0 | loss: 0.016642 | average reward: 1.0\n",
      "Step 269/467 | Example step 1 | Pass 1 | loss: -0.020155 | average reward: 0.5\n",
      "Step 269/467 | Average reward: 0.2890625 | Average sequence length: 166.12\n",
      "Step 270/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 270/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 270/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 270/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 270/467 | Average reward: 0.109375 | Average sequence length: 186.23\n",
      "Step 271/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 271/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 271/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 271/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 271/467 | Average reward: 0.12890625 | Average sequence length: 161.00\n",
      "Step 272/467 | Example step 0 | Pass 0 | loss: -0.001910 | average reward: 0.125\n",
      "Step 272/467 | Example step 0 | Pass 1 | loss: -0.001570 | average reward: 0.125\n",
      "Step 272/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 272/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 272/467 | Average reward: 0.08203125 | Average sequence length: 173.30\n",
      "Step 273/467 | Example step 0 | Pass 0 | loss: -0.006082 | average reward: 0.25\n",
      "Step 273/467 | Example step 0 | Pass 1 | loss: -0.004185 | average reward: 0.375\n",
      "Step 273/467 | Example step 1 | Pass 0 | loss: -0.005131 | average reward: 0.5\n",
      "Step 273/467 | Example step 1 | Pass 1 | loss: -0.000372 | average reward: 0.625\n",
      "Step 273/467 | Average reward: 0.18359375 | Average sequence length: 159.99\n",
      "Step 274/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 274/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 274/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 274/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 274/467 | Average reward: 0.078125 | Average sequence length: 180.62\n",
      "Step 275/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 275/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 275/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 275/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 275/467 | Average reward: 0.29296875 | Average sequence length: 157.88\n",
      "Step 276/467 | Example step 0 | Pass 0 | loss: 0.009975 | average reward: 0.5\n",
      "Step 276/467 | Example step 0 | Pass 1 | loss: -0.012250 | average reward: 0.25\n",
      "Step 276/467 | Example step 1 | Pass 0 | loss: 0.011000 | average reward: 0.125\n",
      "Step 276/467 | Example step 1 | Pass 1 | loss: -0.005338 | average reward: 0.0\n",
      "Step 276/467 | Average reward: 0.26953125 | Average sequence length: 155.86\n",
      "Step 277/467 | Example step 0 | Pass 0 | loss: -0.004992 | average reward: 0.0\n",
      "Step 277/467 | Example step 0 | Pass 1 | loss: 0.009894 | average reward: 0.125\n",
      "Step 277/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 277/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 277/467 | Average reward: 0.2734375 | Average sequence length: 182.43\n",
      "Step 278/467 | Example step 0 | Pass 0 | loss: 0.011050 | average reward: 0.375\n",
      "Step 278/467 | Example step 0 | Pass 1 | loss: -0.012247 | average reward: 0.0\n",
      "Step 278/467 | Example step 1 | Pass 0 | loss: -0.007232 | average reward: 0.625\n",
      "Step 278/467 | Example step 1 | Pass 1 | loss: 0.003662 | average reward: 0.75\n",
      "Step 278/467 | Average reward: 0.19921875 | Average sequence length: 156.66\n",
      "Step 279/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 279/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 279/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 279/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 279/467 | Average reward: 0.0625 | Average sequence length: 174.44\n",
      "Step 280/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 280/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 280/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 280/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 280/467 | Average reward: 0.3046875 | Average sequence length: 159.72\n",
      "Step 281/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 281/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 281/467 | Example step 1 | Pass 0 | loss: -0.004920 | average reward: 0.0\n",
      "Step 281/467 | Example step 1 | Pass 1 | loss: 0.002788 | average reward: 0.125\n",
      "Step 281/467 | Average reward: 0.16796875 | Average sequence length: 196.91\n",
      "Step 282/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 282/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 282/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 282/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 282/467 | Average reward: 0.09765625 | Average sequence length: 168.50\n",
      "Step 283/467 | Example step 0 | Pass 0 | loss: 0.001732 | average reward: 1.0\n",
      "Step 283/467 | Example step 0 | Pass 1 | loss: -0.005006 | average reward: 0.875\n",
      "Step 283/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 283/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 283/467 | Average reward: 0.26953125 | Average sequence length: 150.48\n",
      "Step 284/467 | Example step 0 | Pass 0 | loss: -0.004521 | average reward: 0.5\n",
      "Step 284/467 | Example step 0 | Pass 1 | loss: -0.000291 | average reward: 0.625\n",
      "Step 284/467 | Example step 1 | Pass 0 | loss: 0.001884 | average reward: 0.125\n",
      "Step 284/467 | Example step 1 | Pass 1 | loss: -0.001078 | average reward: 0.0\n",
      "Step 284/467 | Average reward: 0.23046875 | Average sequence length: 168.89\n",
      "Step 285/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 285/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 285/467 | Example step 1 | Pass 0 | loss: -0.021941 | average reward: 0.625\n",
      "Step 285/467 | Example step 1 | Pass 1 | loss: 0.002335 | average reward: 0.875\n",
      "Step 285/467 | Average reward: 0.19921875 | Average sequence length: 155.05\n",
      "Step 286/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 286/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 286/467 | Example step 1 | Pass 0 | loss: 0.004721 | average reward: 0.125\n",
      "Step 286/467 | Example step 1 | Pass 1 | loss: -0.004639 | average reward: 0.0\n",
      "Step 286/467 | Average reward: 0.1484375 | Average sequence length: 192.99\n",
      "Step 287/467 | Example step 0 | Pass 0 | loss: 0.004300 | average reward: 1.0\n",
      "Step 287/467 | Example step 0 | Pass 1 | loss: -0.004878 | average reward: 0.75\n",
      "Step 287/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 287/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 287/467 | Average reward: 0.2890625 | Average sequence length: 161.33\n",
      "Step 288/467 | Example step 0 | Pass 0 | loss: -0.007391 | average reward: 0.0\n",
      "Step 288/467 | Example step 0 | Pass 1 | loss: 0.012002 | average reward: 0.25\n",
      "Step 288/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 288/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 288/467 | Average reward: 0.125 | Average sequence length: 193.32\n",
      "Step 289/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 289/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 289/467 | Example step 1 | Pass 0 | loss: -0.005144 | average reward: 0.125\n",
      "Step 289/467 | Example step 1 | Pass 1 | loss: -0.002629 | average reward: 0.125\n",
      "Step 289/467 | Average reward: 0.15234375 | Average sequence length: 165.87\n",
      "Step 290/467 | Example step 0 | Pass 0 | loss: -0.003786 | average reward: 0.0\n",
      "Step 290/467 | Example step 0 | Pass 1 | loss: 0.006803 | average reward: 0.125\n",
      "Step 290/467 | Example step 1 | Pass 0 | loss: -0.004430 | average reward: 0.25\n",
      "Step 290/467 | Example step 1 | Pass 1 | loss: -0.004587 | average reward: 0.375\n",
      "Step 290/467 | Average reward: 0.3046875 | Average sequence length: 172.24\n",
      "Step 291/467 | Example step 0 | Pass 0 | loss: -0.001872 | average reward: 0.25\n",
      "Step 291/467 | Example step 0 | Pass 1 | loss: -0.000535 | average reward: 0.125\n",
      "Step 291/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 291/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 291/467 | Average reward: 0.15234375 | Average sequence length: 173.91\n",
      "Step 292/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 292/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 292/467 | Example step 1 | Pass 0 | loss: -0.005845 | average reward: 0.5\n",
      "Step 292/467 | Example step 1 | Pass 1 | loss: 0.001242 | average reward: 0.625\n",
      "Step 292/467 | Average reward: 0.18359375 | Average sequence length: 176.96\n",
      "Step 293/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 293/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 293/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 293/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 293/467 | Average reward: 0.09765625 | Average sequence length: 187.48\n",
      "Step 294/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 294/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 294/467 | Example step 1 | Pass 0 | loss: 0.011483 | average reward: 0.375\n",
      "Step 294/467 | Example step 1 | Pass 1 | loss: -0.009330 | average reward: 0.0\n",
      "Step 294/467 | Average reward: 0.13671875 | Average sequence length: 189.67\n",
      "Step 295/467 | Example step 0 | Pass 0 | loss: -0.008782 | average reward: 0.0\n",
      "Step 295/467 | Example step 0 | Pass 1 | loss: 0.004403 | average reward: 0.25\n",
      "Step 295/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 295/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 295/467 | Average reward: 0.1875 | Average sequence length: 184.22\n",
      "Step 296/467 | Example step 0 | Pass 0 | loss: 0.003889 | average reward: 0.375\n",
      "Step 296/467 | Example step 0 | Pass 1 | loss: -0.000128 | average reward: 0.25\n",
      "Step 296/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 296/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 296/467 | Average reward: 0.09375 | Average sequence length: 168.34\n",
      "Step 297/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 297/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 297/467 | Example step 1 | Pass 0 | loss: -0.000907 | average reward: 0.5\n",
      "Step 297/467 | Example step 1 | Pass 1 | loss: -0.000222 | average reward: 0.375\n",
      "Step 297/467 | Average reward: 0.26953125 | Average sequence length: 175.97\n",
      "Step 298/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 298/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 298/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 298/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 298/467 | Average reward: 0.19140625 | Average sequence length: 158.57\n",
      "Step 299/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 299/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 299/467 | Example step 1 | Pass 0 | loss: -0.005779 | average reward: 0.75\n",
      "Step 299/467 | Example step 1 | Pass 1 | loss: 0.003941 | average reward: 0.875\n",
      "Step 299/467 | Average reward: 0.23046875 | Average sequence length: 160.80\n",
      "Step 300 | Pass@1: 0.1800, Pass@2: 0.2325, Pass@3: 0.2700, Pass@4: 0.3000, Pass@5: 0.3250, Pass@6: 0.3400, Pass@7: 0.3550, Pass@8: 0.3725\n",
      "Step 300/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 300/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 300/467 | Example step 1 | Pass 0 | loss: -0.003803 | average reward: 0.5\n",
      "Step 300/467 | Example step 1 | Pass 1 | loss: -0.000940 | average reward: 0.5\n",
      "Step 300/467 | Average reward: 0.16796875 | Average sequence length: 179.60\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000300.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000300.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 301/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 301/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 301/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 301/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 301/467 | Average reward: 0.12109375 | Average sequence length: 164.77\n",
      "Step 302/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 302/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 302/467 | Example step 1 | Pass 0 | loss: -0.001501 | average reward: 0.25\n",
      "Step 302/467 | Example step 1 | Pass 1 | loss: -0.003141 | average reward: 0.375\n",
      "Step 302/467 | Average reward: 0.23046875 | Average sequence length: 153.56\n",
      "Step 303/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 303/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 303/467 | Example step 1 | Pass 0 | loss: -0.007068 | average reward: 0.0\n",
      "Step 303/467 | Example step 1 | Pass 1 | loss: 0.006782 | average reward: 0.25\n",
      "Step 303/467 | Average reward: 0.08203125 | Average sequence length: 201.85\n",
      "Step 304/467 | Example step 0 | Pass 0 | loss: -0.008170 | average reward: 0.5\n",
      "Step 304/467 | Example step 0 | Pass 1 | loss: 0.002517 | average reward: 0.75\n",
      "Step 304/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 304/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 304/467 | Average reward: 0.1640625 | Average sequence length: 166.57\n",
      "Step 305/467 | Example step 0 | Pass 0 | loss: -0.000226 | average reward: 0.625\n",
      "Step 305/467 | Example step 0 | Pass 1 | loss: -0.000401 | average reward: 0.625\n",
      "Step 305/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 305/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 305/467 | Average reward: 0.14453125 | Average sequence length: 160.71\n",
      "Step 306/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 306/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 306/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 306/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 306/467 | Average reward: 0.20703125 | Average sequence length: 163.56\n",
      "Step 307/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 307/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 307/467 | Example step 1 | Pass 0 | loss: 0.006269 | average reward: 0.875\n",
      "Step 307/467 | Example step 1 | Pass 1 | loss: -0.006408 | average reward: 0.5\n",
      "Step 307/467 | Average reward: 0.22265625 | Average sequence length: 185.58\n",
      "Step 308/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 308/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 308/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 308/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 308/467 | Average reward: 0.0859375 | Average sequence length: 187.73\n",
      "Step 309/467 | Example step 0 | Pass 0 | loss: -0.003618 | average reward: 0.875\n",
      "Step 309/467 | Example step 0 | Pass 1 | loss: 0.001341 | average reward: 1.0\n",
      "Step 309/467 | Example step 1 | Pass 0 | loss: 0.000556 | average reward: 0.5\n",
      "Step 309/467 | Example step 1 | Pass 1 | loss: 0.003740 | average reward: 0.625\n",
      "Step 309/467 | Average reward: 0.2421875 | Average sequence length: 166.30\n",
      "Step 310/467 | Example step 0 | Pass 0 | loss: 0.008811 | average reward: 0.375\n",
      "Step 310/467 | Example step 0 | Pass 1 | loss: -0.006676 | average reward: 0.125\n",
      "Step 310/467 | Example step 1 | Pass 0 | loss: -0.007878 | average reward: 0.25\n",
      "Step 310/467 | Example step 1 | Pass 1 | loss: -0.000602 | average reward: 0.5\n",
      "Step 310/467 | Average reward: 0.23046875 | Average sequence length: 164.17\n",
      "Step 311/467 | Example step 0 | Pass 0 | loss: -0.001655 | average reward: 0.125\n",
      "Step 311/467 | Example step 0 | Pass 1 | loss: -0.000643 | average reward: 0.125\n",
      "Step 311/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 311/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 311/467 | Average reward: 0.2890625 | Average sequence length: 158.03\n",
      "Step 312/467 | Example step 0 | Pass 0 | loss: -0.000143 | average reward: 0.125\n",
      "Step 312/467 | Example step 0 | Pass 1 | loss: 0.000909 | average reward: 0.125\n",
      "Step 312/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 312/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 312/467 | Average reward: 0.1953125 | Average sequence length: 155.57\n",
      "Step 313/467 | Example step 0 | Pass 0 | loss: -0.004055 | average reward: 0.75\n",
      "Step 313/467 | Example step 0 | Pass 1 | loss: -0.002720 | average reward: 0.875\n",
      "Step 313/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 313/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 313/467 | Average reward: 0.16796875 | Average sequence length: 156.81\n",
      "Step 314/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 314/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 314/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 314/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 314/467 | Average reward: 0.0859375 | Average sequence length: 163.14\n",
      "Step 315/467 | Example step 0 | Pass 0 | loss: -0.002461 | average reward: 0.125\n",
      "Step 315/467 | Example step 0 | Pass 1 | loss: 0.001806 | average reward: 0.25\n",
      "Step 315/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 315/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 315/467 | Average reward: 0.1640625 | Average sequence length: 193.84\n",
      "Step 316/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 316/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 316/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 316/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 316/467 | Average reward: 0.24609375 | Average sequence length: 143.48\n",
      "Step 317/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 317/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 317/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 317/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 317/467 | Average reward: 0.234375 | Average sequence length: 184.41\n",
      "Step 318/467 | Example step 0 | Pass 0 | loss: -0.002107 | average reward: 0.125\n",
      "Step 318/467 | Example step 0 | Pass 1 | loss: 0.004206 | average reward: 0.25\n",
      "Step 318/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 318/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 318/467 | Average reward: 0.0859375 | Average sequence length: 189.76\n",
      "Step 319/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 319/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 319/467 | Example step 1 | Pass 0 | loss: 0.001878 | average reward: 1.0\n",
      "Step 319/467 | Example step 1 | Pass 1 | loss: -0.003506 | average reward: 0.875\n",
      "Step 319/467 | Average reward: 0.25 | Average sequence length: 140.94\n",
      "Step 320/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 320/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 320/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 320/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 320/467 | Average reward: 0.23828125 | Average sequence length: 185.91\n",
      "Step 321/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 321/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 321/467 | Example step 1 | Pass 0 | loss: -0.002844 | average reward: 0.0\n",
      "Step 321/467 | Example step 1 | Pass 1 | loss: 0.004150 | average reward: 0.125\n",
      "Step 321/467 | Average reward: 0.1171875 | Average sequence length: 174.10\n",
      "Step 322/467 | Example step 0 | Pass 0 | loss: 0.000870 | average reward: 1.0\n",
      "Step 322/467 | Example step 0 | Pass 1 | loss: -0.003360 | average reward: 0.875\n",
      "Step 322/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 322/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 322/467 | Average reward: 0.16796875 | Average sequence length: 162.34\n",
      "Step 323/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 323/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 323/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 323/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 323/467 | Average reward: 0.05859375 | Average sequence length: 181.89\n",
      "Step 324/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 324/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 324/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 324/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 324/467 | Average reward: 0.14453125 | Average sequence length: 166.94\n",
      "Step 325/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 325/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 325/467 | Example step 1 | Pass 0 | loss: -0.001671 | average reward: 0.0\n",
      "Step 325/467 | Example step 1 | Pass 1 | loss: 0.000906 | average reward: 0.125\n",
      "Step 325/467 | Average reward: 0.13671875 | Average sequence length: 171.13\n",
      "Step 326/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 326/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 326/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 326/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 326/467 | Average reward: 0.12890625 | Average sequence length: 179.39\n",
      "Step 327/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 327/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 327/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 327/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 327/467 | Average reward: 0.08203125 | Average sequence length: 183.57\n",
      "Step 328/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 328/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 328/467 | Example step 1 | Pass 0 | loss: 0.000527 | average reward: 0.875\n",
      "Step 328/467 | Example step 1 | Pass 1 | loss: -0.001370 | average reward: 0.875\n",
      "Step 328/467 | Average reward: 0.22265625 | Average sequence length: 176.30\n",
      "Step 329/467 | Example step 0 | Pass 0 | loss: -0.009497 | average reward: 0.0\n",
      "Step 329/467 | Example step 0 | Pass 1 | loss: 0.008139 | average reward: 0.375\n",
      "Step 329/467 | Example step 1 | Pass 0 | loss: 0.005650 | average reward: 0.25\n",
      "Step 329/467 | Example step 1 | Pass 1 | loss: -0.006587 | average reward: 0.0\n",
      "Step 329/467 | Average reward: 0.1015625 | Average sequence length: 166.62\n",
      "Step 330/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 330/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 330/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 330/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 330/467 | Average reward: 0.15625 | Average sequence length: 162.36\n",
      "Step 331/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 331/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 331/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 331/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 331/467 | Average reward: 0.1640625 | Average sequence length: 174.69\n",
      "Step 332/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 332/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 332/467 | Example step 1 | Pass 0 | loss: -0.005640 | average reward: 0.0\n",
      "Step 332/467 | Example step 1 | Pass 1 | loss: 0.014731 | average reward: 0.25\n",
      "Step 332/467 | Average reward: 0.07421875 | Average sequence length: 171.96\n",
      "Step 333/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 333/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 333/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 333/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 333/467 | Average reward: 0.26953125 | Average sequence length: 160.77\n",
      "Step 334/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 334/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 334/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 334/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 334/467 | Average reward: 0.33984375 | Average sequence length: 151.45\n",
      "Step 335/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 335/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 335/467 | Example step 1 | Pass 0 | loss: -0.009234 | average reward: 0.375\n",
      "Step 335/467 | Example step 1 | Pass 1 | loss: -0.002460 | average reward: 0.375\n",
      "Step 335/467 | Average reward: 0.1640625 | Average sequence length: 191.43\n",
      "Step 336/467 | Example step 0 | Pass 0 | loss: -0.002376 | average reward: 0.125\n",
      "Step 336/467 | Example step 0 | Pass 1 | loss: -0.000387 | average reward: 0.125\n",
      "Step 336/467 | Example step 1 | Pass 0 | loss: 0.009057 | average reward: 0.5\n",
      "Step 336/467 | Example step 1 | Pass 1 | loss: -0.003418 | average reward: 0.25\n",
      "Step 336/467 | Average reward: 0.22265625 | Average sequence length: 164.00\n",
      "Step 337/467 | Example step 0 | Pass 0 | loss: -0.009615 | average reward: 0.625\n",
      "Step 337/467 | Example step 0 | Pass 1 | loss: 0.006332 | average reward: 0.875\n",
      "Step 337/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 337/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 337/467 | Average reward: 0.30078125 | Average sequence length: 163.88\n",
      "Step 338/467 | Example step 0 | Pass 0 | loss: 0.008379 | average reward: 1.0\n",
      "Step 338/467 | Example step 0 | Pass 1 | loss: -0.011136 | average reward: 0.625\n",
      "Step 338/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 338/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 338/467 | Average reward: 0.26953125 | Average sequence length: 168.29\n",
      "Step 339/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 339/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 339/467 | Example step 1 | Pass 0 | loss: 0.005060 | average reward: 0.5\n",
      "Step 339/467 | Example step 1 | Pass 1 | loss: -0.006235 | average reward: 0.25\n",
      "Step 339/467 | Average reward: 0.234375 | Average sequence length: 157.75\n",
      "Step 340/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 340/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 340/467 | Example step 1 | Pass 0 | loss: -0.001415 | average reward: 0.0\n",
      "Step 340/467 | Example step 1 | Pass 1 | loss: 0.003247 | average reward: 0.125\n",
      "Step 340/467 | Average reward: 0.09375 | Average sequence length: 180.97\n",
      "Step 341/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 341/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 341/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 341/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 341/467 | Average reward: 0.24609375 | Average sequence length: 171.22\n",
      "Step 342/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 342/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 342/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 342/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 342/467 | Average reward: 0.1640625 | Average sequence length: 183.12\n",
      "Step 343/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 343/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 343/467 | Example step 1 | Pass 0 | loss: -0.024034 | average reward: 0.125\n",
      "Step 343/467 | Example step 1 | Pass 1 | loss: 0.001093 | average reward: 0.625\n",
      "Step 343/467 | Average reward: 0.16796875 | Average sequence length: 182.66\n",
      "Step 344/467 | Example step 0 | Pass 0 | loss: 0.005229 | average reward: 0.875\n",
      "Step 344/467 | Example step 0 | Pass 1 | loss: -0.005269 | average reward: 0.625\n",
      "Step 344/467 | Example step 1 | Pass 0 | loss: -0.007428 | average reward: 0.375\n",
      "Step 344/467 | Example step 1 | Pass 1 | loss: -0.001288 | average reward: 0.25\n",
      "Step 344/467 | Average reward: 0.21875 | Average sequence length: 169.65\n",
      "Step 345/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 345/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 345/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 345/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 345/467 | Average reward: 0.19921875 | Average sequence length: 172.03\n",
      "Step 346/467 | Example step 0 | Pass 0 | loss: -0.007590 | average reward: 0.125\n",
      "Step 346/467 | Example step 0 | Pass 1 | loss: 0.009518 | average reward: 0.375\n",
      "Step 346/467 | Example step 1 | Pass 0 | loss: 0.008636 | average reward: 0.75\n",
      "Step 346/467 | Example step 1 | Pass 1 | loss: -0.008250 | average reward: 0.25\n",
      "Step 346/467 | Average reward: 0.171875 | Average sequence length: 147.04\n",
      "Step 347/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 347/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 347/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 347/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 347/467 | Average reward: 0.08203125 | Average sequence length: 180.93\n",
      "Step 348/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 348/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 348/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 348/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 348/467 | Average reward: 0.3828125 | Average sequence length: 152.18\n",
      "Step 349/467 | Example step 0 | Pass 0 | loss: -0.000349 | average reward: 0.125\n",
      "Step 349/467 | Example step 0 | Pass 1 | loss: 0.005833 | average reward: 0.25\n",
      "Step 349/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 349/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 349/467 | Average reward: 0.15625 | Average sequence length: 170.35\n",
      "Step 350/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 350/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 350/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 350/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 350/467 | Average reward: 0.1484375 | Average sequence length: 180.06\n",
      "Step 351/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 351/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 351/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 351/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 351/467 | Average reward: 0.3046875 | Average sequence length: 165.93\n",
      "Step 352/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 352/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 352/467 | Example step 1 | Pass 0 | loss: 0.015870 | average reward: 0.5\n",
      "Step 352/467 | Example step 1 | Pass 1 | loss: -0.011552 | average reward: 0.0\n",
      "Step 352/467 | Average reward: 0.22265625 | Average sequence length: 173.78\n",
      "Step 353/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 353/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 353/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 353/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 353/467 | Average reward: 0.08203125 | Average sequence length: 176.14\n",
      "Step 354/467 | Example step 0 | Pass 0 | loss: -0.007965 | average reward: 0.875\n",
      "Step 354/467 | Example step 0 | Pass 1 | loss: 0.004304 | average reward: 1.0\n",
      "Step 354/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 354/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 354/467 | Average reward: 0.23828125 | Average sequence length: 168.73\n",
      "Step 355/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 355/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 355/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 355/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 355/467 | Average reward: 0.20703125 | Average sequence length: 163.84\n",
      "Step 356/467 | Example step 0 | Pass 0 | loss: 0.001473 | average reward: 0.125\n",
      "Step 356/467 | Example step 0 | Pass 1 | loss: -0.002719 | average reward: 0.0\n",
      "Step 356/467 | Example step 1 | Pass 0 | loss: -0.000313 | average reward: 0.125\n",
      "Step 356/467 | Example step 1 | Pass 1 | loss: 0.000079 | average reward: 0.125\n",
      "Step 356/467 | Average reward: 0.2109375 | Average sequence length: 172.48\n",
      "Step 357/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 357/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 357/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 357/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 357/467 | Average reward: 0.203125 | Average sequence length: 147.75\n",
      "Step 358/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 358/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 358/467 | Example step 1 | Pass 0 | loss: -0.002085 | average reward: 0.25\n",
      "Step 358/467 | Example step 1 | Pass 1 | loss: -0.005983 | average reward: 0.25\n",
      "Step 358/467 | Average reward: 0.09375 | Average sequence length: 179.69\n",
      "Step 359/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 359/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 359/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 359/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 359/467 | Average reward: 0.19140625 | Average sequence length: 153.88\n",
      "Step 360 | Pass@1: 0.1950, Pass@2: 0.2450, Pass@3: 0.2925, Pass@4: 0.3250, Pass@5: 0.3425, Pass@6: 0.3725, Pass@7: 0.3875, Pass@8: 0.3975\n",
      "Step 360/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 360/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 360/467 | Example step 1 | Pass 0 | loss: 0.003058 | average reward: 0.75\n",
      "Step 360/467 | Example step 1 | Pass 1 | loss: -0.000336 | average reward: 0.75\n",
      "Step 360/467 | Average reward: 0.2578125 | Average sequence length: 153.66\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000360.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000360.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 361/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 361/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 361/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 361/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 361/467 | Average reward: 0.20703125 | Average sequence length: 167.21\n",
      "Step 362/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 362/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 362/467 | Example step 1 | Pass 0 | loss: 0.005737 | average reward: 0.25\n",
      "Step 362/467 | Example step 1 | Pass 1 | loss: 0.002398 | average reward: 0.25\n",
      "Step 362/467 | Average reward: 0.1796875 | Average sequence length: 156.20\n",
      "Step 363/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 363/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 363/467 | Example step 1 | Pass 0 | loss: -0.002265 | average reward: 0.25\n",
      "Step 363/467 | Example step 1 | Pass 1 | loss: -0.001037 | average reward: 0.25\n",
      "Step 363/467 | Average reward: 0.19140625 | Average sequence length: 172.86\n",
      "Step 364/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 364/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 364/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 364/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 364/467 | Average reward: 0.2109375 | Average sequence length: 173.33\n",
      "Step 365/467 | Example step 0 | Pass 0 | loss: -0.014159 | average reward: 0.125\n",
      "Step 365/467 | Example step 0 | Pass 1 | loss: 0.007093 | average reward: 0.875\n",
      "Step 365/467 | Example step 1 | Pass 0 | loss: -0.008380 | average reward: 0.125\n",
      "Step 365/467 | Example step 1 | Pass 1 | loss: 0.004774 | average reward: 0.25\n",
      "Step 365/467 | Average reward: 0.30859375 | Average sequence length: 180.16\n",
      "Step 366/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 366/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 366/467 | Example step 1 | Pass 0 | loss: -0.001817 | average reward: 0.625\n",
      "Step 366/467 | Example step 1 | Pass 1 | loss: -0.003983 | average reward: 0.625\n",
      "Step 366/467 | Average reward: 0.19921875 | Average sequence length: 184.05\n",
      "Step 367/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 367/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 367/467 | Example step 1 | Pass 0 | loss: 0.003582 | average reward: 0.25\n",
      "Step 367/467 | Example step 1 | Pass 1 | loss: -0.004943 | average reward: 0.125\n",
      "Step 367/467 | Average reward: 0.22265625 | Average sequence length: 187.28\n",
      "Step 368/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 368/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 368/467 | Example step 1 | Pass 0 | loss: -0.002532 | average reward: 0.25\n",
      "Step 368/467 | Example step 1 | Pass 1 | loss: -0.001156 | average reward: 0.375\n",
      "Step 368/467 | Average reward: 0.11328125 | Average sequence length: 165.16\n",
      "Step 369/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 369/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 369/467 | Example step 1 | Pass 0 | loss: 0.004343 | average reward: 0.5\n",
      "Step 369/467 | Example step 1 | Pass 1 | loss: -0.006893 | average reward: 0.25\n",
      "Step 369/467 | Average reward: 0.34375 | Average sequence length: 164.08\n",
      "Step 370/467 | Example step 0 | Pass 0 | loss: 0.000442 | average reward: 0.125\n",
      "Step 370/467 | Example step 0 | Pass 1 | loss: -0.001439 | average reward: 0.125\n",
      "Step 370/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 370/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 370/467 | Average reward: 0.203125 | Average sequence length: 183.67\n",
      "Step 371/467 | Example step 0 | Pass 0 | loss: 0.000141 | average reward: 0.375\n",
      "Step 371/467 | Example step 0 | Pass 1 | loss: -0.007350 | average reward: 0.25\n",
      "Step 371/467 | Example step 1 | Pass 0 | loss: -0.003566 | average reward: 0.5\n",
      "Step 371/467 | Example step 1 | Pass 1 | loss: 0.005152 | average reward: 0.625\n",
      "Step 371/467 | Average reward: 0.2109375 | Average sequence length: 171.82\n",
      "Step 372/467 | Example step 0 | Pass 0 | loss: -0.003075 | average reward: 0.0\n",
      "Step 372/467 | Example step 0 | Pass 1 | loss: 0.003297 | average reward: 0.125\n",
      "Step 372/467 | Example step 1 | Pass 0 | loss: -0.002934 | average reward: 0.875\n",
      "Step 372/467 | Example step 1 | Pass 1 | loss: -0.009786 | average reward: 0.75\n",
      "Step 372/467 | Average reward: 0.18359375 | Average sequence length: 169.92\n",
      "Step 373/467 | Example step 0 | Pass 0 | loss: -0.002271 | average reward: 0.875\n",
      "Step 373/467 | Example step 0 | Pass 1 | loss: 0.001886 | average reward: 1.0\n",
      "Step 373/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 373/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 373/467 | Average reward: 0.18359375 | Average sequence length: 167.50\n",
      "Step 374/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 374/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 374/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 374/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 374/467 | Average reward: 0.19140625 | Average sequence length: 162.77\n",
      "Step 375/467 | Example step 0 | Pass 0 | loss: -0.006326 | average reward: 0.375\n",
      "Step 375/467 | Example step 0 | Pass 1 | loss: -0.000244 | average reward: 0.375\n",
      "Step 375/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 375/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 375/467 | Average reward: 0.2421875 | Average sequence length: 164.09\n",
      "Step 376/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 376/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 376/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 376/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 376/467 | Average reward: 0.1875 | Average sequence length: 152.77\n",
      "Step 377/467 | Example step 0 | Pass 0 | loss: -0.004729 | average reward: 0.375\n",
      "Step 377/467 | Example step 0 | Pass 1 | loss: 0.001793 | average reward: 0.5\n",
      "Step 377/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 377/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 377/467 | Average reward: 0.1953125 | Average sequence length: 174.61\n",
      "Step 378/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 378/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 378/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 378/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 378/467 | Average reward: 0.15625 | Average sequence length: 175.73\n",
      "Step 379/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 379/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 379/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 379/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 379/467 | Average reward: 0.125 | Average sequence length: 181.14\n",
      "Step 380/467 | Example step 0 | Pass 0 | loss: 0.000606 | average reward: 0.875\n",
      "Step 380/467 | Example step 0 | Pass 1 | loss: -0.008155 | average reward: 0.75\n",
      "Step 380/467 | Example step 1 | Pass 0 | loss: -0.001996 | average reward: 0.75\n",
      "Step 380/467 | Example step 1 | Pass 1 | loss: -0.008731 | average reward: 0.625\n",
      "Step 380/467 | Average reward: 0.37890625 | Average sequence length: 157.20\n",
      "Step 381/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 381/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 381/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 381/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 381/467 | Average reward: 0.23046875 | Average sequence length: 150.82\n",
      "Step 382/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 382/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 382/467 | Example step 1 | Pass 0 | loss: -0.014062 | average reward: 0.25\n",
      "Step 382/467 | Example step 1 | Pass 1 | loss: 0.008890 | average reward: 0.75\n",
      "Step 382/467 | Average reward: 0.06640625 | Average sequence length: 186.10\n",
      "Step 383/467 | Example step 0 | Pass 0 | loss: -0.007893 | average reward: 0.375\n",
      "Step 383/467 | Example step 0 | Pass 1 | loss: 0.004262 | average reward: 0.625\n",
      "Step 383/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 383/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 383/467 | Average reward: 0.1640625 | Average sequence length: 166.69\n",
      "Step 384/467 | Example step 0 | Pass 0 | loss: 0.000090 | average reward: 0.125\n",
      "Step 384/467 | Example step 0 | Pass 1 | loss: -0.004929 | average reward: 0.0\n",
      "Step 384/467 | Example step 1 | Pass 0 | loss: -0.001179 | average reward: 0.875\n",
      "Step 384/467 | Example step 1 | Pass 1 | loss: 0.000240 | average reward: 1.0\n",
      "Step 384/467 | Average reward: 0.3203125 | Average sequence length: 169.04\n",
      "Step 385/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 385/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 385/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 385/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 385/467 | Average reward: 0.20703125 | Average sequence length: 193.56\n",
      "Step 386/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 386/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 386/467 | Example step 1 | Pass 0 | loss: -0.009723 | average reward: 0.5\n",
      "Step 386/467 | Example step 1 | Pass 1 | loss: 0.000439 | average reward: 0.75\n",
      "Step 386/467 | Average reward: 0.1796875 | Average sequence length: 201.95\n",
      "Step 387/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 387/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 387/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 387/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 387/467 | Average reward: 0.2734375 | Average sequence length: 173.02\n",
      "Step 388/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 388/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 388/467 | Example step 1 | Pass 0 | loss: 0.002738 | average reward: 0.125\n",
      "Step 388/467 | Example step 1 | Pass 1 | loss: -0.002231 | average reward: 0.0\n",
      "Step 388/467 | Average reward: 0.3515625 | Average sequence length: 176.85\n",
      "Step 389/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 389/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 389/467 | Example step 1 | Pass 0 | loss: 0.006199 | average reward: 0.25\n",
      "Step 389/467 | Example step 1 | Pass 1 | loss: 0.002701 | average reward: 0.25\n",
      "Step 389/467 | Average reward: 0.25 | Average sequence length: 149.96\n",
      "Step 390/467 | Example step 0 | Pass 0 | loss: 0.002415 | average reward: 1.0\n",
      "Step 390/467 | Example step 0 | Pass 1 | loss: -0.004718 | average reward: 0.875\n",
      "Step 390/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 390/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 390/467 | Average reward: 0.1640625 | Average sequence length: 163.47\n",
      "Step 391/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 391/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 391/467 | Example step 1 | Pass 0 | loss: 0.008375 | average reward: 0.625\n",
      "Step 391/467 | Example step 1 | Pass 1 | loss: -0.019574 | average reward: 0.25\n",
      "Step 391/467 | Average reward: 0.125 | Average sequence length: 171.25\n",
      "Step 392/467 | Example step 0 | Pass 0 | loss: 0.002880 | average reward: 0.25\n",
      "Step 392/467 | Example step 0 | Pass 1 | loss: -0.002244 | average reward: 0.125\n",
      "Step 392/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 392/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 392/467 | Average reward: 0.234375 | Average sequence length: 195.22\n",
      "Step 393/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 393/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 393/467 | Example step 1 | Pass 0 | loss: 0.004067 | average reward: 1.0\n",
      "Step 393/467 | Example step 1 | Pass 1 | loss: -0.012349 | average reward: 0.75\n",
      "Step 393/467 | Average reward: 0.1875 | Average sequence length: 200.11\n",
      "Step 394/467 | Example step 0 | Pass 0 | loss: 0.002341 | average reward: 0.125\n",
      "Step 394/467 | Example step 0 | Pass 1 | loss: -0.002419 | average reward: 0.0\n",
      "Step 394/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 394/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 394/467 | Average reward: 0.08984375 | Average sequence length: 189.20\n",
      "Step 395/467 | Example step 0 | Pass 0 | loss: -0.001665 | average reward: 0.75\n",
      "Step 395/467 | Example step 0 | Pass 1 | loss: 0.003618 | average reward: 0.625\n",
      "Step 395/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 395/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 395/467 | Average reward: 0.18359375 | Average sequence length: 206.77\n",
      "Step 396/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 396/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 396/467 | Example step 1 | Pass 0 | loss: 0.002650 | average reward: 1.0\n",
      "Step 396/467 | Example step 1 | Pass 1 | loss: -0.002845 | average reward: 0.875\n",
      "Step 396/467 | Average reward: 0.23046875 | Average sequence length: 163.15\n",
      "Step 397/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 397/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 397/467 | Example step 1 | Pass 0 | loss: 0.004605 | average reward: 0.75\n",
      "Step 397/467 | Example step 1 | Pass 1 | loss: -0.004986 | average reward: 0.75\n",
      "Step 397/467 | Average reward: 0.19140625 | Average sequence length: 158.80\n",
      "Step 398/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 398/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 398/467 | Example step 1 | Pass 0 | loss: 0.000870 | average reward: 0.125\n",
      "Step 398/467 | Example step 1 | Pass 1 | loss: -0.001690 | average reward: 0.0\n",
      "Step 398/467 | Average reward: 0.32421875 | Average sequence length: 170.81\n",
      "Step 399/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 399/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 399/467 | Example step 1 | Pass 0 | loss: -0.014375 | average reward: 0.125\n",
      "Step 399/467 | Example step 1 | Pass 1 | loss: 0.016526 | average reward: 0.5\n",
      "Step 399/467 | Average reward: 0.203125 | Average sequence length: 183.87\n",
      "Step 400/467 | Example step 0 | Pass 0 | loss: -0.001031 | average reward: 0.75\n",
      "Step 400/467 | Example step 0 | Pass 1 | loss: -0.005802 | average reward: 0.625\n",
      "Step 400/467 | Example step 1 | Pass 0 | loss: 0.007649 | average reward: 0.125\n",
      "Step 400/467 | Example step 1 | Pass 1 | loss: -0.005292 | average reward: 0.0\n",
      "Step 400/467 | Average reward: 0.22265625 | Average sequence length: 163.30\n",
      "Step 401/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 401/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 401/467 | Example step 1 | Pass 0 | loss: 0.007368 | average reward: 0.625\n",
      "Step 401/467 | Example step 1 | Pass 1 | loss: -0.014954 | average reward: 0.125\n",
      "Step 401/467 | Average reward: 0.35546875 | Average sequence length: 149.61\n",
      "Step 402/467 | Example step 0 | Pass 0 | loss: 0.005281 | average reward: 0.5\n",
      "Step 402/467 | Example step 0 | Pass 1 | loss: -0.009704 | average reward: 0.125\n",
      "Step 402/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 402/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 402/467 | Average reward: 0.296875 | Average sequence length: 159.20\n",
      "Step 403/467 | Example step 0 | Pass 0 | loss: 0.008983 | average reward: 0.125\n",
      "Step 403/467 | Example step 0 | Pass 1 | loss: -0.002709 | average reward: 0.0\n",
      "Step 403/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 403/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 403/467 | Average reward: 0.109375 | Average sequence length: 197.66\n",
      "Step 404/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 404/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 404/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 404/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 404/467 | Average reward: 0.109375 | Average sequence length: 169.88\n",
      "Step 405/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 405/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 405/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 405/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 405/467 | Average reward: 0.203125 | Average sequence length: 159.82\n",
      "Step 406/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 406/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 406/467 | Example step 1 | Pass 0 | loss: 0.001095 | average reward: 0.5\n",
      "Step 406/467 | Example step 1 | Pass 1 | loss: 0.006813 | average reward: 0.625\n",
      "Step 406/467 | Average reward: 0.2734375 | Average sequence length: 188.83\n",
      "Step 407/467 | Example step 0 | Pass 0 | loss: 0.003601 | average reward: 1.0\n",
      "Step 407/467 | Example step 0 | Pass 1 | loss: -0.004547 | average reward: 0.75\n",
      "Step 407/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 407/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 407/467 | Average reward: 0.14453125 | Average sequence length: 177.67\n",
      "Step 408/467 | Example step 0 | Pass 0 | loss: 0.003925 | average reward: 0.125\n",
      "Step 408/467 | Example step 0 | Pass 1 | loss: -0.002942 | average reward: 0.0\n",
      "Step 408/467 | Example step 1 | Pass 0 | loss: 0.003771 | average reward: 0.875\n",
      "Step 408/467 | Example step 1 | Pass 1 | loss: -0.007711 | average reward: 0.75\n",
      "Step 408/467 | Average reward: 0.15625 | Average sequence length: 159.72\n",
      "Step 409/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 409/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 409/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 409/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 409/467 | Average reward: 0.22265625 | Average sequence length: 169.51\n",
      "Step 410/467 | Example step 0 | Pass 0 | loss: -0.004255 | average reward: 0.125\n",
      "Step 410/467 | Example step 0 | Pass 1 | loss: 0.006975 | average reward: 0.375\n",
      "Step 410/467 | Example step 1 | Pass 0 | loss: 0.001770 | average reward: 0.125\n",
      "Step 410/467 | Example step 1 | Pass 1 | loss: -0.001808 | average reward: 0.0\n",
      "Step 410/467 | Average reward: 0.046875 | Average sequence length: 183.17\n",
      "Step 411/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 411/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 411/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 411/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 411/467 | Average reward: 0.18359375 | Average sequence length: 178.15\n",
      "Step 412/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 412/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 412/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 412/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 412/467 | Average reward: 0.2890625 | Average sequence length: 165.16\n",
      "Step 413/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 413/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 413/467 | Example step 1 | Pass 0 | loss: -0.003647 | average reward: 0.5\n",
      "Step 413/467 | Example step 1 | Pass 1 | loss: 0.001660 | average reward: 0.625\n",
      "Step 413/467 | Average reward: 0.30078125 | Average sequence length: 169.70\n",
      "Step 414/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 414/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 414/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 414/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 414/467 | Average reward: 0.13671875 | Average sequence length: 150.34\n",
      "Step 415/467 | Example step 0 | Pass 0 | loss: 0.007010 | average reward: 0.25\n",
      "Step 415/467 | Example step 0 | Pass 1 | loss: -0.008048 | average reward: 0.0\n",
      "Step 415/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 415/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 415/467 | Average reward: 0.12109375 | Average sequence length: 152.02\n",
      "Step 416/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 416/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 416/467 | Example step 1 | Pass 0 | loss: -0.000724 | average reward: 0.125\n",
      "Step 416/467 | Example step 1 | Pass 1 | loss: -0.002138 | average reward: 0.125\n",
      "Step 416/467 | Average reward: 0.23828125 | Average sequence length: 181.75\n",
      "Step 417/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 417/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 417/467 | Example step 1 | Pass 0 | loss: -0.001721 | average reward: 0.0\n",
      "Step 417/467 | Example step 1 | Pass 1 | loss: 0.003652 | average reward: 0.125\n",
      "Step 417/467 | Average reward: 0.03515625 | Average sequence length: 175.18\n",
      "Step 418/467 | Example step 0 | Pass 0 | loss: 0.006912 | average reward: 0.625\n",
      "Step 418/467 | Example step 0 | Pass 1 | loss: -0.003246 | average reward: 0.375\n",
      "Step 418/467 | Example step 1 | Pass 0 | loss: -0.000115 | average reward: 0.125\n",
      "Step 418/467 | Example step 1 | Pass 1 | loss: -0.003510 | average reward: 0.0\n",
      "Step 418/467 | Average reward: 0.09765625 | Average sequence length: 181.02\n",
      "Step 419/467 | Example step 0 | Pass 0 | loss: -0.002105 | average reward: 0.375\n",
      "Step 419/467 | Example step 0 | Pass 1 | loss: -0.004726 | average reward: 0.375\n",
      "Step 419/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 419/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 419/467 | Average reward: 0.09375 | Average sequence length: 190.57\n",
      "Step 420 | Pass@1: 0.1750, Pass@2: 0.2400, Pass@3: 0.2775, Pass@4: 0.3075, Pass@5: 0.3300, Pass@6: 0.3450, Pass@7: 0.3650, Pass@8: 0.3900\n",
      "Step 420/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 420/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 420/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 420/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 420/467 | Average reward: 0.125 | Average sequence length: 171.32\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000420.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000420.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "Step 421/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 421/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 421/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 421/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 421/467 | Average reward: 0.1328125 | Average sequence length: 172.23\n",
      "Step 422/467 | Example step 0 | Pass 0 | loss: -0.006707 | average reward: 0.0\n",
      "Step 422/467 | Example step 0 | Pass 1 | loss: 0.008019 | average reward: 0.25\n",
      "Step 422/467 | Example step 1 | Pass 0 | loss: -0.004530 | average reward: 0.5\n",
      "Step 422/467 | Example step 1 | Pass 1 | loss: -0.002791 | average reward: 0.625\n",
      "Step 422/467 | Average reward: 0.18359375 | Average sequence length: 179.39\n",
      "Step 423/467 | Example step 0 | Pass 0 | loss: 0.003205 | average reward: 0.125\n",
      "Step 423/467 | Example step 0 | Pass 1 | loss: -0.002227 | average reward: 0.0\n",
      "Step 423/467 | Example step 1 | Pass 0 | loss: -0.009602 | average reward: 0.125\n",
      "Step 423/467 | Example step 1 | Pass 1 | loss: 0.005421 | average reward: 0.25\n",
      "Step 423/467 | Average reward: 0.140625 | Average sequence length: 183.62\n",
      "Step 424/467 | Example step 0 | Pass 0 | loss: -0.007185 | average reward: 0.625\n",
      "Step 424/467 | Example step 0 | Pass 1 | loss: 0.003213 | average reward: 0.75\n",
      "Step 424/467 | Example step 1 | Pass 0 | loss: 0.003626 | average reward: 0.125\n",
      "Step 424/467 | Example step 1 | Pass 1 | loss: -0.004104 | average reward: 0.0\n",
      "Step 424/467 | Average reward: 0.14453125 | Average sequence length: 176.62\n",
      "Step 425/467 | Example step 0 | Pass 0 | loss: -0.002433 | average reward: 0.5\n",
      "Step 425/467 | Example step 0 | Pass 1 | loss: 0.002096 | average reward: 0.5\n",
      "Step 425/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 425/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 425/467 | Average reward: 0.28515625 | Average sequence length: 172.69\n",
      "Step 426/467 | Example step 0 | Pass 0 | loss: -0.002360 | average reward: 0.0\n",
      "Step 426/467 | Example step 0 | Pass 1 | loss: 0.002066 | average reward: 0.125\n",
      "Step 426/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 426/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 426/467 | Average reward: 0.2421875 | Average sequence length: 159.31\n",
      "Step 427/467 | Example step 0 | Pass 0 | loss: 0.007196 | average reward: 0.125\n",
      "Step 427/467 | Example step 0 | Pass 1 | loss: -0.001542 | average reward: 0.0\n",
      "Step 427/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 427/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 427/467 | Average reward: 0.07421875 | Average sequence length: 187.26\n",
      "Step 428/467 | Example step 0 | Pass 0 | loss: -0.001234 | average reward: 0.125\n",
      "Step 428/467 | Example step 0 | Pass 1 | loss: 0.002479 | average reward: 0.125\n",
      "Step 428/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 428/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 428/467 | Average reward: 0.19140625 | Average sequence length: 159.66\n",
      "Step 429/467 | Example step 0 | Pass 0 | loss: -0.000917 | average reward: 0.0\n",
      "Step 429/467 | Example step 0 | Pass 1 | loss: 0.003279 | average reward: 0.125\n",
      "Step 429/467 | Example step 1 | Pass 0 | loss: -0.001081 | average reward: 0.375\n",
      "Step 429/467 | Example step 1 | Pass 1 | loss: -0.006535 | average reward: 0.25\n",
      "Step 429/467 | Average reward: 0.16015625 | Average sequence length: 146.61\n",
      "Step 430/467 | Example step 0 | Pass 0 | loss: -0.002724 | average reward: 0.0\n",
      "Step 430/467 | Example step 0 | Pass 1 | loss: 0.004236 | average reward: 0.125\n",
      "Step 430/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 430/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 430/467 | Average reward: 0.1171875 | Average sequence length: 165.86\n",
      "Step 431/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 431/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 431/467 | Example step 1 | Pass 0 | loss: -0.009513 | average reward: 0.375\n",
      "Step 431/467 | Example step 1 | Pass 1 | loss: -0.000013 | average reward: 0.5\n",
      "Step 431/467 | Average reward: 0.15234375 | Average sequence length: 164.92\n",
      "Step 432/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 432/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 432/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 432/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 432/467 | Average reward: 0.30078125 | Average sequence length: 180.53\n",
      "Step 433/467 | Example step 0 | Pass 0 | loss: -0.002841 | average reward: 0.875\n",
      "Step 433/467 | Example step 0 | Pass 1 | loss: 0.001669 | average reward: 1.0\n",
      "Step 433/467 | Example step 1 | Pass 0 | loss: -0.014079 | average reward: 0.875\n",
      "Step 433/467 | Example step 1 | Pass 1 | loss: 0.001168 | average reward: 1.0\n",
      "Step 433/467 | Average reward: 0.2734375 | Average sequence length: 180.79\n",
      "Step 434/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 434/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 434/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 434/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 434/467 | Average reward: 0.17578125 | Average sequence length: 173.06\n",
      "Step 435/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 435/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 435/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 435/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 435/467 | Average reward: 0.12109375 | Average sequence length: 206.26\n",
      "Step 436/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 436/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 436/467 | Example step 1 | Pass 0 | loss: -0.000985 | average reward: 0.875\n",
      "Step 436/467 | Example step 1 | Pass 1 | loss: -0.001817 | average reward: 0.875\n",
      "Step 436/467 | Average reward: 0.2421875 | Average sequence length: 178.59\n",
      "Step 437/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 437/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 437/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 437/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 437/467 | Average reward: 0.109375 | Average sequence length: 171.75\n",
      "Step 438/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 438/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 438/467 | Example step 1 | Pass 0 | loss: 0.006205 | average reward: 0.5\n",
      "Step 438/467 | Example step 1 | Pass 1 | loss: 0.000416 | average reward: 0.5\n",
      "Step 438/467 | Average reward: 0.33203125 | Average sequence length: 153.79\n",
      "Step 439/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 439/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 439/467 | Example step 1 | Pass 0 | loss: -0.013400 | average reward: 0.125\n",
      "Step 439/467 | Example step 1 | Pass 1 | loss: 0.006572 | average reward: 0.625\n",
      "Step 439/467 | Average reward: 0.23828125 | Average sequence length: 179.92\n",
      "Step 440/467 | Example step 0 | Pass 0 | loss: 0.010940 | average reward: 0.625\n",
      "Step 440/467 | Example step 0 | Pass 1 | loss: -0.012884 | average reward: 0.125\n",
      "Step 440/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 440/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 440/467 | Average reward: 0.2890625 | Average sequence length: 156.32\n",
      "Step 441/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 441/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 441/467 | Example step 1 | Pass 0 | loss: 0.002586 | average reward: 0.25\n",
      "Step 441/467 | Example step 1 | Pass 1 | loss: -0.005961 | average reward: 0.125\n",
      "Step 441/467 | Average reward: 0.21875 | Average sequence length: 169.93\n",
      "Step 442/467 | Example step 0 | Pass 0 | loss: -0.001706 | average reward: 0.0\n",
      "Step 442/467 | Example step 0 | Pass 1 | loss: 0.000492 | average reward: 0.125\n",
      "Step 442/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 442/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 442/467 | Average reward: 0.140625 | Average sequence length: 164.00\n",
      "Step 443/467 | Example step 0 | Pass 0 | loss: 0.000921 | average reward: 0.375\n",
      "Step 443/467 | Example step 0 | Pass 1 | loss: -0.013472 | average reward: 0.125\n",
      "Step 443/467 | Example step 1 | Pass 0 | loss: -0.005337 | average reward: 0.125\n",
      "Step 443/467 | Example step 1 | Pass 1 | loss: 0.001802 | average reward: 0.25\n",
      "Step 443/467 | Average reward: 0.265625 | Average sequence length: 149.97\n",
      "Step 444/467 | Example step 0 | Pass 0 | loss: 0.001363 | average reward: 0.125\n",
      "Step 444/467 | Example step 0 | Pass 1 | loss: -0.003475 | average reward: 0.0\n",
      "Step 444/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 444/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 444/467 | Average reward: 0.2265625 | Average sequence length: 150.80\n",
      "Step 445/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 1.0\n",
      "Step 445/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 1.0\n",
      "Step 445/467 | Example step 1 | Pass 0 | loss: 0.001882 | average reward: 0.875\n",
      "Step 445/467 | Example step 1 | Pass 1 | loss: -0.002703 | average reward: 0.5\n",
      "Step 445/467 | Average reward: 0.26953125 | Average sequence length: 181.96\n",
      "Step 446/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 446/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 446/467 | Example step 1 | Pass 0 | loss: -0.002431 | average reward: 0.25\n",
      "Step 446/467 | Example step 1 | Pass 1 | loss: 0.002140 | average reward: 0.5\n",
      "Step 446/467 | Average reward: 0.1796875 | Average sequence length: 171.54\n",
      "Step 447/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 447/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 447/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 447/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 447/467 | Average reward: 0.37890625 | Average sequence length: 173.66\n",
      "Step 448/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 448/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 448/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 448/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 448/467 | Average reward: 0.17578125 | Average sequence length: 157.50\n",
      "Step 449/467 | Example step 0 | Pass 0 | loss: 0.005562 | average reward: 0.375\n",
      "Step 449/467 | Example step 0 | Pass 1 | loss: -0.007522 | average reward: 0.125\n",
      "Step 449/467 | Example step 1 | Pass 0 | loss: -0.014137 | average reward: 0.125\n",
      "Step 449/467 | Example step 1 | Pass 1 | loss: 0.011107 | average reward: 0.5\n",
      "Step 449/467 | Average reward: 0.21484375 | Average sequence length: 162.49\n",
      "Step 450/467 | Example step 0 | Pass 0 | loss: 0.000135 | average reward: 0.875\n",
      "Step 450/467 | Example step 0 | Pass 1 | loss: -0.002390 | average reward: 0.75\n",
      "Step 450/467 | Example step 1 | Pass 0 | loss: 0.001432 | average reward: 0.125\n",
      "Step 450/467 | Example step 1 | Pass 1 | loss: -0.000602 | average reward: 0.125\n",
      "Step 450/467 | Average reward: 0.21484375 | Average sequence length: 173.75\n",
      "Step 451/467 | Example step 0 | Pass 0 | loss: -0.001901 | average reward: 0.0\n",
      "Step 451/467 | Example step 0 | Pass 1 | loss: 0.005369 | average reward: 0.125\n",
      "Step 451/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 451/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 451/467 | Average reward: 0.0703125 | Average sequence length: 164.29\n",
      "Step 452/467 | Example step 0 | Pass 0 | loss: -0.000793 | average reward: 0.125\n",
      "Step 452/467 | Example step 0 | Pass 1 | loss: 0.004035 | average reward: 0.125\n",
      "Step 452/467 | Example step 1 | Pass 0 | loss: 0.003567 | average reward: 0.375\n",
      "Step 452/467 | Example step 1 | Pass 1 | loss: 0.000222 | average reward: 0.25\n",
      "Step 452/467 | Average reward: 0.140625 | Average sequence length: 194.00\n",
      "Step 453/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 453/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 453/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 453/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 453/467 | Average reward: 0.2578125 | Average sequence length: 168.35\n",
      "Step 454/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 454/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 454/467 | Example step 1 | Pass 0 | loss: 0.000791 | average reward: 0.125\n",
      "Step 454/467 | Example step 1 | Pass 1 | loss: 0.000667 | average reward: 0.125\n",
      "Step 454/467 | Average reward: 0.16015625 | Average sequence length: 162.51\n",
      "Step 455/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 455/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 455/467 | Example step 1 | Pass 0 | loss: -0.002654 | average reward: 0.875\n",
      "Step 455/467 | Example step 1 | Pass 1 | loss: 0.001578 | average reward: 1.0\n",
      "Step 455/467 | Average reward: 0.19140625 | Average sequence length: 170.09\n",
      "Step 456/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 456/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 456/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 456/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 456/467 | Average reward: 0.15234375 | Average sequence length: 178.91\n",
      "Step 457/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 457/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 457/467 | Example step 1 | Pass 0 | loss: -0.005344 | average reward: 0.875\n",
      "Step 457/467 | Example step 1 | Pass 1 | loss: -0.000581 | average reward: 0.75\n",
      "Step 457/467 | Average reward: 0.328125 | Average sequence length: 145.87\n",
      "Step 458/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 458/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 458/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 458/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 458/467 | Average reward: 0.05859375 | Average sequence length: 176.52\n",
      "Step 459/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 459/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 459/467 | Example step 1 | Pass 0 | loss: -0.007436 | average reward: 0.125\n",
      "Step 459/467 | Example step 1 | Pass 1 | loss: 0.005661 | average reward: 0.375\n",
      "Step 459/467 | Average reward: 0.29296875 | Average sequence length: 175.09\n",
      "Step 460/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 460/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 460/467 | Example step 1 | Pass 0 | loss: 0.000664 | average reward: 0.125\n",
      "Step 460/467 | Example step 1 | Pass 1 | loss: 0.002931 | average reward: 0.25\n",
      "Step 460/467 | Average reward: 0.15234375 | Average sequence length: 166.28\n",
      "Step 461/467 | Example step 0 | Pass 0 | loss: 0.013777 | average reward: 0.625\n",
      "Step 461/467 | Example step 0 | Pass 1 | loss: -0.020672 | average reward: 0.125\n",
      "Step 461/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 461/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 461/467 | Average reward: 0.13671875 | Average sequence length: 177.62\n",
      "Step 462/467 | Example step 0 | Pass 0 | loss: 0.010474 | average reward: 0.875\n",
      "Step 462/467 | Example step 0 | Pass 1 | loss: -0.007829 | average reward: 0.375\n",
      "Step 462/467 | Example step 1 | Pass 0 | loss: -0.002894 | average reward: 0.0\n",
      "Step 462/467 | Example step 1 | Pass 1 | loss: 0.001181 | average reward: 0.125\n",
      "Step 462/467 | Average reward: 0.30078125 | Average sequence length: 160.14\n",
      "Step 463/467 | Example step 0 | Pass 0 | loss: -0.008802 | average reward: 0.375\n",
      "Step 463/467 | Example step 0 | Pass 1 | loss: 0.002601 | average reward: 0.5\n",
      "Step 463/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 463/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 463/467 | Average reward: 0.25 | Average sequence length: 167.80\n",
      "Step 464/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 464/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 464/467 | Example step 1 | Pass 0 | loss: 0.000260 | average reward: 0.125\n",
      "Step 464/467 | Example step 1 | Pass 1 | loss: -0.003867 | average reward: 0.0\n",
      "Step 464/467 | Average reward: 0.1953125 | Average sequence length: 176.34\n",
      "Step 465/467 | Example step 0 | Pass 0 | loss: -0.009297 | average reward: 0.5\n",
      "Step 465/467 | Example step 0 | Pass 1 | loss: -0.003437 | average reward: 0.75\n",
      "Step 465/467 | Example step 1 | Pass 0 | loss: -0.003896 | average reward: 0.875\n",
      "Step 465/467 | Example step 1 | Pass 1 | loss: 0.002617 | average reward: 1.0\n",
      "Step 465/467 | Average reward: 0.203125 | Average sequence length: 168.23\n",
      "Step 466/467 | Example step 0 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 466/467 | Example step 0 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 466/467 | Example step 1 | Pass 0 | loss: -0.000000 | average reward: 0.0\n",
      "Step 466/467 | Example step 1 | Pass 1 | loss: -0.000000 | average reward: 0.0\n",
      "Step 466/467 | Average reward: 0.36328125 | Average sequence length: 174.32\n",
      "saved model to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/model_000466.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32/meta_000466.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat2/chatrl_checkpoints/d32\n",
      "wandb: updating run metadata\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:    lrm ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb: pass@1 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñá\n",
      "wandb: pass@2 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "wandb: pass@3 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá\n",
      "wandb: pass@4 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá\n",
      "wandb: pass@5 ‚ñÅ‚ñÑ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá\n",
      "wandb: pass@6 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá\n",
      "wandb: pass@7 ‚ñÅ‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá\n",
      "wandb: pass@8 ‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà\n",
      "wandb: reward ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÉ\n",
      "wandb:     +2 ...\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:    lrm 0.00214\n",
      "wandb: pass@1 0.175\n",
      "wandb: pass@2 0.24\n",
      "wandb: pass@3 0.2775\n",
      "wandb: pass@4 0.3075\n",
      "wandb: pass@5 0.33\n",
      "wandb: pass@6 0.345\n",
      "wandb: pass@7 0.365\n",
      "wandb: pass@8 0.39\n",
      "wandb: reward 0.36328\n",
      "wandb:     +2 ...\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-8 at: https://wandb.ai/ericsilberstein-self/my-nanochat-rl/runs/1dhrcxg0\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat-rl\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251231_150432-1dhrcxg0/logs\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/rl_train_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22de44b4-671e-41d0-b6d6-ecea25f626ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/chatrl_checkpoints/d32 with step 360\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2251/2251 [00:00<00:00, 361116.02 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2376/2376 [00:00<00:00, 542733.16 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 253580.11 examples/s]\n",
      "final: 1571/2376 (66.12%)\n",
      "ARC-Easy accuracy: 66.12%\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1119/1119 [00:00<00:00, 220069.69 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [00:00<00:00, 321373.19 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 140735.82 examples/s]\n",
      "final: 557/1172 (47.53%)\n",
      "ARC-Challenge accuracy: 47.53%\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14042/14042 [00:00<00:00, 596703.41 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1531/1531 [00:00<00:00, 371183.78 examples/s]\n",
      "Generating dev split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [00:00<00:00, 146062.64 examples/s]\n",
      "Generating auxiliary_train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99842/99842 [00:00<00:00, 392409.55 examples/s]\n",
      "final: 5546/14042 (39.50%)\n",
      "MMLU accuracy: 39.50%\n",
      "\u001b[KRank 7 | 33/164 (20.12%)]\n",
      "\u001b[KRank 0 | 27/165 (16.36%)]\n",
      "\u001b[KRank 1 | 44/165 (26.67%)]\n",
      "\u001b[KRank 2 | 27/165 (16.36%)]\n",
      "\u001b[KRank 6 | 25/165 (15.15%)]\n",
      "\u001b[KRank 5 | 28/165 (16.97%)]\n",
      "\u001b[KRank 4 | 26/165 (15.76%)]\n",
      "\u001b[KRank 3 | 44/165 (26.67%)]\n",
      "==================================================\n",
      "final: 254/1319 (19.26%)\n",
      "GSM8K accuracy: 19.26%\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:00<00:00, 32500.16 examples/s]\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 1 | 3/21 (14.29%)]\n",
      "\u001b[KRank 7 | 1/20 (5.00%)]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 3 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 2/20 (10.00%)]\n",
      "==================================================\n",
      "final: 15/164 (9.15%)\n",
      "HumanEval accuracy: 9.15%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 29/32 (90.62%)]\n",
      "\u001b[KRank 0 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 5 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 31/32 (96.88%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/run_outputs/rl_chat_eval_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d5072e-6bf0-4fa5-a94b-403186793e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chat evaluation rl\n",
      "timestamp: 2025-12-31 17:00:49\n",
      "\n",
      "- source: rl\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d32\n",
      "- step: 360\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.6612\n",
      "- ARC-Challenge: 0.4753\n",
      "- MMLU: 0.3950\n",
      "- GSM8K: 0.1926\n",
      "- HumanEval: 0.0915\n",
      "- SpellingBee: 0.9688\n",
      "- ChatCORE metric: 0.3824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../files_copied_from_servers/challenge-38-train-d32/report/chat-evaluation-rl.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a4190-1f2d-4b77-9657-c866e79a3279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
