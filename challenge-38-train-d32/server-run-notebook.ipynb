{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4fb844-de00-4459-b225-d7fc63738640",
   "metadata": {},
   "source": [
    "## Server run notebook\n",
    "\n",
    "This is not the main notebook. See `instructions.ipynb`.\n",
    "\n",
    "I'll use this notebook to record and check things along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ade605-0222-491e-96b5-c56a8db8b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 22 15:45:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51fd9e-c493-4958-908c-d8c528219b7b",
   "metadata": {},
   "source": [
    "## hit this problem\n",
    "\n",
    "Doing the first test run. Looks like https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip is no longer accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a958c6-b861-4034-b44b-c16818dae4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank2]:     result = func(*args)\n",
      "[rank2]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank2]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank2]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank7]: Traceback (most recent call last):\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank7]:     return _run_code(code, main_globals, None,\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank7]:     exec(code, run_globals)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank7]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank7]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank7]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank7]:     with urllib.request.urlopen(url) as response:\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank7]:     return opener.open(url, data, timeout)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank7]:     response = meth(req, response)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank7]:     response = self.parent.error(\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank7]:     return self._call_chain(*args)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank7]:     result = func(*args)\n",
      "[rank7]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank7]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank7]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank5]: Traceback (most recent call last):\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank5]:     return _run_code(code, main_globals, None,\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank5]:     exec(code, run_globals)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank5]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank5]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank5]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank5]:     with urllib.request.urlopen(url) as response:\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank5]:     return opener.open(url, data, timeout)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank5]:     response = meth(req, response)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank5]:     response = self.parent.error(\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank5]:     return self._call_chain(*args)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank5]:     result = func(*args)\n",
      "[rank5]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank5]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank5]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank6]: Traceback (most recent call last):\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank6]:     return _run_code(code, main_globals, None,\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank6]:     exec(code, run_globals)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank6]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank6]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank6]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank6]:     with urllib.request.urlopen(url) as response:\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank6]:     return opener.open(url, data, timeout)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank6]:     response = meth(req, response)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank6]:     response = self.parent.error(\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank6]:     return self._call_chain(*args)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank6]:     result = func(*args)\n",
      "[rank6]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank6]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank6]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank3]:     return _run_code(code, main_globals, None,\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank3]:     exec(code, run_globals)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank3]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank3]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank3]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank3]:     with urllib.request.urlopen(url) as response:\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank3]:     return opener.open(url, data, timeout)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank3]:     response = meth(req, response)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank3]:     response = self.parent.error(\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank3]:     return self._call_chain(*args)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank3]:     result = func(*args)\n",
      "[rank3]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank3]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank3]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "    download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "  File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "    with urllib.request.urlopen(url) as response:\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "    response = self.parent.error(\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_train.py\", line 205, in <module>\n",
      "[rank0]:     results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/scripts/my_base_eval.py\", line 41, in evaluate_model\n",
      "[rank0]:     download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)\n",
      "[rank0]:   File \"/home/ubuntu/learn-nanochat/my_nanochat/my_nanochat/my_common.py\", line 95, in download_file_with_lock\n",
      "[rank0]:     with urllib.request.urlopen(url) as response:\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "[rank0]:     return opener.open(url, data, timeout)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "[rank0]:     response = meth(req, response)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "[rank0]:     response = self.parent.error(\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "[rank0]:     return self._call_chain(*args)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "[rank0]:     result = func(*args)\n",
      "[rank0]:   File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "[rank0]:     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "[rank0]: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mchallenge-38-1\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251222_154945-byeg46fu/logs\u001b[0m\n",
      "[rank1]:[W1222 15:52:01.322717052 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank4]:[W1222 15:52:01.649825257 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1222 15:52:01.719296376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank7]:[W1222 15:52:01.778273131 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank2]:[W1222 15:52:01.908061259 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank6]:[W1222 15:52:01.117893743 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank5]:[W1222 15:52:01.229616340 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1222 15:52:02.000000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21232 closing signal SIGTERM\n",
      "W1222 15:52:02.001000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21234 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21235 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21236 closing signal SIGTERM\n",
      "W1222 15:52:02.002000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21237 closing signal SIGTERM\n",
      "W1222 15:52:02.003000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21238 closing signal SIGTERM\n",
      "W1222 15:52:02.003000 21195 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 21239 closing signal SIGTERM\n",
      "E1222 15:52:05.223000 21195 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 21233) of binary: /home/ubuntu/learn-nanochat/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_base_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-12-22_15:52:02\n",
      "  host      : 192-222-52-230\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 21233)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!tail -200 /home/ubuntu/mynanochat2/run_outputs/base_train_output_001.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df23bad-063e-4033-bf8c-53f31764453a",
   "metadata": {},
   "source": [
    "### repeating test run\n",
    "\n",
    "...after scp'ing base_eval from my laptop\n",
    "\n",
    "logging to `base_train_output_002.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6486ab-03a9-4ca7-bf6b-4df213dc48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb:            train/lrm ‚ñÅ\n",
      "wandb:            train/mfu ‚ñÅ\n",
      "wandb:    train/tok_per_sec ‚ñÅ\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:          core_metric -0.02267\n",
      "wandb:                 step 10\n",
      "wandb: total_training_flops 63331869759897600\n",
      "wandb:  total_training_time 0\n",
      "wandb:             train/dt 22.59089\n",
      "wandb:      train/grad_norm 0.53604\n",
      "wandb:           train/loss 11.09035\n",
      "wandb:            train/lrm 1\n",
      "wandb:            train/mfu 3.54326\n",
      "wandb:    train/tok_per_sec 23207\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-1 at: https://wandb.ai/ericsilberstein-self/my-nanochat/runs/0ie1fin2\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251222_155927-0ie1fin2/logs\n",
      "[W1222 16:01:23.002992110 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1222 16:01:23.007993651 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1222 16:01:24.529711010 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -25 /home/ubuntu/mynanochat2/run_outputs/base_train_output_002.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc1e22-97ff-46d2-aba6-8f8737475f48",
   "metadata": {},
   "source": [
    "### confirm can load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f3494a-adfc-44c3-95da-552971a662c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type, compute_init\n",
    "from my_nanochat.my_checkpoint_manager import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b30aee-9aa5-4ec4-8765-0a03e6c36b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 10\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type() \n",
    "_, _, _, _, device = compute_init(device_type)\n",
    "model, tokenizer, meta_data = load_model('base', model_tag='d32', device=device, phase='eval')\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2117a99-780d-4c60-a21f-57907e82ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1006, 345, 400, 311, 4236, 3328, 281, 261, 24142, 4398]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autocast_ctx:\n",
    "    tokens = list(model.generate(tokenizer.encode(\"First take a right on Main St.\", prepend=tokenizer.get_bos_token_id()), max_tokens=10))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e939dd49-cfe5-4155-a0e0-90f134da8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We are can be introduced examples of the utmost background'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea6f8d-3bbc-4ea8-a2af-a52546e1cf41",
   "metadata": {},
   "source": [
    "that looks ok, also check wandb\n",
    "\n",
    "look ok, do real run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f039456-ed60-433b-a537-29d0a048f674",
   "metadata": {},
   "source": [
    "### full run followed by base_loss followed by base_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06b799e-76e3-45fd-a534-73e194e86958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 71658/71680 (99.97%) | loss: 2.372278 | grad norm: 0.1515 | lrm: 0.00 | dt: 1705.46ms | tok/sec: 307,416 | mfu: 46.93 | total time: 1961.10m\n",
      "step 71659/71680 (99.97%) | loss: 2.378669 | grad norm: 0.1473 | lrm: 0.00 | dt: 2489.89ms | tok/sec: 210,566 | mfu: 32.15 | total time: 1961.14m\n",
      "step 71660/71680 (99.97%) | loss: 2.409759 | grad norm: 0.1564 | lrm: 0.00 | dt: 1513.21ms | tok/sec: 346,474 | mfu: 52.90 | total time: 1961.17m\n",
      "step 71661/71680 (99.97%) | loss: 2.400042 | grad norm: 0.1685 | lrm: 0.00 | dt: 1523.07ms | tok/sec: 344,230 | mfu: 52.56 | total time: 1961.19m\n",
      "step 71662/71680 (99.97%) | loss: 2.398359 | grad norm: 0.1515 | lrm: 0.00 | dt: 1516.79ms | tok/sec: 345,656 | mfu: 52.77 | total time: 1961.22m\n",
      "step 71663/71680 (99.98%) | loss: 2.397501 | grad norm: 0.1507 | lrm: 0.00 | dt: 1519.49ms | tok/sec: 345,041 | mfu: 52.68 | total time: 1961.24m\n",
      "step 71664/71680 (99.98%) | loss: 2.383298 | grad norm: 0.1942 | lrm: 0.00 | dt: 1520.90ms | tok/sec: 344,722 | mfu: 52.63 | total time: 1961.27m\n",
      "step 71665/71680 (99.98%) | loss: 2.390323 | grad norm: 0.1416 | lrm: 0.00 | dt: 1521.79ms | tok/sec: 344,519 | mfu: 52.60 | total time: 1961.29m\n",
      "step 71666/71680 (99.98%) | loss: 2.355329 | grad norm: 0.1843 | lrm: 0.00 | dt: 1771.04ms | tok/sec: 296,034 | mfu: 45.20 | total time: 1961.32m\n",
      "step 71667/71680 (99.98%) | loss: 2.353283 | grad norm: 0.1577 | lrm: 0.00 | dt: 2382.30ms | tok/sec: 220,075 | mfu: 33.60 | total time: 1961.36m\n",
      "step 71668/71680 (99.98%) | loss: 2.363530 | grad norm: 0.1529 | lrm: 0.00 | dt: 1513.02ms | tok/sec: 346,517 | mfu: 52.90 | total time: 1961.39m\n",
      "step 71669/71680 (99.98%) | loss: 2.366316 | grad norm: 0.1513 | lrm: 0.00 | dt: 1520.08ms | tok/sec: 344,907 | mfu: 52.66 | total time: 1961.41m\n",
      "step 71670/71680 (99.99%) | loss: 2.393173 | grad norm: 0.1577 | lrm: 0.00 | dt: 1680.22ms | tok/sec: 312,035 | mfu: 47.64 | total time: 1961.44m\n",
      "step 71671/71680 (99.99%) | loss: 2.391595 | grad norm: 0.1541 | lrm: 0.00 | dt: 2504.23ms | tok/sec: 209,360 | mfu: 31.96 | total time: 1961.48m\n",
      "step 71672/71680 (99.99%) | loss: 2.388799 | grad norm: 0.1471 | lrm: 0.00 | dt: 1674.57ms | tok/sec: 313,087 | mfu: 47.80 | total time: 1961.51m\n",
      "step 71673/71680 (99.99%) | loss: 2.400435 | grad norm: 0.1521 | lrm: 0.00 | dt: 2529.72ms | tok/sec: 207,251 | mfu: 31.64 | total time: 1961.55m\n",
      "step 71674/71680 (99.99%) | loss: 2.389731 | grad norm: 0.1486 | lrm: 0.00 | dt: 2542.54ms | tok/sec: 206,206 | mfu: 31.48 | total time: 1961.60m\n",
      "step 71675/71680 (99.99%) | loss: 2.394191 | grad norm: 0.1513 | lrm: 0.00 | dt: 1509.01ms | tok/sec: 347,438 | mfu: 53.04 | total time: 1961.62m\n",
      "step 71676/71680 (99.99%) | loss: 2.359824 | grad norm: 0.1483 | lrm: 0.00 | dt: 1518.78ms | tok/sec: 345,203 | mfu: 52.70 | total time: 1961.65m\n",
      "step 71677/71680 (100.00%) | loss: 2.362267 | grad norm: 0.1668 | lrm: 0.00 | dt: 1506.43ms | tok/sec: 348,034 | mfu: 53.14 | total time: 1961.67m\n",
      "step 71678/71680 (100.00%) | loss: 2.368727 | grad norm: 0.1709 | lrm: 0.00 | dt: 1511.36ms | tok/sec: 346,897 | mfu: 52.96 | total time: 1961.70m\n",
      "step 71679/71680 (100.00%) | loss: 2.374543 | grad norm: 0.1510 | lrm: 0.00 | dt: 1509.54ms | tok/sec: 347,317 | mfu: 53.03 | total time: 1961.72m\n",
      "step 71680 | Validation bpb: 0.7233\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.5860 | centered: 0.4480 | time: 1.22s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.3100 | centered: 0.3100 | time: 1.18s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.6280 | centered: 0.6280 | time: 1.19s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.7140 | centered: 0.6187 | time: 1.38s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.4500 | centered: 0.2667 | time: 1.51s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.7100 | centered: 0.4200 | time: 0.23s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2560 | centered: 0.0700 | time: 1.68s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7560 | centered: 0.5120 | time: 1.20s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3880 | centered: 0.1840 | time: 1.14s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.5060 | centered: 0.5060 | time: 1.15s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.5940 | centered: 0.4587 | time: 2.90s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.7473 | centered: 0.4945 | time: 0.60s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.6000 | centered: 0.2000 | time: 1.11s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0860 | centered: 0.0860 | time: 1.21s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2478 | centered: 0.0598 | time: 1.27s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3720 | centered: 0.3720 | time: 1.20s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1762 | centered: 0.1762 | time: 0.50s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0312 | centered: 0.0312 | time: 0.08s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.4360 | centered: 0.4360 | time: 2.04s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.2860 | centered: 0.2860 | time: 1.27s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.6080 | centered: -0.0316 | time: 4.00s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2520 | centered: 0.1771 | time: 4.75s\n",
      "Step 71680: CORE metric: 0.3050\n",
      "<|bos|>The capital of France is Paris. It is the largest city in the country\n",
      "<|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will\n",
      "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
      "<|bos|>My favorite color is blue. I love the color blue. I love\n",
      "<|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x\n",
      "[W1224 02:02:25.615379342 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.648592094 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.848795559 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:25.850615391 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:27.822987331 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat2/base_checkpoints/d32/model_071680.pt\n",
      "saved metadata to /home/ubuntu/mynanochat2/base_checkpoints/d32/meta_071680.json\n",
      "saved optimizer to /home/ubuntu/mynanochat2/base_checkpoints/d32/optim_071680_rank0.pt\n",
      "Peak memory usage: 77017.79MiB\n",
      "Total training time: 1961.72m\n",
      "Minimum validation bpb: 0.7233\n",
      "wandb: updating run metadata\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:          core_metric ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:                 step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb: total_training_flops ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "wandb:  total_training_time ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:             train/dt ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:      train/grad_norm ‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ\n",
      "wandb:           train/loss ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:            train/lrm ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
      "wandb:            train/mfu ‚ñÜ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñà‚ñà\n",
      "wandb:    train/tok_per_sec ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñá‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÇ‚ñá‚ñá‚ñá‚ñà\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:          core_metric 0.30497\n",
      "wandb:                 step 71680\n",
      "wandb: total_training_flops 4.53962842438946e+20\n",
      "wandb:  total_training_time 117703.30327\n",
      "wandb:             train/dt 3.49672\n",
      "wandb:      train/grad_norm 0.15324\n",
      "wandb:           train/loss 2.40304\n",
      "wandb:            train/lrm 0.00558\n",
      "wandb:            train/mfu 22.89155\n",
      "wandb:    train/tok_per_sec 149937\n",
      "wandb:                   +1 ...\n",
      "wandb: \n",
      "wandb: üöÄ View run challenge-38-4 at: https://wandb.ai/ericsilberstein-self/my-nanochat/runs/oo52nxvr\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ericsilberstein-self/my-nanochat\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251222_161000-oo52nxvr/logs\n",
      "[W1224 02:02:45.251346943 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:46.264167514 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1224 02:02:46.974340210 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -100 $RUN_OUTPUTS_DIR/base_train_output_004.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59509b-62d8-4fc3-a02d-57b948091c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16aa9f9-a350-44fe-98e4-a54f6bc10eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d32\n",
      "user_config: {'device_batch_size': 32, 'split_tokens': 10485760, 'device_type': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 71680\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "train bpb: 0.7271\n",
      "val bpb: 0.7235\n",
      "<|bos|>The capital of France is Paris. It is the largest city in the country and the second largest in Europe\n",
      "<|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic number 79 and the atomic\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will be Tuesday. If today is\n",
      "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune\n",
      "<|bos|>My favorite color is blue. I love the color blue. I love the color blue. I love\n",
      "<|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x + 3 = 13\n"
     ]
    }
   ],
   "source": [
    "!cat $RUN_OUTPUTS_DIR/base_loss_output_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cdb06-e817-4bd4-b4f4-1b89d19798a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b26837-d3e3-45ff-841d-15c40ab617f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat2/base_checkpoints/d32 with step 71680\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.5822 | centered: 0.4429 | time: 23.57s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.2976 | centered: 0.2976 | time: 5.03s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.6202 | centered: 0.6202 | time: 47.94s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.7226 | centered: 0.6302 | time: 6.66s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.4343 | centered: 0.2457 | time: 3.54s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.7100 | centered: 0.4200 | time: 0.28s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2375 | centered: 0.0469 | time: 4.10s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7394 | centered: 0.4788 | time: 4.66s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3880 | centered: 0.1840 | time: 1.17s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.4910 | centered: 0.4910 | time: 11.64s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.5888 | centered: 0.4518 | time: 55.79s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.7473 | centered: 0.4945 | time: 0.64s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5998 | centered: 0.1997 | time: 2.88s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0780 | centered: 0.0780 | time: 2.62s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2478 | centered: 0.0598 | time: 1.27s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3720 | centered: 0.3720 | time: 3.31s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1762 | centered: 0.1762 | time: 0.65s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0312 | centered: 0.0312 | time: 0.08s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.3996 | centered: 0.3996 | time: 41.08s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.2937 | centered: 0.2937 | time: 19.63s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.6174 | centered: -0.0068 | time: 17.23s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2524 | centered: 0.1776 | time: 94.39s\n",
      "================================================================================\n",
      "Model: base_model (step 71680)\n",
      "================================================================================\n",
      "Task                               , Accuracy  , Centered  \n",
      "hellaswag_zeroshot                 , 0.582155  , 0.442873  \n",
      "jeopardy                           , 0.297591  , 0.297591  \n",
      "bigbench_qa_wikidata               , 0.620196  , 0.620196  \n",
      "arc_easy                           , 0.722643  , 0.630191  \n",
      "arc_challenge                      , 0.434300  , 0.245734  \n",
      "copa                               , 0.710000  , 0.420000  \n",
      "commonsense_qa                     , 0.237510  , 0.046888  \n",
      "piqa                               , 0.739391  , 0.478781  \n",
      "openbook_qa                        , 0.388000  , 0.184000  \n",
      "lambada_openai                     , 0.490976  , 0.490976  \n",
      "hellaswag                          , 0.588827  , 0.451769  \n",
      "winograd                           , 0.747253  , 0.494506  \n",
      "winogrande                         , 0.599842  , 0.199684  \n",
      "bigbench_dyck_languages            , 0.078000  , 0.078000  \n",
      "agi_eval_lsat_ar                   , 0.247826  , 0.059783  \n",
      "bigbench_cs_algorithms             , 0.371970  , 0.371970  \n",
      "bigbench_operators                 , 0.176190  , 0.176190  \n",
      "bigbench_repeat_copy_logic         , 0.031250  , 0.031250  \n",
      "squad                              , 0.399622  , 0.399622  \n",
      "coqa                               , 0.293749  , 0.293749  \n",
      "boolq                              , 0.617431  , -0.006760 \n",
      "bigbench_language_identification   , 0.252400  , 0.177558  \n",
      "CORE                               ,           , 0.299298  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $RUN_OUTPUTS_DIR/base_eval_output.001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b12a6-59e2-413a-8295-8175d3482d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6532c0-15f3-40bb-aa86-1185832606c5",
   "metadata": {},
   "source": [
    "### also cat reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499bba88-77d9-4185-8473-42056b9496f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nanochat training report\n",
      "\n",
      "Generated: 2025-12-22 16:07:10\n",
      "\n",
      "## Environment\n",
      "\n",
      "### Git Information\n",
      "- Branch: master\n",
      "- Commit: bc88d72 (dirty)\n",
      "- Message: getting ready to work on gpu machine for challenge 38: train d32\n",
      "\n",
      "### Hardware\n",
      "- Platform: Linux\n",
      "- CPUs: 104 cores (208 logical)\n",
      "- Memory: 1771.7 GB\n",
      "- GPUs: 8x NVIDIA H100 80GB HBM3\n",
      "- GPU Memory: 633.5 GB total\n",
      "- CUDA Version: 12.8\n",
      "\n",
      "### Software\n",
      "- Python: 3.10.12\n",
      "- PyTorch: 2.9.0+cu128\n",
      "\n",
      "Run started: 2025-12-22 16:07:10\n",
      "\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/header.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c636572a-b52b-4a3d-810d-d1693b7a90cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model training\n",
      "timestamp: 2025-12-24 02:02:38\n",
      "\n",
      "- run: challenge-38-4\n",
      "- device_type: \n",
      "- depth: 32\n",
      "- max_seq_len: 2048\n",
      "- num_iterations: -1\n",
      "- target_param_data_ratio: 20\n",
      "- device_batch_size: 8\n",
      "- total_batch_size: 524,288\n",
      "- embedding_lr: 0.2000\n",
      "- unembedding_lr: 0.0040\n",
      "- weight_decay: 0.0000\n",
      "- matrix_lr: 0.0200\n",
      "- grad_clip: 1.0000\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- eval_every: 250\n",
      "- eval_tokens: 10,485,760\n",
      "- core_metric_every: 2000\n",
      "- core_metric_max_per_task: 500\n",
      "- sample_every: 2000\n",
      "- model_tag: \n",
      "- Number of parameters: 1,879,048,192\n",
      "- Number of FLOPs per token: 1.207960e+10\n",
      "- Calculated number of iterations: 71,680\n",
      "- Number of training tokens: 37,580,963,840\n",
      "- Tokens : Params ratio: 20.0000\n",
      "- DDP world size: 8\n",
      "- warmup_ratio: 0.0000\n",
      "- warmdown_ratio: 0.2000\n",
      "- final_lr_frac: 0.0000\n",
      "- Minimum validation bpb: 0.7233\n",
      "- Final validation bpb: 0.7233\n",
      "- CORE metric estimate: 0.3050\n",
      "- MFU %: 53.03%\n",
      "- Total training flops: 4.539628e+20\n",
      "- Total training time: 1961.72m\n",
      "- Peak memory usage: 77017.79MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-training.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9e42b2-a825-468e-9054-693dbfb3929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model loss\n",
      "timestamp: 2025-12-24 02:05:23\n",
      "\n",
      "- train bpb: 0.7271\n",
      "- val bpb: 0.7235\n",
      "- sample 0: <|bos|>The capital of France is Paris. It is the largest city in the country and the second largest in Europe\n",
      "- sample 1: <|bos|>The chemical symbol of gold is Au. Gold is a chemical element with the atomic number 79 and the atomic\n",
      "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today is Monday, then tomorrow will be Tuesday. If today is\n",
      "- sample 3: <|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.\n",
      "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune\n",
      "- sample 5: <|bos|>My favorite color is blue. I love the color blue. I love the color blue. I love\n",
      "- sample 6: <|bos|>If 5*x + 3 = 13, then x is equal to 3.\n",
      "If 5*x + 3 = 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-loss.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569d70e6-056b-436b-81af-438ebdd96fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Base model evaluation\n",
      "timestamp: 2025-12-24 02:12:40\n",
      "\n",
      "- Model: base_model (step 71680)\n",
      "- CORE metric: 0.2993\n",
      "- hellaswag_zeroshot: 0.4429\n",
      "- jeopardy: 0.2976\n",
      "- bigbench_qa_wikidata: 0.6202\n",
      "- arc_easy: 0.6302\n",
      "- arc_challenge: 0.2457\n",
      "- copa: 0.4200\n",
      "- commonsense_qa: 0.0469\n",
      "- piqa: 0.4788\n",
      "- openbook_qa: 0.1840\n",
      "- lambada_openai: 0.4910\n",
      "- hellaswag: 0.4518\n",
      "- winograd: 0.4945\n",
      "- winogrande: 0.1997\n",
      "- bigbench_dyck_languages: 0.0780\n",
      "- agi_eval_lsat_ar: 0.0598\n",
      "- bigbench_cs_algorithms: 0.3720\n",
      "- bigbench_operators: 0.1762\n",
      "- bigbench_repeat_copy_logic: 0.0312\n",
      "- squad: 0.3996\n",
      "- coqa: 0.2937\n",
      "- boolq: -0.0068\n",
      "- bigbench_language_identification: 0.1776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat $NANOCHAT_BASE_DIR/report/base-model-evaluation.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a3e09-3b6b-4dec-88f8-592c2ac1b830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
