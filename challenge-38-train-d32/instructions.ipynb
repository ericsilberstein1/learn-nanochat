{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd5181f-a1e8-43e9-b533-22b38069260b",
   "metadata": {},
   "source": [
    "## Train d32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87261115-9192-46c6-8d06-7640b0b74c71",
   "metadata": {},
   "source": [
    "I'll do the training on lambda cloud with new storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa75ff7-9b74-4e05-ba18-0330db52cb6a",
   "metadata": {},
   "source": [
    "### storage\n",
    "\n",
    "How much? Really rough...\n",
    "\n",
    "- 800 parquet files @ 90 mb each = 72 gb\n",
    "- room for 10 checkpoints @ 8 gb each (?) = 80 gb\n",
    "- eval stuff = 200 mb\n",
    "\n",
    "500 gb should be plenty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447f64a-b9da-4543-98ed-be8e4b5bcab9",
   "metadata": {},
   "source": [
    "### Machine setup\n",
    "\n",
    "```\n",
    "ssh ssh ubuntu@[ip]\n",
    "\n",
    "# ssh key for git\n",
    "ssh-keygen -t ed25519 -C \"lambda-cloud\"\n",
    "cat ~/.ssh/id_ed25519.pub\n",
    "copy into github UI (https://github.com/settings/keys)\n",
    "\n",
    "git config --global user.email \"ericsilberstein@gmail.com\"\n",
    "git config --global user.name \"Eric Silberstein\"\n",
    "\n",
    "# clone this repo\n",
    "git clone git@github.com:ericsilberstein1/learn-nanochat.git\n",
    "\n",
    "# UV\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "echo '. \"$HOME/.cargo/env\"' >> .bashrc\n",
    "\n",
    "echo 'export NANOCHAT_BASE_DIR=\"/home/ubuntu/mynanochat\"' >> .bashrc\n",
    "echo 'export RUN_OUTPUTS_DIR=\"$NANOCHAT_BASE_DIR/run_outputs\"' >> .bashrc\n",
    "echo 'export OMP_NUM_THREADS=1' >> .bashrc\n",
    "\n",
    "# in .bashrc add\n",
    "# export WANDB_API_KEY=\"XXX\"\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "cd learn-nanochat\n",
    "\n",
    "uv sync\n",
    "source .venv/bin/activate\n",
    "\n",
    "# for now until organize this better\n",
    "uv tool install maturin\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "cd -\n",
    "\n",
    "# looks like lambda automatically runs jupyter but for now at least let me run it\n",
    "# in the way I understand\n",
    "uv run jupyter lab --port=7001\n",
    "jupyter server list\n",
    "\n",
    "# ON MY LAPTOP make a tunnel to jupyter\n",
    "ssh -N -L 7001:localhost:7001 ubuntu@[ip]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c066b86-b027-4430-8872-3e553956d39f",
   "metadata": {},
   "source": [
    "### Run...\n",
    "\n",
    "In tmux shell\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "mkdir -p $NANOCHAT_BASE_DIR\n",
    "mkdir -p $RUN_OUTPUTS_DIR\n",
    "\n",
    "cd challenge-38-train-d32\n",
    "\n",
    "curl -L -o $NANOCHAT_BASE_DIR/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl\n",
    "\n",
    "export PYTHONPATH=../my_nanochat/\n",
    "\n",
    "python -m nanochat.dataset -n 800\n",
    "\n",
    "python -m scripts.my_tok_train --max_chars=4000000000\n",
    "\n",
    "# do a short run to confirm it runs to completion\n",
    "\n",
    "python -m my_nanochat.my_report reset\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_train -- --depth=32 --device_batch_size=8 --num_iterations=10 --run=challenge-38-1 > $RUN_OUTPUTS_DIR/base_train_output_001.txt 2>&1\n",
    "\n",
    "# if good do the full run and then the base evals\n",
    "\n",
    "python -m my_nanochat.my_report reset\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_train -- --depth=32 --device_batch_size=8 --run=challenge-38-2 > $RUN_OUTPUTS_DIR/base_train_output_002.txt 2>&1\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_loss -- --model_tag=d32 > $RUN_OUTPUTS_DIR/base_loss_output_001.txt 2>&1\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --model-tag=d32 > $RUN_OUTPUTS_DIR/base_eval_output.001.txt 2>&1\n",
    "\n",
    "# do mid training and chat eval\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d32 --device_batch_size=8 --run=challenge-38-3 > $RUN_OUTPUTS_DIR/mid_train_output_001.txt 2>&1\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --model-tag=d32 --source=mid > $RUN_OUTPUTS_DIR/mid_chat_eval_output_001.txt 2>&1\n",
    "\n",
    "# do sft and chat eval on sft\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- --model-tag=d32 --run=challenge-38-4 > $RUN_OUTPUTS_DIR/sft_train_output_001.txt 2>&1\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --model-tag=d32 --source=sft > $RUN_OUTPUTS_DIR/sft_chat_eval_output_001.txt 2>&1\n",
    "\n",
    "# run final report\n",
    "\n",
    "python -m my_nanochat.my_report\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6af639-3583-4766-b9ce-1338cd5d5d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
