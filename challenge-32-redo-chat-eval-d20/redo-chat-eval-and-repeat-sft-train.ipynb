{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31bba19-5f4f-4add-9171-1f06bc2adb5a",
   "metadata": {},
   "source": [
    "### Redo chat eval and repeat SFT train d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b27c1-1e2d-4776-945e-7c337e7c8150",
   "metadata": {},
   "source": [
    "This notebook is to get back on the GPU machine and do two things from `challenge-30-sft-train-d20/look-into-observations.ipynb`\n",
    "\n",
    "- I measured chat eval accuracies wrong because the engine didn't yet support calling the python calculator. I want to measure mid and sft again.\n",
    "  \n",
    "- I want to measure the validation loss more frequently to help figure out if the increase from step 0 to 100 is concerning.\n",
    "\n",
    "I'll again use the 8xH100 and again get all the commands ready in this notebook.\n",
    "\n",
    "Follow the instructions here to get the machine ready: `challenge-28-midtrain-d20/midtrain-d20.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22806591-bf4e-4705-8bbe-b178f25202ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 21 19:35:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2dc5c-f5aa-4279-8308-4c3cd0309908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e5021-90ef-4f2e-9242-ab7687c6e463",
   "metadata": {},
   "source": [
    "#### Do chat eval on mid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5a3c13-f921-4a8f-8abb-775181a1a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 19:35:34.959000 13187 torch/distributed/run.py:803] \n",
      "W1121 19:35:34.959000 13187 torch/distributed/run.py:803] *****************************************\n",
      "W1121 19:35:34.959000 13187 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 19:35:34.959000 13187 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 9.00kB [00:00, 17.6MB/s]\n",
      "ARC-Easy/train-00000-of-00001.parquet: 100%|â–ˆ| 331k/331k [00:00<00:00, 1.18MB/s]\n",
      "ARC-Easy/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆ| 346k/346k [00:00<00:00, 1.97MB/s]\n",
      "ARC-Easy/validation-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 86.1k/86.1k [00:00<00:00, 6\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2251/2251 [00:00<00:00, 352843.20 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 543443.47 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 570/570 [00:00<00:00, 256683.84 examples/s]\n",
      "final: 1026/2376 (43.18%)\n",
      "ARC-Easy accuracy: 43.18%\n",
      "ARC-Challenge/train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 190k/190k [00:00<00:00, 558\n",
      "ARC-Challenge/test-00000-of-00001.parque(â€¦): 100%|â–ˆ| 204k/204k [00:00<00:00, 1.0\n",
      "ARC-Challenge/validation-00000-of-00001.(â€¦): 100%|â–ˆ| 55.7k/55.7k [00:00<00:00, 4\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1119/1119 [00:00<00:00, 227803.05 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 324942.11 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 299/299 [00:00<00:00, 139825.72 examples/s]\n",
      "final: 389/1172 (33.19%)\n",
      "ARC-Challenge accuracy: 33.19%\n",
      "README.md: 53.2kB [00:00, 106MB/s]\n",
      "dataset_infos.json: 138kB [00:00, 220MB/s]\n",
      "all/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50M/3.50M [00:00<00:00, 8.43MB/s]\n",
      "all/validation-00000-of-00001.parquet: 100%|â–ˆ| 408k/408k [00:00<00:00, 2.19MB/s]\n",
      "all/dev-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76.5k/76.5k [00:00<00:00, 520kB/s]\n",
      "all/auxiliary_train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 7\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 14042/14042 [00:00<00:00, 562256.96 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 1531/1531 [00:00<00:00, 368309.69 examples/\n",
      "Generating dev split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 146330.84 examples/s]\n",
      "Generating auxiliary_train split: 100%|â–ˆ| 99842/99842 [00:00<00:00, 407322.35 ex\n",
      "final: 4644/14042 (33.07%)\n",
      "MMLU accuracy: 33.07%\n",
      "README.md: 7.94kB [00:00, 25.4MB/s]\n",
      "main/train-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆ| 2.31M/2.31M [00:00<00:00, 8.47MB/s]\n",
      "main/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419k/419k [00:00<00:00, 3.61MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 490286.78 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 373828.43 examples/s]\n",
      "\u001b[KRank 6 | 6/165 (3.64%)]\n",
      "\u001b[KRank 0 | 4/165 (2.42%)]\n",
      "\u001b[KRank 4 | 8/165 (4.85%)]\n",
      "\u001b[KRank 7 | 6/164 (3.66%)]\n",
      "\u001b[KRank 3 | 11/165 (6.67%)]\n",
      "\u001b[KRank 5 | 2/165 (1.21%)]\n",
      "\u001b[KRank 2 | 3/165 (1.82%)]\n",
      "\u001b[KRank 1 | 5/165 (3.03%)]\n",
      "==================================================\n",
      "final: 45/1319 (3.41%)\n",
      "GSM8K accuracy: 3.41%\n",
      "README.md: 6.52kB [00:00, 23.2MB/s]\n",
      "openai_humaneval/test-00000-of-00001.par(â€¦): 100%|â–ˆ| 83.9k/83.9k [00:00<00:00, 2\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 40062.08 examples/s]\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "\u001b[KRank 2 | 3/21 (14.29%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]]\n",
      "\u001b[KRank 7 | 1/20 (5.00%)]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "==================================================\n",
      "final: 11/164 (6.71%)\n",
      "HumanEval accuracy: 6.71%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 5 | 31/32 (96.88%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]\n",
      "\u001b[KRank 1 | 31/32 (96.88%)]\n",
      "==================================================\n",
      "final: 251/256 (98.05%)\n",
      "SpellingBee accuracy: 98.05%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7d2ba-29fd-47a2-aee9-e0b14f2e1d03",
   "metadata": {},
   "source": [
    "#### Do chat eval on sft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56d4e3b-bdfc-455a-9a8a-e6015398ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 19:44:58.832000 16684 torch/distributed/run.py:803] \n",
      "W1121 19:44:58.832000 16684 torch/distributed/run.py:803] *****************************************\n",
      "W1121 19:44:58.832000 16684 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 19:44:58.832000 16684 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1047/2376 (44.07%)\n",
      "ARC-Easy accuracy: 44.07%\n",
      "final: 372/1172 (31.74%)\n",
      "ARC-Challenge accuracy: 31.74%\n",
      "final: 4541/14042 (32.34%)\n",
      "MMLU accuracy: 32.34%\n",
      "\u001b[KRank 0 | 10/165 (6.06%)]\n",
      "\u001b[KRank 5 | 6/165 (3.64%)]]\n",
      "\u001b[KRank 4 | 7/165 (4.24%)]]\n",
      "\u001b[KRank 2 | 9/165 (5.45%)]]\n",
      "\u001b[KRank 7 | 7/164 (4.27%)]\n",
      "\u001b[KRank 1 | 11/165 (6.67%)]\n",
      "\u001b[KRank 6 | 8/165 (4.85%)]\n",
      "\u001b[KRank 3 | 13/165 (7.88%)]\n",
      "==================================================\n",
      "final: 71/1319 (5.38%)\n",
      "GSM8K accuracy: 5.38%\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 5 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "==================================================\n",
      "final: 10/164 (6.10%)\n",
      "HumanEval accuracy: 6.10%\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 5 | 30/32 (93.75%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003838f5-69f0-42dc-87bb-d53ac24b9ad9",
   "metadata": {},
   "source": [
    "#### Do SFT train with more frequent evaluation of validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5577dc93-2ad7-42f3-be47-bf6c566fc966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 19:51:49.406000 19509 torch/distributed/run.py:803] \n",
      "W1121 19:51:49.406000 19509 torch/distributed/run.py:803] *****************************************\n",
      "W1121 19:51:49.406000 19509 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 19:51:49.406000 19509 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding eval_every = 25\n",
      "overriding run = challenge-32-1\n",
      "user_config: {'run': 'challenge-32-1', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 25, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run pvbowvrd (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-32-redo-chat-eval-d20/wandb/run-20251121_195202-pvbowvrd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-32-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/pvbowvrd\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 2.24kB [00:00, 8.77MB/s]\n",
      "data/train-00000-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 204MB/s]\n",
      "data/train-00001-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:00<00:00, 243MB/s]\n",
      "data/train-00002-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231M/231M [00:00<00:00, 273MB/s]\n",
      "data/train-00003-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232M/232M [00:00<00:00, 296MB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 48.2M/48.2M [00:00<00:00, 83.4MB/s]\n",
      "Generating train split: 100%|â–ˆ| 460341/460341 [00:03<00:00, 124840.72 examples/s\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 24229/24229 [00:00<00:00, 130391.56 examples/s]\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00701 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00701 | Training loss: 0.764178| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 1.094233| lrm: 0.997147| num_tokens: 11,283\n",
      "Step 00003/00701 | Training loss: 1.216006| lrm: 0.995720| num_tokens: 12,682\n",
      "Step 00004/00701 | Training loss: 0.784353| lrm: 0.994294| num_tokens: 9,509\n",
      "Step 00005/00701 | Training loss: 0.847236| lrm: 0.992867| num_tokens: 8,344\n",
      "Step 00006/00701 | Training loss: 0.690105| lrm: 0.991441| num_tokens: 8,763\n",
      "Step 00007/00701 | Training loss: 0.417752| lrm: 0.990014| num_tokens: 10,664\n",
      "Step 00008/00701 | Training loss: 1.029844| lrm: 0.988588| num_tokens: 11,584\n",
      "Step 00009/00701 | Training loss: 0.565623| lrm: 0.987161| num_tokens: 9,920\n",
      "Step 00010/00701 | Training loss: 0.405964| lrm: 0.985735| num_tokens: 15,002\n",
      "Step 00011/00701 | Training loss: 1.074546| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.879793| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.957320| lrm: 0.981455| num_tokens: 10,802\n",
      "Step 00014/00701 | Training loss: 0.773008| lrm: 0.980029| num_tokens: 7,541\n",
      "Step 00015/00701 | Training loss: 0.880106| lrm: 0.978602| num_tokens: 10,515\n",
      "Step 00016/00701 | Training loss: 1.074671| lrm: 0.977175| num_tokens: 14,469\n",
      "Step 00017/00701 | Training loss: 0.539944| lrm: 0.975749| num_tokens: 7,715\n",
      "Step 00018/00701 | Training loss: 1.228569| lrm: 0.974322| num_tokens: 11,655\n",
      "Step 00019/00701 | Training loss: 0.591309| lrm: 0.972896| num_tokens: 11,228\n",
      "Step 00020/00701 | Training loss: 0.413286| lrm: 0.971469| num_tokens: 10,678\n",
      "Step 00021/00701 | Training loss: 0.757234| lrm: 0.970043| num_tokens: 12,324\n",
      "Step 00022/00701 | Training loss: 1.113977| lrm: 0.968616| num_tokens: 9,331\n",
      "Step 00023/00701 | Training loss: 0.692595| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.845343| lrm: 0.965763| num_tokens: 10,715\n",
      "Step 00025 | Validation loss: 1.012588\n",
      "Step 00025/00701 | Training loss: 0.631336| lrm: 0.964337| num_tokens: 12,383\n",
      "Step 00026/00701 | Training loss: 1.034890| lrm: 0.962910| num_tokens: 12,576\n",
      "Step 00027/00701 | Training loss: 0.919199| lrm: 0.961484| num_tokens: 8,359\n",
      "Step 00028/00701 | Training loss: 0.933134| lrm: 0.960057| num_tokens: 10,178\n",
      "Step 00029/00701 | Training loss: 0.933458| lrm: 0.958631| num_tokens: 10,629\n",
      "Step 00030/00701 | Training loss: 0.588206| lrm: 0.957204| num_tokens: 9,143\n",
      "Step 00031/00701 | Training loss: 1.142920| lrm: 0.955777| num_tokens: 7,481\n",
      "Step 00032/00701 | Training loss: 0.859159| lrm: 0.954351| num_tokens: 9,972\n",
      "Step 00033/00701 | Training loss: 0.841789| lrm: 0.952924| num_tokens: 11,270\n",
      "Step 00034/00701 | Training loss: 1.043059| lrm: 0.951498| num_tokens: 11,177\n",
      "Step 00035/00701 | Training loss: 0.845001| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.931393| lrm: 0.948645| num_tokens: 9,343\n",
      "Step 00037/00701 | Training loss: 1.067934| lrm: 0.947218| num_tokens: 10,954\n",
      "Step 00038/00701 | Training loss: 0.719889| lrm: 0.945792| num_tokens: 6,793\n",
      "Step 00039/00701 | Training loss: 0.610449| lrm: 0.944365| num_tokens: 5,849\n",
      "Step 00040/00701 | Training loss: 0.941661| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.620335| lrm: 0.941512| num_tokens: 14,344\n",
      "Step 00042/00701 | Training loss: 1.180998| lrm: 0.940086| num_tokens: 11,223\n",
      "Step 00043/00701 | Training loss: 0.999089| lrm: 0.938659| num_tokens: 11,061\n",
      "Step 00044/00701 | Training loss: 1.086356| lrm: 0.937233| num_tokens: 12,666\n",
      "Step 00045/00701 | Training loss: 1.067328| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.688457| lrm: 0.934379| num_tokens: 9,630\n",
      "Step 00047/00701 | Training loss: 1.076704| lrm: 0.932953| num_tokens: 12,891\n",
      "Step 00048/00701 | Training loss: 0.733129| lrm: 0.931526| num_tokens: 10,059\n",
      "Step 00049/00701 | Training loss: 0.540922| lrm: 0.930100| num_tokens: 11,237\n",
      "Step 00050 | Validation loss: 1.014873\n",
      "Step 00050/00701 | Training loss: 0.757630| lrm: 0.928673| num_tokens: 9,079\n",
      "Step 00051/00701 | Training loss: 0.276231| lrm: 0.927247| num_tokens: 9,590\n",
      "Step 00052/00701 | Training loss: 0.831585| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 1.003339| lrm: 0.924394| num_tokens: 7,924\n",
      "Step 00054/00701 | Training loss: 0.374489| lrm: 0.922967| num_tokens: 9,631\n",
      "Step 00055/00701 | Training loss: 0.898600| lrm: 0.921541| num_tokens: 6,508\n",
      "Step 00056/00701 | Training loss: 1.395498| lrm: 0.920114| num_tokens: 10,513\n",
      "Step 00057/00701 | Training loss: 1.008230| lrm: 0.918688| num_tokens: 9,782\n",
      "Step 00058/00701 | Training loss: 1.359835| lrm: 0.917261| num_tokens: 13,372\n",
      "Step 00059/00701 | Training loss: 1.058974| lrm: 0.915835| num_tokens: 7,944\n",
      "Step 00060/00701 | Training loss: 1.142303| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.256187| lrm: 0.912981| num_tokens: 13,711\n",
      "Step 00062/00701 | Training loss: 0.939758| lrm: 0.911555| num_tokens: 12,159\n",
      "Step 00063/00701 | Training loss: 0.571522| lrm: 0.910128| num_tokens: 10,617\n",
      "Step 00064/00701 | Training loss: 0.667873| lrm: 0.908702| num_tokens: 8,098\n",
      "Step 00065/00701 | Training loss: 0.489255| lrm: 0.907275| num_tokens: 11,033\n",
      "Step 00066/00701 | Training loss: 1.025763| lrm: 0.905849| num_tokens: 9,531\n",
      "Step 00067/00701 | Training loss: 0.555463| lrm: 0.904422| num_tokens: 11,573\n",
      "Step 00068/00701 | Training loss: 1.318524| lrm: 0.902996| num_tokens: 12,923\n",
      "Step 00069/00701 | Training loss: 0.845442| lrm: 0.901569| num_tokens: 7,943\n",
      "Step 00070/00701 | Training loss: 0.477253| lrm: 0.900143| num_tokens: 10,775\n",
      "Step 00071/00701 | Training loss: 0.733535| lrm: 0.898716| num_tokens: 9,193\n",
      "Step 00072/00701 | Training loss: 0.948834| lrm: 0.897290| num_tokens: 9,378\n",
      "Step 00073/00701 | Training loss: 1.221339| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.724585| lrm: 0.894437| num_tokens: 7,685\n",
      "Step 00075 | Validation loss: 1.015615\n",
      "Step 00075/00701 | Training loss: 0.700249| lrm: 0.893010| num_tokens: 6,807\n",
      "Step 00076/00701 | Training loss: 0.833897| lrm: 0.891583| num_tokens: 7,530\n",
      "Step 00077/00701 | Training loss: 0.855963| lrm: 0.890157| num_tokens: 9,319\n",
      "Step 00078/00701 | Training loss: 0.614304| lrm: 0.888730| num_tokens: 11,560\n",
      "Step 00079/00701 | Training loss: 0.583495| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080/00701 | Training loss: 1.003100| lrm: 0.885877| num_tokens: 11,674\n",
      "Step 00081/00701 | Training loss: 0.516477| lrm: 0.884451| num_tokens: 7,783\n",
      "Step 00082/00701 | Training loss: 0.684139| lrm: 0.883024| num_tokens: 14,544\n",
      "Step 00083/00701 | Training loss: 0.538602| lrm: 0.881598| num_tokens: 11,069\n",
      "Step 00084/00701 | Training loss: 0.749787| lrm: 0.880171| num_tokens: 13,568\n",
      "Step 00085/00701 | Training loss: 1.147327| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.988395| lrm: 0.877318| num_tokens: 9,908\n",
      "Step 00087/00701 | Training loss: 1.020715| lrm: 0.875892| num_tokens: 9,627\n",
      "Step 00088/00701 | Training loss: 0.862806| lrm: 0.874465| num_tokens: 11,064\n",
      "Step 00089/00701 | Training loss: 1.392760| lrm: 0.873039| num_tokens: 9,523\n",
      "Step 00090/00701 | Training loss: 0.960280| lrm: 0.871612| num_tokens: 10,566\n",
      "Step 00091/00701 | Training loss: 0.752077| lrm: 0.870185| num_tokens: 10,761\n",
      "Step 00092/00701 | Training loss: 0.867564| lrm: 0.868759| num_tokens: 10,418\n",
      "Step 00093/00701 | Training loss: 0.869506| lrm: 0.867332| num_tokens: 9,591\n",
      "Step 00094/00701 | Training loss: 0.729506| lrm: 0.865906| num_tokens: 9,464\n",
      "Step 00095/00701 | Training loss: 0.956916| lrm: 0.864479| num_tokens: 9,395\n",
      "Step 00096/00701 | Training loss: 1.119000| lrm: 0.863053| num_tokens: 13,239\n",
      "Step 00097/00701 | Training loss: 1.008160| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.756194| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 1.138832| lrm: 0.858773| num_tokens: 8,989\n",
      "Step 00100 | Validation loss: 1.015306\n",
      "Step 00100/00701 | Training loss: 1.958841| lrm: 0.857347| num_tokens: 10,527\n",
      "Step 00101/00701 | Training loss: 0.437790| lrm: 0.855920| num_tokens: 9,838\n",
      "Step 00102/00701 | Training loss: 0.568284| lrm: 0.854494| num_tokens: 10,665\n",
      "Step 00103/00701 | Training loss: 0.717765| lrm: 0.853067| num_tokens: 11,272\n",
      "Step 00104/00701 | Training loss: 0.710880| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.763315| lrm: 0.850214| num_tokens: 10,153\n",
      "Step 00106/00701 | Training loss: 0.827930| lrm: 0.848787| num_tokens: 11,156\n",
      "Step 00107/00701 | Training loss: 1.045651| lrm: 0.847361| num_tokens: 10,161\n",
      "Step 00108/00701 | Training loss: 0.941354| lrm: 0.845934| num_tokens: 8,469\n",
      "Step 00109/00701 | Training loss: 0.682846| lrm: 0.844508| num_tokens: 8,740\n",
      "Step 00110/00701 | Training loss: 0.817387| lrm: 0.843081| num_tokens: 12,241\n",
      "Step 00111/00701 | Training loss: 0.705901| lrm: 0.841655| num_tokens: 10,737\n",
      "Step 00112/00701 | Training loss: 1.482489| lrm: 0.840228| num_tokens: 14,866\n",
      "Step 00113/00701 | Training loss: 0.758295| lrm: 0.838802| num_tokens: 16,098\n",
      "Step 00114/00701 | Training loss: 1.106907| lrm: 0.837375| num_tokens: 14,309\n",
      "Step 00115/00701 | Training loss: 0.317910| lrm: 0.835949| num_tokens: 11,090\n",
      "Step 00116/00701 | Training loss: 0.795544| lrm: 0.834522| num_tokens: 9,127\n",
      "Step 00117/00701 | Training loss: 0.923248| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.644946| lrm: 0.831669| num_tokens: 9,686\n",
      "Step 00119/00701 | Training loss: 0.644762| lrm: 0.830243| num_tokens: 14,012\n",
      "Step 00120/00701 | Training loss: 0.973936| lrm: 0.828816| num_tokens: 12,993\n",
      "Step 00121/00701 | Training loss: 1.081907| lrm: 0.827389| num_tokens: 16,514\n",
      "Step 00122/00701 | Training loss: 0.821376| lrm: 0.825963| num_tokens: 7,264\n",
      "Step 00123/00701 | Training loss: 0.874317| lrm: 0.824536| num_tokens: 6,636\n",
      "Step 00124/00701 | Training loss: 0.697677| lrm: 0.823110| num_tokens: 11,608\n",
      "Step 00125 | Validation loss: 1.014454\n",
      "Step 00125/00701 | Training loss: 0.982392| lrm: 0.821683| num_tokens: 5,773\n",
      "Step 00126/00701 | Training loss: 0.630014| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 1.057103| lrm: 0.818830| num_tokens: 10,148\n",
      "Step 00128/00701 | Training loss: 0.453627| lrm: 0.817404| num_tokens: 10,627\n",
      "Step 00129/00701 | Training loss: 0.829464| lrm: 0.815977| num_tokens: 8,873\n",
      "Step 00130/00701 | Training loss: 0.952434| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.937041| lrm: 0.813124| num_tokens: 11,228\n",
      "Step 00132/00701 | Training loss: 0.809734| lrm: 0.811698| num_tokens: 12,012\n",
      "Step 00133/00701 | Training loss: 0.909297| lrm: 0.810271| num_tokens: 16,435\n",
      "Step 00134/00701 | Training loss: 0.699939| lrm: 0.808845| num_tokens: 10,741\n",
      "Step 00135/00701 | Training loss: 1.373389| lrm: 0.807418| num_tokens: 13,492\n",
      "Step 00136/00701 | Training loss: 0.505784| lrm: 0.805991| num_tokens: 10,194\n",
      "Step 00137/00701 | Training loss: 0.884917| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.742323| lrm: 0.803138| num_tokens: 11,043\n",
      "Step 00139/00701 | Training loss: 0.959518| lrm: 0.801712| num_tokens: 12,147\n",
      "Step 00140/00701 | Training loss: 0.964850| lrm: 0.800285| num_tokens: 13,088\n",
      "Step 00141/00701 | Training loss: 0.808198| lrm: 0.798859| num_tokens: 13,647\n",
      "Step 00142/00701 | Training loss: 0.469787| lrm: 0.797432| num_tokens: 10,286\n",
      "Step 00143/00701 | Training loss: 1.172585| lrm: 0.796006| num_tokens: 8,956\n",
      "Step 00144/00701 | Training loss: 1.254873| lrm: 0.794579| num_tokens: 14,847\n",
      "Step 00145/00701 | Training loss: 0.959649| lrm: 0.793153| num_tokens: 14,649\n",
      "Step 00146/00701 | Training loss: 0.574411| lrm: 0.791726| num_tokens: 11,999\n",
      "Step 00147/00701 | Training loss: 0.644380| lrm: 0.790300| num_tokens: 8,022\n",
      "Step 00148/00701 | Training loss: 1.042744| lrm: 0.788873| num_tokens: 11,348\n",
      "Step 00149/00701 | Training loss: 1.078937| lrm: 0.787447| num_tokens: 14,182\n",
      "Step 00150 | Validation loss: 1.014185\n",
      "Step 00150/00701 | Training loss: 0.625308| lrm: 0.786020| num_tokens: 10,776\n",
      "Step 00151/00701 | Training loss: 0.676057| lrm: 0.784593| num_tokens: 14,039\n",
      "Step 00152/00701 | Training loss: 0.787590| lrm: 0.783167| num_tokens: 8,526\n",
      "Step 00153/00701 | Training loss: 0.828399| lrm: 0.781740| num_tokens: 12,782\n",
      "Step 00154/00701 | Training loss: 0.759717| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.834439| lrm: 0.778887| num_tokens: 10,624\n",
      "Step 00156/00701 | Training loss: 0.573935| lrm: 0.777461| num_tokens: 13,575\n",
      "Step 00157/00701 | Training loss: 1.127871| lrm: 0.776034| num_tokens: 13,319\n",
      "Step 00158/00701 | Training loss: 0.542029| lrm: 0.774608| num_tokens: 7,492\n",
      "Step 00159/00701 | Training loss: 0.719536| lrm: 0.773181| num_tokens: 9,274\n",
      "Step 00160/00701 | Training loss: 0.645737| lrm: 0.771755| num_tokens: 9,399\n",
      "Step 00161/00701 | Training loss: 0.981392| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.985021| lrm: 0.768902| num_tokens: 11,779\n",
      "Step 00163/00701 | Training loss: 0.492602| lrm: 0.767475| num_tokens: 7,485\n",
      "Step 00164/00701 | Training loss: 0.418143| lrm: 0.766049| num_tokens: 9,081\n",
      "Step 00165/00701 | Training loss: 0.757723| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.955325| lrm: 0.763195| num_tokens: 12,309\n",
      "Step 00167/00701 | Training loss: 0.903830| lrm: 0.761769| num_tokens: 10,547\n",
      "Step 00168/00701 | Training loss: 0.702581| lrm: 0.760342| num_tokens: 9,717\n",
      "Step 00169/00701 | Training loss: 0.880465| lrm: 0.758916| num_tokens: 9,577\n",
      "Step 00170/00701 | Training loss: 1.305716| lrm: 0.757489| num_tokens: 13,464\n",
      "Step 00171/00701 | Training loss: 0.685026| lrm: 0.756063| num_tokens: 10,156\n",
      "Step 00172/00701 | Training loss: 0.960002| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.971125| lrm: 0.753210| num_tokens: 13,931\n",
      "Step 00174/00701 | Training loss: 0.943994| lrm: 0.751783| num_tokens: 9,132\n",
      "Step 00175 | Validation loss: 1.014507\n",
      "Step 00175/00701 | Training loss: 1.049300| lrm: 0.750357| num_tokens: 9,835\n",
      "Step 00176/00701 | Training loss: 0.990621| lrm: 0.748930| num_tokens: 11,244\n",
      "Step 00177/00701 | Training loss: 0.982740| lrm: 0.747504| num_tokens: 14,670\n",
      "Step 00178/00701 | Training loss: 0.633802| lrm: 0.746077| num_tokens: 9,624\n",
      "Step 00179/00701 | Training loss: 0.955341| lrm: 0.744650| num_tokens: 11,085\n",
      "Step 00180/00701 | Training loss: 0.547517| lrm: 0.743224| num_tokens: 11,989\n",
      "Step 00181/00701 | Training loss: 0.876587| lrm: 0.741797| num_tokens: 15,749\n",
      "Step 00182/00701 | Training loss: 1.184740| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.484286| lrm: 0.738944| num_tokens: 12,476\n",
      "Step 00184/00701 | Training loss: 0.966469| lrm: 0.737518| num_tokens: 13,569\n",
      "Step 00185/00701 | Training loss: 0.667648| lrm: 0.736091| num_tokens: 11,265\n",
      "Step 00186/00701 | Training loss: 0.843417| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.481050| lrm: 0.733238| num_tokens: 13,609\n",
      "Step 00188/00701 | Training loss: 0.963117| lrm: 0.731812| num_tokens: 9,421\n",
      "Step 00189/00701 | Training loss: 0.970144| lrm: 0.730385| num_tokens: 9,300\n",
      "Step 00190/00701 | Training loss: 0.459478| lrm: 0.728959| num_tokens: 15,818\n",
      "Step 00191/00701 | Training loss: 0.905324| lrm: 0.727532| num_tokens: 12,340\n",
      "Step 00192/00701 | Training loss: 0.532317| lrm: 0.726106| num_tokens: 9,962\n",
      "Step 00193/00701 | Training loss: 1.101328| lrm: 0.724679| num_tokens: 10,650\n",
      "Step 00194/00701 | Training loss: 1.216312| lrm: 0.723252| num_tokens: 8,192\n",
      "Step 00195/00701 | Training loss: 0.546517| lrm: 0.721826| num_tokens: 14,326\n",
      "Step 00196/00701 | Training loss: 0.765698| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 1.041850| lrm: 0.718973| num_tokens: 10,264\n",
      "Step 00198/00701 | Training loss: 0.583058| lrm: 0.717546| num_tokens: 9,718\n",
      "Step 00199/00701 | Training loss: 0.615927| lrm: 0.716120| num_tokens: 10,337\n",
      "Step 00200 | Validation loss: 1.014421\n",
      "final: 339/1024 (33.11%)\n",
      "final: 414/1024 (40.43%)\n",
      "Step 00200 | mmlu_acc: 0.331055, arc_easy_acc: 0.404297\n",
      "Step 00200/00701 | Training loss: 0.845378| lrm: 0.714693| num_tokens: 17,787\n",
      "Step 00201/00701 | Training loss: 0.879927| lrm: 0.713267| num_tokens: 9,676\n",
      "Step 00202/00701 | Training loss: 1.930434| lrm: 0.711840| num_tokens: 15,694\n",
      "Step 00203/00701 | Training loss: 1.074959| lrm: 0.710414| num_tokens: 8,344\n",
      "Step 00204/00701 | Training loss: 0.992222| lrm: 0.708987| num_tokens: 11,398\n",
      "Step 00205/00701 | Training loss: 0.654538| lrm: 0.707561| num_tokens: 11,165\n",
      "Step 00206/00701 | Training loss: 0.903481| lrm: 0.706134| num_tokens: 10,515\n",
      "Step 00207/00701 | Training loss: 0.640345| lrm: 0.704708| num_tokens: 11,281\n",
      "Step 00208/00701 | Training loss: 1.289638| lrm: 0.703281| num_tokens: 13,195\n",
      "Step 00209/00701 | Training loss: 0.568610| lrm: 0.701854| num_tokens: 10,620\n",
      "Step 00210/00701 | Training loss: 0.716853| lrm: 0.700428| num_tokens: 11,855\n",
      "Step 00211/00701 | Training loss: 1.064708| lrm: 0.699001| num_tokens: 11,439\n",
      "Step 00212/00701 | Training loss: 1.011101| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.736134| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 1.103621| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.676737| lrm: 0.693295| num_tokens: 10,049\n",
      "Step 00216/00701 | Training loss: 0.753848| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 1.000786| lrm: 0.690442| num_tokens: 8,208\n",
      "Step 00218/00701 | Training loss: 0.477385| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.884848| lrm: 0.687589| num_tokens: 14,707\n",
      "Step 00220/00701 | Training loss: 0.930056| lrm: 0.686163| num_tokens: 8,471\n",
      "Step 00221/00701 | Training loss: 0.769151| lrm: 0.684736| num_tokens: 7,856\n",
      "Step 00222/00701 | Training loss: 1.106047| lrm: 0.683310| num_tokens: 7,401\n",
      "Step 00223/00701 | Training loss: 1.150687| lrm: 0.681883| num_tokens: 11,899\n",
      "Step 00224/00701 | Training loss: 0.712680| lrm: 0.680456| num_tokens: 10,060\n",
      "Step 00225 | Validation loss: 1.014292\n",
      "Step 00225/00701 | Training loss: 1.127207| lrm: 0.679030| num_tokens: 8,888\n",
      "Step 00226/00701 | Training loss: 0.646008| lrm: 0.677603| num_tokens: 10,747\n",
      "Step 00227/00701 | Training loss: 0.594616| lrm: 0.676177| num_tokens: 10,038\n",
      "Step 00228/00701 | Training loss: 0.703623| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 1.137542| lrm: 0.673324| num_tokens: 9,397\n",
      "Step 00230/00701 | Training loss: 1.015136| lrm: 0.671897| num_tokens: 7,844\n",
      "Step 00231/00701 | Training loss: 1.210365| lrm: 0.670471| num_tokens: 11,685\n",
      "Step 00232/00701 | Training loss: 1.646267| lrm: 0.669044| num_tokens: 10,805\n",
      "Step 00233/00701 | Training loss: 0.740633| lrm: 0.667618| num_tokens: 10,203\n",
      "Step 00234/00701 | Training loss: 0.881856| lrm: 0.666191| num_tokens: 14,225\n",
      "Step 00235/00701 | Training loss: 1.032576| lrm: 0.664765| num_tokens: 9,748\n",
      "Step 00236/00701 | Training loss: 1.011036| lrm: 0.663338| num_tokens: 7,453\n",
      "Step 00237/00701 | Training loss: 0.844662| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 1.006171| lrm: 0.660485| num_tokens: 13,482\n",
      "Step 00239/00701 | Training loss: 1.265922| lrm: 0.659058| num_tokens: 12,515\n",
      "Step 00240/00701 | Training loss: 0.559400| lrm: 0.657632| num_tokens: 12,885\n",
      "Step 00241/00701 | Training loss: 1.004618| lrm: 0.656205| num_tokens: 11,284\n",
      "Step 00242/00701 | Training loss: 0.900325| lrm: 0.654779| num_tokens: 8,141\n",
      "Step 00243/00701 | Training loss: 0.666061| lrm: 0.653352| num_tokens: 10,581\n",
      "Step 00244/00701 | Training loss: 0.467079| lrm: 0.651926| num_tokens: 6,552\n",
      "Step 00245/00701 | Training loss: 1.031599| lrm: 0.650499| num_tokens: 12,896\n",
      "Step 00246/00701 | Training loss: 0.325215| lrm: 0.649073| num_tokens: 8,779\n",
      "Step 00247/00701 | Training loss: 0.560501| lrm: 0.647646| num_tokens: 10,640\n",
      "Step 00248/00701 | Training loss: 0.806268| lrm: 0.646220| num_tokens: 14,873\n",
      "Step 00249/00701 | Training loss: 0.705971| lrm: 0.644793| num_tokens: 8,755\n",
      "Step 00250 | Validation loss: 1.014295\n",
      "Step 00250/00701 | Training loss: 1.023324| lrm: 0.643367| num_tokens: 11,347\n",
      "Step 00251/00701 | Training loss: 1.206303| lrm: 0.641940| num_tokens: 13,773\n",
      "Step 00252/00701 | Training loss: 0.692755| lrm: 0.640514| num_tokens: 10,831\n",
      "Step 00253/00701 | Training loss: 0.820451| lrm: 0.639087| num_tokens: 14,171\n",
      "Step 00254/00701 | Training loss: 0.863075| lrm: 0.637660| num_tokens: 8,633\n",
      "Step 00255/00701 | Training loss: 0.743490| lrm: 0.636234| num_tokens: 10,649\n",
      "Step 00256/00701 | Training loss: 1.409417| lrm: 0.634807| num_tokens: 10,565\n",
      "Step 00257/00701 | Training loss: 0.538415| lrm: 0.633381| num_tokens: 8,375\n",
      "Step 00258/00701 | Training loss: 1.019478| lrm: 0.631954| num_tokens: 11,893\n",
      "Step 00259/00701 | Training loss: 0.910125| lrm: 0.630528| num_tokens: 6,289\n",
      "Step 00260/00701 | Training loss: 0.455162| lrm: 0.629101| num_tokens: 12,731\n",
      "Step 00261/00701 | Training loss: 0.677806| lrm: 0.627675| num_tokens: 11,069\n",
      "Step 00262/00701 | Training loss: 0.695140| lrm: 0.626248| num_tokens: 10,292\n",
      "Step 00263/00701 | Training loss: 0.622163| lrm: 0.624822| num_tokens: 6,973\n",
      "Step 00264/00701 | Training loss: 1.046281| lrm: 0.623395| num_tokens: 9,513\n",
      "Step 00265/00701 | Training loss: 0.819913| lrm: 0.621969| num_tokens: 11,233\n",
      "Step 00266/00701 | Training loss: 0.844480| lrm: 0.620542| num_tokens: 11,266\n",
      "Step 00267/00701 | Training loss: 0.547258| lrm: 0.619116| num_tokens: 8,955\n",
      "Step 00268/00701 | Training loss: 0.921316| lrm: 0.617689| num_tokens: 12,249\n",
      "Step 00269/00701 | Training loss: 0.756631| lrm: 0.616262| num_tokens: 9,576\n",
      "Step 00270/00701 | Training loss: 1.143186| lrm: 0.614836| num_tokens: 12,109\n",
      "Step 00271/00701 | Training loss: 1.391892| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.857130| lrm: 0.611983| num_tokens: 13,072\n",
      "Step 00273/00701 | Training loss: 0.621210| lrm: 0.610556| num_tokens: 9,778\n",
      "Step 00274/00701 | Training loss: 0.654004| lrm: 0.609130| num_tokens: 12,238\n",
      "Step 00275 | Validation loss: 1.014174\n",
      "Step 00275/00701 | Training loss: 0.582321| lrm: 0.607703| num_tokens: 10,854\n",
      "Step 00276/00701 | Training loss: 0.704756| lrm: 0.606277| num_tokens: 10,720\n",
      "Step 00277/00701 | Training loss: 0.571386| lrm: 0.604850| num_tokens: 7,570\n",
      "Step 00278/00701 | Training loss: 0.822126| lrm: 0.603424| num_tokens: 11,628\n",
      "Step 00279/00701 | Training loss: 0.891461| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280/00701 | Training loss: 0.777576| lrm: 0.600571| num_tokens: 8,970\n",
      "Step 00281/00701 | Training loss: 1.004701| lrm: 0.599144| num_tokens: 14,104\n",
      "Step 00282/00701 | Training loss: 0.994041| lrm: 0.597718| num_tokens: 10,950\n",
      "Step 00283/00701 | Training loss: 0.619954| lrm: 0.596291| num_tokens: 8,906\n",
      "Step 00284/00701 | Training loss: 0.431018| lrm: 0.594864| num_tokens: 12,614\n",
      "Step 00285/00701 | Training loss: 0.862729| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.599557| lrm: 0.592011| num_tokens: 11,914\n",
      "Step 00287/00701 | Training loss: 0.264645| lrm: 0.590585| num_tokens: 9,547\n",
      "Step 00288/00701 | Training loss: 0.679426| lrm: 0.589158| num_tokens: 5,007\n",
      "Step 00289/00701 | Training loss: 0.685504| lrm: 0.587732| num_tokens: 9,433\n",
      "Step 00290/00701 | Training loss: 1.072361| lrm: 0.586305| num_tokens: 11,035\n",
      "Step 00291/00701 | Training loss: 0.996917| lrm: 0.584879| num_tokens: 10,852\n",
      "Step 00292/00701 | Training loss: 0.948740| lrm: 0.583452| num_tokens: 11,659\n",
      "Step 00293/00701 | Training loss: 0.827863| lrm: 0.582026| num_tokens: 15,038\n",
      "Step 00294/00701 | Training loss: 0.726827| lrm: 0.580599| num_tokens: 9,439\n",
      "Step 00295/00701 | Training loss: 1.109004| lrm: 0.579173| num_tokens: 6,998\n",
      "Step 00296/00701 | Training loss: 1.221313| lrm: 0.577746| num_tokens: 12,267\n",
      "Step 00297/00701 | Training loss: 0.579244| lrm: 0.576320| num_tokens: 17,571\n",
      "Step 00298/00701 | Training loss: 1.035867| lrm: 0.574893| num_tokens: 10,103\n",
      "Step 00299/00701 | Training loss: 0.640537| lrm: 0.573466| num_tokens: 7,731\n",
      "Step 00300 | Validation loss: 1.014061\n",
      "Step 00300/00701 | Training loss: 0.749817| lrm: 0.572040| num_tokens: 8,914\n",
      "Step 00301/00701 | Training loss: 0.830195| lrm: 0.570613| num_tokens: 12,830\n",
      "Step 00302/00701 | Training loss: 0.769041| lrm: 0.569187| num_tokens: 12,209\n",
      "Step 00303/00701 | Training loss: 0.957454| lrm: 0.567760| num_tokens: 11,589\n",
      "Step 00304/00701 | Training loss: 0.612609| lrm: 0.566334| num_tokens: 8,228\n",
      "Step 00305/00701 | Training loss: 0.862258| lrm: 0.564907| num_tokens: 14,637\n",
      "Step 00306/00701 | Training loss: 0.886539| lrm: 0.563481| num_tokens: 11,570\n",
      "Step 00307/00701 | Training loss: 1.095869| lrm: 0.562054| num_tokens: 12,881\n",
      "Step 00308/00701 | Training loss: 0.664495| lrm: 0.560628| num_tokens: 6,839\n",
      "Step 00309/00701 | Training loss: 0.939375| lrm: 0.559201| num_tokens: 11,009\n",
      "Step 00310/00701 | Training loss: 0.582776| lrm: 0.557775| num_tokens: 12,118\n",
      "Step 00311/00701 | Training loss: 1.269233| lrm: 0.556348| num_tokens: 10,305\n",
      "Step 00312/00701 | Training loss: 0.747852| lrm: 0.554922| num_tokens: 11,427\n",
      "Step 00313/00701 | Training loss: 0.741804| lrm: 0.553495| num_tokens: 8,436\n",
      "Step 00314/00701 | Training loss: 1.104442| lrm: 0.552068| num_tokens: 11,858\n",
      "Step 00315/00701 | Training loss: 0.531881| lrm: 0.550642| num_tokens: 14,168\n",
      "Step 00316/00701 | Training loss: 1.124552| lrm: 0.549215| num_tokens: 6,451\n",
      "Step 00317/00701 | Training loss: 0.407068| lrm: 0.547789| num_tokens: 11,643\n",
      "Step 00318/00701 | Training loss: 0.710711| lrm: 0.546362| num_tokens: 10,043\n",
      "Step 00319/00701 | Training loss: 1.214894| lrm: 0.544936| num_tokens: 11,387\n",
      "Step 00320/00701 | Training loss: 0.615694| lrm: 0.543509| num_tokens: 6,437\n",
      "Step 00321/00701 | Training loss: 1.083281| lrm: 0.542083| num_tokens: 8,880\n",
      "Step 00322/00701 | Training loss: 0.747682| lrm: 0.540656| num_tokens: 4,147\n",
      "Step 00323/00701 | Training loss: 0.792882| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.979005| lrm: 0.537803| num_tokens: 11,807\n",
      "Step 00325 | Validation loss: 1.014068\n",
      "Step 00325/00701 | Training loss: 0.632749| lrm: 0.536377| num_tokens: 9,002\n",
      "Step 00326/00701 | Training loss: 0.998862| lrm: 0.534950| num_tokens: 7,345\n",
      "Step 00327/00701 | Training loss: 0.707223| lrm: 0.533524| num_tokens: 12,006\n",
      "Step 00328/00701 | Training loss: 1.044177| lrm: 0.532097| num_tokens: 12,714\n",
      "Step 00329/00701 | Training loss: 0.657435| lrm: 0.530670| num_tokens: 10,357\n",
      "Step 00330/00701 | Training loss: 1.009030| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.806243| lrm: 0.527817| num_tokens: 7,793\n",
      "Step 00332/00701 | Training loss: 0.728986| lrm: 0.526391| num_tokens: 10,629\n",
      "Step 00333/00701 | Training loss: 1.273031| lrm: 0.524964| num_tokens: 9,785\n",
      "Step 00334/00701 | Training loss: 0.377093| lrm: 0.523538| num_tokens: 12,100\n",
      "Step 00335/00701 | Training loss: 0.444217| lrm: 0.522111| num_tokens: 7,312\n",
      "Step 00336/00701 | Training loss: 1.150263| lrm: 0.520685| num_tokens: 12,905\n",
      "Step 00337/00701 | Training loss: 0.767464| lrm: 0.519258| num_tokens: 11,898\n",
      "Step 00338/00701 | Training loss: 0.378277| lrm: 0.517832| num_tokens: 10,202\n",
      "Step 00339/00701 | Training loss: 0.822905| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340/00701 | Training loss: 0.775416| lrm: 0.514979| num_tokens: 11,148\n",
      "Step 00341/00701 | Training loss: 1.192602| lrm: 0.513552| num_tokens: 8,979\n",
      "Step 00342/00701 | Training loss: 0.705687| lrm: 0.512126| num_tokens: 6,240\n",
      "Step 00343/00701 | Training loss: 1.134750| lrm: 0.510699| num_tokens: 11,617\n",
      "Step 00344/00701 | Training loss: 0.560962| lrm: 0.509272| num_tokens: 7,701\n",
      "Step 00345/00701 | Training loss: 0.799624| lrm: 0.507846| num_tokens: 12,529\n",
      "Step 00346/00701 | Training loss: 0.887286| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.678939| lrm: 0.504993| num_tokens: 11,169\n",
      "Step 00348/00701 | Training loss: 1.320593| lrm: 0.503566| num_tokens: 9,517\n",
      "Step 00349/00701 | Training loss: 0.664887| lrm: 0.502140| num_tokens: 10,291\n",
      "Step 00350 | Validation loss: 1.014273\n",
      "Step 00350/00701 | Training loss: 0.848572| lrm: 0.500713| num_tokens: 9,018\n",
      "Step 00351/00701 | Training loss: 0.767525| lrm: 0.499287| num_tokens: 11,729\n",
      "Step 00352/00701 | Training loss: 1.034308| lrm: 0.497860| num_tokens: 12,675\n",
      "Step 00353/00701 | Training loss: 0.863143| lrm: 0.496434| num_tokens: 8,383\n",
      "Step 00354/00701 | Training loss: 0.754820| lrm: 0.495007| num_tokens: 10,059\n",
      "Step 00355/00701 | Training loss: 0.835301| lrm: 0.493581| num_tokens: 10,087\n",
      "Step 00356/00701 | Training loss: 0.656433| lrm: 0.492154| num_tokens: 8,333\n",
      "Step 00357/00701 | Training loss: 0.654794| lrm: 0.490728| num_tokens: 8,266\n",
      "Step 00358/00701 | Training loss: 0.682072| lrm: 0.489301| num_tokens: 9,973\n",
      "Step 00359/00701 | Training loss: 0.710012| lrm: 0.487874| num_tokens: 9,013\n",
      "Step 00360/00701 | Training loss: 0.537312| lrm: 0.486448| num_tokens: 8,617\n",
      "Step 00361/00701 | Training loss: 0.742228| lrm: 0.485021| num_tokens: 11,847\n",
      "Step 00362/00701 | Training loss: 1.221150| lrm: 0.483595| num_tokens: 11,199\n",
      "Step 00363/00701 | Training loss: 0.463247| lrm: 0.482168| num_tokens: 11,212\n",
      "Step 00364/00701 | Training loss: 0.675648| lrm: 0.480742| num_tokens: 13,298\n",
      "Step 00365/00701 | Training loss: 0.519500| lrm: 0.479315| num_tokens: 7,758\n",
      "Step 00366/00701 | Training loss: 0.952804| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.769803| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 1.134583| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.822451| lrm: 0.473609| num_tokens: 13,353\n",
      "Step 00370/00701 | Training loss: 0.742981| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.924606| lrm: 0.470756| num_tokens: 11,940\n",
      "Step 00372/00701 | Training loss: 0.935277| lrm: 0.469330| num_tokens: 11,410\n",
      "Step 00373/00701 | Training loss: 0.552088| lrm: 0.467903| num_tokens: 8,816\n",
      "Step 00374/00701 | Training loss: 0.854385| lrm: 0.466476| num_tokens: 13,098\n",
      "Step 00375 | Validation loss: 1.014231\n",
      "Step 00375/00701 | Training loss: 1.160170| lrm: 0.465050| num_tokens: 13,724\n",
      "Step 00376/00701 | Training loss: 1.184432| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.673699| lrm: 0.462197| num_tokens: 13,890\n",
      "Step 00378/00701 | Training loss: 0.665714| lrm: 0.460770| num_tokens: 11,856\n",
      "Step 00379/00701 | Training loss: 0.895130| lrm: 0.459344| num_tokens: 7,960\n",
      "Step 00380/00701 | Training loss: 1.288097| lrm: 0.457917| num_tokens: 8,519\n",
      "Step 00381/00701 | Training loss: 0.906956| lrm: 0.456491| num_tokens: 14,234\n",
      "Step 00382/00701 | Training loss: 0.644419| lrm: 0.455064| num_tokens: 13,353\n",
      "Step 00383/00701 | Training loss: 0.529567| lrm: 0.453638| num_tokens: 15,146\n",
      "Step 00384/00701 | Training loss: 0.641313| lrm: 0.452211| num_tokens: 12,589\n",
      "Step 00385/00701 | Training loss: 0.433184| lrm: 0.450785| num_tokens: 10,105\n",
      "Step 00386/00701 | Training loss: 1.022636| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 1.038233| lrm: 0.447932| num_tokens: 9,869\n",
      "Step 00388/00701 | Training loss: 0.915865| lrm: 0.446505| num_tokens: 11,292\n",
      "Step 00389/00701 | Training loss: 0.833658| lrm: 0.445078| num_tokens: 15,855\n",
      "Step 00390/00701 | Training loss: 0.506161| lrm: 0.443652| num_tokens: 12,768\n",
      "Step 00391/00701 | Training loss: 0.675117| lrm: 0.442225| num_tokens: 13,893\n",
      "Step 00392/00701 | Training loss: 0.354696| lrm: 0.440799| num_tokens: 10,495\n",
      "Step 00393/00701 | Training loss: 1.292900| lrm: 0.439372| num_tokens: 13,459\n",
      "Step 00394/00701 | Training loss: 0.992198| lrm: 0.437946| num_tokens: 13,515\n",
      "Step 00395/00701 | Training loss: 0.849720| lrm: 0.436519| num_tokens: 10,006\n",
      "Step 00396/00701 | Training loss: 0.961503| lrm: 0.435093| num_tokens: 11,009\n",
      "Step 00397/00701 | Training loss: 1.088174| lrm: 0.433666| num_tokens: 8,178\n",
      "Step 00398/00701 | Training loss: 0.997874| lrm: 0.432240| num_tokens: 8,070\n",
      "Step 00399/00701 | Training loss: 0.901939| lrm: 0.430813| num_tokens: 8,559\n",
      "Step 00400 | Validation loss: 1.013911\n",
      "final: 357/1024 (34.86%)\n",
      "final: 435/1024 (42.48%)\n",
      "Step 00400 | mmlu_acc: 0.348633, arc_easy_acc: 0.424805\n",
      "Step 00400/00701 | Training loss: 0.454243| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.742644| lrm: 0.427960| num_tokens: 6,689\n",
      "Step 00402/00701 | Training loss: 0.666540| lrm: 0.426534| num_tokens: 7,153\n",
      "Step 00403/00701 | Training loss: 1.253213| lrm: 0.425107| num_tokens: 15,333\n",
      "Step 00404/00701 | Training loss: 0.862550| lrm: 0.423680| num_tokens: 8,839\n",
      "Step 00405/00701 | Training loss: 0.693636| lrm: 0.422254| num_tokens: 10,865\n",
      "Step 00406/00701 | Training loss: 0.684180| lrm: 0.420827| num_tokens: 9,689\n",
      "Step 00407/00701 | Training loss: 0.559208| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.621292| lrm: 0.417974| num_tokens: 12,826\n",
      "Step 00409/00701 | Training loss: 0.599226| lrm: 0.416548| num_tokens: 8,557\n",
      "Step 00410/00701 | Training loss: 0.608920| lrm: 0.415121| num_tokens: 13,374\n",
      "Step 00411/00701 | Training loss: 0.885401| lrm: 0.413695| num_tokens: 12,917\n",
      "Step 00412/00701 | Training loss: 0.749639| lrm: 0.412268| num_tokens: 11,922\n",
      "Step 00413/00701 | Training loss: 0.861072| lrm: 0.410842| num_tokens: 7,315\n",
      "Step 00414/00701 | Training loss: 0.985339| lrm: 0.409415| num_tokens: 14,723\n",
      "Step 00415/00701 | Training loss: 0.746226| lrm: 0.407989| num_tokens: 13,804\n",
      "Step 00416/00701 | Training loss: 1.085811| lrm: 0.406562| num_tokens: 11,956\n",
      "Step 00417/00701 | Training loss: 0.659007| lrm: 0.405136| num_tokens: 12,018\n",
      "Step 00418/00701 | Training loss: 0.373208| lrm: 0.403709| num_tokens: 3,817\n",
      "Step 00419/00701 | Training loss: 0.974597| lrm: 0.402282| num_tokens: 9,950\n",
      "Step 00420/00701 | Training loss: 1.278141| lrm: 0.400856| num_tokens: 10,386\n",
      "Step 00421/00701 | Training loss: 0.826476| lrm: 0.399429| num_tokens: 12,796\n",
      "Step 00422/00701 | Training loss: 0.635011| lrm: 0.398003| num_tokens: 13,674\n",
      "Step 00423/00701 | Training loss: 0.579451| lrm: 0.396576| num_tokens: 15,302\n",
      "Step 00424/00701 | Training loss: 0.717058| lrm: 0.395150| num_tokens: 14,939\n",
      "Step 00425 | Validation loss: 1.013763\n",
      "Step 00425/00701 | Training loss: 0.857192| lrm: 0.393723| num_tokens: 7,669\n",
      "Step 00426/00701 | Training loss: 0.916674| lrm: 0.392297| num_tokens: 6,524\n",
      "Step 00427/00701 | Training loss: 0.865549| lrm: 0.390870| num_tokens: 8,753\n",
      "Step 00428/00701 | Training loss: 0.789057| lrm: 0.389444| num_tokens: 9,846\n",
      "Step 00429/00701 | Training loss: 0.686809| lrm: 0.388017| num_tokens: 10,688\n",
      "Step 00430/00701 | Training loss: 0.808068| lrm: 0.386591| num_tokens: 11,206\n",
      "Step 00431/00701 | Training loss: 1.606390| lrm: 0.385164| num_tokens: 8,919\n",
      "Step 00432/00701 | Training loss: 1.025770| lrm: 0.383738| num_tokens: 13,388\n",
      "Step 00433/00701 | Training loss: 0.596469| lrm: 0.382311| num_tokens: 10,474\n",
      "Step 00434/00701 | Training loss: 0.864936| lrm: 0.380884| num_tokens: 11,897\n",
      "Step 00435/00701 | Training loss: 0.674664| lrm: 0.379458| num_tokens: 12,165\n",
      "Step 00436/00701 | Training loss: 0.786336| lrm: 0.378031| num_tokens: 13,113\n",
      "Step 00437/00701 | Training loss: 0.948364| lrm: 0.376605| num_tokens: 12,838\n",
      "Step 00438/00701 | Training loss: 1.265276| lrm: 0.375178| num_tokens: 10,430\n",
      "Step 00439/00701 | Training loss: 0.821557| lrm: 0.373752| num_tokens: 11,832\n",
      "Step 00440/00701 | Training loss: 0.651464| lrm: 0.372325| num_tokens: 10,469\n",
      "Step 00441/00701 | Training loss: 0.782027| lrm: 0.370899| num_tokens: 11,808\n",
      "Step 00442/00701 | Training loss: 0.820730| lrm: 0.369472| num_tokens: 12,062\n",
      "Step 00443/00701 | Training loss: 0.764762| lrm: 0.368046| num_tokens: 10,750\n",
      "Step 00444/00701 | Training loss: 0.690839| lrm: 0.366619| num_tokens: 13,119\n",
      "Step 00445/00701 | Training loss: 0.798584| lrm: 0.365193| num_tokens: 9,910\n",
      "Step 00446/00701 | Training loss: 0.515923| lrm: 0.363766| num_tokens: 13,290\n",
      "Step 00447/00701 | Training loss: 1.072742| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.923815| lrm: 0.360913| num_tokens: 10,547\n",
      "Step 00449/00701 | Training loss: 0.823304| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450 | Validation loss: 1.013843\n",
      "Step 00450/00701 | Training loss: 1.065238| lrm: 0.358060| num_tokens: 15,847\n",
      "Step 00451/00701 | Training loss: 0.882172| lrm: 0.356633| num_tokens: 10,672\n",
      "Step 00452/00701 | Training loss: 0.809970| lrm: 0.355207| num_tokens: 9,304\n",
      "Step 00453/00701 | Training loss: 0.773180| lrm: 0.353780| num_tokens: 12,538\n",
      "Step 00454/00701 | Training loss: 0.933291| lrm: 0.352354| num_tokens: 7,953\n",
      "Step 00455/00701 | Training loss: 0.887379| lrm: 0.350927| num_tokens: 7,755\n",
      "Step 00456/00701 | Training loss: 1.000237| lrm: 0.349501| num_tokens: 10,549\n",
      "Step 00457/00701 | Training loss: 0.606404| lrm: 0.348074| num_tokens: 10,105\n",
      "Step 00458/00701 | Training loss: 0.836370| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.750362| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460/00701 | Training loss: 0.748918| lrm: 0.343795| num_tokens: 8,420\n",
      "Step 00461/00701 | Training loss: 0.808228| lrm: 0.342368| num_tokens: 12,615\n",
      "Step 00462/00701 | Training loss: 0.565056| lrm: 0.340942| num_tokens: 9,887\n",
      "Step 00463/00701 | Training loss: 1.127418| lrm: 0.339515| num_tokens: 7,312\n",
      "Step 00464/00701 | Training loss: 0.689689| lrm: 0.338088| num_tokens: 9,797\n",
      "Step 00465/00701 | Training loss: 1.060315| lrm: 0.336662| num_tokens: 8,338\n",
      "Step 00466/00701 | Training loss: 0.932195| lrm: 0.335235| num_tokens: 11,435\n",
      "Step 00467/00701 | Training loss: 0.654687| lrm: 0.333809| num_tokens: 10,835\n",
      "Step 00468/00701 | Training loss: 0.753378| lrm: 0.332382| num_tokens: 9,671\n",
      "Step 00469/00701 | Training loss: 0.441589| lrm: 0.330956| num_tokens: 8,338\n",
      "Step 00470/00701 | Training loss: 1.075974| lrm: 0.329529| num_tokens: 8,700\n",
      "Step 00471/00701 | Training loss: 0.941245| lrm: 0.328103| num_tokens: 15,219\n",
      "Step 00472/00701 | Training loss: 1.889066| lrm: 0.326676| num_tokens: 13,448\n",
      "Step 00473/00701 | Training loss: 0.657546| lrm: 0.325250| num_tokens: 9,585\n",
      "Step 00474/00701 | Training loss: 1.148019| lrm: 0.323823| num_tokens: 14,725\n",
      "Step 00475 | Validation loss: 1.013892\n",
      "Step 00475/00701 | Training loss: 0.920903| lrm: 0.322397| num_tokens: 12,688\n",
      "Step 00476/00701 | Training loss: 1.142867| lrm: 0.320970| num_tokens: 11,268\n",
      "Step 00477/00701 | Training loss: 1.008251| lrm: 0.319544| num_tokens: 13,478\n",
      "Step 00478/00701 | Training loss: 1.317170| lrm: 0.318117| num_tokens: 6,838\n",
      "Step 00479/00701 | Training loss: 0.757286| lrm: 0.316690| num_tokens: 13,516\n",
      "Step 00480/00701 | Training loss: 0.405596| lrm: 0.315264| num_tokens: 13,587\n",
      "Step 00481/00701 | Training loss: 1.080713| lrm: 0.313837| num_tokens: 7,015\n",
      "Step 00482/00701 | Training loss: 0.980755| lrm: 0.312411| num_tokens: 9,863\n",
      "Step 00483/00701 | Training loss: 0.688063| lrm: 0.310984| num_tokens: 11,048\n",
      "Step 00484/00701 | Training loss: 0.495362| lrm: 0.309558| num_tokens: 10,661\n",
      "Step 00485/00701 | Training loss: 1.126765| lrm: 0.308131| num_tokens: 10,617\n",
      "Step 00486/00701 | Training loss: 0.395944| lrm: 0.306705| num_tokens: 9,656\n",
      "Step 00487/00701 | Training loss: 0.664867| lrm: 0.305278| num_tokens: 8,590\n",
      "Step 00488/00701 | Training loss: 0.441281| lrm: 0.303852| num_tokens: 11,260\n",
      "Step 00489/00701 | Training loss: 0.947282| lrm: 0.302425| num_tokens: 12,388\n",
      "Step 00490/00701 | Training loss: 0.890746| lrm: 0.300999| num_tokens: 8,471\n",
      "Step 00491/00701 | Training loss: 0.927261| lrm: 0.299572| num_tokens: 8,826\n",
      "Step 00492/00701 | Training loss: 1.446102| lrm: 0.298146| num_tokens: 14,321\n",
      "Step 00493/00701 | Training loss: 0.571892| lrm: 0.296719| num_tokens: 7,284\n",
      "Step 00494/00701 | Training loss: 0.540016| lrm: 0.295292| num_tokens: 8,592\n",
      "Step 00495/00701 | Training loss: 0.869151| lrm: 0.293866| num_tokens: 10,854\n",
      "Step 00496/00701 | Training loss: 0.761761| lrm: 0.292439| num_tokens: 10,975\n",
      "Step 00497/00701 | Training loss: 0.724380| lrm: 0.291013| num_tokens: 11,925\n",
      "Step 00498/00701 | Training loss: 0.978017| lrm: 0.289586| num_tokens: 11,744\n",
      "Step 00499/00701 | Training loss: 0.294904| lrm: 0.288160| num_tokens: 14,263\n",
      "Step 00500 | Validation loss: 1.013905\n",
      "Step 00500/00701 | Training loss: 0.895635| lrm: 0.286733| num_tokens: 13,673\n",
      "Step 00501/00701 | Training loss: 0.629991| lrm: 0.285307| num_tokens: 9,954\n",
      "Step 00502/00701 | Training loss: 0.438105| lrm: 0.283880| num_tokens: 9,783\n",
      "Step 00503/00701 | Training loss: 1.390097| lrm: 0.282454| num_tokens: 13,075\n",
      "Step 00504/00701 | Training loss: 1.556905| lrm: 0.281027| num_tokens: 9,767\n",
      "Step 00505/00701 | Training loss: 0.831744| lrm: 0.279601| num_tokens: 8,895\n",
      "Step 00506/00701 | Training loss: 1.359142| lrm: 0.278174| num_tokens: 9,652\n",
      "Step 00507/00701 | Training loss: 0.592218| lrm: 0.276748| num_tokens: 6,756\n",
      "Step 00508/00701 | Training loss: 1.330125| lrm: 0.275321| num_tokens: 18,637\n",
      "Step 00509/00701 | Training loss: 0.861776| lrm: 0.273894| num_tokens: 9,678\n",
      "Step 00510/00701 | Training loss: 0.765998| lrm: 0.272468| num_tokens: 9,942\n",
      "Step 00511/00701 | Training loss: 0.578443| lrm: 0.271041| num_tokens: 5,586\n",
      "Step 00512/00701 | Training loss: 1.013055| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.790395| lrm: 0.268188| num_tokens: 8,743\n",
      "Step 00514/00701 | Training loss: 0.856886| lrm: 0.266762| num_tokens: 9,904\n",
      "Step 00515/00701 | Training loss: 1.181182| lrm: 0.265335| num_tokens: 10,734\n",
      "Step 00516/00701 | Training loss: 1.216494| lrm: 0.263909| num_tokens: 12,039\n",
      "Step 00517/00701 | Training loss: 0.548476| lrm: 0.262482| num_tokens: 10,739\n",
      "Step 00518/00701 | Training loss: 1.135129| lrm: 0.261056| num_tokens: 10,288\n",
      "Step 00519/00701 | Training loss: 0.849683| lrm: 0.259629| num_tokens: 8,447\n",
      "Step 00520/00701 | Training loss: 0.863580| lrm: 0.258203| num_tokens: 9,737\n",
      "Step 00521/00701 | Training loss: 0.971205| lrm: 0.256776| num_tokens: 14,760\n",
      "Step 00522/00701 | Training loss: 1.078097| lrm: 0.255350| num_tokens: 9,452\n",
      "Step 00523/00701 | Training loss: 0.653865| lrm: 0.253923| num_tokens: 11,752\n",
      "Step 00524/00701 | Training loss: 0.975016| lrm: 0.252496| num_tokens: 7,305\n",
      "Step 00525 | Validation loss: 1.013812\n",
      "Step 00525/00701 | Training loss: 1.540869| lrm: 0.251070| num_tokens: 10,430\n",
      "Step 00526/00701 | Training loss: 1.127627| lrm: 0.249643| num_tokens: 9,485\n",
      "Step 00527/00701 | Training loss: 1.166523| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 1.189167| lrm: 0.246790| num_tokens: 14,142\n",
      "Step 00529/00701 | Training loss: 1.138450| lrm: 0.245364| num_tokens: 14,963\n",
      "Step 00530/00701 | Training loss: 0.463651| lrm: 0.243937| num_tokens: 8,920\n",
      "Step 00531/00701 | Training loss: 0.614086| lrm: 0.242511| num_tokens: 10,497\n",
      "Step 00532/00701 | Training loss: 1.154144| lrm: 0.241084| num_tokens: 8,047\n",
      "Step 00533/00701 | Training loss: 1.048627| lrm: 0.239658| num_tokens: 11,665\n",
      "Step 00534/00701 | Training loss: 1.164469| lrm: 0.238231| num_tokens: 12,661\n",
      "Step 00535/00701 | Training loss: 0.645072| lrm: 0.236805| num_tokens: 9,332\n",
      "Step 00536/00701 | Training loss: 0.510616| lrm: 0.235378| num_tokens: 10,270\n",
      "Step 00537/00701 | Training loss: 0.694718| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.646773| lrm: 0.232525| num_tokens: 10,264\n",
      "Step 00539/00701 | Training loss: 0.808624| lrm: 0.231098| num_tokens: 13,034\n",
      "Step 00540/00701 | Training loss: 0.814689| lrm: 0.229672| num_tokens: 7,601\n",
      "Step 00541/00701 | Training loss: 1.028647| lrm: 0.228245| num_tokens: 15,932\n",
      "Step 00542/00701 | Training loss: 1.242172| lrm: 0.226819| num_tokens: 10,867\n",
      "Step 00543/00701 | Training loss: 0.368499| lrm: 0.225392| num_tokens: 11,887\n",
      "Step 00544/00701 | Training loss: 0.968831| lrm: 0.223966| num_tokens: 11,865\n",
      "Step 00545/00701 | Training loss: 0.765257| lrm: 0.222539| num_tokens: 13,438\n",
      "Step 00546/00701 | Training loss: 0.794371| lrm: 0.221113| num_tokens: 13,844\n",
      "Step 00547/00701 | Training loss: 1.263812| lrm: 0.219686| num_tokens: 6,023\n",
      "Step 00548/00701 | Training loss: 0.742048| lrm: 0.218260| num_tokens: 13,321\n",
      "Step 00549/00701 | Training loss: 1.074258| lrm: 0.216833| num_tokens: 8,358\n",
      "Step 00550 | Validation loss: 1.013607\n",
      "Step 00550/00701 | Training loss: 0.633758| lrm: 0.215407| num_tokens: 13,487\n",
      "Step 00551/00701 | Training loss: 0.763589| lrm: 0.213980| num_tokens: 11,180\n",
      "Step 00552/00701 | Training loss: 0.773540| lrm: 0.212553| num_tokens: 8,538\n",
      "Step 00553/00701 | Training loss: 0.257355| lrm: 0.211127| num_tokens: 10,572\n",
      "Step 00554/00701 | Training loss: 0.976021| lrm: 0.209700| num_tokens: 12,982\n",
      "Step 00555/00701 | Training loss: 0.892144| lrm: 0.208274| num_tokens: 11,588\n",
      "Step 00556/00701 | Training loss: 0.912548| lrm: 0.206847| num_tokens: 7,432\n",
      "Step 00557/00701 | Training loss: 0.651532| lrm: 0.205421| num_tokens: 9,354\n",
      "Step 00558/00701 | Training loss: 0.881066| lrm: 0.203994| num_tokens: 10,187\n",
      "Step 00559/00701 | Training loss: 1.005775| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560/00701 | Training loss: 0.509841| lrm: 0.201141| num_tokens: 13,597\n",
      "Step 00561/00701 | Training loss: 1.171588| lrm: 0.199715| num_tokens: 6,689\n",
      "Step 00562/00701 | Training loss: 0.242795| lrm: 0.198288| num_tokens: 8,682\n",
      "Step 00563/00701 | Training loss: 0.940966| lrm: 0.196862| num_tokens: 12,881\n",
      "Step 00564/00701 | Training loss: 0.944668| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 1.070088| lrm: 0.194009| num_tokens: 13,874\n",
      "Step 00566/00701 | Training loss: 0.997429| lrm: 0.192582| num_tokens: 12,213\n",
      "Step 00567/00701 | Training loss: 0.921782| lrm: 0.191155| num_tokens: 18,532\n",
      "Step 00568/00701 | Training loss: 0.701802| lrm: 0.189729| num_tokens: 10,003\n",
      "Step 00569/00701 | Training loss: 0.892025| lrm: 0.188302| num_tokens: 9,476\n",
      "Step 00570/00701 | Training loss: 1.257241| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.721322| lrm: 0.185449| num_tokens: 13,088\n",
      "Step 00572/00701 | Training loss: 0.690830| lrm: 0.184023| num_tokens: 7,260\n",
      "Step 00573/00701 | Training loss: 1.087654| lrm: 0.182596| num_tokens: 10,391\n",
      "Step 00574/00701 | Training loss: 0.744796| lrm: 0.181170| num_tokens: 14,540\n",
      "Step 00575 | Validation loss: 1.013660\n",
      "Step 00575/00701 | Training loss: 0.424153| lrm: 0.179743| num_tokens: 7,583\n",
      "Step 00576/00701 | Training loss: 1.752447| lrm: 0.178317| num_tokens: 8,834\n",
      "Step 00577/00701 | Training loss: 0.723040| lrm: 0.176890| num_tokens: 13,422\n",
      "Step 00578/00701 | Training loss: 0.578224| lrm: 0.175464| num_tokens: 8,190\n",
      "Step 00579/00701 | Training loss: 0.488362| lrm: 0.174037| num_tokens: 10,220\n",
      "Step 00580/00701 | Training loss: 0.994607| lrm: 0.172611| num_tokens: 9,082\n",
      "Step 00581/00701 | Training loss: 1.045885| lrm: 0.171184| num_tokens: 13,484\n",
      "Step 00582/00701 | Training loss: 1.097919| lrm: 0.169757| num_tokens: 8,858\n",
      "Step 00583/00701 | Training loss: 0.591561| lrm: 0.168331| num_tokens: 12,740\n",
      "Step 00584/00701 | Training loss: 0.824342| lrm: 0.166904| num_tokens: 13,825\n",
      "Step 00585/00701 | Training loss: 0.677158| lrm: 0.165478| num_tokens: 14,923\n",
      "Step 00586/00701 | Training loss: 0.977670| lrm: 0.164051| num_tokens: 9,923\n",
      "Step 00587/00701 | Training loss: 0.767077| lrm: 0.162625| num_tokens: 7,792\n",
      "Step 00588/00701 | Training loss: 0.932080| lrm: 0.161198| num_tokens: 12,331\n",
      "Step 00589/00701 | Training loss: 0.472680| lrm: 0.159772| num_tokens: 9,040\n",
      "Step 00590/00701 | Training loss: 0.545042| lrm: 0.158345| num_tokens: 10,334\n",
      "Step 00591/00701 | Training loss: 0.759983| lrm: 0.156919| num_tokens: 8,577\n",
      "Step 00592/00701 | Training loss: 0.980561| lrm: 0.155492| num_tokens: 11,743\n",
      "Step 00593/00701 | Training loss: 1.069966| lrm: 0.154066| num_tokens: 11,795\n",
      "Step 00594/00701 | Training loss: 0.248874| lrm: 0.152639| num_tokens: 11,224\n",
      "Step 00595/00701 | Training loss: 0.461772| lrm: 0.151213| num_tokens: 8,408\n",
      "Step 00596/00701 | Training loss: 0.745493| lrm: 0.149786| num_tokens: 5,658\n",
      "Step 00597/00701 | Training loss: 1.320049| lrm: 0.148359| num_tokens: 10,836\n",
      "Step 00598/00701 | Training loss: 0.748893| lrm: 0.146933| num_tokens: 9,759\n",
      "Step 00599/00701 | Training loss: 1.424068| lrm: 0.145506| num_tokens: 8,267\n",
      "Step 00600 | Validation loss: 1.013714\n",
      "final: 363/1024 (35.45%)\n",
      "final: 469/1024 (45.80%)\n",
      "Step 00600 | mmlu_acc: 0.354492, arc_easy_acc: 0.458008\n",
      "Step 00600/00701 | Training loss: 0.723380| lrm: 0.144080| num_tokens: 10,446\n",
      "Step 00601/00701 | Training loss: 1.064465| lrm: 0.142653| num_tokens: 11,003\n",
      "Step 00602/00701 | Training loss: 0.785875| lrm: 0.141227| num_tokens: 8,234\n",
      "Step 00603/00701 | Training loss: 1.098591| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.965271| lrm: 0.138374| num_tokens: 13,517\n",
      "Step 00605/00701 | Training loss: 0.762046| lrm: 0.136947| num_tokens: 10,453\n",
      "Step 00606/00701 | Training loss: 0.927332| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.645033| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.612977| lrm: 0.132668| num_tokens: 12,833\n",
      "Step 00609/00701 | Training loss: 1.124654| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610/00701 | Training loss: 0.497692| lrm: 0.129815| num_tokens: 14,179\n",
      "Step 00611/00701 | Training loss: 0.972622| lrm: 0.128388| num_tokens: 9,321\n",
      "Step 00612/00701 | Training loss: 0.846366| lrm: 0.126961| num_tokens: 9,523\n",
      "Step 00613/00701 | Training loss: 1.079903| lrm: 0.125535| num_tokens: 16,542\n",
      "Step 00614/00701 | Training loss: 0.551494| lrm: 0.124108| num_tokens: 15,348\n",
      "Step 00615/00701 | Training loss: 0.921877| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.618798| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.719866| lrm: 0.119829| num_tokens: 11,180\n",
      "Step 00618/00701 | Training loss: 0.565163| lrm: 0.118402| num_tokens: 8,842\n",
      "Step 00619/00701 | Training loss: 0.705776| lrm: 0.116976| num_tokens: 9,213\n",
      "Step 00620/00701 | Training loss: 0.640088| lrm: 0.115549| num_tokens: 12,340\n",
      "Step 00621/00701 | Training loss: 0.484958| lrm: 0.114123| num_tokens: 7,718\n",
      "Step 00622/00701 | Training loss: 0.964482| lrm: 0.112696| num_tokens: 7,350\n",
      "Step 00623/00701 | Training loss: 0.904016| lrm: 0.111270| num_tokens: 14,519\n",
      "Step 00624/00701 | Training loss: 1.413878| lrm: 0.109843| num_tokens: 12,368\n",
      "Step 00625 | Validation loss: 1.013754\n",
      "Step 00625/00701 | Training loss: 0.577413| lrm: 0.108417| num_tokens: 6,842\n",
      "Step 00626/00701 | Training loss: 0.732772| lrm: 0.106990| num_tokens: 11,577\n",
      "Step 00627/00701 | Training loss: 0.487635| lrm: 0.105563| num_tokens: 8,498\n",
      "Step 00628/00701 | Training loss: 0.769856| lrm: 0.104137| num_tokens: 6,965\n",
      "Step 00629/00701 | Training loss: 0.607147| lrm: 0.102710| num_tokens: 12,391\n",
      "Step 00630/00701 | Training loss: 0.562338| lrm: 0.101284| num_tokens: 10,252\n",
      "Step 00631/00701 | Training loss: 0.767581| lrm: 0.099857| num_tokens: 12,194\n",
      "Step 00632/00701 | Training loss: 0.644749| lrm: 0.098431| num_tokens: 10,964\n",
      "Step 00633/00701 | Training loss: 0.911044| lrm: 0.097004| num_tokens: 11,198\n",
      "Step 00634/00701 | Training loss: 1.007322| lrm: 0.095578| num_tokens: 7,808\n",
      "Step 00635/00701 | Training loss: 0.765236| lrm: 0.094151| num_tokens: 12,053\n",
      "Step 00636/00701 | Training loss: 0.800849| lrm: 0.092725| num_tokens: 8,138\n",
      "Step 00637/00701 | Training loss: 0.860442| lrm: 0.091298| num_tokens: 13,101\n",
      "Step 00638/00701 | Training loss: 0.563172| lrm: 0.089872| num_tokens: 8,339\n",
      "Step 00639/00701 | Training loss: 0.688798| lrm: 0.088445| num_tokens: 13,057\n",
      "Step 00640/00701 | Training loss: 0.971889| lrm: 0.087019| num_tokens: 12,703\n",
      "Step 00641/00701 | Training loss: 0.911186| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.350464| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.889130| lrm: 0.082739| num_tokens: 8,878\n",
      "Step 00644/00701 | Training loss: 0.839599| lrm: 0.081312| num_tokens: 8,861\n",
      "Step 00645/00701 | Training loss: 1.239201| lrm: 0.079886| num_tokens: 13,446\n",
      "Step 00646/00701 | Training loss: 0.758240| lrm: 0.078459| num_tokens: 12,789\n",
      "Step 00647/00701 | Training loss: 1.062389| lrm: 0.077033| num_tokens: 8,241\n",
      "Step 00648/00701 | Training loss: 0.856197| lrm: 0.075606| num_tokens: 10,330\n",
      "Step 00649/00701 | Training loss: 0.663327| lrm: 0.074180| num_tokens: 12,278\n",
      "Step 00650 | Validation loss: 1.013768\n",
      "Step 00650/00701 | Training loss: 1.329244| lrm: 0.072753| num_tokens: 7,245\n",
      "Step 00651/00701 | Training loss: 1.297611| lrm: 0.071327| num_tokens: 13,989\n",
      "Step 00652/00701 | Training loss: 1.143795| lrm: 0.069900| num_tokens: 11,516\n",
      "Step 00653/00701 | Training loss: 0.644582| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.799120| lrm: 0.067047| num_tokens: 8,702\n",
      "Step 00655/00701 | Training loss: 0.892061| lrm: 0.065621| num_tokens: 11,510\n",
      "Step 00656/00701 | Training loss: 1.253838| lrm: 0.064194| num_tokens: 11,144\n",
      "Step 00657/00701 | Training loss: 0.389799| lrm: 0.062767| num_tokens: 10,336\n",
      "Step 00658/00701 | Training loss: 1.089223| lrm: 0.061341| num_tokens: 9,779\n",
      "Step 00659/00701 | Training loss: 0.744336| lrm: 0.059914| num_tokens: 8,105\n",
      "Step 00660/00701 | Training loss: 0.794556| lrm: 0.058488| num_tokens: 10,476\n",
      "Step 00661/00701 | Training loss: 1.118363| lrm: 0.057061| num_tokens: 11,310\n",
      "Step 00662/00701 | Training loss: 0.642573| lrm: 0.055635| num_tokens: 7,871\n",
      "Step 00663/00701 | Training loss: 0.708490| lrm: 0.054208| num_tokens: 9,855\n",
      "Step 00664/00701 | Training loss: 0.667032| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.655358| lrm: 0.051355| num_tokens: 9,697\n",
      "Step 00666/00701 | Training loss: 1.036489| lrm: 0.049929| num_tokens: 10,710\n",
      "Step 00667/00701 | Training loss: 0.849911| lrm: 0.048502| num_tokens: 13,511\n",
      "Step 00668/00701 | Training loss: 0.641352| lrm: 0.047076| num_tokens: 11,917\n",
      "Step 00669/00701 | Training loss: 1.120035| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670/00701 | Training loss: 0.755385| lrm: 0.044223| num_tokens: 9,513\n",
      "Step 00671/00701 | Training loss: 0.789511| lrm: 0.042796| num_tokens: 10,895\n",
      "Step 00672/00701 | Training loss: 0.653746| lrm: 0.041369| num_tokens: 10,537\n",
      "Step 00673/00701 | Training loss: 0.694198| lrm: 0.039943| num_tokens: 9,859\n",
      "Step 00674/00701 | Training loss: 0.622455| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675 | Validation loss: 1.013749\n",
      "Step 00675/00701 | Training loss: 0.611741| lrm: 0.037090| num_tokens: 10,400\n",
      "Step 00676/00701 | Training loss: 0.859537| lrm: 0.035663| num_tokens: 10,101\n",
      "Step 00677/00701 | Training loss: 1.020869| lrm: 0.034237| num_tokens: 16,334\n",
      "Step 00678/00701 | Training loss: 0.339280| lrm: 0.032810| num_tokens: 12,035\n",
      "Step 00679/00701 | Training loss: 0.790631| lrm: 0.031384| num_tokens: 8,362\n",
      "Step 00680/00701 | Training loss: 1.207559| lrm: 0.029957| num_tokens: 12,050\n",
      "Step 00681/00701 | Training loss: 0.679650| lrm: 0.028531| num_tokens: 9,425\n",
      "Step 00682/00701 | Training loss: 0.719481| lrm: 0.027104| num_tokens: 14,504\n",
      "Step 00683/00701 | Training loss: 0.925945| lrm: 0.025678| num_tokens: 9,745\n",
      "Step 00684/00701 | Training loss: 1.595622| lrm: 0.024251| num_tokens: 12,770\n",
      "Step 00685/00701 | Training loss: 0.594208| lrm: 0.022825| num_tokens: 9,268\n",
      "Step 00686/00701 | Training loss: 0.530522| lrm: 0.021398| num_tokens: 10,168\n",
      "Step 00687/00701 | Training loss: 0.782289| lrm: 0.019971| num_tokens: 13,350\n",
      "Step 00688/00701 | Training loss: 0.705681| lrm: 0.018545| num_tokens: 10,458\n",
      "Step 00689/00701 | Training loss: 0.504768| lrm: 0.017118| num_tokens: 10,652\n",
      "Step 00690/00701 | Training loss: 0.424696| lrm: 0.015692| num_tokens: 8,627\n",
      "Step 00691/00701 | Training loss: 0.544062| lrm: 0.014265| num_tokens: 10,642\n",
      "Step 00692/00701 | Training loss: 0.909885| lrm: 0.012839| num_tokens: 11,930\n",
      "Step 00693/00701 | Training loss: 1.140530| lrm: 0.011412| num_tokens: 9,686\n",
      "Step 00694/00701 | Training loss: 1.221102| lrm: 0.009986| num_tokens: 10,109\n",
      "Step 00695/00701 | Training loss: 1.040090| lrm: 0.008559| num_tokens: 8,528\n",
      "Step 00696/00701 | Training loss: 0.528351| lrm: 0.007133| num_tokens: 12,544\n",
      "Step 00697/00701 | Training loss: 1.213478| lrm: 0.005706| num_tokens: 10,314\n",
      "Step 00698/00701 | Training loss: 0.544541| lrm: 0.004280| num_tokens: 8,859\n",
      "Step 00699/00701 | Training loss: 0.529829| lrm: 0.002853| num_tokens: 11,147\n",
      "Step 00700 | Validation loss: 1.013690\n",
      "final: 360/1024 (35.16%)\n",
      "final: 459/1024 (44.82%)\n",
      "Step 00700 | mmlu_acc: 0.351562, arc_easy_acc: 0.448242\n",
      "[W1121 19:55:17.606639571 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:17.649866698 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:17.667583310 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:17.704811824 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:17.772276444 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 4.2KB/4.2KB (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 4.2KB/4.2KB (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading config.yaml 4.2KB/4.2KB (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading config.yaml 4.2KB/4.2KB (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading config.yaml 4.2KB/4.2KB (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–â–„â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–â–†â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–ƒâ–†â–…â–„â–†â–ˆâ–†â–ƒâ–ƒâ–…â–ƒâ–‚â–ƒâ–†â–ƒâ–ƒâ–…â–ƒâ–†â–ƒâ–â–…â–‡â–…â–ƒâ–â–‚â–†â–‚â–„â–…â–ƒâ–ƒâ–‚â–…â–ƒâ–‚â–„â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–â–‚â–‡â–†â–†â–†â–â–†â–„â–„â–ƒâ–ƒâ–„â–…â–‚â–ƒâ–ƒâ–â–ƒâ–†â–„â–…â–‡â–‚â–…â–„â–ƒâ–†â–‚â–‚â–‚â–„â–‡â–ˆâ–…â–„â–…â–ˆâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–ƒâ–‡â–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.44824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.35156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 0.52983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-32-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/pvbowvrd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_195202-pvbowvrd/logs\u001b[0m\n",
      "[W1121 19:55:22.575312485 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:22.669903917 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 19:55:22.965219088 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --eval_every=25 --run=challenge-32-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a5d4e-b964-4df1-b615-3aae94a11c13",
   "metadata": {},
   "source": [
    "#### Redo with...\n",
    "\n",
    "While here, redo with even more frequent evaluation of val loss, more steps for each evaluation, but skip evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f59bde1-2fd7-4af7-97db-d6481e808686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 19:56:39.029000 23301 torch/distributed/run.py:803] \n",
      "W1121 19:56:39.029000 23301 torch/distributed/run.py:803] *****************************************\n",
      "W1121 19:56:39.029000 23301 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 19:56:39.029000 23301 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding eval_every = 10\n",
      "overriding eval_steps = 200\n",
      "overriding eval_metrics_every = 1000\n",
      "overriding run = challenge-32-2\n",
      "user_config: {'run': 'challenge-32-2', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 10, 'eval_steps': 200, 'eval_metrics_every': 1000, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run vvrkum95 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run vvrkum95 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m setting up run vvrkum95 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m setting up run vvrkum95 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-32-redo-chat-eval-d20/wandb/run-20251121_195651-vvrkum95\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-32-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/vvrkum95\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.019773\n",
      "Step 00000/00701 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00701 | Training loss: 0.765644| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 1.093265| lrm: 0.997147| num_tokens: 11,283\n",
      "Step 00003/00701 | Training loss: 1.215839| lrm: 0.995720| num_tokens: 12,682\n",
      "Step 00004/00701 | Training loss: 0.786859| lrm: 0.994294| num_tokens: 9,509\n",
      "Step 00005/00701 | Training loss: 0.846886| lrm: 0.992867| num_tokens: 8,344\n",
      "Step 00006/00701 | Training loss: 0.690973| lrm: 0.991441| num_tokens: 8,763\n",
      "Step 00007/00701 | Training loss: 0.417658| lrm: 0.990014| num_tokens: 10,664\n",
      "Step 00008/00701 | Training loss: 1.030312| lrm: 0.988588| num_tokens: 11,584\n",
      "Step 00009/00701 | Training loss: 0.566435| lrm: 0.987161| num_tokens: 9,920\n",
      "Step 00010 | Validation loss: 1.020069\n",
      "Step 00010/00701 | Training loss: 0.404886| lrm: 0.985735| num_tokens: 15,002\n",
      "Step 00011/00701 | Training loss: 1.073687| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.879981| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.957063| lrm: 0.981455| num_tokens: 10,802\n",
      "Step 00014/00701 | Training loss: 0.768687| lrm: 0.980029| num_tokens: 7,541\n",
      "Step 00015/00701 | Training loss: 0.879559| lrm: 0.978602| num_tokens: 10,515\n",
      "Step 00016/00701 | Training loss: 1.074760| lrm: 0.977175| num_tokens: 14,469\n",
      "Step 00017/00701 | Training loss: 0.539607| lrm: 0.975749| num_tokens: 7,715\n",
      "Step 00018/00701 | Training loss: 1.229464| lrm: 0.974322| num_tokens: 11,655\n",
      "Step 00019/00701 | Training loss: 0.591836| lrm: 0.972896| num_tokens: 11,228\n",
      "Step 00020 | Validation loss: 1.020790\n",
      "Step 00020/00701 | Training loss: 0.410747| lrm: 0.971469| num_tokens: 10,678\n",
      "Step 00021/00701 | Training loss: 0.761239| lrm: 0.970043| num_tokens: 12,324\n",
      "Step 00022/00701 | Training loss: 1.112874| lrm: 0.968616| num_tokens: 9,331\n",
      "Step 00023/00701 | Training loss: 0.692753| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.845410| lrm: 0.965763| num_tokens: 10,715\n",
      "Step 00025/00701 | Training loss: 0.629688| lrm: 0.964337| num_tokens: 12,383\n",
      "Step 00026/00701 | Training loss: 1.035196| lrm: 0.962910| num_tokens: 12,576\n",
      "Step 00027/00701 | Training loss: 0.918767| lrm: 0.961484| num_tokens: 8,359\n",
      "Step 00028/00701 | Training loss: 0.931334| lrm: 0.960057| num_tokens: 10,178\n",
      "Step 00029/00701 | Training loss: 0.935726| lrm: 0.958631| num_tokens: 10,629\n",
      "Step 00030 | Validation loss: 1.021778\n",
      "Step 00030/00701 | Training loss: 0.588895| lrm: 0.957204| num_tokens: 9,143\n",
      "Step 00031/00701 | Training loss: 1.143904| lrm: 0.955777| num_tokens: 7,481\n",
      "Step 00032/00701 | Training loss: 0.861302| lrm: 0.954351| num_tokens: 9,972\n",
      "Step 00033/00701 | Training loss: 0.839960| lrm: 0.952924| num_tokens: 11,270\n",
      "Step 00034/00701 | Training loss: 1.042606| lrm: 0.951498| num_tokens: 11,177\n",
      "Step 00035/00701 | Training loss: 0.846186| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.930600| lrm: 0.948645| num_tokens: 9,343\n",
      "Step 00037/00701 | Training loss: 1.067322| lrm: 0.947218| num_tokens: 10,954\n",
      "Step 00038/00701 | Training loss: 0.720290| lrm: 0.945792| num_tokens: 6,793\n",
      "Step 00039/00701 | Training loss: 0.610579| lrm: 0.944365| num_tokens: 5,849\n",
      "Step 00040 | Validation loss: 1.022777\n",
      "Step 00040/00701 | Training loss: 0.941526| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.619544| lrm: 0.941512| num_tokens: 14,344\n",
      "Step 00042/00701 | Training loss: 1.181919| lrm: 0.940086| num_tokens: 11,223\n",
      "Step 00043/00701 | Training loss: 0.999029| lrm: 0.938659| num_tokens: 11,061\n",
      "Step 00044/00701 | Training loss: 1.084281| lrm: 0.937233| num_tokens: 12,666\n",
      "Step 00045/00701 | Training loss: 1.066277| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.688366| lrm: 0.934379| num_tokens: 9,630\n",
      "Step 00047/00701 | Training loss: 1.079397| lrm: 0.932953| num_tokens: 12,891\n",
      "Step 00048/00701 | Training loss: 0.732694| lrm: 0.931526| num_tokens: 10,059\n",
      "Step 00049/00701 | Training loss: 0.537439| lrm: 0.930100| num_tokens: 11,237\n",
      "Step 00050 | Validation loss: 1.023494\n",
      "Step 00050/00701 | Training loss: 0.757994| lrm: 0.928673| num_tokens: 9,079\n",
      "Step 00051/00701 | Training loss: 0.275688| lrm: 0.927247| num_tokens: 9,590\n",
      "Step 00052/00701 | Training loss: 0.831239| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 1.002588| lrm: 0.924394| num_tokens: 7,924\n",
      "Step 00054/00701 | Training loss: 0.374015| lrm: 0.922967| num_tokens: 9,631\n",
      "Step 00055/00701 | Training loss: 0.898350| lrm: 0.921541| num_tokens: 6,508\n",
      "Step 00056/00701 | Training loss: 1.396053| lrm: 0.920114| num_tokens: 10,513\n",
      "Step 00057/00701 | Training loss: 1.008685| lrm: 0.918688| num_tokens: 9,782\n",
      "Step 00058/00701 | Training loss: 1.360180| lrm: 0.917261| num_tokens: 13,372\n",
      "Step 00059/00701 | Training loss: 1.059755| lrm: 0.915835| num_tokens: 7,944\n",
      "Step 00060 | Validation loss: 1.023959\n",
      "Step 00060/00701 | Training loss: 1.141880| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.256442| lrm: 0.912981| num_tokens: 13,711\n",
      "Step 00062/00701 | Training loss: 0.938621| lrm: 0.911555| num_tokens: 12,159\n",
      "Step 00063/00701 | Training loss: 0.573507| lrm: 0.910128| num_tokens: 10,617\n",
      "Step 00064/00701 | Training loss: 0.669048| lrm: 0.908702| num_tokens: 8,098\n",
      "Step 00065/00701 | Training loss: 0.488126| lrm: 0.907275| num_tokens: 11,033\n",
      "Step 00066/00701 | Training loss: 1.024594| lrm: 0.905849| num_tokens: 9,531\n",
      "Step 00067/00701 | Training loss: 0.553682| lrm: 0.904422| num_tokens: 11,573\n",
      "Step 00068/00701 | Training loss: 1.318847| lrm: 0.902996| num_tokens: 12,923\n",
      "Step 00069/00701 | Training loss: 0.845823| lrm: 0.901569| num_tokens: 7,943\n",
      "Step 00070 | Validation loss: 1.024193\n",
      "Step 00070/00701 | Training loss: 0.475501| lrm: 0.900143| num_tokens: 10,775\n",
      "Step 00071/00701 | Training loss: 0.734144| lrm: 0.898716| num_tokens: 9,193\n",
      "Step 00072/00701 | Training loss: 0.948009| lrm: 0.897290| num_tokens: 9,378\n",
      "Step 00073/00701 | Training loss: 1.220835| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.725542| lrm: 0.894437| num_tokens: 7,685\n",
      "Step 00075/00701 | Training loss: 0.701049| lrm: 0.893010| num_tokens: 6,807\n",
      "Step 00076/00701 | Training loss: 0.831552| lrm: 0.891583| num_tokens: 7,530\n",
      "Step 00077/00701 | Training loss: 0.856039| lrm: 0.890157| num_tokens: 9,319\n",
      "Step 00078/00701 | Training loss: 0.613650| lrm: 0.888730| num_tokens: 11,560\n",
      "Step 00079/00701 | Training loss: 0.583249| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080 | Validation loss: 1.024254\n",
      "Step 00080/00701 | Training loss: 1.004000| lrm: 0.885877| num_tokens: 11,674\n",
      "Step 00081/00701 | Training loss: 0.514955| lrm: 0.884451| num_tokens: 7,783\n",
      "Step 00082/00701 | Training loss: 0.683685| lrm: 0.883024| num_tokens: 14,544\n",
      "Step 00083/00701 | Training loss: 0.539446| lrm: 0.881598| num_tokens: 11,069\n",
      "Step 00084/00701 | Training loss: 0.749653| lrm: 0.880171| num_tokens: 13,568\n",
      "Step 00085/00701 | Training loss: 1.148319| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.990841| lrm: 0.877318| num_tokens: 9,908\n",
      "Step 00087/00701 | Training loss: 1.019954| lrm: 0.875892| num_tokens: 9,627\n",
      "Step 00088/00701 | Training loss: 0.861117| lrm: 0.874465| num_tokens: 11,064\n",
      "Step 00089/00701 | Training loss: 1.391632| lrm: 0.873039| num_tokens: 9,523\n",
      "Step 00090 | Validation loss: 1.024125\n",
      "Step 00090/00701 | Training loss: 0.960510| lrm: 0.871612| num_tokens: 10,566\n",
      "Step 00091/00701 | Training loss: 0.750845| lrm: 0.870185| num_tokens: 10,761\n",
      "Step 00092/00701 | Training loss: 0.868610| lrm: 0.868759| num_tokens: 10,418\n",
      "Step 00093/00701 | Training loss: 0.866674| lrm: 0.867332| num_tokens: 9,591\n",
      "Step 00094/00701 | Training loss: 0.728941| lrm: 0.865906| num_tokens: 9,464\n",
      "Step 00095/00701 | Training loss: 0.956923| lrm: 0.864479| num_tokens: 9,395\n",
      "Step 00096/00701 | Training loss: 1.120106| lrm: 0.863053| num_tokens: 13,239\n",
      "Step 00097/00701 | Training loss: 1.008739| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.756371| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 1.139280| lrm: 0.858773| num_tokens: 8,989\n",
      "Step 00100 | Validation loss: 1.023871\n",
      "Step 00100/00701 | Training loss: 1.957842| lrm: 0.857347| num_tokens: 10,527\n",
      "Step 00101/00701 | Training loss: 0.440057| lrm: 0.855920| num_tokens: 9,838\n",
      "Step 00102/00701 | Training loss: 0.567389| lrm: 0.854494| num_tokens: 10,665\n",
      "Step 00103/00701 | Training loss: 0.716657| lrm: 0.853067| num_tokens: 11,272\n",
      "Step 00104/00701 | Training loss: 0.710491| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.768463| lrm: 0.850214| num_tokens: 10,153\n",
      "Step 00106/00701 | Training loss: 0.828154| lrm: 0.848787| num_tokens: 11,156\n",
      "Step 00107/00701 | Training loss: 1.047368| lrm: 0.847361| num_tokens: 10,161\n",
      "Step 00108/00701 | Training loss: 0.940920| lrm: 0.845934| num_tokens: 8,469\n",
      "Step 00109/00701 | Training loss: 0.683787| lrm: 0.844508| num_tokens: 8,740\n",
      "Step 00110 | Validation loss: 1.023720\n",
      "Step 00110/00701 | Training loss: 0.817427| lrm: 0.843081| num_tokens: 12,241\n",
      "Step 00111/00701 | Training loss: 0.707103| lrm: 0.841655| num_tokens: 10,737\n",
      "Step 00112/00701 | Training loss: 1.483258| lrm: 0.840228| num_tokens: 14,866\n",
      "Step 00113/00701 | Training loss: 0.758820| lrm: 0.838802| num_tokens: 16,098\n",
      "Step 00114/00701 | Training loss: 1.107895| lrm: 0.837375| num_tokens: 14,309\n",
      "Step 00115/00701 | Training loss: 0.319969| lrm: 0.835949| num_tokens: 11,090\n",
      "Step 00116/00701 | Training loss: 0.797398| lrm: 0.834522| num_tokens: 9,127\n",
      "Step 00117/00701 | Training loss: 0.922887| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.644102| lrm: 0.831669| num_tokens: 9,686\n",
      "Step 00119/00701 | Training loss: 0.644645| lrm: 0.830243| num_tokens: 14,012\n",
      "Step 00120 | Validation loss: 1.023253\n",
      "Step 00120/00701 | Training loss: 0.973334| lrm: 0.828816| num_tokens: 12,993\n",
      "Step 00121/00701 | Training loss: 1.081658| lrm: 0.827389| num_tokens: 16,514\n",
      "Step 00122/00701 | Training loss: 0.819888| lrm: 0.825963| num_tokens: 7,264\n",
      "Step 00123/00701 | Training loss: 0.874628| lrm: 0.824536| num_tokens: 6,636\n",
      "Step 00124/00701 | Training loss: 0.697536| lrm: 0.823110| num_tokens: 11,608\n",
      "Step 00125/00701 | Training loss: 0.981306| lrm: 0.821683| num_tokens: 5,773\n",
      "Step 00126/00701 | Training loss: 0.626914| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 1.058301| lrm: 0.818830| num_tokens: 10,148\n",
      "Step 00128/00701 | Training loss: 0.452782| lrm: 0.817404| num_tokens: 10,627\n",
      "Step 00129/00701 | Training loss: 0.829168| lrm: 0.815977| num_tokens: 8,873\n",
      "Step 00130 | Validation loss: 1.022921\n",
      "Step 00130/00701 | Training loss: 0.953769| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.938996| lrm: 0.813124| num_tokens: 11,228\n",
      "Step 00132/00701 | Training loss: 0.808957| lrm: 0.811698| num_tokens: 12,012\n",
      "Step 00133/00701 | Training loss: 0.909079| lrm: 0.810271| num_tokens: 16,435\n",
      "Step 00134/00701 | Training loss: 0.700476| lrm: 0.808845| num_tokens: 10,741\n",
      "Step 00135/00701 | Training loss: 1.374750| lrm: 0.807418| num_tokens: 13,492\n",
      "Step 00136/00701 | Training loss: 0.507394| lrm: 0.805991| num_tokens: 10,194\n",
      "Step 00137/00701 | Training loss: 0.886260| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.741971| lrm: 0.803138| num_tokens: 11,043\n",
      "Step 00139/00701 | Training loss: 0.959702| lrm: 0.801712| num_tokens: 12,147\n",
      "Step 00140 | Validation loss: 1.022773\n",
      "Step 00140/00701 | Training loss: 0.964752| lrm: 0.800285| num_tokens: 13,088\n",
      "Step 00141/00701 | Training loss: 0.807827| lrm: 0.798859| num_tokens: 13,647\n",
      "Step 00142/00701 | Training loss: 0.469592| lrm: 0.797432| num_tokens: 10,286\n",
      "Step 00143/00701 | Training loss: 1.172892| lrm: 0.796006| num_tokens: 8,956\n",
      "Step 00144/00701 | Training loss: 1.255215| lrm: 0.794579| num_tokens: 14,847\n",
      "Step 00145/00701 | Training loss: 0.958524| lrm: 0.793153| num_tokens: 14,649\n",
      "Step 00146/00701 | Training loss: 0.574844| lrm: 0.791726| num_tokens: 11,999\n",
      "Step 00147/00701 | Training loss: 0.643906| lrm: 0.790300| num_tokens: 8,022\n",
      "Step 00148/00701 | Training loss: 1.042696| lrm: 0.788873| num_tokens: 11,348\n",
      "Step 00149/00701 | Training loss: 1.078359| lrm: 0.787447| num_tokens: 14,182\n",
      "Step 00150 | Validation loss: 1.022764\n",
      "Step 00150/00701 | Training loss: 0.625897| lrm: 0.786020| num_tokens: 10,776\n",
      "Step 00151/00701 | Training loss: 0.675805| lrm: 0.784593| num_tokens: 14,039\n",
      "Step 00152/00701 | Training loss: 0.787014| lrm: 0.783167| num_tokens: 8,526\n",
      "Step 00153/00701 | Training loss: 0.827774| lrm: 0.781740| num_tokens: 12,782\n",
      "Step 00154/00701 | Training loss: 0.758814| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.834747| lrm: 0.778887| num_tokens: 10,624\n",
      "Step 00156/00701 | Training loss: 0.575670| lrm: 0.777461| num_tokens: 13,575\n",
      "Step 00157/00701 | Training loss: 1.128045| lrm: 0.776034| num_tokens: 13,319\n",
      "Step 00158/00701 | Training loss: 0.543485| lrm: 0.774608| num_tokens: 7,492\n",
      "Step 00159/00701 | Training loss: 0.721283| lrm: 0.773181| num_tokens: 9,274\n",
      "Step 00160 | Validation loss: 1.023049\n",
      "Step 00160/00701 | Training loss: 0.647029| lrm: 0.771755| num_tokens: 9,399\n",
      "Step 00161/00701 | Training loss: 0.981610| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.984469| lrm: 0.768902| num_tokens: 11,779\n",
      "Step 00163/00701 | Training loss: 0.492063| lrm: 0.767475| num_tokens: 7,485\n",
      "Step 00164/00701 | Training loss: 0.417343| lrm: 0.766049| num_tokens: 9,081\n",
      "Step 00165/00701 | Training loss: 0.756278| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.955279| lrm: 0.763195| num_tokens: 12,309\n",
      "Step 00167/00701 | Training loss: 0.901390| lrm: 0.761769| num_tokens: 10,547\n",
      "Step 00168/00701 | Training loss: 0.703279| lrm: 0.760342| num_tokens: 9,717\n",
      "Step 00169/00701 | Training loss: 0.880467| lrm: 0.758916| num_tokens: 9,577\n",
      "Step 00170 | Validation loss: 1.023152\n",
      "Step 00170/00701 | Training loss: 1.305460| lrm: 0.757489| num_tokens: 13,464\n",
      "Step 00171/00701 | Training loss: 0.684255| lrm: 0.756063| num_tokens: 10,156\n",
      "Step 00172/00701 | Training loss: 0.960775| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.970161| lrm: 0.753210| num_tokens: 13,931\n",
      "Step 00174/00701 | Training loss: 0.944795| lrm: 0.751783| num_tokens: 9,132\n",
      "Step 00175/00701 | Training loss: 1.049901| lrm: 0.750357| num_tokens: 9,835\n",
      "Step 00176/00701 | Training loss: 0.990373| lrm: 0.748930| num_tokens: 11,244\n",
      "Step 00177/00701 | Training loss: 0.983501| lrm: 0.747504| num_tokens: 14,670\n",
      "Step 00178/00701 | Training loss: 0.632865| lrm: 0.746077| num_tokens: 9,624\n",
      "Step 00179/00701 | Training loss: 0.955605| lrm: 0.744650| num_tokens: 11,085\n",
      "Step 00180 | Validation loss: 1.023179\n",
      "Step 00180/00701 | Training loss: 0.549602| lrm: 0.743224| num_tokens: 11,989\n",
      "Step 00181/00701 | Training loss: 0.873518| lrm: 0.741797| num_tokens: 15,749\n",
      "Step 00182/00701 | Training loss: 1.184748| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.485959| lrm: 0.738944| num_tokens: 12,476\n",
      "Step 00184/00701 | Training loss: 0.966821| lrm: 0.737518| num_tokens: 13,569\n",
      "Step 00185/00701 | Training loss: 0.667754| lrm: 0.736091| num_tokens: 11,265\n",
      "Step 00186/00701 | Training loss: 0.843045| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.480628| lrm: 0.733238| num_tokens: 13,609\n",
      "Step 00188/00701 | Training loss: 0.963056| lrm: 0.731812| num_tokens: 9,421\n",
      "Step 00189/00701 | Training loss: 0.971286| lrm: 0.730385| num_tokens: 9,300\n",
      "Step 00190 | Validation loss: 1.023114\n",
      "Step 00190/00701 | Training loss: 0.459680| lrm: 0.728959| num_tokens: 15,818\n",
      "Step 00191/00701 | Training loss: 0.903680| lrm: 0.727532| num_tokens: 12,340\n",
      "Step 00192/00701 | Training loss: 0.531411| lrm: 0.726106| num_tokens: 9,962\n",
      "Step 00193/00701 | Training loss: 1.100807| lrm: 0.724679| num_tokens: 10,650\n",
      "Step 00194/00701 | Training loss: 1.215591| lrm: 0.723252| num_tokens: 8,192\n",
      "Step 00195/00701 | Training loss: 0.547470| lrm: 0.721826| num_tokens: 14,326\n",
      "Step 00196/00701 | Training loss: 0.761478| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 1.041635| lrm: 0.718973| num_tokens: 10,264\n",
      "Step 00198/00701 | Training loss: 0.581570| lrm: 0.717546| num_tokens: 9,718\n",
      "Step 00199/00701 | Training loss: 0.615942| lrm: 0.716120| num_tokens: 10,337\n",
      "Step 00200 | Validation loss: 1.023044\n",
      "Step 00200/00701 | Training loss: 0.845666| lrm: 0.714693| num_tokens: 17,787\n",
      "Step 00201/00701 | Training loss: 0.880687| lrm: 0.713267| num_tokens: 9,676\n",
      "Step 00202/00701 | Training loss: 1.930813| lrm: 0.711840| num_tokens: 15,694\n",
      "Step 00203/00701 | Training loss: 1.075771| lrm: 0.710414| num_tokens: 8,344\n",
      "Step 00204/00701 | Training loss: 0.992266| lrm: 0.708987| num_tokens: 11,398\n",
      "Step 00205/00701 | Training loss: 0.653815| lrm: 0.707561| num_tokens: 11,165\n",
      "Step 00206/00701 | Training loss: 0.902360| lrm: 0.706134| num_tokens: 10,515\n",
      "Step 00207/00701 | Training loss: 0.641463| lrm: 0.704708| num_tokens: 11,281\n",
      "Step 00208/00701 | Training loss: 1.288818| lrm: 0.703281| num_tokens: 13,195\n",
      "Step 00209/00701 | Training loss: 0.568972| lrm: 0.701854| num_tokens: 10,620\n",
      "Step 00210 | Validation loss: 1.023052\n",
      "Step 00210/00701 | Training loss: 0.717201| lrm: 0.700428| num_tokens: 11,855\n",
      "Step 00211/00701 | Training loss: 1.064510| lrm: 0.699001| num_tokens: 11,439\n",
      "Step 00212/00701 | Training loss: 1.011722| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.737121| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 1.101669| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.677400| lrm: 0.693295| num_tokens: 10,049\n",
      "Step 00216/00701 | Training loss: 0.753976| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 0.998689| lrm: 0.690442| num_tokens: 8,208\n",
      "Step 00218/00701 | Training loss: 0.478448| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.884784| lrm: 0.687589| num_tokens: 14,707\n",
      "Step 00220 | Validation loss: 1.022972\n",
      "Step 00220/00701 | Training loss: 0.930169| lrm: 0.686163| num_tokens: 8,471\n",
      "Step 00221/00701 | Training loss: 0.773054| lrm: 0.684736| num_tokens: 7,856\n",
      "Step 00222/00701 | Training loss: 1.105618| lrm: 0.683310| num_tokens: 7,401\n",
      "Step 00223/00701 | Training loss: 1.151561| lrm: 0.681883| num_tokens: 11,899\n",
      "Step 00224/00701 | Training loss: 0.710102| lrm: 0.680456| num_tokens: 10,060\n",
      "Step 00225/00701 | Training loss: 1.127651| lrm: 0.679030| num_tokens: 8,888\n",
      "Step 00226/00701 | Training loss: 0.645946| lrm: 0.677603| num_tokens: 10,747\n",
      "Step 00227/00701 | Training loss: 0.594553| lrm: 0.676177| num_tokens: 10,038\n",
      "Step 00228/00701 | Training loss: 0.704930| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 1.136733| lrm: 0.673324| num_tokens: 9,397\n",
      "Step 00230 | Validation loss: 1.023022\n",
      "Step 00230/00701 | Training loss: 1.013790| lrm: 0.671897| num_tokens: 7,844\n",
      "Step 00231/00701 | Training loss: 1.208863| lrm: 0.670471| num_tokens: 11,685\n",
      "Step 00232/00701 | Training loss: 1.646671| lrm: 0.669044| num_tokens: 10,805\n",
      "Step 00233/00701 | Training loss: 0.740452| lrm: 0.667618| num_tokens: 10,203\n",
      "Step 00234/00701 | Training loss: 0.882918| lrm: 0.666191| num_tokens: 14,225\n",
      "Step 00235/00701 | Training loss: 1.032792| lrm: 0.664765| num_tokens: 9,748\n",
      "Step 00236/00701 | Training loss: 1.010435| lrm: 0.663338| num_tokens: 7,453\n",
      "Step 00237/00701 | Training loss: 0.843530| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 1.006141| lrm: 0.660485| num_tokens: 13,482\n",
      "Step 00239/00701 | Training loss: 1.264249| lrm: 0.659058| num_tokens: 12,515\n",
      "Step 00240 | Validation loss: 1.022965\n",
      "Step 00240/00701 | Training loss: 0.561239| lrm: 0.657632| num_tokens: 12,885\n",
      "Step 00241/00701 | Training loss: 1.005949| lrm: 0.656205| num_tokens: 11,284\n",
      "Step 00242/00701 | Training loss: 0.899531| lrm: 0.654779| num_tokens: 8,141\n",
      "Step 00243/00701 | Training loss: 0.666967| lrm: 0.653352| num_tokens: 10,581\n",
      "Step 00244/00701 | Training loss: 0.467356| lrm: 0.651926| num_tokens: 6,552\n",
      "Step 00245/00701 | Training loss: 1.031839| lrm: 0.650499| num_tokens: 12,896\n",
      "Step 00246/00701 | Training loss: 0.325334| lrm: 0.649073| num_tokens: 8,779\n",
      "Step 00247/00701 | Training loss: 0.559202| lrm: 0.647646| num_tokens: 10,640\n",
      "Step 00248/00701 | Training loss: 0.805794| lrm: 0.646220| num_tokens: 14,873\n",
      "Step 00249/00701 | Training loss: 0.708320| lrm: 0.644793| num_tokens: 8,755\n",
      "Step 00250 | Validation loss: 1.022901\n",
      "Step 00250/00701 | Training loss: 1.023668| lrm: 0.643367| num_tokens: 11,347\n",
      "Step 00251/00701 | Training loss: 1.205934| lrm: 0.641940| num_tokens: 13,773\n",
      "Step 00252/00701 | Training loss: 0.694086| lrm: 0.640514| num_tokens: 10,831\n",
      "Step 00253/00701 | Training loss: 0.820792| lrm: 0.639087| num_tokens: 14,171\n",
      "Step 00254/00701 | Training loss: 0.862156| lrm: 0.637660| num_tokens: 8,633\n",
      "Step 00255/00701 | Training loss: 0.743774| lrm: 0.636234| num_tokens: 10,649\n",
      "Step 00256/00701 | Training loss: 1.409174| lrm: 0.634807| num_tokens: 10,565\n",
      "Step 00257/00701 | Training loss: 0.535965| lrm: 0.633381| num_tokens: 8,375\n",
      "Step 00258/00701 | Training loss: 1.020694| lrm: 0.631954| num_tokens: 11,893\n",
      "Step 00259/00701 | Training loss: 0.908033| lrm: 0.630528| num_tokens: 6,289\n",
      "Step 00260 | Validation loss: 1.022852\n",
      "Step 00260/00701 | Training loss: 0.454173| lrm: 0.629101| num_tokens: 12,731\n",
      "Step 00261/00701 | Training loss: 0.677951| lrm: 0.627675| num_tokens: 11,069\n",
      "Step 00262/00701 | Training loss: 0.695397| lrm: 0.626248| num_tokens: 10,292\n",
      "Step 00263/00701 | Training loss: 0.624408| lrm: 0.624822| num_tokens: 6,973\n",
      "Step 00264/00701 | Training loss: 1.044622| lrm: 0.623395| num_tokens: 9,513\n",
      "Step 00265/00701 | Training loss: 0.820570| lrm: 0.621969| num_tokens: 11,233\n",
      "Step 00266/00701 | Training loss: 0.843503| lrm: 0.620542| num_tokens: 11,266\n",
      "Step 00267/00701 | Training loss: 0.545377| lrm: 0.619116| num_tokens: 8,955\n",
      "Step 00268/00701 | Training loss: 0.922791| lrm: 0.617689| num_tokens: 12,249\n",
      "Step 00269/00701 | Training loss: 0.757123| lrm: 0.616262| num_tokens: 9,576\n",
      "Step 00270 | Validation loss: 1.022795\n",
      "Step 00270/00701 | Training loss: 1.143037| lrm: 0.614836| num_tokens: 12,109\n",
      "Step 00271/00701 | Training loss: 1.393336| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.856603| lrm: 0.611983| num_tokens: 13,072\n",
      "Step 00273/00701 | Training loss: 0.621533| lrm: 0.610556| num_tokens: 9,778\n",
      "Step 00274/00701 | Training loss: 0.653637| lrm: 0.609130| num_tokens: 12,238\n",
      "Step 00275/00701 | Training loss: 0.584032| lrm: 0.607703| num_tokens: 10,854\n",
      "Step 00276/00701 | Training loss: 0.706952| lrm: 0.606277| num_tokens: 10,720\n",
      "Step 00277/00701 | Training loss: 0.570256| lrm: 0.604850| num_tokens: 7,570\n",
      "Step 00278/00701 | Training loss: 0.822875| lrm: 0.603424| num_tokens: 11,628\n",
      "Step 00279/00701 | Training loss: 0.891188| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280 | Validation loss: 1.022745\n",
      "Step 00280/00701 | Training loss: 0.776512| lrm: 0.600571| num_tokens: 8,970\n",
      "Step 00281/00701 | Training loss: 1.005086| lrm: 0.599144| num_tokens: 14,104\n",
      "Step 00282/00701 | Training loss: 0.993503| lrm: 0.597718| num_tokens: 10,950\n",
      "Step 00283/00701 | Training loss: 0.621874| lrm: 0.596291| num_tokens: 8,906\n",
      "Step 00284/00701 | Training loss: 0.430026| lrm: 0.594864| num_tokens: 12,614\n",
      "Step 00285/00701 | Training loss: 0.862773| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.598865| lrm: 0.592011| num_tokens: 11,914\n",
      "Step 00287/00701 | Training loss: 0.263399| lrm: 0.590585| num_tokens: 9,547\n",
      "Step 00288/00701 | Training loss: 0.677851| lrm: 0.589158| num_tokens: 5,007\n",
      "Step 00289/00701 | Training loss: 0.686807| lrm: 0.587732| num_tokens: 9,433\n",
      "Step 00290 | Validation loss: 1.022772\n",
      "Step 00290/00701 | Training loss: 1.073745| lrm: 0.586305| num_tokens: 11,035\n",
      "Step 00291/00701 | Training loss: 0.997810| lrm: 0.584879| num_tokens: 10,852\n",
      "Step 00292/00701 | Training loss: 0.947048| lrm: 0.583452| num_tokens: 11,659\n",
      "Step 00293/00701 | Training loss: 0.828613| lrm: 0.582026| num_tokens: 15,038\n",
      "Step 00294/00701 | Training loss: 0.726651| lrm: 0.580599| num_tokens: 9,439\n",
      "Step 00295/00701 | Training loss: 1.109742| lrm: 0.579173| num_tokens: 6,998\n",
      "Step 00296/00701 | Training loss: 1.221178| lrm: 0.577746| num_tokens: 12,267\n",
      "Step 00297/00701 | Training loss: 0.578633| lrm: 0.576320| num_tokens: 17,571\n",
      "Step 00298/00701 | Training loss: 1.037081| lrm: 0.574893| num_tokens: 10,103\n",
      "Step 00299/00701 | Training loss: 0.640378| lrm: 0.573466| num_tokens: 7,731\n",
      "Step 00300 | Validation loss: 1.022743\n",
      "Step 00300/00701 | Training loss: 0.750991| lrm: 0.572040| num_tokens: 8,914\n",
      "Step 00301/00701 | Training loss: 0.829310| lrm: 0.570613| num_tokens: 12,830\n",
      "Step 00302/00701 | Training loss: 0.768714| lrm: 0.569187| num_tokens: 12,209\n",
      "Step 00303/00701 | Training loss: 0.957043| lrm: 0.567760| num_tokens: 11,589\n",
      "Step 00304/00701 | Training loss: 0.611140| lrm: 0.566334| num_tokens: 8,228\n",
      "Step 00305/00701 | Training loss: 0.862920| lrm: 0.564907| num_tokens: 14,637\n",
      "Step 00306/00701 | Training loss: 0.887393| lrm: 0.563481| num_tokens: 11,570\n",
      "Step 00307/00701 | Training loss: 1.095217| lrm: 0.562054| num_tokens: 12,881\n",
      "Step 00308/00701 | Training loss: 0.664257| lrm: 0.560628| num_tokens: 6,839\n",
      "Step 00309/00701 | Training loss: 0.938823| lrm: 0.559201| num_tokens: 11,009\n",
      "Step 00310 | Validation loss: 1.022658\n",
      "Step 00310/00701 | Training loss: 0.583158| lrm: 0.557775| num_tokens: 12,118\n",
      "Step 00311/00701 | Training loss: 1.270011| lrm: 0.556348| num_tokens: 10,305\n",
      "Step 00312/00701 | Training loss: 0.747578| lrm: 0.554922| num_tokens: 11,427\n",
      "Step 00313/00701 | Training loss: 0.741343| lrm: 0.553495| num_tokens: 8,436\n",
      "Step 00314/00701 | Training loss: 1.104566| lrm: 0.552068| num_tokens: 11,858\n",
      "Step 00315/00701 | Training loss: 0.532629| lrm: 0.550642| num_tokens: 14,168\n",
      "Step 00316/00701 | Training loss: 1.127302| lrm: 0.549215| num_tokens: 6,451\n",
      "Step 00317/00701 | Training loss: 0.405976| lrm: 0.547789| num_tokens: 11,643\n",
      "Step 00318/00701 | Training loss: 0.708642| lrm: 0.546362| num_tokens: 10,043\n",
      "Step 00319/00701 | Training loss: 1.214356| lrm: 0.544936| num_tokens: 11,387\n",
      "Step 00320 | Validation loss: 1.022691\n",
      "Step 00320/00701 | Training loss: 0.614258| lrm: 0.543509| num_tokens: 6,437\n",
      "Step 00321/00701 | Training loss: 1.082890| lrm: 0.542083| num_tokens: 8,880\n",
      "Step 00322/00701 | Training loss: 0.750064| lrm: 0.540656| num_tokens: 4,147\n",
      "Step 00323/00701 | Training loss: 0.793836| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.978195| lrm: 0.537803| num_tokens: 11,807\n",
      "Step 00325/00701 | Training loss: 0.633367| lrm: 0.536377| num_tokens: 9,002\n",
      "Step 00326/00701 | Training loss: 1.000540| lrm: 0.534950| num_tokens: 7,345\n",
      "Step 00327/00701 | Training loss: 0.707461| lrm: 0.533524| num_tokens: 12,006\n",
      "Step 00328/00701 | Training loss: 1.044078| lrm: 0.532097| num_tokens: 12,714\n",
      "Step 00329/00701 | Training loss: 0.656710| lrm: 0.530670| num_tokens: 10,357\n",
      "Step 00330 | Validation loss: 1.022787\n",
      "Step 00330/00701 | Training loss: 1.009948| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.806607| lrm: 0.527817| num_tokens: 7,793\n",
      "Step 00332/00701 | Training loss: 0.728675| lrm: 0.526391| num_tokens: 10,629\n",
      "Step 00333/00701 | Training loss: 1.273534| lrm: 0.524964| num_tokens: 9,785\n",
      "Step 00334/00701 | Training loss: 0.376733| lrm: 0.523538| num_tokens: 12,100\n",
      "Step 00335/00701 | Training loss: 0.442932| lrm: 0.522111| num_tokens: 7,312\n",
      "Step 00336/00701 | Training loss: 1.151221| lrm: 0.520685| num_tokens: 12,905\n",
      "Step 00337/00701 | Training loss: 0.769321| lrm: 0.519258| num_tokens: 11,898\n",
      "Step 00338/00701 | Training loss: 0.378906| lrm: 0.517832| num_tokens: 10,202\n",
      "Step 00339/00701 | Training loss: 0.820943| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340 | Validation loss: 1.022885\n",
      "Step 00340/00701 | Training loss: 0.775247| lrm: 0.514979| num_tokens: 11,148\n",
      "Step 00341/00701 | Training loss: 1.193709| lrm: 0.513552| num_tokens: 8,979\n",
      "Step 00342/00701 | Training loss: 0.706573| lrm: 0.512126| num_tokens: 6,240\n",
      "Step 00343/00701 | Training loss: 1.133342| lrm: 0.510699| num_tokens: 11,617\n",
      "Step 00344/00701 | Training loss: 0.561095| lrm: 0.509272| num_tokens: 7,701\n",
      "Step 00345/00701 | Training loss: 0.800434| lrm: 0.507846| num_tokens: 12,529\n",
      "Step 00346/00701 | Training loss: 0.887832| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.678679| lrm: 0.504993| num_tokens: 11,169\n",
      "Step 00348/00701 | Training loss: 1.321707| lrm: 0.503566| num_tokens: 9,517\n",
      "Step 00349/00701 | Training loss: 0.666209| lrm: 0.502140| num_tokens: 10,291\n",
      "Step 00350 | Validation loss: 1.022966\n",
      "Step 00350/00701 | Training loss: 0.849223| lrm: 0.500713| num_tokens: 9,018\n",
      "Step 00351/00701 | Training loss: 0.766820| lrm: 0.499287| num_tokens: 11,729\n",
      "Step 00352/00701 | Training loss: 1.032630| lrm: 0.497860| num_tokens: 12,675\n",
      "Step 00353/00701 | Training loss: 0.864944| lrm: 0.496434| num_tokens: 8,383\n",
      "Step 00354/00701 | Training loss: 0.756854| lrm: 0.495007| num_tokens: 10,059\n",
      "Step 00355/00701 | Training loss: 0.835998| lrm: 0.493581| num_tokens: 10,087\n",
      "Step 00356/00701 | Training loss: 0.656841| lrm: 0.492154| num_tokens: 8,333\n",
      "Step 00357/00701 | Training loss: 0.653330| lrm: 0.490728| num_tokens: 8,266\n",
      "Step 00358/00701 | Training loss: 0.681483| lrm: 0.489301| num_tokens: 9,973\n",
      "Step 00359/00701 | Training loss: 0.708238| lrm: 0.487874| num_tokens: 9,013\n",
      "Step 00360 | Validation loss: 1.023000\n",
      "Step 00360/00701 | Training loss: 0.535522| lrm: 0.486448| num_tokens: 8,617\n",
      "Step 00361/00701 | Training loss: 0.742898| lrm: 0.485021| num_tokens: 11,847\n",
      "Step 00362/00701 | Training loss: 1.219663| lrm: 0.483595| num_tokens: 11,199\n",
      "Step 00363/00701 | Training loss: 0.464471| lrm: 0.482168| num_tokens: 11,212\n",
      "Step 00364/00701 | Training loss: 0.673743| lrm: 0.480742| num_tokens: 13,298\n",
      "Step 00365/00701 | Training loss: 0.519901| lrm: 0.479315| num_tokens: 7,758\n",
      "Step 00366/00701 | Training loss: 0.951585| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.769403| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 1.136534| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.823284| lrm: 0.473609| num_tokens: 13,353\n",
      "Step 00370 | Validation loss: 1.022925\n",
      "Step 00370/00701 | Training loss: 0.745428| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.924362| lrm: 0.470756| num_tokens: 11,940\n",
      "Step 00372/00701 | Training loss: 0.933456| lrm: 0.469330| num_tokens: 11,410\n",
      "Step 00373/00701 | Training loss: 0.553711| lrm: 0.467903| num_tokens: 8,816\n",
      "Step 00374/00701 | Training loss: 0.851824| lrm: 0.466476| num_tokens: 13,098\n",
      "Step 00375/00701 | Training loss: 1.159923| lrm: 0.465050| num_tokens: 13,724\n",
      "Step 00376/00701 | Training loss: 1.183475| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.672891| lrm: 0.462197| num_tokens: 13,890\n",
      "Step 00378/00701 | Training loss: 0.667807| lrm: 0.460770| num_tokens: 11,856\n",
      "Step 00379/00701 | Training loss: 0.897298| lrm: 0.459344| num_tokens: 7,960\n",
      "Step 00380 | Validation loss: 1.022824\n",
      "Step 00380/00701 | Training loss: 1.286101| lrm: 0.457917| num_tokens: 8,519\n",
      "Step 00381/00701 | Training loss: 0.907047| lrm: 0.456491| num_tokens: 14,234\n",
      "Step 00382/00701 | Training loss: 0.644269| lrm: 0.455064| num_tokens: 13,353\n",
      "Step 00383/00701 | Training loss: 0.527792| lrm: 0.453638| num_tokens: 15,146\n",
      "Step 00384/00701 | Training loss: 0.641418| lrm: 0.452211| num_tokens: 12,589\n",
      "Step 00385/00701 | Training loss: 0.434862| lrm: 0.450785| num_tokens: 10,105\n",
      "Step 00386/00701 | Training loss: 1.021977| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 1.038127| lrm: 0.447932| num_tokens: 9,869\n",
      "Step 00388/00701 | Training loss: 0.914548| lrm: 0.446505| num_tokens: 11,292\n",
      "Step 00389/00701 | Training loss: 0.833396| lrm: 0.445078| num_tokens: 15,855\n",
      "Step 00390 | Validation loss: 1.022745\n",
      "Step 00390/00701 | Training loss: 0.505678| lrm: 0.443652| num_tokens: 12,768\n",
      "Step 00391/00701 | Training loss: 0.675418| lrm: 0.442225| num_tokens: 13,893\n",
      "Step 00392/00701 | Training loss: 0.354972| lrm: 0.440799| num_tokens: 10,495\n",
      "Step 00393/00701 | Training loss: 1.292128| lrm: 0.439372| num_tokens: 13,459\n",
      "Step 00394/00701 | Training loss: 0.993268| lrm: 0.437946| num_tokens: 13,515\n",
      "Step 00395/00701 | Training loss: 0.848723| lrm: 0.436519| num_tokens: 10,006\n",
      "Step 00396/00701 | Training loss: 0.962983| lrm: 0.435093| num_tokens: 11,009\n",
      "Step 00397/00701 | Training loss: 1.089575| lrm: 0.433666| num_tokens: 8,178\n",
      "Step 00398/00701 | Training loss: 0.997524| lrm: 0.432240| num_tokens: 8,070\n",
      "Step 00399/00701 | Training loss: 0.898684| lrm: 0.430813| num_tokens: 8,559\n",
      "Step 00400 | Validation loss: 1.022611\n",
      "Step 00400/00701 | Training loss: 0.453928| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.743930| lrm: 0.427960| num_tokens: 6,689\n",
      "Step 00402/00701 | Training loss: 0.663190| lrm: 0.426534| num_tokens: 7,153\n",
      "Step 00403/00701 | Training loss: 1.254052| lrm: 0.425107| num_tokens: 15,333\n",
      "Step 00404/00701 | Training loss: 0.862241| lrm: 0.423680| num_tokens: 8,839\n",
      "Step 00405/00701 | Training loss: 0.696824| lrm: 0.422254| num_tokens: 10,865\n",
      "Step 00406/00701 | Training loss: 0.684362| lrm: 0.420827| num_tokens: 9,689\n",
      "Step 00407/00701 | Training loss: 0.558184| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.624185| lrm: 0.417974| num_tokens: 12,826\n",
      "Step 00409/00701 | Training loss: 0.599004| lrm: 0.416548| num_tokens: 8,557\n",
      "Step 00410 | Validation loss: 1.022534\n",
      "Step 00410/00701 | Training loss: 0.610392| lrm: 0.415121| num_tokens: 13,374\n",
      "Step 00411/00701 | Training loss: 0.885416| lrm: 0.413695| num_tokens: 12,917\n",
      "Step 00412/00701 | Training loss: 0.749210| lrm: 0.412268| num_tokens: 11,922\n",
      "Step 00413/00701 | Training loss: 0.856767| lrm: 0.410842| num_tokens: 7,315\n",
      "Step 00414/00701 | Training loss: 0.986571| lrm: 0.409415| num_tokens: 14,723\n",
      "Step 00415/00701 | Training loss: 0.745766| lrm: 0.407989| num_tokens: 13,804\n",
      "Step 00416/00701 | Training loss: 1.086293| lrm: 0.406562| num_tokens: 11,956\n",
      "Step 00417/00701 | Training loss: 0.659645| lrm: 0.405136| num_tokens: 12,018\n",
      "Step 00418/00701 | Training loss: 0.373247| lrm: 0.403709| num_tokens: 3,817\n",
      "Step 00419/00701 | Training loss: 0.973176| lrm: 0.402282| num_tokens: 9,950\n",
      "Step 00420 | Validation loss: 1.022487\n",
      "Step 00420/00701 | Training loss: 1.280561| lrm: 0.400856| num_tokens: 10,386\n",
      "Step 00421/00701 | Training loss: 0.828125| lrm: 0.399429| num_tokens: 12,796\n",
      "Step 00422/00701 | Training loss: 0.634934| lrm: 0.398003| num_tokens: 13,674\n",
      "Step 00423/00701 | Training loss: 0.580069| lrm: 0.396576| num_tokens: 15,302\n",
      "Step 00424/00701 | Training loss: 0.716347| lrm: 0.395150| num_tokens: 14,939\n",
      "Step 00425/00701 | Training loss: 0.857471| lrm: 0.393723| num_tokens: 7,669\n",
      "Step 00426/00701 | Training loss: 0.917307| lrm: 0.392297| num_tokens: 6,524\n",
      "Step 00427/00701 | Training loss: 0.864762| lrm: 0.390870| num_tokens: 8,753\n",
      "Step 00428/00701 | Training loss: 0.786749| lrm: 0.389444| num_tokens: 9,846\n",
      "Step 00429/00701 | Training loss: 0.688262| lrm: 0.388017| num_tokens: 10,688\n",
      "Step 00430 | Validation loss: 1.022472\n",
      "Step 00430/00701 | Training loss: 0.806756| lrm: 0.386591| num_tokens: 11,206\n",
      "Step 00431/00701 | Training loss: 1.603916| lrm: 0.385164| num_tokens: 8,919\n",
      "Step 00432/00701 | Training loss: 1.025534| lrm: 0.383738| num_tokens: 13,388\n",
      "Step 00433/00701 | Training loss: 0.595770| lrm: 0.382311| num_tokens: 10,474\n",
      "Step 00434/00701 | Training loss: 0.864001| lrm: 0.380884| num_tokens: 11,897\n",
      "Step 00435/00701 | Training loss: 0.674309| lrm: 0.379458| num_tokens: 12,165\n",
      "Step 00436/00701 | Training loss: 0.786697| lrm: 0.378031| num_tokens: 13,113\n",
      "Step 00437/00701 | Training loss: 0.948160| lrm: 0.376605| num_tokens: 12,838\n",
      "Step 00438/00701 | Training loss: 1.265784| lrm: 0.375178| num_tokens: 10,430\n",
      "Step 00439/00701 | Training loss: 0.818162| lrm: 0.373752| num_tokens: 11,832\n",
      "Step 00440 | Validation loss: 1.022416\n",
      "Step 00440/00701 | Training loss: 0.651902| lrm: 0.372325| num_tokens: 10,469\n",
      "Step 00441/00701 | Training loss: 0.781544| lrm: 0.370899| num_tokens: 11,808\n",
      "Step 00442/00701 | Training loss: 0.820921| lrm: 0.369472| num_tokens: 12,062\n",
      "Step 00443/00701 | Training loss: 0.765333| lrm: 0.368046| num_tokens: 10,750\n",
      "Step 00444/00701 | Training loss: 0.689322| lrm: 0.366619| num_tokens: 13,119\n",
      "Step 00445/00701 | Training loss: 0.800216| lrm: 0.365193| num_tokens: 9,910\n",
      "Step 00446/00701 | Training loss: 0.514143| lrm: 0.363766| num_tokens: 13,290\n",
      "Step 00447/00701 | Training loss: 1.073326| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.923856| lrm: 0.360913| num_tokens: 10,547\n",
      "Step 00449/00701 | Training loss: 0.824562| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450 | Validation loss: 1.022477\n",
      "Step 00450/00701 | Training loss: 1.065093| lrm: 0.358060| num_tokens: 15,847\n",
      "Step 00451/00701 | Training loss: 0.883704| lrm: 0.356633| num_tokens: 10,672\n",
      "Step 00452/00701 | Training loss: 0.809520| lrm: 0.355207| num_tokens: 9,304\n",
      "Step 00453/00701 | Training loss: 0.773506| lrm: 0.353780| num_tokens: 12,538\n",
      "Step 00454/00701 | Training loss: 0.933203| lrm: 0.352354| num_tokens: 7,953\n",
      "Step 00455/00701 | Training loss: 0.887156| lrm: 0.350927| num_tokens: 7,755\n",
      "Step 00456/00701 | Training loss: 1.000414| lrm: 0.349501| num_tokens: 10,549\n",
      "Step 00457/00701 | Training loss: 0.606392| lrm: 0.348074| num_tokens: 10,105\n",
      "Step 00458/00701 | Training loss: 0.837167| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.746804| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460 | Validation loss: 1.022517\n",
      "Step 00460/00701 | Training loss: 0.748599| lrm: 0.343795| num_tokens: 8,420\n",
      "Step 00461/00701 | Training loss: 0.807633| lrm: 0.342368| num_tokens: 12,615\n",
      "Step 00462/00701 | Training loss: 0.567512| lrm: 0.340942| num_tokens: 9,887\n",
      "Step 00463/00701 | Training loss: 1.126877| lrm: 0.339515| num_tokens: 7,312\n",
      "Step 00464/00701 | Training loss: 0.690590| lrm: 0.338088| num_tokens: 9,797\n",
      "Step 00465/00701 | Training loss: 1.061798| lrm: 0.336662| num_tokens: 8,338\n",
      "Step 00466/00701 | Training loss: 0.934142| lrm: 0.335235| num_tokens: 11,435\n",
      "Step 00467/00701 | Training loss: 0.654063| lrm: 0.333809| num_tokens: 10,835\n",
      "Step 00468/00701 | Training loss: 0.752903| lrm: 0.332382| num_tokens: 9,671\n",
      "Step 00469/00701 | Training loss: 0.444350| lrm: 0.330956| num_tokens: 8,338\n",
      "Step 00470 | Validation loss: 1.022553\n",
      "Step 00470/00701 | Training loss: 1.076223| lrm: 0.329529| num_tokens: 8,700\n",
      "Step 00471/00701 | Training loss: 0.941440| lrm: 0.328103| num_tokens: 15,219\n",
      "Step 00472/00701 | Training loss: 1.888751| lrm: 0.326676| num_tokens: 13,448\n",
      "Step 00473/00701 | Training loss: 0.657994| lrm: 0.325250| num_tokens: 9,585\n",
      "Step 00474/00701 | Training loss: 1.146513| lrm: 0.323823| num_tokens: 14,725\n",
      "Step 00475/00701 | Training loss: 0.921611| lrm: 0.322397| num_tokens: 12,688\n",
      "Step 00476/00701 | Training loss: 1.140619| lrm: 0.320970| num_tokens: 11,268\n",
      "Step 00477/00701 | Training loss: 1.008031| lrm: 0.319544| num_tokens: 13,478\n",
      "Step 00478/00701 | Training loss: 1.317510| lrm: 0.318117| num_tokens: 6,838\n",
      "Step 00479/00701 | Training loss: 0.758147| lrm: 0.316690| num_tokens: 13,516\n",
      "Step 00480 | Validation loss: 1.022578\n",
      "Step 00480/00701 | Training loss: 0.405285| lrm: 0.315264| num_tokens: 13,587\n",
      "Step 00481/00701 | Training loss: 1.081623| lrm: 0.313837| num_tokens: 7,015\n",
      "Step 00482/00701 | Training loss: 0.982490| lrm: 0.312411| num_tokens: 9,863\n",
      "Step 00483/00701 | Training loss: 0.687307| lrm: 0.310984| num_tokens: 11,048\n",
      "Step 00484/00701 | Training loss: 0.495754| lrm: 0.309558| num_tokens: 10,661\n",
      "Step 00485/00701 | Training loss: 1.126421| lrm: 0.308131| num_tokens: 10,617\n",
      "Step 00486/00701 | Training loss: 0.394199| lrm: 0.306705| num_tokens: 9,656\n",
      "Step 00487/00701 | Training loss: 0.664975| lrm: 0.305278| num_tokens: 8,590\n",
      "Step 00488/00701 | Training loss: 0.442125| lrm: 0.303852| num_tokens: 11,260\n",
      "Step 00489/00701 | Training loss: 0.946465| lrm: 0.302425| num_tokens: 12,388\n",
      "Step 00490 | Validation loss: 1.022555\n",
      "Step 00490/00701 | Training loss: 0.892557| lrm: 0.300999| num_tokens: 8,471\n",
      "Step 00491/00701 | Training loss: 0.927210| lrm: 0.299572| num_tokens: 8,826\n",
      "Step 00492/00701 | Training loss: 1.445579| lrm: 0.298146| num_tokens: 14,321\n",
      "Step 00493/00701 | Training loss: 0.573202| lrm: 0.296719| num_tokens: 7,284\n",
      "Step 00494/00701 | Training loss: 0.540198| lrm: 0.295292| num_tokens: 8,592\n",
      "Step 00495/00701 | Training loss: 0.869817| lrm: 0.293866| num_tokens: 10,854\n",
      "Step 00496/00701 | Training loss: 0.760508| lrm: 0.292439| num_tokens: 10,975\n",
      "Step 00497/00701 | Training loss: 0.724258| lrm: 0.291013| num_tokens: 11,925\n",
      "Step 00498/00701 | Training loss: 0.978404| lrm: 0.289586| num_tokens: 11,744\n",
      "Step 00499/00701 | Training loss: 0.295009| lrm: 0.288160| num_tokens: 14,263\n",
      "Step 00500 | Validation loss: 1.022517\n",
      "Step 00500/00701 | Training loss: 0.895711| lrm: 0.286733| num_tokens: 13,673\n",
      "Step 00501/00701 | Training loss: 0.629074| lrm: 0.285307| num_tokens: 9,954\n",
      "Step 00502/00701 | Training loss: 0.435075| lrm: 0.283880| num_tokens: 9,783\n",
      "Step 00503/00701 | Training loss: 1.390642| lrm: 0.282454| num_tokens: 13,075\n",
      "Step 00504/00701 | Training loss: 1.558832| lrm: 0.281027| num_tokens: 9,767\n",
      "Step 00505/00701 | Training loss: 0.832481| lrm: 0.279601| num_tokens: 8,895\n",
      "Step 00506/00701 | Training loss: 1.359295| lrm: 0.278174| num_tokens: 9,652\n",
      "Step 00507/00701 | Training loss: 0.594392| lrm: 0.276748| num_tokens: 6,756\n",
      "Step 00508/00701 | Training loss: 1.328425| lrm: 0.275321| num_tokens: 18,637\n",
      "Step 00509/00701 | Training loss: 0.862259| lrm: 0.273894| num_tokens: 9,678\n",
      "Step 00510 | Validation loss: 1.022536\n",
      "Step 00510/00701 | Training loss: 0.768932| lrm: 0.272468| num_tokens: 9,942\n",
      "Step 00511/00701 | Training loss: 0.578484| lrm: 0.271041| num_tokens: 5,586\n",
      "Step 00512/00701 | Training loss: 1.011372| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.791628| lrm: 0.268188| num_tokens: 8,743\n",
      "Step 00514/00701 | Training loss: 0.857878| lrm: 0.266762| num_tokens: 9,904\n",
      "Step 00515/00701 | Training loss: 1.179952| lrm: 0.265335| num_tokens: 10,734\n",
      "Step 00516/00701 | Training loss: 1.217106| lrm: 0.263909| num_tokens: 12,039\n",
      "Step 00517/00701 | Training loss: 0.547410| lrm: 0.262482| num_tokens: 10,739\n",
      "Step 00518/00701 | Training loss: 1.136655| lrm: 0.261056| num_tokens: 10,288\n",
      "Step 00519/00701 | Training loss: 0.849780| lrm: 0.259629| num_tokens: 8,447\n",
      "Step 00520 | Validation loss: 1.022481\n",
      "Step 00520/00701 | Training loss: 0.862736| lrm: 0.258203| num_tokens: 9,737\n",
      "Step 00521/00701 | Training loss: 0.970762| lrm: 0.256776| num_tokens: 14,760\n",
      "Step 00522/00701 | Training loss: 1.078244| lrm: 0.255350| num_tokens: 9,452\n",
      "Step 00523/00701 | Training loss: 0.653673| lrm: 0.253923| num_tokens: 11,752\n",
      "Step 00524/00701 | Training loss: 0.975984| lrm: 0.252496| num_tokens: 7,305\n",
      "Step 00525/00701 | Training loss: 1.539481| lrm: 0.251070| num_tokens: 10,430\n",
      "Step 00526/00701 | Training loss: 1.127333| lrm: 0.249643| num_tokens: 9,485\n",
      "Step 00527/00701 | Training loss: 1.167069| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 1.190217| lrm: 0.246790| num_tokens: 14,142\n",
      "Step 00529/00701 | Training loss: 1.138768| lrm: 0.245364| num_tokens: 14,963\n",
      "Step 00530 | Validation loss: 1.022406\n",
      "Step 00530/00701 | Training loss: 0.463539| lrm: 0.243937| num_tokens: 8,920\n",
      "Step 00531/00701 | Training loss: 0.615001| lrm: 0.242511| num_tokens: 10,497\n",
      "Step 00532/00701 | Training loss: 1.154005| lrm: 0.241084| num_tokens: 8,047\n",
      "Step 00533/00701 | Training loss: 1.048255| lrm: 0.239658| num_tokens: 11,665\n",
      "Step 00534/00701 | Training loss: 1.163793| lrm: 0.238231| num_tokens: 12,661\n",
      "Step 00535/00701 | Training loss: 0.643556| lrm: 0.236805| num_tokens: 9,332\n",
      "Step 00536/00701 | Training loss: 0.510072| lrm: 0.235378| num_tokens: 10,270\n",
      "Step 00537/00701 | Training loss: 0.697713| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.648191| lrm: 0.232525| num_tokens: 10,264\n",
      "Step 00539/00701 | Training loss: 0.809876| lrm: 0.231098| num_tokens: 13,034\n",
      "Step 00540 | Validation loss: 1.022313\n",
      "Step 00540/00701 | Training loss: 0.814862| lrm: 0.229672| num_tokens: 7,601\n",
      "Step 00541/00701 | Training loss: 1.028138| lrm: 0.228245| num_tokens: 15,932\n",
      "Step 00542/00701 | Training loss: 1.241461| lrm: 0.226819| num_tokens: 10,867\n",
      "Step 00543/00701 | Training loss: 0.368496| lrm: 0.225392| num_tokens: 11,887\n",
      "Step 00544/00701 | Training loss: 0.969180| lrm: 0.223966| num_tokens: 11,865\n",
      "Step 00545/00701 | Training loss: 0.764936| lrm: 0.222539| num_tokens: 13,438\n",
      "Step 00546/00701 | Training loss: 0.793908| lrm: 0.221113| num_tokens: 13,844\n",
      "Step 00547/00701 | Training loss: 1.263603| lrm: 0.219686| num_tokens: 6,023\n",
      "Step 00548/00701 | Training loss: 0.741583| lrm: 0.218260| num_tokens: 13,321\n",
      "Step 00549/00701 | Training loss: 1.073801| lrm: 0.216833| num_tokens: 8,358\n",
      "Step 00550 | Validation loss: 1.022306\n",
      "Step 00550/00701 | Training loss: 0.635105| lrm: 0.215407| num_tokens: 13,487\n",
      "Step 00551/00701 | Training loss: 0.763167| lrm: 0.213980| num_tokens: 11,180\n",
      "Step 00552/00701 | Training loss: 0.774400| lrm: 0.212553| num_tokens: 8,538\n",
      "Step 00553/00701 | Training loss: 0.256588| lrm: 0.211127| num_tokens: 10,572\n",
      "Step 00554/00701 | Training loss: 0.976247| lrm: 0.209700| num_tokens: 12,982\n",
      "Step 00555/00701 | Training loss: 0.891236| lrm: 0.208274| num_tokens: 11,588\n",
      "Step 00556/00701 | Training loss: 0.913182| lrm: 0.206847| num_tokens: 7,432\n",
      "Step 00557/00701 | Training loss: 0.654261| lrm: 0.205421| num_tokens: 9,354\n",
      "Step 00558/00701 | Training loss: 0.880581| lrm: 0.203994| num_tokens: 10,187\n",
      "Step 00559/00701 | Training loss: 1.005726| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560 | Validation loss: 1.022362\n",
      "Step 00560/00701 | Training loss: 0.512785| lrm: 0.201141| num_tokens: 13,597\n",
      "Step 00561/00701 | Training loss: 1.173465| lrm: 0.199715| num_tokens: 6,689\n",
      "Step 00562/00701 | Training loss: 0.242424| lrm: 0.198288| num_tokens: 8,682\n",
      "Step 00563/00701 | Training loss: 0.941932| lrm: 0.196862| num_tokens: 12,881\n",
      "Step 00564/00701 | Training loss: 0.945415| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 1.068251| lrm: 0.194009| num_tokens: 13,874\n",
      "Step 00566/00701 | Training loss: 0.997027| lrm: 0.192582| num_tokens: 12,213\n",
      "Step 00567/00701 | Training loss: 0.921283| lrm: 0.191155| num_tokens: 18,532\n",
      "Step 00568/00701 | Training loss: 0.704697| lrm: 0.189729| num_tokens: 10,003\n",
      "Step 00569/00701 | Training loss: 0.889696| lrm: 0.188302| num_tokens: 9,476\n",
      "Step 00570 | Validation loss: 1.022332\n",
      "Step 00570/00701 | Training loss: 1.258604| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.722219| lrm: 0.185449| num_tokens: 13,088\n",
      "Step 00572/00701 | Training loss: 0.690869| lrm: 0.184023| num_tokens: 7,260\n",
      "Step 00573/00701 | Training loss: 1.087271| lrm: 0.182596| num_tokens: 10,391\n",
      "Step 00574/00701 | Training loss: 0.742345| lrm: 0.181170| num_tokens: 14,540\n",
      "Step 00575/00701 | Training loss: 0.423914| lrm: 0.179743| num_tokens: 7,583\n",
      "Step 00576/00701 | Training loss: 1.752595| lrm: 0.178317| num_tokens: 8,834\n",
      "Step 00577/00701 | Training loss: 0.722734| lrm: 0.176890| num_tokens: 13,422\n",
      "Step 00578/00701 | Training loss: 0.578532| lrm: 0.175464| num_tokens: 8,190\n",
      "Step 00579/00701 | Training loss: 0.489269| lrm: 0.174037| num_tokens: 10,220\n",
      "Step 00580 | Validation loss: 1.022343\n",
      "Step 00580/00701 | Training loss: 0.994607| lrm: 0.172611| num_tokens: 9,082\n",
      "Step 00581/00701 | Training loss: 1.045251| lrm: 0.171184| num_tokens: 13,484\n",
      "Step 00582/00701 | Training loss: 1.097232| lrm: 0.169757| num_tokens: 8,858\n",
      "Step 00583/00701 | Training loss: 0.591985| lrm: 0.168331| num_tokens: 12,740\n",
      "Step 00584/00701 | Training loss: 0.824178| lrm: 0.166904| num_tokens: 13,825\n",
      "Step 00585/00701 | Training loss: 0.676330| lrm: 0.165478| num_tokens: 14,923\n",
      "Step 00586/00701 | Training loss: 0.978041| lrm: 0.164051| num_tokens: 9,923\n",
      "Step 00587/00701 | Training loss: 0.764418| lrm: 0.162625| num_tokens: 7,792\n",
      "Step 00588/00701 | Training loss: 0.930967| lrm: 0.161198| num_tokens: 12,331\n",
      "Step 00589/00701 | Training loss: 0.472872| lrm: 0.159772| num_tokens: 9,040\n",
      "Step 00590 | Validation loss: 1.022382\n",
      "Step 00590/00701 | Training loss: 0.544830| lrm: 0.158345| num_tokens: 10,334\n",
      "Step 00591/00701 | Training loss: 0.757875| lrm: 0.156919| num_tokens: 8,577\n",
      "Step 00592/00701 | Training loss: 0.979237| lrm: 0.155492| num_tokens: 11,743\n",
      "Step 00593/00701 | Training loss: 1.070970| lrm: 0.154066| num_tokens: 11,795\n",
      "Step 00594/00701 | Training loss: 0.245915| lrm: 0.152639| num_tokens: 11,224\n",
      "Step 00595/00701 | Training loss: 0.466318| lrm: 0.151213| num_tokens: 8,408\n",
      "Step 00596/00701 | Training loss: 0.746211| lrm: 0.149786| num_tokens: 5,658\n",
      "Step 00597/00701 | Training loss: 1.316927| lrm: 0.148359| num_tokens: 10,836\n",
      "Step 00598/00701 | Training loss: 0.748976| lrm: 0.146933| num_tokens: 9,759\n",
      "Step 00599/00701 | Training loss: 1.425558| lrm: 0.145506| num_tokens: 8,267\n",
      "Step 00600 | Validation loss: 1.022407\n",
      "Step 00600/00701 | Training loss: 0.721479| lrm: 0.144080| num_tokens: 10,446\n",
      "Step 00601/00701 | Training loss: 1.063708| lrm: 0.142653| num_tokens: 11,003\n",
      "Step 00602/00701 | Training loss: 0.786609| lrm: 0.141227| num_tokens: 8,234\n",
      "Step 00603/00701 | Training loss: 1.099983| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.966313| lrm: 0.138374| num_tokens: 13,517\n",
      "Step 00605/00701 | Training loss: 0.762746| lrm: 0.136947| num_tokens: 10,453\n",
      "Step 00606/00701 | Training loss: 0.927853| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.645923| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.613412| lrm: 0.132668| num_tokens: 12,833\n",
      "Step 00609/00701 | Training loss: 1.123323| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610 | Validation loss: 1.022416\n",
      "Step 00610/00701 | Training loss: 0.497374| lrm: 0.129815| num_tokens: 14,179\n",
      "Step 00611/00701 | Training loss: 0.972448| lrm: 0.128388| num_tokens: 9,321\n",
      "Step 00612/00701 | Training loss: 0.846865| lrm: 0.126961| num_tokens: 9,523\n",
      "Step 00613/00701 | Training loss: 1.079312| lrm: 0.125535| num_tokens: 16,542\n",
      "Step 00614/00701 | Training loss: 0.551214| lrm: 0.124108| num_tokens: 15,348\n",
      "Step 00615/00701 | Training loss: 0.921661| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.618214| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.719504| lrm: 0.119829| num_tokens: 11,180\n",
      "Step 00618/00701 | Training loss: 0.565791| lrm: 0.118402| num_tokens: 8,842\n",
      "Step 00619/00701 | Training loss: 0.704979| lrm: 0.116976| num_tokens: 9,213\n",
      "Step 00620 | Validation loss: 1.022435\n",
      "Step 00620/00701 | Training loss: 0.641051| lrm: 0.115549| num_tokens: 12,340\n",
      "Step 00621/00701 | Training loss: 0.487597| lrm: 0.114123| num_tokens: 7,718\n",
      "Step 00622/00701 | Training loss: 0.965580| lrm: 0.112696| num_tokens: 7,350\n",
      "Step 00623/00701 | Training loss: 0.903505| lrm: 0.111270| num_tokens: 14,519\n",
      "Step 00624/00701 | Training loss: 1.415140| lrm: 0.109843| num_tokens: 12,368\n",
      "Step 00625/00701 | Training loss: 0.578853| lrm: 0.108417| num_tokens: 6,842\n",
      "Step 00626/00701 | Training loss: 0.732524| lrm: 0.106990| num_tokens: 11,577\n",
      "Step 00627/00701 | Training loss: 0.488446| lrm: 0.105563| num_tokens: 8,498\n",
      "Step 00628/00701 | Training loss: 0.769923| lrm: 0.104137| num_tokens: 6,965\n",
      "Step 00629/00701 | Training loss: 0.606300| lrm: 0.102710| num_tokens: 12,391\n",
      "Step 00630 | Validation loss: 1.022483\n",
      "Step 00630/00701 | Training loss: 0.563831| lrm: 0.101284| num_tokens: 10,252\n",
      "Step 00631/00701 | Training loss: 0.768622| lrm: 0.099857| num_tokens: 12,194\n",
      "Step 00632/00701 | Training loss: 0.644890| lrm: 0.098431| num_tokens: 10,964\n",
      "Step 00633/00701 | Training loss: 0.911795| lrm: 0.097004| num_tokens: 11,198\n",
      "Step 00634/00701 | Training loss: 1.005049| lrm: 0.095578| num_tokens: 7,808\n",
      "Step 00635/00701 | Training loss: 0.765011| lrm: 0.094151| num_tokens: 12,053\n",
      "Step 00636/00701 | Training loss: 0.798818| lrm: 0.092725| num_tokens: 8,138\n",
      "Step 00637/00701 | Training loss: 0.861956| lrm: 0.091298| num_tokens: 13,101\n",
      "Step 00638/00701 | Training loss: 0.562583| lrm: 0.089872| num_tokens: 8,339\n",
      "Step 00639/00701 | Training loss: 0.688392| lrm: 0.088445| num_tokens: 13,057\n",
      "Step 00640 | Validation loss: 1.022456\n",
      "Step 00640/00701 | Training loss: 0.970931| lrm: 0.087019| num_tokens: 12,703\n",
      "Step 00641/00701 | Training loss: 0.912153| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.349773| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.889259| lrm: 0.082739| num_tokens: 8,878\n",
      "Step 00644/00701 | Training loss: 0.841106| lrm: 0.081312| num_tokens: 8,861\n",
      "Step 00645/00701 | Training loss: 1.237710| lrm: 0.079886| num_tokens: 13,446\n",
      "Step 00646/00701 | Training loss: 0.756671| lrm: 0.078459| num_tokens: 12,789\n",
      "Step 00647/00701 | Training loss: 1.062499| lrm: 0.077033| num_tokens: 8,241\n",
      "Step 00648/00701 | Training loss: 0.856502| lrm: 0.075606| num_tokens: 10,330\n",
      "Step 00649/00701 | Training loss: 0.664216| lrm: 0.074180| num_tokens: 12,278\n",
      "Step 00650 | Validation loss: 1.022477\n",
      "Step 00650/00701 | Training loss: 1.332348| lrm: 0.072753| num_tokens: 7,245\n",
      "Step 00651/00701 | Training loss: 1.299629| lrm: 0.071327| num_tokens: 13,989\n",
      "Step 00652/00701 | Training loss: 1.143123| lrm: 0.069900| num_tokens: 11,516\n",
      "Step 00653/00701 | Training loss: 0.644336| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.798760| lrm: 0.067047| num_tokens: 8,702\n",
      "Step 00655/00701 | Training loss: 0.891719| lrm: 0.065621| num_tokens: 11,510\n",
      "Step 00656/00701 | Training loss: 1.253937| lrm: 0.064194| num_tokens: 11,144\n",
      "Step 00657/00701 | Training loss: 0.386900| lrm: 0.062767| num_tokens: 10,336\n",
      "Step 00658/00701 | Training loss: 1.090364| lrm: 0.061341| num_tokens: 9,779\n",
      "Step 00659/00701 | Training loss: 0.746789| lrm: 0.059914| num_tokens: 8,105\n",
      "Step 00660 | Validation loss: 1.022452\n",
      "Step 00660/00701 | Training loss: 0.794924| lrm: 0.058488| num_tokens: 10,476\n",
      "Step 00661/00701 | Training loss: 1.114480| lrm: 0.057061| num_tokens: 11,310\n",
      "Step 00662/00701 | Training loss: 0.644325| lrm: 0.055635| num_tokens: 7,871\n",
      "Step 00663/00701 | Training loss: 0.707858| lrm: 0.054208| num_tokens: 9,855\n",
      "Step 00664/00701 | Training loss: 0.666295| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.657944| lrm: 0.051355| num_tokens: 9,697\n",
      "Step 00666/00701 | Training loss: 1.036197| lrm: 0.049929| num_tokens: 10,710\n",
      "Step 00667/00701 | Training loss: 0.848241| lrm: 0.048502| num_tokens: 13,511\n",
      "Step 00668/00701 | Training loss: 0.641283| lrm: 0.047076| num_tokens: 11,917\n",
      "Step 00669/00701 | Training loss: 1.120504| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670 | Validation loss: 1.022403\n",
      "Step 00670/00701 | Training loss: 0.757563| lrm: 0.044223| num_tokens: 9,513\n",
      "Step 00671/00701 | Training loss: 0.787720| lrm: 0.042796| num_tokens: 10,895\n",
      "Step 00672/00701 | Training loss: 0.654976| lrm: 0.041369| num_tokens: 10,537\n",
      "Step 00673/00701 | Training loss: 0.695997| lrm: 0.039943| num_tokens: 9,859\n",
      "Step 00674/00701 | Training loss: 0.622599| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675/00701 | Training loss: 0.610805| lrm: 0.037090| num_tokens: 10,400\n",
      "Step 00676/00701 | Training loss: 0.857317| lrm: 0.035663| num_tokens: 10,101\n",
      "Step 00677/00701 | Training loss: 1.020497| lrm: 0.034237| num_tokens: 16,334\n",
      "Step 00678/00701 | Training loss: 0.339332| lrm: 0.032810| num_tokens: 12,035\n",
      "Step 00679/00701 | Training loss: 0.788136| lrm: 0.031384| num_tokens: 8,362\n",
      "Step 00680 | Validation loss: 1.022382\n",
      "Step 00680/00701 | Training loss: 1.206766| lrm: 0.029957| num_tokens: 12,050\n",
      "Step 00681/00701 | Training loss: 0.679935| lrm: 0.028531| num_tokens: 9,425\n",
      "Step 00682/00701 | Training loss: 0.720819| lrm: 0.027104| num_tokens: 14,504\n",
      "Step 00683/00701 | Training loss: 0.925189| lrm: 0.025678| num_tokens: 9,745\n",
      "Step 00684/00701 | Training loss: 1.594777| lrm: 0.024251| num_tokens: 12,770\n",
      "Step 00685/00701 | Training loss: 0.594906| lrm: 0.022825| num_tokens: 9,268\n",
      "Step 00686/00701 | Training loss: 0.530567| lrm: 0.021398| num_tokens: 10,168\n",
      "Step 00687/00701 | Training loss: 0.782724| lrm: 0.019971| num_tokens: 13,350\n",
      "Step 00688/00701 | Training loss: 0.705711| lrm: 0.018545| num_tokens: 10,458\n",
      "Step 00689/00701 | Training loss: 0.505559| lrm: 0.017118| num_tokens: 10,652\n",
      "Step 00690 | Validation loss: 1.022378\n",
      "Step 00690/00701 | Training loss: 0.425209| lrm: 0.015692| num_tokens: 8,627\n",
      "Step 00691/00701 | Training loss: 0.546082| lrm: 0.014265| num_tokens: 10,642\n",
      "Step 00692/00701 | Training loss: 0.910318| lrm: 0.012839| num_tokens: 11,930\n",
      "Step 00693/00701 | Training loss: 1.139833| lrm: 0.011412| num_tokens: 9,686\n",
      "Step 00694/00701 | Training loss: 1.220765| lrm: 0.009986| num_tokens: 10,109\n",
      "Step 00695/00701 | Training loss: 1.039768| lrm: 0.008559| num_tokens: 8,528\n",
      "Step 00696/00701 | Training loss: 0.528098| lrm: 0.007133| num_tokens: 12,544\n",
      "Step 00697/00701 | Training loss: 1.214602| lrm: 0.005706| num_tokens: 10,314\n",
      "Step 00698/00701 | Training loss: 0.545012| lrm: 0.004280| num_tokens: 8,859\n",
      "Step 00699/00701 | Training loss: 0.529634| lrm: 0.002853| num_tokens: 11,147\n",
      "Step 00700 | Validation loss: 1.022344\n",
      "final: 360/1024 (35.16%)\n",
      "final: 455/1024 (44.43%)\n",
      "Step 00700 | mmlu_acc: 0.351562, arc_easy_acc: 0.444336\n",
      "[W1121 20:04:37.097320927 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:37.173420556 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:37.188049305 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:37.247891773 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:38.554097767 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading summary, console lines 784-786 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–‚â–„â–‚â–„â–„â–„â–‡â–ƒâ–‚â–…â–ƒâ–…â–„â–„â–…â–ƒâ–…â–†â–‚â–‡â–â–†â–…â–‚â–‚â–„â–…â–„â–ˆâ–„â–„â–ˆâ–…â–†â–‚â–„â–„â–ƒâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–â–‚â–†â–…â–…â–…â–‡â–…â–‚â–ƒâ–‚â–ƒâ–„â–…â–‚â–‚â–‚â–„â–‡â–†â–ˆâ–â–„â–…â–‚â–ˆâ–‚â–‚â–„â–†â–…â–†â–„â–„â–ƒâ–†â–‡â–…â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–‚â–‡â–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.44434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.35156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 0.52963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.02234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-32-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/vvrkum95\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_195651-vvrkum95/logs\u001b[0m\n",
      "[W1121 20:04:42.991175721 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:42.021734286 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:04:42.412141657 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --eval_every=10 --eval_steps=200 --eval_metrics_every=1000 --run=challenge-32-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee56e2a-f7be-4534-9212-7ec04980166e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333f5594-10e7-4f27-848a-40aee13c1f53",
   "metadata": {},
   "source": [
    "#### What happens with an increased target_examples_per_step?\n",
    "\n",
    "While here try one more thing. Change target_examples_per_step to 64. This will give us 2 grad accum steps. First did:\n",
    "\n",
    "```\n",
    "cd /home/ubuntu/mynanochat/chatsft_checkpoints\n",
    "cp -r d20 d20-teps-32\n",
    "```\n",
    "\n",
    "(But realized later unnecessary becuase this new one will have half the number of steps and so get a different filename.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85012ae-2c6d-49a8-8a5a-f8c8a4f1c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:08:43.589000 25639 torch/distributed/run.py:803] \n",
      "W1121 20:08:43.589000 25639 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:08:43.589000 25639 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:08:43.589000 25639 torch/distributed/run.py:803] *****************************************\n",
      "overriding model_tag = d20\n",
      "overriding target_examples_per_step = 64\n",
      "overriding eval_every = 25\n",
      "overriding run = challenge-32-3\n",
      "user_config: {'run': 'challenge-32-3', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 64, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 25, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 3ijepqr7 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-32-redo-chat-eval-d20/wandb/run-20251121_200855-3ijepqr7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-32-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/3ijepqr7\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 64\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 2\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00350 | Training loss: 0.764929| lrm: 1.000000| num_tokens: 23,697\n",
      "Step 00001/00350 | Training loss: 1.216009| lrm: 0.997143| num_tokens: 23,965\n",
      "Step 00002/00350 | Training loss: 0.846789| lrm: 0.994286| num_tokens: 17,853\n",
      "Step 00003/00350 | Training loss: 0.417952| lrm: 0.991429| num_tokens: 19,427\n",
      "Step 00004/00350 | Training loss: 0.567021| lrm: 0.988571| num_tokens: 21,504\n",
      "Step 00005/00350 | Training loss: 1.074257| lrm: 0.985714| num_tokens: 25,506\n",
      "Step 00006/00350 | Training loss: 0.957608| lrm: 0.982857| num_tokens: 23,495\n",
      "Step 00007/00350 | Training loss: 0.883710| lrm: 0.980000| num_tokens: 18,056\n",
      "Step 00008/00350 | Training loss: 0.538877| lrm: 0.977143| num_tokens: 22,184\n",
      "Step 00009/00350 | Training loss: 0.593634| lrm: 0.974286| num_tokens: 22,883\n",
      "Step 00010/00350 | Training loss: 0.760241| lrm: 0.971429| num_tokens: 23,002\n",
      "Step 00011/00350 | Training loss: 0.692235| lrm: 0.968571| num_tokens: 18,725\n",
      "Step 00012/00350 | Training loss: 0.632046| lrm: 0.965714| num_tokens: 23,098\n",
      "Step 00013/00350 | Training loss: 0.919621| lrm: 0.962857| num_tokens: 20,935\n",
      "Step 00014/00350 | Training loss: 0.936437| lrm: 0.960000| num_tokens: 20,807\n",
      "Step 00015/00350 | Training loss: 1.143659| lrm: 0.957143| num_tokens: 16,624\n",
      "Step 00016/00350 | Training loss: 0.840078| lrm: 0.954286| num_tokens: 21,242\n",
      "Step 00017/00350 | Training loss: 0.843577| lrm: 0.951429| num_tokens: 22,642\n",
      "Step 00018/00350 | Training loss: 1.065358| lrm: 0.948571| num_tokens: 20,297\n",
      "Step 00019/00350 | Training loss: 0.610522| lrm: 0.945714| num_tokens: 12,642\n",
      "Step 00020/00350 | Training loss: 0.618021| lrm: 0.942857| num_tokens: 27,161\n",
      "Step 00021/00350 | Training loss: 0.995855| lrm: 0.940000| num_tokens: 22,284\n",
      "Step 00022/00350 | Training loss: 1.070644| lrm: 0.937143| num_tokens: 26,025\n",
      "Step 00023/00350 | Training loss: 1.078125| lrm: 0.934286| num_tokens: 22,521\n",
      "Step 00024/00350 | Training loss: 0.543267| lrm: 0.931429| num_tokens: 21,296\n",
      "Step 00025 | Validation loss: 1.012896\n",
      "Step 00025/00350 | Training loss: 0.276985| lrm: 0.928571| num_tokens: 18,669\n",
      "Step 00026/00350 | Training loss: 1.003969| lrm: 0.925714| num_tokens: 17,675\n",
      "Step 00027/00350 | Training loss: 0.897419| lrm: 0.922857| num_tokens: 16,139\n",
      "Step 00028/00350 | Training loss: 1.008760| lrm: 0.920000| num_tokens: 20,295\n",
      "Step 00029/00350 | Training loss: 1.061941| lrm: 0.917143| num_tokens: 21,316\n",
      "Step 00030/00350 | Training loss: 1.259118| lrm: 0.914286| num_tokens: 26,372\n",
      "Step 00031/00350 | Training loss: 0.574248| lrm: 0.911429| num_tokens: 22,776\n",
      "Step 00032/00350 | Training loss: 0.486753| lrm: 0.908571| num_tokens: 19,131\n",
      "Step 00033/00350 | Training loss: 0.559916| lrm: 0.905714| num_tokens: 21,104\n",
      "Step 00034/00350 | Training loss: 0.844196| lrm: 0.902857| num_tokens: 20,866\n",
      "Step 00035/00350 | Training loss: 0.732925| lrm: 0.900000| num_tokens: 19,968\n",
      "Step 00036/00350 | Training loss: 1.221412| lrm: 0.897143| num_tokens: 18,438\n",
      "Step 00037/00350 | Training loss: 0.703696| lrm: 0.894286| num_tokens: 14,492\n",
      "Step 00038/00350 | Training loss: 0.856445| lrm: 0.891429| num_tokens: 16,849\n",
      "Step 00039/00350 | Training loss: 0.583740| lrm: 0.888571| num_tokens: 21,072\n",
      "Step 00040/00350 | Training loss: 0.514015| lrm: 0.885714| num_tokens: 19,457\n",
      "Step 00041/00350 | Training loss: 0.539162| lrm: 0.882857| num_tokens: 25,613\n",
      "Step 00042/00350 | Training loss: 1.147459| lrm: 0.880000| num_tokens: 22,356\n",
      "Step 00043/00350 | Training loss: 1.019458| lrm: 0.877143| num_tokens: 19,535\n",
      "Step 00044/00350 | Training loss: 1.390440| lrm: 0.874286| num_tokens: 20,587\n",
      "Step 00045/00350 | Training loss: 0.753992| lrm: 0.871429| num_tokens: 21,327\n",
      "Step 00046/00350 | Training loss: 0.868762| lrm: 0.868571| num_tokens: 20,009\n",
      "Step 00047/00350 | Training loss: 0.959293| lrm: 0.865714| num_tokens: 18,859\n",
      "Step 00048/00350 | Training loss: 1.008208| lrm: 0.862857| num_tokens: 27,557\n",
      "Step 00049/00350 | Training loss: 1.139434| lrm: 0.860000| num_tokens: 17,537\n",
      "Step 00050 | Validation loss: 1.015344\n",
      "Step 00050/00350 | Training loss: 0.436913| lrm: 0.857143| num_tokens: 20,365\n",
      "Step 00051/00350 | Training loss: 0.715109| lrm: 0.854286| num_tokens: 21,937\n",
      "Step 00052/00350 | Training loss: 0.757876| lrm: 0.851429| num_tokens: 18,381\n",
      "Step 00053/00350 | Training loss: 1.046114| lrm: 0.848571| num_tokens: 21,317\n",
      "Step 00054/00350 | Training loss: 0.682926| lrm: 0.845714| num_tokens: 17,209\n",
      "Step 00055/00350 | Training loss: 0.704802| lrm: 0.842857| num_tokens: 22,978\n",
      "Step 00056/00350 | Training loss: 0.758594| lrm: 0.840000| num_tokens: 30,964\n",
      "Step 00057/00350 | Training loss: 0.319952| lrm: 0.837143| num_tokens: 25,399\n",
      "Step 00058/00350 | Training loss: 0.923423| lrm: 0.834286| num_tokens: 21,344\n",
      "Step 00059/00350 | Training loss: 0.645473| lrm: 0.831429| num_tokens: 23,698\n",
      "Step 00060/00350 | Training loss: 1.085117| lrm: 0.828571| num_tokens: 29,507\n",
      "Step 00061/00350 | Training loss: 0.878105| lrm: 0.825714| num_tokens: 13,900\n",
      "Step 00062/00350 | Training loss: 0.985616| lrm: 0.822857| num_tokens: 17,381\n",
      "Step 00063/00350 | Training loss: 1.056372| lrm: 0.820000| num_tokens: 20,273\n",
      "Step 00064/00350 | Training loss: 0.833928| lrm: 0.817143| num_tokens: 19,500\n",
      "Step 00065/00350 | Training loss: 0.940961| lrm: 0.814286| num_tokens: 22,975\n",
      "Step 00066/00350 | Training loss: 0.911986| lrm: 0.811429| num_tokens: 28,447\n",
      "Step 00067/00350 | Training loss: 1.377412| lrm: 0.808571| num_tokens: 24,233\n",
      "Step 00068/00350 | Training loss: 0.883933| lrm: 0.805714| num_tokens: 20,465\n",
      "Step 00069/00350 | Training loss: 0.957316| lrm: 0.802857| num_tokens: 23,190\n",
      "Step 00070/00350 | Training loss: 0.806635| lrm: 0.800000| num_tokens: 26,735\n",
      "Step 00071/00350 | Training loss: 1.171566| lrm: 0.797143| num_tokens: 19,242\n",
      "Step 00072/00350 | Training loss: 0.960354| lrm: 0.794286| num_tokens: 29,496\n",
      "Step 00073/00350 | Training loss: 0.646362| lrm: 0.791429| num_tokens: 20,021\n",
      "Step 00074/00350 | Training loss: 1.076940| lrm: 0.788571| num_tokens: 25,530\n",
      "Step 00075 | Validation loss: 1.014202\n",
      "Step 00075/00350 | Training loss: 0.673267| lrm: 0.785714| num_tokens: 24,815\n",
      "Step 00076/00350 | Training loss: 0.828686| lrm: 0.782857| num_tokens: 21,308\n",
      "Step 00077/00350 | Training loss: 0.836653| lrm: 0.780000| num_tokens: 20,567\n",
      "Step 00078/00350 | Training loss: 1.126100| lrm: 0.777143| num_tokens: 26,894\n",
      "Step 00079/00350 | Training loss: 0.720045| lrm: 0.774286| num_tokens: 16,766\n",
      "Step 00080/00350 | Training loss: 0.982984| lrm: 0.771429| num_tokens: 18,936\n",
      "Step 00081/00350 | Training loss: 0.492559| lrm: 0.768571| num_tokens: 19,264\n",
      "Step 00082/00350 | Training loss: 0.763292| lrm: 0.765714| num_tokens: 21,127\n",
      "Step 00083/00350 | Training loss: 0.906631| lrm: 0.762857| num_tokens: 22,856\n",
      "Step 00084/00350 | Training loss: 0.881405| lrm: 0.760000| num_tokens: 19,294\n",
      "Step 00085/00350 | Training loss: 0.690887| lrm: 0.757143| num_tokens: 23,620\n",
      "Step 00086/00350 | Training loss: 0.972422| lrm: 0.754286| num_tokens: 23,649\n",
      "Step 00087/00350 | Training loss: 1.049066| lrm: 0.751429| num_tokens: 18,967\n",
      "Step 00088/00350 | Training loss: 0.983854| lrm: 0.748571| num_tokens: 25,914\n",
      "Step 00089/00350 | Training loss: 0.956735| lrm: 0.745714| num_tokens: 20,709\n",
      "Step 00090/00350 | Training loss: 0.876099| lrm: 0.742857| num_tokens: 27,738\n",
      "Step 00091/00350 | Training loss: 0.491476| lrm: 0.740000| num_tokens: 24,043\n",
      "Step 00092/00350 | Training loss: 0.666084| lrm: 0.737143| num_tokens: 24,834\n",
      "Step 00093/00350 | Training loss: 0.480408| lrm: 0.734286| num_tokens: 25,468\n",
      "Step 00094/00350 | Training loss: 0.969952| lrm: 0.731429| num_tokens: 18,721\n",
      "Step 00095/00350 | Training loss: 0.905823| lrm: 0.728571| num_tokens: 28,158\n",
      "Step 00096/00350 | Training loss: 1.102486| lrm: 0.725714| num_tokens: 20,612\n",
      "Step 00097/00350 | Training loss: 0.547026| lrm: 0.722857| num_tokens: 22,518\n",
      "Step 00098/00350 | Training loss: 1.042285| lrm: 0.720000| num_tokens: 22,477\n",
      "Step 00099/00350 | Training loss: 0.616081| lrm: 0.717143| num_tokens: 20,055\n",
      "Step 00100 | Validation loss: 1.013824\n",
      "Step 00100/00350 | Training loss: 0.881635| lrm: 0.714286| num_tokens: 27,463\n",
      "Step 00101/00350 | Training loss: 1.075785| lrm: 0.711429| num_tokens: 24,038\n",
      "Step 00102/00350 | Training loss: 0.654532| lrm: 0.708571| num_tokens: 22,563\n",
      "Step 00103/00350 | Training loss: 0.642589| lrm: 0.705714| num_tokens: 21,796\n",
      "Step 00104/00350 | Training loss: 0.569849| lrm: 0.702857| num_tokens: 23,815\n",
      "Step 00105/00350 | Training loss: 1.066323| lrm: 0.700000| num_tokens: 23,294\n",
      "Step 00106/00350 | Training loss: 0.736903| lrm: 0.697143| num_tokens: 25,927\n",
      "Step 00107/00350 | Training loss: 0.675663| lrm: 0.694286| num_tokens: 20,937\n",
      "Step 00108/00350 | Training loss: 1.014037| lrm: 0.691429| num_tokens: 16,842\n",
      "Step 00109/00350 | Training loss: 0.884912| lrm: 0.688571| num_tokens: 24,244\n",
      "Step 00110/00350 | Training loss: 0.777056| lrm: 0.685714| num_tokens: 16,327\n",
      "Step 00111/00350 | Training loss: 1.151198| lrm: 0.682857| num_tokens: 19,300\n",
      "Step 00112/00350 | Training loss: 1.128626| lrm: 0.680000| num_tokens: 18,948\n",
      "Step 00113/00350 | Training loss: 0.599003| lrm: 0.677143| num_tokens: 20,785\n",
      "Step 00114/00350 | Training loss: 1.135798| lrm: 0.674286| num_tokens: 21,737\n",
      "Step 00115/00350 | Training loss: 1.209277| lrm: 0.671429| num_tokens: 19,529\n",
      "Step 00116/00350 | Training loss: 0.740416| lrm: 0.668571| num_tokens: 21,008\n",
      "Step 00117/00350 | Training loss: 1.032104| lrm: 0.665714| num_tokens: 23,973\n",
      "Step 00118/00350 | Training loss: 0.844008| lrm: 0.662857| num_tokens: 14,807\n",
      "Step 00119/00350 | Training loss: 1.263059| lrm: 0.660000| num_tokens: 25,997\n",
      "Step 00120/00350 | Training loss: 1.008893| lrm: 0.657143| num_tokens: 24,169\n",
      "Step 00121/00350 | Training loss: 0.665962| lrm: 0.654286| num_tokens: 18,722\n",
      "Step 00122/00350 | Training loss: 1.031037| lrm: 0.651429| num_tokens: 19,448\n",
      "Step 00123/00350 | Training loss: 0.559107| lrm: 0.648571| num_tokens: 19,419\n",
      "Step 00124/00350 | Training loss: 0.714178| lrm: 0.645714| num_tokens: 23,628\n",
      "Step 00125 | Validation loss: 1.013773\n",
      "Step 00125/00350 | Training loss: 1.207566| lrm: 0.642857| num_tokens: 25,120\n",
      "Step 00126/00350 | Training loss: 0.820074| lrm: 0.640000| num_tokens: 25,002\n",
      "Step 00127/00350 | Training loss: 0.742096| lrm: 0.637143| num_tokens: 19,282\n",
      "Step 00128/00350 | Training loss: 0.538788| lrm: 0.634286| num_tokens: 18,940\n",
      "Step 00129/00350 | Training loss: 0.912543| lrm: 0.631429| num_tokens: 18,182\n",
      "Step 00130/00350 | Training loss: 0.678163| lrm: 0.628571| num_tokens: 23,800\n",
      "Step 00131/00350 | Training loss: 0.621516| lrm: 0.625714| num_tokens: 17,265\n",
      "Step 00132/00350 | Training loss: 0.819643| lrm: 0.622857| num_tokens: 20,746\n",
      "Step 00133/00350 | Training loss: 0.547227| lrm: 0.620000| num_tokens: 20,221\n",
      "Step 00134/00350 | Training loss: 0.757715| lrm: 0.617143| num_tokens: 21,825\n",
      "Step 00135/00350 | Training loss: 1.393938| lrm: 0.614286| num_tokens: 18,025\n",
      "Step 00136/00350 | Training loss: 0.621761| lrm: 0.611429| num_tokens: 22,850\n",
      "Step 00137/00350 | Training loss: 0.582508| lrm: 0.608571| num_tokens: 23,092\n",
      "Step 00138/00350 | Training loss: 0.571254| lrm: 0.605714| num_tokens: 18,290\n",
      "Step 00139/00350 | Training loss: 0.891085| lrm: 0.602857| num_tokens: 22,403\n",
      "Step 00140/00350 | Training loss: 1.005159| lrm: 0.600000| num_tokens: 23,074\n",
      "Step 00141/00350 | Training loss: 0.620758| lrm: 0.597143| num_tokens: 19,856\n",
      "Step 00142/00350 | Training loss: 0.862563| lrm: 0.594286| num_tokens: 22,863\n",
      "Step 00143/00350 | Training loss: 0.259898| lrm: 0.591429| num_tokens: 21,461\n",
      "Step 00144/00350 | Training loss: 0.679577| lrm: 0.588571| num_tokens: 14,440\n",
      "Step 00145/00350 | Training loss: 1.001055| lrm: 0.585714| num_tokens: 21,887\n",
      "Step 00146/00350 | Training loss: 0.828657| lrm: 0.582857| num_tokens: 26,697\n",
      "Step 00147/00350 | Training loss: 1.109481| lrm: 0.580000| num_tokens: 16,437\n",
      "Step 00148/00350 | Training loss: 0.579880| lrm: 0.577143| num_tokens: 29,838\n",
      "Step 00149/00350 | Training loss: 0.642250| lrm: 0.574286| num_tokens: 17,834\n",
      "Step 00150 | Validation loss: 1.013765\n",
      "Step 00150/00350 | Training loss: 0.829344| lrm: 0.571429| num_tokens: 21,744\n",
      "Step 00151/00350 | Training loss: 0.958288| lrm: 0.568571| num_tokens: 23,798\n",
      "Step 00152/00350 | Training loss: 0.865891| lrm: 0.565714| num_tokens: 22,865\n",
      "Step 00153/00350 | Training loss: 1.097633| lrm: 0.562857| num_tokens: 24,451\n",
      "Step 00154/00350 | Training loss: 0.938938| lrm: 0.560000| num_tokens: 17,848\n",
      "Step 00155/00350 | Training loss: 1.269958| lrm: 0.557143| num_tokens: 22,423\n",
      "Step 00156/00350 | Training loss: 0.741281| lrm: 0.554286| num_tokens: 19,863\n",
      "Step 00157/00350 | Training loss: 0.531085| lrm: 0.551429| num_tokens: 26,026\n",
      "Step 00158/00350 | Training loss: 0.408566| lrm: 0.548571| num_tokens: 18,094\n",
      "Step 00159/00350 | Training loss: 1.214632| lrm: 0.545714| num_tokens: 21,430\n",
      "Step 00160/00350 | Training loss: 1.084739| lrm: 0.542857| num_tokens: 15,317\n",
      "Step 00161/00350 | Training loss: 0.793025| lrm: 0.540000| num_tokens: 16,230\n",
      "Step 00162/00350 | Training loss: 0.634549| lrm: 0.537143| num_tokens: 20,809\n",
      "Step 00163/00350 | Training loss: 0.708413| lrm: 0.534286| num_tokens: 19,351\n",
      "Step 00164/00350 | Training loss: 0.656981| lrm: 0.531429| num_tokens: 23,071\n",
      "Step 00165/00350 | Training loss: 0.807560| lrm: 0.528571| num_tokens: 15,879\n",
      "Step 00166/00350 | Training loss: 1.271721| lrm: 0.525714| num_tokens: 20,414\n",
      "Step 00167/00350 | Training loss: 0.442954| lrm: 0.522857| num_tokens: 19,412\n",
      "Step 00168/00350 | Training loss: 0.766080| lrm: 0.520000| num_tokens: 24,803\n",
      "Step 00169/00350 | Training loss: 0.825088| lrm: 0.517143| num_tokens: 15,824\n",
      "Step 00170/00350 | Training loss: 1.193275| lrm: 0.514286| num_tokens: 20,127\n",
      "Step 00171/00350 | Training loss: 1.133996| lrm: 0.511429| num_tokens: 17,857\n",
      "Step 00172/00350 | Training loss: 0.800048| lrm: 0.508571| num_tokens: 20,230\n",
      "Step 00173/00350 | Training loss: 0.681242| lrm: 0.505714| num_tokens: 22,015\n",
      "Step 00174/00350 | Training loss: 0.666848| lrm: 0.502857| num_tokens: 19,808\n",
      "Step 00175 | Validation loss: 1.013792\n",
      "Step 00175/00350 | Training loss: 0.767319| lrm: 0.500000| num_tokens: 20,747\n",
      "Step 00176/00350 | Training loss: 0.865099| lrm: 0.497143| num_tokens: 21,058\n",
      "Step 00177/00350 | Training loss: 0.836632| lrm: 0.494286| num_tokens: 20,146\n",
      "Step 00178/00350 | Training loss: 0.659048| lrm: 0.491429| num_tokens: 16,599\n",
      "Step 00179/00350 | Training loss: 0.712752| lrm: 0.488571| num_tokens: 18,986\n",
      "Step 00180/00350 | Training loss: 0.743313| lrm: 0.485714| num_tokens: 20,464\n",
      "Step 00181/00350 | Training loss: 0.464126| lrm: 0.482857| num_tokens: 22,411\n",
      "Step 00182/00350 | Training loss: 0.521694| lrm: 0.480000| num_tokens: 21,056\n",
      "Step 00183/00350 | Training loss: 0.773231| lrm: 0.477143| num_tokens: 20,563\n",
      "Step 00184/00350 | Training loss: 0.823797| lrm: 0.474286| num_tokens: 27,316\n",
      "Step 00185/00350 | Training loss: 0.923504| lrm: 0.471429| num_tokens: 22,579\n",
      "Step 00186/00350 | Training loss: 0.550587| lrm: 0.468571| num_tokens: 20,226\n",
      "Step 00187/00350 | Training loss: 1.158396| lrm: 0.465714| num_tokens: 26,822\n",
      "Step 00188/00350 | Training loss: 0.671942| lrm: 0.462857| num_tokens: 23,350\n",
      "Step 00189/00350 | Training loss: 0.895385| lrm: 0.460000| num_tokens: 19,816\n",
      "Step 00190/00350 | Training loss: 0.907366| lrm: 0.457143| num_tokens: 22,753\n",
      "Step 00191/00350 | Training loss: 0.529308| lrm: 0.454286| num_tokens: 28,499\n",
      "Step 00192/00350 | Training loss: 0.434809| lrm: 0.451429| num_tokens: 22,694\n",
      "Step 00193/00350 | Training loss: 1.030825| lrm: 0.448571| num_tokens: 21,027\n",
      "Step 00194/00350 | Training loss: 0.832617| lrm: 0.445714| num_tokens: 27,147\n",
      "Step 00195/00350 | Training loss: 0.675124| lrm: 0.442857| num_tokens: 26,661\n",
      "Step 00196/00350 | Training loss: 1.291059| lrm: 0.440000| num_tokens: 23,954\n",
      "Step 00197/00350 | Training loss: 0.849981| lrm: 0.437143| num_tokens: 23,521\n",
      "Step 00198/00350 | Training loss: 1.091390| lrm: 0.434286| num_tokens: 19,187\n",
      "Step 00199/00350 | Training loss: 0.898843| lrm: 0.431429| num_tokens: 16,629\n",
      "Step 00200 | Validation loss: 1.013662\n",
      "final: 341/1024 (33.30%)\n",
      "final: 442/1024 (43.16%)\n",
      "Step 00200 | mmlu_acc: 0.333008, arc_easy_acc: 0.431641\n",
      "Step 00200/00350 | Training loss: 0.743400| lrm: 0.428571| num_tokens: 16,715\n",
      "Step 00201/00350 | Training loss: 1.252614| lrm: 0.425714| num_tokens: 22,486\n",
      "Step 00202/00350 | Training loss: 0.693749| lrm: 0.422857| num_tokens: 19,704\n",
      "Step 00203/00350 | Training loss: 0.557802| lrm: 0.420000| num_tokens: 15,983\n",
      "Step 00204/00350 | Training loss: 0.599012| lrm: 0.417143| num_tokens: 21,383\n",
      "Step 00205/00350 | Training loss: 0.884143| lrm: 0.414286| num_tokens: 26,291\n",
      "Step 00206/00350 | Training loss: 0.856832| lrm: 0.411429| num_tokens: 19,237\n",
      "Step 00207/00350 | Training loss: 0.750140| lrm: 0.408571| num_tokens: 28,527\n",
      "Step 00208/00350 | Training loss: 0.663193| lrm: 0.405714| num_tokens: 23,974\n",
      "Step 00209/00350 | Training loss: 0.973766| lrm: 0.402857| num_tokens: 13,767\n",
      "Step 00210/00350 | Training loss: 0.829596| lrm: 0.400000| num_tokens: 23,182\n",
      "Step 00211/00350 | Training loss: 0.579453| lrm: 0.397143| num_tokens: 28,976\n",
      "Step 00212/00350 | Training loss: 0.856607| lrm: 0.394286| num_tokens: 22,608\n",
      "Step 00213/00350 | Training loss: 0.861917| lrm: 0.391429| num_tokens: 15,277\n",
      "Step 00214/00350 | Training loss: 0.688621| lrm: 0.388571| num_tokens: 20,534\n",
      "Step 00215/00350 | Training loss: 1.603737| lrm: 0.385714| num_tokens: 20,125\n",
      "Step 00216/00350 | Training loss: 0.598247| lrm: 0.382857| num_tokens: 23,862\n",
      "Step 00217/00350 | Training loss: 0.674006| lrm: 0.380000| num_tokens: 24,062\n",
      "Step 00218/00350 | Training loss: 0.946923| lrm: 0.377143| num_tokens: 25,951\n",
      "Step 00219/00350 | Training loss: 0.821425| lrm: 0.374286| num_tokens: 22,262\n",
      "Step 00220/00350 | Training loss: 0.782259| lrm: 0.371429| num_tokens: 22,277\n",
      "Step 00221/00350 | Training loss: 0.767220| lrm: 0.368571| num_tokens: 22,812\n",
      "Step 00222/00350 | Training loss: 0.800535| lrm: 0.365714| num_tokens: 23,029\n",
      "Step 00223/00350 | Training loss: 1.072080| lrm: 0.362857| num_tokens: 28,319\n",
      "Step 00224/00350 | Training loss: 0.824452| lrm: 0.360000| num_tokens: 19,408\n",
      "Step 00225 | Validation loss: 1.013462\n",
      "Step 00225/00350 | Training loss: 0.883758| lrm: 0.357143| num_tokens: 26,519\n",
      "Step 00226/00350 | Training loss: 0.775794| lrm: 0.354286| num_tokens: 21,842\n",
      "Step 00227/00350 | Training loss: 0.886275| lrm: 0.351429| num_tokens: 15,708\n",
      "Step 00228/00350 | Training loss: 0.610202| lrm: 0.348571| num_tokens: 20,654\n",
      "Step 00229/00350 | Training loss: 0.748568| lrm: 0.345714| num_tokens: 20,836\n",
      "Step 00230/00350 | Training loss: 0.809325| lrm: 0.342857| num_tokens: 21,035\n",
      "Step 00231/00350 | Training loss: 1.127410| lrm: 0.340000| num_tokens: 17,199\n",
      "Step 00232/00350 | Training loss: 1.061152| lrm: 0.337143| num_tokens: 18,135\n",
      "Step 00233/00350 | Training loss: 0.654453| lrm: 0.334286| num_tokens: 22,270\n",
      "Step 00234/00350 | Training loss: 0.442296| lrm: 0.331429| num_tokens: 18,009\n",
      "Step 00235/00350 | Training loss: 0.942179| lrm: 0.328571| num_tokens: 23,919\n",
      "Step 00236/00350 | Training loss: 0.657771| lrm: 0.325714| num_tokens: 23,033\n",
      "Step 00237/00350 | Training loss: 0.923197| lrm: 0.322857| num_tokens: 27,413\n",
      "Step 00238/00350 | Training loss: 1.008688| lrm: 0.320000| num_tokens: 24,746\n",
      "Step 00239/00350 | Training loss: 0.762190| lrm: 0.317143| num_tokens: 20,354\n",
      "Step 00240/00350 | Training loss: 1.082911| lrm: 0.314286| num_tokens: 20,602\n",
      "Step 00241/00350 | Training loss: 0.689520| lrm: 0.311429| num_tokens: 20,911\n",
      "Step 00242/00350 | Training loss: 1.128872| lrm: 0.308571| num_tokens: 21,278\n",
      "Step 00243/00350 | Training loss: 0.663569| lrm: 0.305714| num_tokens: 18,246\n",
      "Step 00244/00350 | Training loss: 0.946522| lrm: 0.302857| num_tokens: 23,648\n",
      "Step 00245/00350 | Training loss: 0.925393| lrm: 0.300000| num_tokens: 17,297\n",
      "Step 00246/00350 | Training loss: 0.574690| lrm: 0.297143| num_tokens: 21,605\n",
      "Step 00247/00350 | Training loss: 0.870378| lrm: 0.294286| num_tokens: 19,446\n",
      "Step 00248/00350 | Training loss: 0.726404| lrm: 0.291429| num_tokens: 22,900\n",
      "Step 00249/00350 | Training loss: 0.296627| lrm: 0.288571| num_tokens: 26,007\n",
      "Step 00250 | Validation loss: 1.013535\n",
      "Step 00250/00350 | Training loss: 0.631163| lrm: 0.285714| num_tokens: 23,627\n",
      "Step 00251/00350 | Training loss: 1.390546| lrm: 0.282857| num_tokens: 22,858\n",
      "Step 00252/00350 | Training loss: 0.831599| lrm: 0.280000| num_tokens: 18,662\n",
      "Step 00253/00350 | Training loss: 0.596892| lrm: 0.277143| num_tokens: 16,408\n",
      "Step 00254/00350 | Training loss: 0.862581| lrm: 0.274286| num_tokens: 28,315\n",
      "Step 00255/00350 | Training loss: 0.578921| lrm: 0.271429| num_tokens: 15,528\n",
      "Step 00256/00350 | Training loss: 0.792211| lrm: 0.268571| num_tokens: 20,164\n",
      "Step 00257/00350 | Training loss: 1.181795| lrm: 0.265714| num_tokens: 20,638\n",
      "Step 00258/00350 | Training loss: 0.550389| lrm: 0.262857| num_tokens: 22,778\n",
      "Step 00259/00350 | Training loss: 0.850613| lrm: 0.260000| num_tokens: 18,735\n",
      "Step 00260/00350 | Training loss: 0.970184| lrm: 0.257143| num_tokens: 24,497\n",
      "Step 00261/00350 | Training loss: 0.653433| lrm: 0.254286| num_tokens: 21,204\n",
      "Step 00262/00350 | Training loss: 1.542098| lrm: 0.251429| num_tokens: 17,735\n",
      "Step 00263/00350 | Training loss: 1.166348| lrm: 0.248571| num_tokens: 14,150\n",
      "Step 00264/00350 | Training loss: 1.140811| lrm: 0.245714| num_tokens: 29,105\n",
      "Step 00265/00350 | Training loss: 0.612862| lrm: 0.242857| num_tokens: 19,417\n",
      "Step 00266/00350 | Training loss: 1.049325| lrm: 0.240000| num_tokens: 19,712\n",
      "Step 00267/00350 | Training loss: 0.644293| lrm: 0.237143| num_tokens: 21,993\n",
      "Step 00268/00350 | Training loss: 0.700914| lrm: 0.234286| num_tokens: 16,721\n",
      "Step 00269/00350 | Training loss: 0.809253| lrm: 0.231429| num_tokens: 23,298\n",
      "Step 00270/00350 | Training loss: 1.027470| lrm: 0.228571| num_tokens: 23,533\n",
      "Step 00271/00350 | Training loss: 0.368280| lrm: 0.225714| num_tokens: 22,754\n",
      "Step 00272/00350 | Training loss: 0.765791| lrm: 0.222857| num_tokens: 25,303\n",
      "Step 00273/00350 | Training loss: 1.264360| lrm: 0.220000| num_tokens: 19,867\n",
      "Step 00274/00350 | Training loss: 1.074740| lrm: 0.217143| num_tokens: 21,679\n",
      "Step 00275 | Validation loss: 1.013514\n",
      "Step 00275/00350 | Training loss: 0.763413| lrm: 0.214286| num_tokens: 24,667\n",
      "Step 00276/00350 | Training loss: 0.257714| lrm: 0.211429| num_tokens: 19,110\n",
      "Step 00277/00350 | Training loss: 0.892929| lrm: 0.208571| num_tokens: 24,570\n",
      "Step 00278/00350 | Training loss: 0.660621| lrm: 0.205714| num_tokens: 16,786\n",
      "Step 00279/00350 | Training loss: 1.007091| lrm: 0.202857| num_tokens: 18,952\n",
      "Step 00280/00350 | Training loss: 1.177460| lrm: 0.200000| num_tokens: 20,286\n",
      "Step 00281/00350 | Training loss: 0.940824| lrm: 0.197143| num_tokens: 21,563\n",
      "Step 00282/00350 | Training loss: 1.069730| lrm: 0.194286| num_tokens: 24,760\n",
      "Step 00283/00350 | Training loss: 0.919260| lrm: 0.191429| num_tokens: 30,745\n",
      "Step 00284/00350 | Training loss: 0.891295| lrm: 0.188571| num_tokens: 19,479\n",
      "Step 00285/00350 | Training loss: 0.722666| lrm: 0.185714| num_tokens: 22,158\n",
      "Step 00286/00350 | Training loss: 1.091535| lrm: 0.182857| num_tokens: 17,651\n",
      "Step 00287/00350 | Training loss: 0.425346| lrm: 0.180000| num_tokens: 22,123\n",
      "Step 00288/00350 | Training loss: 0.724442| lrm: 0.177143| num_tokens: 22,256\n",
      "Step 00289/00350 | Training loss: 0.488895| lrm: 0.174286| num_tokens: 18,410\n",
      "Step 00290/00350 | Training loss: 1.044474| lrm: 0.171429| num_tokens: 22,566\n",
      "Step 00291/00350 | Training loss: 0.597814| lrm: 0.168571| num_tokens: 21,598\n",
      "Step 00292/00350 | Training loss: 0.677322| lrm: 0.165714| num_tokens: 28,748\n",
      "Step 00293/00350 | Training loss: 0.766798| lrm: 0.162857| num_tokens: 17,715\n",
      "Step 00294/00350 | Training loss: 0.473460| lrm: 0.160000| num_tokens: 21,371\n",
      "Step 00295/00350 | Training loss: 0.761640| lrm: 0.157143| num_tokens: 18,911\n",
      "Step 00296/00350 | Training loss: 1.070050| lrm: 0.154286| num_tokens: 23,538\n",
      "Step 00297/00350 | Training loss: 0.467338| lrm: 0.151429| num_tokens: 19,632\n",
      "Step 00298/00350 | Training loss: 1.316435| lrm: 0.148571| num_tokens: 16,494\n",
      "Step 00299/00350 | Training loss: 1.426041| lrm: 0.145714| num_tokens: 18,026\n",
      "Step 00300 | Validation loss: 1.013547\n",
      "Step 00300/00350 | Training loss: 1.064087| lrm: 0.142857| num_tokens: 21,449\n",
      "Step 00301/00350 | Training loss: 1.098715| lrm: 0.140000| num_tokens: 16,213\n",
      "Step 00302/00350 | Training loss: 0.762859| lrm: 0.137143| num_tokens: 23,970\n",
      "Step 00303/00350 | Training loss: 0.647280| lrm: 0.134286| num_tokens: 22,077\n",
      "Step 00304/00350 | Training loss: 1.128208| lrm: 0.131429| num_tokens: 21,754\n",
      "Step 00305/00350 | Training loss: 0.971285| lrm: 0.128571| num_tokens: 23,500\n",
      "Step 00306/00350 | Training loss: 1.077626| lrm: 0.125714| num_tokens: 26,065\n",
      "Step 00307/00350 | Training loss: 0.922134| lrm: 0.122857| num_tokens: 27,293\n",
      "Step 00308/00350 | Training loss: 0.719219| lrm: 0.120000| num_tokens: 19,394\n",
      "Step 00309/00350 | Training loss: 0.706073| lrm: 0.117143| num_tokens: 18,055\n",
      "Step 00310/00350 | Training loss: 0.492713| lrm: 0.114286| num_tokens: 20,058\n",
      "Step 00311/00350 | Training loss: 0.904332| lrm: 0.111429| num_tokens: 21,869\n",
      "Step 00312/00350 | Training loss: 0.590133| lrm: 0.108571| num_tokens: 19,210\n",
      "Step 00313/00350 | Training loss: 0.486698| lrm: 0.105714| num_tokens: 20,075\n",
      "Step 00314/00350 | Training loss: 0.607718| lrm: 0.102857| num_tokens: 19,356\n",
      "Step 00315/00350 | Training loss: 0.769173| lrm: 0.100000| num_tokens: 22,446\n",
      "Step 00316/00350 | Training loss: 0.912629| lrm: 0.097143| num_tokens: 22,162\n",
      "Step 00317/00350 | Training loss: 0.767535| lrm: 0.094286| num_tokens: 19,861\n",
      "Step 00318/00350 | Training loss: 0.861920| lrm: 0.091429| num_tokens: 21,239\n",
      "Step 00319/00350 | Training loss: 0.688453| lrm: 0.088571| num_tokens: 21,396\n",
      "Step 00320/00350 | Training loss: 0.911514| lrm: 0.085714| num_tokens: 22,836\n",
      "Step 00321/00350 | Training loss: 0.890679| lrm: 0.082857| num_tokens: 20,235\n",
      "Step 00322/00350 | Training loss: 1.237376| lrm: 0.080000| num_tokens: 22,307\n",
      "Step 00323/00350 | Training loss: 1.062386| lrm: 0.077143| num_tokens: 21,030\n",
      "Step 00324/00350 | Training loss: 0.661721| lrm: 0.074286| num_tokens: 22,608\n",
      "Step 00325 | Validation loss: 1.013565\n",
      "Step 00325/00350 | Training loss: 1.299510| lrm: 0.071429| num_tokens: 21,234\n",
      "Step 00326/00350 | Training loss: 0.647976| lrm: 0.068571| num_tokens: 20,120\n",
      "Step 00327/00350 | Training loss: 0.892722| lrm: 0.065714| num_tokens: 20,212\n",
      "Step 00328/00350 | Training loss: 0.386133| lrm: 0.062857| num_tokens: 21,480\n",
      "Step 00329/00350 | Training loss: 0.749796| lrm: 0.060000| num_tokens: 17,884\n",
      "Step 00330/00350 | Training loss: 1.118533| lrm: 0.057143| num_tokens: 21,786\n",
      "Step 00331/00350 | Training loss: 0.708341| lrm: 0.054286| num_tokens: 17,726\n",
      "Step 00332/00350 | Training loss: 1.654420| lrm: 0.051429| num_tokens: 19,345\n",
      "Step 00333/00350 | Training loss: 0.848859| lrm: 0.048571| num_tokens: 24,221\n",
      "Step 00334/00350 | Training loss: 1.122094| lrm: 0.045714| num_tokens: 22,116\n",
      "Step 00335/00350 | Training loss: 0.788455| lrm: 0.042857| num_tokens: 20,408\n",
      "Step 00336/00350 | Training loss: 0.696672| lrm: 0.040000| num_tokens: 20,396\n",
      "Step 00337/00350 | Training loss: 0.617865| lrm: 0.037143| num_tokens: 20,835\n",
      "Step 00338/00350 | Training loss: 1.021176| lrm: 0.034286| num_tokens: 26,435\n",
      "Step 00339/00350 | Training loss: 0.797400| lrm: 0.031429| num_tokens: 20,397\n",
      "Step 00340/00350 | Training loss: 0.678952| lrm: 0.028571| num_tokens: 21,475\n",
      "Step 00341/00350 | Training loss: 0.924904| lrm: 0.025714| num_tokens: 24,249\n",
      "Step 00342/00350 | Training loss: 0.594336| lrm: 0.022857| num_tokens: 22,038\n",
      "Step 00343/00350 | Training loss: 0.781416| lrm: 0.020000| num_tokens: 23,518\n",
      "Step 00344/00350 | Training loss: 0.506033| lrm: 0.017143| num_tokens: 21,110\n",
      "Step 00345/00350 | Training loss: 0.547892| lrm: 0.014286| num_tokens: 19,269\n",
      "Step 00346/00350 | Training loss: 1.140603| lrm: 0.011429| num_tokens: 21,616\n",
      "Step 00347/00350 | Training loss: 1.042098| lrm: 0.008571| num_tokens: 18,637\n",
      "Step 00348/00350 | Training loss: 1.216339| lrm: 0.005714| num_tokens: 22,858\n",
      "Step 00349 | Validation loss: 1.013553\n",
      "final: 357/1024 (34.86%)\n",
      "final: 461/1024 (45.02%)\n",
      "Step 00349 | mmlu_acc: 0.348633, arc_easy_acc: 0.450195\n",
      "[W1121 20:11:04.059208679 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:04.099389445 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:04.508251403 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:05.599740631 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:05.887307823 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000349.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000349.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 360-365, summary, console lines 372-382 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–„â–…â–‡â–„â–„â–‡â–â–„â–‡â–‚â–â–‚â–„â–…â–†â–‚â–ƒâ–†â–â–„â–„â–†â–„â–ƒâ–ˆâ–‚â–„â–ƒâ–„â–…â–â–‡â–ƒâ–‚â–…â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–†â–„â–‚â–„â–ƒâ–‡â–†â–†â–†â–…â–†â–‡â–†â–ƒâ–„â–ƒâ–„â–ˆâ–„â–ˆâ–„â–‡â–„â–…â–ƒâ–†â–†â–ƒâ–‡â–‡â–…â–‡â–†â–‚â–‡â–…â–…â–â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–„â–ˆâ–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.4502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.34863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 22858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 1.21634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-32-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/3ijepqr7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_200855-3ijepqr7/logs\u001b[0m\n",
      "[W1121 20:11:09.978438360 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:09.103440793 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:11:09.169122339 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --target_examples_per_step=64 --eval_every=25 --run=challenge-32-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c8922-b3c3-4b17-8179-fbc293283489",
   "metadata": {},
   "source": [
    "#### Do chat eval on this sft model\n",
    "\n",
    "Run chat eval on the model trained just above where target_examples_per_step was 64 leading to ~half the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647d04e1-9a8f-4ea6-9ee2-a7514dbf1c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:20:35.302000 30188 torch/distributed/run.py:803] \n",
      "W1121 20:20:35.302000 30188 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:20:35.302000 30188 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:20:35.302000 30188 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 349\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1049/2376 (44.15%)\n",
      "ARC-Easy accuracy: 44.15%\n",
      "final: 371/1172 (31.66%)\n",
      "ARC-Challenge accuracy: 31.66%\n",
      "final: 4538/14042 (32.32%)\n",
      "MMLU accuracy: 32.32%\n",
      "\u001b[KRank 7 | 7/164 (4.27%)]]\n",
      "\u001b[KRank 6 | 8/165 (4.85%)]]\n",
      "\u001b[KRank 5 | 6/165 (3.64%)]]\n",
      "\u001b[KRank 0 | 11/165 (6.67%)]\n",
      "\u001b[KRank 2 | 9/165 (5.45%)]]\n",
      "\u001b[KRank 3 | 14/165 (8.48%)]\n",
      "\u001b[KRank 4 | 6/165 (3.64%)]\n",
      "\u001b[KRank 1 | 10/165 (6.06%)]\n",
      "==================================================\n",
      "final: 71/1319 (5.38%)\n",
      "GSM8K accuracy: 5.38%\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 5 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "==================================================\n",
      "final: 10/164 (6.10%)\n",
      "HumanEval accuracy: 6.10%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 5 | 31/32 (96.88%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]\n",
      "==================================================\n",
      "final: 249/256 (97.27%)\n",
      "SpellingBee accuracy: 97.27%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --model-tag=d20 --step=349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d09db5-1a28-4b2b-9100-d96529dce7a1",
   "metadata": {},
   "source": [
    "### Do chat eval on the 700 step SFT model\n",
    "\n",
    "I hate to do because it's so slow, but when I started running the command above without specifying steps by mistake, and interrupted and started again, I thought the ARC-Easy accuracy didn't match what I measured near the top of this notebook and that's concerning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765c78e5-c860-4b3b-8e89-86fd9027ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:28:07.911000 33105 torch/distributed/run.py:803] \n",
      "W1121 20:28:07.911000 33105 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:28:07.911000 33105 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:28:07.911000 33105 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1048/2376 (44.11%)\n",
      "ARC-Easy accuracy: 44.11%\n",
      "final: 373/1172 (31.83%)\n",
      "ARC-Challenge accuracy: 31.83%\n",
      "final: 4504/14042 (32.08%)\n",
      "MMLU accuracy: 32.08%\n",
      "\u001b[KRank 4 | 7/165 (4.24%)]]\n",
      "\u001b[KRank 7 | 7/164 (4.27%)]]\n",
      "\u001b[KRank 6 | 7/165 (4.24%)]]\n",
      "\u001b[KRank 0 | 14/165 (8.48%)]\n",
      "\u001b[KRank 5 | 7/165 (4.24%)]\n",
      "\u001b[KRank 1 | 8/165 (4.85%)]]\n",
      "\u001b[KRank 3 | 12/165 (7.27%)]\n",
      "\u001b[KRank 2 | 8/165 (4.85%)]\n",
      "==================================================\n",
      "final: 70/1319 (5.31%)\n",
      "GSM8K accuracy: 5.31%\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 5 | 2/20 (10.00%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "==================================================\n",
      "final: 11/164 (6.71%)\n",
      "HumanEval accuracy: 6.71%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "\u001b[KRank 5 | 30/32 (93.75%)]\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001a210-b65b-42f9-85a4-28d1fadc508d",
   "metadata": {},
   "source": [
    "^ The accuracies are different. That shouldn't be. Nothing about changing the engine, of changing how often we evaluate the validation loss should change the trained model.\n",
    "\n",
    "#### Do SFT train one more time with the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a791aa-e4b9-4811-bf0f-fca2970a8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:38:39.772000 36279 torch/distributed/run.py:803] \n",
      "W1121 20:38:39.772000 36279 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:38:39.772000 36279 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:38:39.772000 36279 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-32-4\n",
      "user_config: {'run': 'challenge-32-4', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 100, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-32-redo-chat-eval-d20/wandb/run-20251121_203851-se8tsmc2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-32-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/se8tsmc2\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00701 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00701 | Training loss: 0.765703| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 1.094785| lrm: 0.997147| num_tokens: 11,283\n",
      "Step 00003/00701 | Training loss: 1.216676| lrm: 0.995720| num_tokens: 12,682\n",
      "Step 00004/00701 | Training loss: 0.786972| lrm: 0.994294| num_tokens: 9,509\n",
      "Step 00005/00701 | Training loss: 0.847359| lrm: 0.992867| num_tokens: 8,344\n",
      "Step 00006/00701 | Training loss: 0.694152| lrm: 0.991441| num_tokens: 8,763\n",
      "Step 00007/00701 | Training loss: 0.418476| lrm: 0.990014| num_tokens: 10,664\n",
      "Step 00008/00701 | Training loss: 1.029977| lrm: 0.988588| num_tokens: 11,584\n",
      "Step 00009/00701 | Training loss: 0.565761| lrm: 0.987161| num_tokens: 9,920\n",
      "Step 00010/00701 | Training loss: 0.404951| lrm: 0.985735| num_tokens: 15,002\n",
      "Step 00011/00701 | Training loss: 1.073448| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.878830| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.958819| lrm: 0.981455| num_tokens: 10,802\n",
      "Step 00014/00701 | Training loss: 0.772789| lrm: 0.980029| num_tokens: 7,541\n",
      "Step 00015/00701 | Training loss: 0.879849| lrm: 0.978602| num_tokens: 10,515\n",
      "Step 00016/00701 | Training loss: 1.074197| lrm: 0.977175| num_tokens: 14,469\n",
      "Step 00017/00701 | Training loss: 0.538727| lrm: 0.975749| num_tokens: 7,715\n",
      "Step 00018/00701 | Training loss: 1.228991| lrm: 0.974322| num_tokens: 11,655\n",
      "Step 00019/00701 | Training loss: 0.591910| lrm: 0.972896| num_tokens: 11,228\n",
      "Step 00020/00701 | Training loss: 0.413929| lrm: 0.971469| num_tokens: 10,678\n",
      "Step 00021/00701 | Training loss: 0.762510| lrm: 0.970043| num_tokens: 12,324\n",
      "Step 00022/00701 | Training loss: 1.113417| lrm: 0.968616| num_tokens: 9,331\n",
      "Step 00023/00701 | Training loss: 0.690985| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.845751| lrm: 0.965763| num_tokens: 10,715\n",
      "Step 00025/00701 | Training loss: 0.630881| lrm: 0.964337| num_tokens: 12,383\n",
      "Step 00026/00701 | Training loss: 1.034695| lrm: 0.962910| num_tokens: 12,576\n",
      "Step 00027/00701 | Training loss: 0.919175| lrm: 0.961484| num_tokens: 8,359\n",
      "Step 00028/00701 | Training loss: 0.932310| lrm: 0.960057| num_tokens: 10,178\n",
      "Step 00029/00701 | Training loss: 0.936880| lrm: 0.958631| num_tokens: 10,629\n",
      "Step 00030/00701 | Training loss: 0.589679| lrm: 0.957204| num_tokens: 9,143\n",
      "Step 00031/00701 | Training loss: 1.141760| lrm: 0.955777| num_tokens: 7,481\n",
      "Step 00032/00701 | Training loss: 0.859945| lrm: 0.954351| num_tokens: 9,972\n",
      "Step 00033/00701 | Training loss: 0.839113| lrm: 0.952924| num_tokens: 11,270\n",
      "Step 00034/00701 | Training loss: 1.042715| lrm: 0.951498| num_tokens: 11,177\n",
      "Step 00035/00701 | Training loss: 0.845996| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.931722| lrm: 0.948645| num_tokens: 9,343\n",
      "Step 00037/00701 | Training loss: 1.066865| lrm: 0.947218| num_tokens: 10,954\n",
      "Step 00038/00701 | Training loss: 0.719132| lrm: 0.945792| num_tokens: 6,793\n",
      "Step 00039/00701 | Training loss: 0.609093| lrm: 0.944365| num_tokens: 5,849\n",
      "Step 00040/00701 | Training loss: 0.941540| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.620753| lrm: 0.941512| num_tokens: 14,344\n",
      "Step 00042/00701 | Training loss: 1.180045| lrm: 0.940086| num_tokens: 11,223\n",
      "Step 00043/00701 | Training loss: 0.997076| lrm: 0.938659| num_tokens: 11,061\n",
      "Step 00044/00701 | Training loss: 1.084586| lrm: 0.937233| num_tokens: 12,666\n",
      "Step 00045/00701 | Training loss: 1.066363| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.688904| lrm: 0.934379| num_tokens: 9,630\n",
      "Step 00047/00701 | Training loss: 1.077094| lrm: 0.932953| num_tokens: 12,891\n",
      "Step 00048/00701 | Training loss: 0.733498| lrm: 0.931526| num_tokens: 10,059\n",
      "Step 00049/00701 | Training loss: 0.538327| lrm: 0.930100| num_tokens: 11,237\n",
      "Step 00050/00701 | Training loss: 0.758723| lrm: 0.928673| num_tokens: 9,079\n",
      "Step 00051/00701 | Training loss: 0.274791| lrm: 0.927247| num_tokens: 9,590\n",
      "Step 00052/00701 | Training loss: 0.830608| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 1.004495| lrm: 0.924394| num_tokens: 7,924\n",
      "Step 00054/00701 | Training loss: 0.374034| lrm: 0.922967| num_tokens: 9,631\n",
      "Step 00055/00701 | Training loss: 0.897435| lrm: 0.921541| num_tokens: 6,508\n",
      "Step 00056/00701 | Training loss: 1.396379| lrm: 0.920114| num_tokens: 10,513\n",
      "Step 00057/00701 | Training loss: 1.007091| lrm: 0.918688| num_tokens: 9,782\n",
      "Step 00058/00701 | Training loss: 1.359282| lrm: 0.917261| num_tokens: 13,372\n",
      "Step 00059/00701 | Training loss: 1.058745| lrm: 0.915835| num_tokens: 7,944\n",
      "Step 00060/00701 | Training loss: 1.142027| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.255698| lrm: 0.912981| num_tokens: 13,711\n",
      "Step 00062/00701 | Training loss: 0.939090| lrm: 0.911555| num_tokens: 12,159\n",
      "Step 00063/00701 | Training loss: 0.569921| lrm: 0.910128| num_tokens: 10,617\n",
      "Step 00064/00701 | Training loss: 0.670314| lrm: 0.908702| num_tokens: 8,098\n",
      "Step 00065/00701 | Training loss: 0.488556| lrm: 0.907275| num_tokens: 11,033\n",
      "Step 00066/00701 | Training loss: 1.025622| lrm: 0.905849| num_tokens: 9,531\n",
      "Step 00067/00701 | Training loss: 0.554192| lrm: 0.904422| num_tokens: 11,573\n",
      "Step 00068/00701 | Training loss: 1.319848| lrm: 0.902996| num_tokens: 12,923\n",
      "Step 00069/00701 | Training loss: 0.844010| lrm: 0.901569| num_tokens: 7,943\n",
      "Step 00070/00701 | Training loss: 0.475581| lrm: 0.900143| num_tokens: 10,775\n",
      "Step 00071/00701 | Training loss: 0.733417| lrm: 0.898716| num_tokens: 9,193\n",
      "Step 00072/00701 | Training loss: 0.947881| lrm: 0.897290| num_tokens: 9,378\n",
      "Step 00073/00701 | Training loss: 1.220570| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.723744| lrm: 0.894437| num_tokens: 7,685\n",
      "Step 00075/00701 | Training loss: 0.700502| lrm: 0.893010| num_tokens: 6,807\n",
      "Step 00076/00701 | Training loss: 0.832555| lrm: 0.891583| num_tokens: 7,530\n",
      "Step 00077/00701 | Training loss: 0.855786| lrm: 0.890157| num_tokens: 9,319\n",
      "Step 00078/00701 | Training loss: 0.613368| lrm: 0.888730| num_tokens: 11,560\n",
      "Step 00079/00701 | Training loss: 0.582570| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080/00701 | Training loss: 1.004194| lrm: 0.885877| num_tokens: 11,674\n",
      "Step 00081/00701 | Training loss: 0.518609| lrm: 0.884451| num_tokens: 7,783\n",
      "Step 00082/00701 | Training loss: 0.684067| lrm: 0.883024| num_tokens: 14,544\n",
      "Step 00083/00701 | Training loss: 0.540354| lrm: 0.881598| num_tokens: 11,069\n",
      "Step 00084/00701 | Training loss: 0.749646| lrm: 0.880171| num_tokens: 13,568\n",
      "Step 00085/00701 | Training loss: 1.146870| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.989496| lrm: 0.877318| num_tokens: 9,908\n",
      "Step 00087/00701 | Training loss: 1.022878| lrm: 0.875892| num_tokens: 9,627\n",
      "Step 00088/00701 | Training loss: 0.862351| lrm: 0.874465| num_tokens: 11,064\n",
      "Step 00089/00701 | Training loss: 1.390711| lrm: 0.873039| num_tokens: 9,523\n",
      "Step 00090/00701 | Training loss: 0.959167| lrm: 0.871612| num_tokens: 10,566\n",
      "Step 00091/00701 | Training loss: 0.751789| lrm: 0.870185| num_tokens: 10,761\n",
      "Step 00092/00701 | Training loss: 0.867643| lrm: 0.868759| num_tokens: 10,418\n",
      "Step 00093/00701 | Training loss: 0.867030| lrm: 0.867332| num_tokens: 9,591\n",
      "Step 00094/00701 | Training loss: 0.729346| lrm: 0.865906| num_tokens: 9,464\n",
      "Step 00095/00701 | Training loss: 0.956760| lrm: 0.864479| num_tokens: 9,395\n",
      "Step 00096/00701 | Training loss: 1.120162| lrm: 0.863053| num_tokens: 13,239\n",
      "Step 00097/00701 | Training loss: 1.008645| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.755614| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 1.138209| lrm: 0.858773| num_tokens: 8,989\n",
      "Step 00100 | Validation loss: 1.015304\n",
      "Step 00100/00701 | Training loss: 1.959123| lrm: 0.857347| num_tokens: 10,527\n",
      "Step 00101/00701 | Training loss: 0.436857| lrm: 0.855920| num_tokens: 9,838\n",
      "Step 00102/00701 | Training loss: 0.567780| lrm: 0.854494| num_tokens: 10,665\n",
      "Step 00103/00701 | Training loss: 0.717459| lrm: 0.853067| num_tokens: 11,272\n",
      "Step 00104/00701 | Training loss: 0.710543| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.761850| lrm: 0.850214| num_tokens: 10,153\n",
      "Step 00106/00701 | Training loss: 0.827975| lrm: 0.848787| num_tokens: 11,156\n",
      "Step 00107/00701 | Training loss: 1.047772| lrm: 0.847361| num_tokens: 10,161\n",
      "Step 00108/00701 | Training loss: 0.941865| lrm: 0.845934| num_tokens: 8,469\n",
      "Step 00109/00701 | Training loss: 0.684548| lrm: 0.844508| num_tokens: 8,740\n",
      "Step 00110/00701 | Training loss: 0.816801| lrm: 0.843081| num_tokens: 12,241\n",
      "Step 00111/00701 | Training loss: 0.709181| lrm: 0.841655| num_tokens: 10,737\n",
      "Step 00112/00701 | Training loss: 1.482730| lrm: 0.840228| num_tokens: 14,866\n",
      "Step 00113/00701 | Training loss: 0.757747| lrm: 0.838802| num_tokens: 16,098\n",
      "Step 00114/00701 | Training loss: 1.106718| lrm: 0.837375| num_tokens: 14,309\n",
      "Step 00115/00701 | Training loss: 0.318019| lrm: 0.835949| num_tokens: 11,090\n",
      "Step 00116/00701 | Training loss: 0.796869| lrm: 0.834522| num_tokens: 9,127\n",
      "Step 00117/00701 | Training loss: 0.923604| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.643536| lrm: 0.831669| num_tokens: 9,686\n",
      "Step 00119/00701 | Training loss: 0.645079| lrm: 0.830243| num_tokens: 14,012\n",
      "Step 00120/00701 | Training loss: 0.973948| lrm: 0.828816| num_tokens: 12,993\n",
      "Step 00121/00701 | Training loss: 1.081500| lrm: 0.827389| num_tokens: 16,514\n",
      "Step 00122/00701 | Training loss: 0.821314| lrm: 0.825963| num_tokens: 7,264\n",
      "Step 00123/00701 | Training loss: 0.875987| lrm: 0.824536| num_tokens: 6,636\n",
      "Step 00124/00701 | Training loss: 0.694801| lrm: 0.823110| num_tokens: 11,608\n",
      "Step 00125/00701 | Training loss: 0.979950| lrm: 0.821683| num_tokens: 5,773\n",
      "Step 00126/00701 | Training loss: 0.627960| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 1.057672| lrm: 0.818830| num_tokens: 10,148\n",
      "Step 00128/00701 | Training loss: 0.452421| lrm: 0.817404| num_tokens: 10,627\n",
      "Step 00129/00701 | Training loss: 0.829439| lrm: 0.815977| num_tokens: 8,873\n",
      "Step 00130/00701 | Training loss: 0.953969| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.936194| lrm: 0.813124| num_tokens: 11,228\n",
      "Step 00132/00701 | Training loss: 0.808851| lrm: 0.811698| num_tokens: 12,012\n",
      "Step 00133/00701 | Training loss: 0.909288| lrm: 0.810271| num_tokens: 16,435\n",
      "Step 00134/00701 | Training loss: 0.700173| lrm: 0.808845| num_tokens: 10,741\n",
      "Step 00135/00701 | Training loss: 1.374513| lrm: 0.807418| num_tokens: 13,492\n",
      "Step 00136/00701 | Training loss: 0.507320| lrm: 0.805991| num_tokens: 10,194\n",
      "Step 00137/00701 | Training loss: 0.885732| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.744536| lrm: 0.803138| num_tokens: 11,043\n",
      "Step 00139/00701 | Training loss: 0.960564| lrm: 0.801712| num_tokens: 12,147\n",
      "Step 00140/00701 | Training loss: 0.964397| lrm: 0.800285| num_tokens: 13,088\n",
      "Step 00141/00701 | Training loss: 0.808001| lrm: 0.798859| num_tokens: 13,647\n",
      "Step 00142/00701 | Training loss: 0.469269| lrm: 0.797432| num_tokens: 10,286\n",
      "Step 00143/00701 | Training loss: 1.172966| lrm: 0.796006| num_tokens: 8,956\n",
      "Step 00144/00701 | Training loss: 1.254699| lrm: 0.794579| num_tokens: 14,847\n",
      "Step 00145/00701 | Training loss: 0.959148| lrm: 0.793153| num_tokens: 14,649\n",
      "Step 00146/00701 | Training loss: 0.575748| lrm: 0.791726| num_tokens: 11,999\n",
      "Step 00147/00701 | Training loss: 0.642997| lrm: 0.790300| num_tokens: 8,022\n",
      "Step 00148/00701 | Training loss: 1.041409| lrm: 0.788873| num_tokens: 11,348\n",
      "Step 00149/00701 | Training loss: 1.077932| lrm: 0.787447| num_tokens: 14,182\n",
      "Step 00150/00701 | Training loss: 0.626148| lrm: 0.786020| num_tokens: 10,776\n",
      "Step 00151/00701 | Training loss: 0.676184| lrm: 0.784593| num_tokens: 14,039\n",
      "Step 00152/00701 | Training loss: 0.787180| lrm: 0.783167| num_tokens: 8,526\n",
      "Step 00153/00701 | Training loss: 0.827274| lrm: 0.781740| num_tokens: 12,782\n",
      "Step 00154/00701 | Training loss: 0.760620| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.834562| lrm: 0.778887| num_tokens: 10,624\n",
      "Step 00156/00701 | Training loss: 0.574776| lrm: 0.777461| num_tokens: 13,575\n",
      "Step 00157/00701 | Training loss: 1.128031| lrm: 0.776034| num_tokens: 13,319\n",
      "Step 00158/00701 | Training loss: 0.543016| lrm: 0.774608| num_tokens: 7,492\n",
      "Step 00159/00701 | Training loss: 0.719469| lrm: 0.773181| num_tokens: 9,274\n",
      "Step 00160/00701 | Training loss: 0.646746| lrm: 0.771755| num_tokens: 9,399\n",
      "Step 00161/00701 | Training loss: 0.981454| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.985336| lrm: 0.768902| num_tokens: 11,779\n",
      "Step 00163/00701 | Training loss: 0.492680| lrm: 0.767475| num_tokens: 7,485\n",
      "Step 00164/00701 | Training loss: 0.416399| lrm: 0.766049| num_tokens: 9,081\n",
      "Step 00165/00701 | Training loss: 0.758464| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.955797| lrm: 0.763195| num_tokens: 12,309\n",
      "Step 00167/00701 | Training loss: 0.905863| lrm: 0.761769| num_tokens: 10,547\n",
      "Step 00168/00701 | Training loss: 0.702079| lrm: 0.760342| num_tokens: 9,717\n",
      "Step 00169/00701 | Training loss: 0.881207| lrm: 0.758916| num_tokens: 9,577\n",
      "Step 00170/00701 | Training loss: 1.305481| lrm: 0.757489| num_tokens: 13,464\n",
      "Step 00171/00701 | Training loss: 0.683586| lrm: 0.756063| num_tokens: 10,156\n",
      "Step 00172/00701 | Training loss: 0.961208| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.970107| lrm: 0.753210| num_tokens: 13,931\n",
      "Step 00174/00701 | Training loss: 0.943626| lrm: 0.751783| num_tokens: 9,132\n",
      "Step 00175/00701 | Training loss: 1.050660| lrm: 0.750357| num_tokens: 9,835\n",
      "Step 00176/00701 | Training loss: 0.991866| lrm: 0.748930| num_tokens: 11,244\n",
      "Step 00177/00701 | Training loss: 0.983509| lrm: 0.747504| num_tokens: 14,670\n",
      "Step 00178/00701 | Training loss: 0.632597| lrm: 0.746077| num_tokens: 9,624\n",
      "Step 00179/00701 | Training loss: 0.953974| lrm: 0.744650| num_tokens: 11,085\n",
      "Step 00180/00701 | Training loss: 0.544077| lrm: 0.743224| num_tokens: 11,989\n",
      "Step 00181/00701 | Training loss: 0.875445| lrm: 0.741797| num_tokens: 15,749\n",
      "Step 00182/00701 | Training loss: 1.183460| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.489654| lrm: 0.738944| num_tokens: 12,476\n",
      "Step 00184/00701 | Training loss: 0.968494| lrm: 0.737518| num_tokens: 13,569\n",
      "Step 00185/00701 | Training loss: 0.665198| lrm: 0.736091| num_tokens: 11,265\n",
      "Step 00186/00701 | Training loss: 0.844696| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.480617| lrm: 0.733238| num_tokens: 13,609\n",
      "Step 00188/00701 | Training loss: 0.962791| lrm: 0.731812| num_tokens: 9,421\n",
      "Step 00189/00701 | Training loss: 0.973216| lrm: 0.730385| num_tokens: 9,300\n",
      "Step 00190/00701 | Training loss: 0.460680| lrm: 0.728959| num_tokens: 15,818\n",
      "Step 00191/00701 | Training loss: 0.905954| lrm: 0.727532| num_tokens: 12,340\n",
      "Step 00192/00701 | Training loss: 0.533409| lrm: 0.726106| num_tokens: 9,962\n",
      "Step 00193/00701 | Training loss: 1.101706| lrm: 0.724679| num_tokens: 10,650\n",
      "Step 00194/00701 | Training loss: 1.214628| lrm: 0.723252| num_tokens: 8,192\n",
      "Step 00195/00701 | Training loss: 0.546731| lrm: 0.721826| num_tokens: 14,326\n",
      "Step 00196/00701 | Training loss: 0.760950| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 1.041947| lrm: 0.718973| num_tokens: 10,264\n",
      "Step 00198/00701 | Training loss: 0.583616| lrm: 0.717546| num_tokens: 9,718\n",
      "Step 00199/00701 | Training loss: 0.616562| lrm: 0.716120| num_tokens: 10,337\n",
      "Step 00200 | Validation loss: 1.014468\n",
      "final: 335/1024 (32.71%)\n",
      "final: 425/1024 (41.50%)\n",
      "Step 00200 | mmlu_acc: 0.327148, arc_easy_acc: 0.415039\n",
      "Step 00200/00701 | Training loss: 0.845720| lrm: 0.714693| num_tokens: 17,787\n",
      "Step 00201/00701 | Training loss: 0.879460| lrm: 0.713267| num_tokens: 9,676\n",
      "Step 00202/00701 | Training loss: 1.929568| lrm: 0.711840| num_tokens: 15,694\n",
      "Step 00203/00701 | Training loss: 1.074866| lrm: 0.710414| num_tokens: 8,344\n",
      "Step 00204/00701 | Training loss: 0.991476| lrm: 0.708987| num_tokens: 11,398\n",
      "Step 00205/00701 | Training loss: 0.654308| lrm: 0.707561| num_tokens: 11,165\n",
      "Step 00206/00701 | Training loss: 0.902519| lrm: 0.706134| num_tokens: 10,515\n",
      "Step 00207/00701 | Training loss: 0.640957| lrm: 0.704708| num_tokens: 11,281\n",
      "Step 00208/00701 | Training loss: 1.288813| lrm: 0.703281| num_tokens: 13,195\n",
      "Step 00209/00701 | Training loss: 0.568135| lrm: 0.701854| num_tokens: 10,620\n",
      "Step 00210/00701 | Training loss: 0.715171| lrm: 0.700428| num_tokens: 11,855\n",
      "Step 00211/00701 | Training loss: 1.064558| lrm: 0.699001| num_tokens: 11,439\n",
      "Step 00212/00701 | Training loss: 1.011860| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.736506| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 1.103513| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.675887| lrm: 0.693295| num_tokens: 10,049\n",
      "Step 00216/00701 | Training loss: 0.754241| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 0.998018| lrm: 0.690442| num_tokens: 8,208\n",
      "Step 00218/00701 | Training loss: 0.480404| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.884441| lrm: 0.687589| num_tokens: 14,707\n",
      "Step 00220/00701 | Training loss: 0.930482| lrm: 0.686163| num_tokens: 8,471\n",
      "Step 00221/00701 | Training loss: 0.771921| lrm: 0.684736| num_tokens: 7,856\n",
      "Step 00222/00701 | Training loss: 1.106888| lrm: 0.683310| num_tokens: 7,401\n",
      "Step 00223/00701 | Training loss: 1.152465| lrm: 0.681883| num_tokens: 11,899\n",
      "Step 00224/00701 | Training loss: 0.711156| lrm: 0.680456| num_tokens: 10,060\n",
      "Step 00225/00701 | Training loss: 1.127265| lrm: 0.679030| num_tokens: 8,888\n",
      "Step 00226/00701 | Training loss: 0.646487| lrm: 0.677603| num_tokens: 10,747\n",
      "Step 00227/00701 | Training loss: 0.595495| lrm: 0.676177| num_tokens: 10,038\n",
      "Step 00228/00701 | Training loss: 0.704808| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 1.137906| lrm: 0.673324| num_tokens: 9,397\n",
      "Step 00230/00701 | Training loss: 1.013412| lrm: 0.671897| num_tokens: 7,844\n",
      "Step 00231/00701 | Training loss: 1.209580| lrm: 0.670471| num_tokens: 11,685\n",
      "Step 00232/00701 | Training loss: 1.647364| lrm: 0.669044| num_tokens: 10,805\n",
      "Step 00233/00701 | Training loss: 0.739531| lrm: 0.667618| num_tokens: 10,203\n",
      "Step 00234/00701 | Training loss: 0.882713| lrm: 0.666191| num_tokens: 14,225\n",
      "Step 00235/00701 | Training loss: 1.032054| lrm: 0.664765| num_tokens: 9,748\n",
      "Step 00236/00701 | Training loss: 1.011573| lrm: 0.663338| num_tokens: 7,453\n",
      "Step 00237/00701 | Training loss: 0.844175| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 1.004828| lrm: 0.660485| num_tokens: 13,482\n",
      "Step 00239/00701 | Training loss: 1.263242| lrm: 0.659058| num_tokens: 12,515\n",
      "Step 00240/00701 | Training loss: 0.559495| lrm: 0.657632| num_tokens: 12,885\n",
      "Step 00241/00701 | Training loss: 1.005372| lrm: 0.656205| num_tokens: 11,284\n",
      "Step 00242/00701 | Training loss: 0.901188| lrm: 0.654779| num_tokens: 8,141\n",
      "Step 00243/00701 | Training loss: 0.667003| lrm: 0.653352| num_tokens: 10,581\n",
      "Step 00244/00701 | Training loss: 0.466941| lrm: 0.651926| num_tokens: 6,552\n",
      "Step 00245/00701 | Training loss: 1.032277| lrm: 0.650499| num_tokens: 12,896\n",
      "Step 00246/00701 | Training loss: 0.325229| lrm: 0.649073| num_tokens: 8,779\n",
      "Step 00247/00701 | Training loss: 0.559174| lrm: 0.647646| num_tokens: 10,640\n",
      "Step 00248/00701 | Training loss: 0.806764| lrm: 0.646220| num_tokens: 14,873\n",
      "Step 00249/00701 | Training loss: 0.706023| lrm: 0.644793| num_tokens: 8,755\n",
      "Step 00250/00701 | Training loss: 1.023689| lrm: 0.643367| num_tokens: 11,347\n",
      "Step 00251/00701 | Training loss: 1.206891| lrm: 0.641940| num_tokens: 13,773\n",
      "Step 00252/00701 | Training loss: 0.693755| lrm: 0.640514| num_tokens: 10,831\n",
      "Step 00253/00701 | Training loss: 0.820426| lrm: 0.639087| num_tokens: 14,171\n",
      "Step 00254/00701 | Training loss: 0.861773| lrm: 0.637660| num_tokens: 8,633\n",
      "Step 00255/00701 | Training loss: 0.743282| lrm: 0.636234| num_tokens: 10,649\n",
      "Step 00256/00701 | Training loss: 1.407913| lrm: 0.634807| num_tokens: 10,565\n",
      "Step 00257/00701 | Training loss: 0.534567| lrm: 0.633381| num_tokens: 8,375\n",
      "Step 00258/00701 | Training loss: 1.019286| lrm: 0.631954| num_tokens: 11,893\n",
      "Step 00259/00701 | Training loss: 0.907923| lrm: 0.630528| num_tokens: 6,289\n",
      "Step 00260/00701 | Training loss: 0.455030| lrm: 0.629101| num_tokens: 12,731\n",
      "Step 00261/00701 | Training loss: 0.678678| lrm: 0.627675| num_tokens: 11,069\n",
      "Step 00262/00701 | Training loss: 0.696385| lrm: 0.626248| num_tokens: 10,292\n",
      "Step 00263/00701 | Training loss: 0.623795| lrm: 0.624822| num_tokens: 6,973\n",
      "Step 00264/00701 | Training loss: 1.045582| lrm: 0.623395| num_tokens: 9,513\n",
      "Step 00265/00701 | Training loss: 0.819289| lrm: 0.621969| num_tokens: 11,233\n",
      "Step 00266/00701 | Training loss: 0.845203| lrm: 0.620542| num_tokens: 11,266\n",
      "Step 00267/00701 | Training loss: 0.548245| lrm: 0.619116| num_tokens: 8,955\n",
      "Step 00268/00701 | Training loss: 0.922661| lrm: 0.617689| num_tokens: 12,249\n",
      "Step 00269/00701 | Training loss: 0.756031| lrm: 0.616262| num_tokens: 9,576\n",
      "Step 00270/00701 | Training loss: 1.144055| lrm: 0.614836| num_tokens: 12,109\n",
      "Step 00271/00701 | Training loss: 1.394094| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.857240| lrm: 0.611983| num_tokens: 13,072\n",
      "Step 00273/00701 | Training loss: 0.621448| lrm: 0.610556| num_tokens: 9,778\n",
      "Step 00274/00701 | Training loss: 0.653465| lrm: 0.609130| num_tokens: 12,238\n",
      "Step 00275/00701 | Training loss: 0.583450| lrm: 0.607703| num_tokens: 10,854\n",
      "Step 00276/00701 | Training loss: 0.709049| lrm: 0.606277| num_tokens: 10,720\n",
      "Step 00277/00701 | Training loss: 0.571270| lrm: 0.604850| num_tokens: 7,570\n",
      "Step 00278/00701 | Training loss: 0.823707| lrm: 0.603424| num_tokens: 11,628\n",
      "Step 00279/00701 | Training loss: 0.889827| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280/00701 | Training loss: 0.778168| lrm: 0.600571| num_tokens: 8,970\n",
      "Step 00281/00701 | Training loss: 1.004883| lrm: 0.599144| num_tokens: 14,104\n",
      "Step 00282/00701 | Training loss: 0.994061| lrm: 0.597718| num_tokens: 10,950\n",
      "Step 00283/00701 | Training loss: 0.619776| lrm: 0.596291| num_tokens: 8,906\n",
      "Step 00284/00701 | Training loss: 0.431105| lrm: 0.594864| num_tokens: 12,614\n",
      "Step 00285/00701 | Training loss: 0.862288| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.599885| lrm: 0.592011| num_tokens: 11,914\n",
      "Step 00287/00701 | Training loss: 0.265276| lrm: 0.590585| num_tokens: 9,547\n",
      "Step 00288/00701 | Training loss: 0.679435| lrm: 0.589158| num_tokens: 5,007\n",
      "Step 00289/00701 | Training loss: 0.683922| lrm: 0.587732| num_tokens: 9,433\n",
      "Step 00290/00701 | Training loss: 1.072483| lrm: 0.586305| num_tokens: 11,035\n",
      "Step 00291/00701 | Training loss: 0.998005| lrm: 0.584879| num_tokens: 10,852\n",
      "Step 00292/00701 | Training loss: 0.947795| lrm: 0.583452| num_tokens: 11,659\n",
      "Step 00293/00701 | Training loss: 0.828487| lrm: 0.582026| num_tokens: 15,038\n",
      "Step 00294/00701 | Training loss: 0.724498| lrm: 0.580599| num_tokens: 9,439\n",
      "Step 00295/00701 | Training loss: 1.108657| lrm: 0.579173| num_tokens: 6,998\n",
      "Step 00296/00701 | Training loss: 1.221482| lrm: 0.577746| num_tokens: 12,267\n",
      "Step 00297/00701 | Training loss: 0.579508| lrm: 0.576320| num_tokens: 17,571\n",
      "Step 00298/00701 | Training loss: 1.035756| lrm: 0.574893| num_tokens: 10,103\n",
      "Step 00299/00701 | Training loss: 0.640580| lrm: 0.573466| num_tokens: 7,731\n",
      "Step 00300 | Validation loss: 1.014073\n",
      "Step 00300/00701 | Training loss: 0.750945| lrm: 0.572040| num_tokens: 8,914\n",
      "Step 00301/00701 | Training loss: 0.829139| lrm: 0.570613| num_tokens: 12,830\n",
      "Step 00302/00701 | Training loss: 0.766509| lrm: 0.569187| num_tokens: 12,209\n",
      "Step 00303/00701 | Training loss: 0.956985| lrm: 0.567760| num_tokens: 11,589\n",
      "Step 00304/00701 | Training loss: 0.612140| lrm: 0.566334| num_tokens: 8,228\n",
      "Step 00305/00701 | Training loss: 0.864516| lrm: 0.564907| num_tokens: 14,637\n",
      "Step 00306/00701 | Training loss: 0.886872| lrm: 0.563481| num_tokens: 11,570\n",
      "Step 00307/00701 | Training loss: 1.095519| lrm: 0.562054| num_tokens: 12,881\n",
      "Step 00308/00701 | Training loss: 0.667296| lrm: 0.560628| num_tokens: 6,839\n",
      "Step 00309/00701 | Training loss: 0.938950| lrm: 0.559201| num_tokens: 11,009\n",
      "Step 00310/00701 | Training loss: 0.583239| lrm: 0.557775| num_tokens: 12,118\n",
      "Step 00311/00701 | Training loss: 1.268734| lrm: 0.556348| num_tokens: 10,305\n",
      "Step 00312/00701 | Training loss: 0.748362| lrm: 0.554922| num_tokens: 11,427\n",
      "Step 00313/00701 | Training loss: 0.740734| lrm: 0.553495| num_tokens: 8,436\n",
      "Step 00314/00701 | Training loss: 1.104079| lrm: 0.552068| num_tokens: 11,858\n",
      "Step 00315/00701 | Training loss: 0.533365| lrm: 0.550642| num_tokens: 14,168\n",
      "Step 00316/00701 | Training loss: 1.123267| lrm: 0.549215| num_tokens: 6,451\n",
      "Step 00317/00701 | Training loss: 0.406634| lrm: 0.547789| num_tokens: 11,643\n",
      "Step 00318/00701 | Training loss: 0.710886| lrm: 0.546362| num_tokens: 10,043\n",
      "Step 00319/00701 | Training loss: 1.215066| lrm: 0.544936| num_tokens: 11,387\n",
      "Step 00320/00701 | Training loss: 0.615203| lrm: 0.543509| num_tokens: 6,437\n",
      "Step 00321/00701 | Training loss: 1.081226| lrm: 0.542083| num_tokens: 8,880\n",
      "Step 00322/00701 | Training loss: 0.748067| lrm: 0.540656| num_tokens: 4,147\n",
      "Step 00323/00701 | Training loss: 0.792654| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.977263| lrm: 0.537803| num_tokens: 11,807\n",
      "Step 00325/00701 | Training loss: 0.633755| lrm: 0.536377| num_tokens: 9,002\n",
      "Step 00326/00701 | Training loss: 1.000865| lrm: 0.534950| num_tokens: 7,345\n",
      "Step 00327/00701 | Training loss: 0.706962| lrm: 0.533524| num_tokens: 12,006\n",
      "Step 00328/00701 | Training loss: 1.044621| lrm: 0.532097| num_tokens: 12,714\n",
      "Step 00329/00701 | Training loss: 0.654818| lrm: 0.530670| num_tokens: 10,357\n",
      "Step 00330/00701 | Training loss: 1.010018| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.807390| lrm: 0.527817| num_tokens: 7,793\n",
      "Step 00332/00701 | Training loss: 0.728007| lrm: 0.526391| num_tokens: 10,629\n",
      "Step 00333/00701 | Training loss: 1.273492| lrm: 0.524964| num_tokens: 9,785\n",
      "Step 00334/00701 | Training loss: 0.377229| lrm: 0.523538| num_tokens: 12,100\n",
      "Step 00335/00701 | Training loss: 0.443324| lrm: 0.522111| num_tokens: 7,312\n",
      "Step 00336/00701 | Training loss: 1.151175| lrm: 0.520685| num_tokens: 12,905\n",
      "Step 00337/00701 | Training loss: 0.767367| lrm: 0.519258| num_tokens: 11,898\n",
      "Step 00338/00701 | Training loss: 0.378310| lrm: 0.517832| num_tokens: 10,202\n",
      "Step 00339/00701 | Training loss: 0.823014| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340/00701 | Training loss: 0.773944| lrm: 0.514979| num_tokens: 11,148\n",
      "Step 00341/00701 | Training loss: 1.193401| lrm: 0.513552| num_tokens: 8,979\n",
      "Step 00342/00701 | Training loss: 0.705808| lrm: 0.512126| num_tokens: 6,240\n",
      "Step 00343/00701 | Training loss: 1.134066| lrm: 0.510699| num_tokens: 11,617\n",
      "Step 00344/00701 | Training loss: 0.560552| lrm: 0.509272| num_tokens: 7,701\n",
      "Step 00345/00701 | Training loss: 0.800847| lrm: 0.507846| num_tokens: 12,529\n",
      "Step 00346/00701 | Training loss: 0.888697| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.677881| lrm: 0.504993| num_tokens: 11,169\n",
      "Step 00348/00701 | Training loss: 1.321877| lrm: 0.503566| num_tokens: 9,517\n",
      "Step 00349/00701 | Training loss: 0.665339| lrm: 0.502140| num_tokens: 10,291\n",
      "Step 00350/00701 | Training loss: 0.848163| lrm: 0.500713| num_tokens: 9,018\n",
      "Step 00351/00701 | Training loss: 0.766483| lrm: 0.499287| num_tokens: 11,729\n",
      "Step 00352/00701 | Training loss: 1.032831| lrm: 0.497860| num_tokens: 12,675\n",
      "Step 00353/00701 | Training loss: 0.862712| lrm: 0.496434| num_tokens: 8,383\n",
      "Step 00354/00701 | Training loss: 0.754209| lrm: 0.495007| num_tokens: 10,059\n",
      "Step 00355/00701 | Training loss: 0.835212| lrm: 0.493581| num_tokens: 10,087\n",
      "Step 00356/00701 | Training loss: 0.657221| lrm: 0.492154| num_tokens: 8,333\n",
      "Step 00357/00701 | Training loss: 0.650260| lrm: 0.490728| num_tokens: 8,266\n",
      "Step 00358/00701 | Training loss: 0.680712| lrm: 0.489301| num_tokens: 9,973\n",
      "Step 00359/00701 | Training loss: 0.708725| lrm: 0.487874| num_tokens: 9,013\n",
      "Step 00360/00701 | Training loss: 0.535189| lrm: 0.486448| num_tokens: 8,617\n",
      "Step 00361/00701 | Training loss: 0.742021| lrm: 0.485021| num_tokens: 11,847\n",
      "Step 00362/00701 | Training loss: 1.222075| lrm: 0.483595| num_tokens: 11,199\n",
      "Step 00363/00701 | Training loss: 0.463828| lrm: 0.482168| num_tokens: 11,212\n",
      "Step 00364/00701 | Training loss: 0.674758| lrm: 0.480742| num_tokens: 13,298\n",
      "Step 00365/00701 | Training loss: 0.521566| lrm: 0.479315| num_tokens: 7,758\n",
      "Step 00366/00701 | Training loss: 0.951005| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.772209| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 1.134750| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.823054| lrm: 0.473609| num_tokens: 13,353\n",
      "Step 00370/00701 | Training loss: 0.744797| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.924012| lrm: 0.470756| num_tokens: 11,940\n",
      "Step 00372/00701 | Training loss: 0.934874| lrm: 0.469330| num_tokens: 11,410\n",
      "Step 00373/00701 | Training loss: 0.553819| lrm: 0.467903| num_tokens: 8,816\n",
      "Step 00374/00701 | Training loss: 0.853666| lrm: 0.466476| num_tokens: 13,098\n",
      "Step 00375/00701 | Training loss: 1.158411| lrm: 0.465050| num_tokens: 13,724\n",
      "Step 00376/00701 | Training loss: 1.184731| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.673097| lrm: 0.462197| num_tokens: 13,890\n",
      "Step 00378/00701 | Training loss: 0.665740| lrm: 0.460770| num_tokens: 11,856\n",
      "Step 00379/00701 | Training loss: 0.903188| lrm: 0.459344| num_tokens: 7,960\n",
      "Step 00380/00701 | Training loss: 1.288310| lrm: 0.457917| num_tokens: 8,519\n",
      "Step 00381/00701 | Training loss: 0.907238| lrm: 0.456491| num_tokens: 14,234\n",
      "Step 00382/00701 | Training loss: 0.644410| lrm: 0.455064| num_tokens: 13,353\n",
      "Step 00383/00701 | Training loss: 0.526739| lrm: 0.453638| num_tokens: 15,146\n",
      "Step 00384/00701 | Training loss: 0.641351| lrm: 0.452211| num_tokens: 12,589\n",
      "Step 00385/00701 | Training loss: 0.432022| lrm: 0.450785| num_tokens: 10,105\n",
      "Step 00386/00701 | Training loss: 1.022064| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 1.036198| lrm: 0.447932| num_tokens: 9,869\n",
      "Step 00388/00701 | Training loss: 0.914236| lrm: 0.446505| num_tokens: 11,292\n",
      "Step 00389/00701 | Training loss: 0.833023| lrm: 0.445078| num_tokens: 15,855\n",
      "Step 00390/00701 | Training loss: 0.505606| lrm: 0.443652| num_tokens: 12,768\n",
      "Step 00391/00701 | Training loss: 0.674997| lrm: 0.442225| num_tokens: 13,893\n",
      "Step 00392/00701 | Training loss: 0.354814| lrm: 0.440799| num_tokens: 10,495\n",
      "Step 00393/00701 | Training loss: 1.291457| lrm: 0.439372| num_tokens: 13,459\n",
      "Step 00394/00701 | Training loss: 0.992412| lrm: 0.437946| num_tokens: 13,515\n",
      "Step 00395/00701 | Training loss: 0.849212| lrm: 0.436519| num_tokens: 10,006\n",
      "Step 00396/00701 | Training loss: 0.961434| lrm: 0.435093| num_tokens: 11,009\n",
      "Step 00397/00701 | Training loss: 1.088439| lrm: 0.433666| num_tokens: 8,178\n",
      "Step 00398/00701 | Training loss: 0.999080| lrm: 0.432240| num_tokens: 8,070\n",
      "Step 00399/00701 | Training loss: 0.899722| lrm: 0.430813| num_tokens: 8,559\n",
      "Step 00400 | Validation loss: 1.013958\n",
      "final: 350/1024 (34.18%)\n",
      "final: 436/1024 (42.58%)\n",
      "Step 00400 | mmlu_acc: 0.341797, arc_easy_acc: 0.425781\n",
      "Step 00400/00701 | Training loss: 0.452244| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.743105| lrm: 0.427960| num_tokens: 6,689\n",
      "Step 00402/00701 | Training loss: 0.664265| lrm: 0.426534| num_tokens: 7,153\n",
      "Step 00403/00701 | Training loss: 1.254012| lrm: 0.425107| num_tokens: 15,333\n",
      "Step 00404/00701 | Training loss: 0.861965| lrm: 0.423680| num_tokens: 8,839\n",
      "Step 00405/00701 | Training loss: 0.697616| lrm: 0.422254| num_tokens: 10,865\n",
      "Step 00406/00701 | Training loss: 0.683502| lrm: 0.420827| num_tokens: 9,689\n",
      "Step 00407/00701 | Training loss: 0.558306| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.622112| lrm: 0.417974| num_tokens: 12,826\n",
      "Step 00409/00701 | Training loss: 0.598114| lrm: 0.416548| num_tokens: 8,557\n",
      "Step 00410/00701 | Training loss: 0.609677| lrm: 0.415121| num_tokens: 13,374\n",
      "Step 00411/00701 | Training loss: 0.886405| lrm: 0.413695| num_tokens: 12,917\n",
      "Step 00412/00701 | Training loss: 0.747807| lrm: 0.412268| num_tokens: 11,922\n",
      "Step 00413/00701 | Training loss: 0.858294| lrm: 0.410842| num_tokens: 7,315\n",
      "Step 00414/00701 | Training loss: 0.985323| lrm: 0.409415| num_tokens: 14,723\n",
      "Step 00415/00701 | Training loss: 0.746980| lrm: 0.407989| num_tokens: 13,804\n",
      "Step 00416/00701 | Training loss: 1.085162| lrm: 0.406562| num_tokens: 11,956\n",
      "Step 00417/00701 | Training loss: 0.661297| lrm: 0.405136| num_tokens: 12,018\n",
      "Step 00418/00701 | Training loss: 0.372878| lrm: 0.403709| num_tokens: 3,817\n",
      "Step 00419/00701 | Training loss: 0.974658| lrm: 0.402282| num_tokens: 9,950\n",
      "Step 00420/00701 | Training loss: 1.281026| lrm: 0.400856| num_tokens: 10,386\n",
      "Step 00421/00701 | Training loss: 0.828327| lrm: 0.399429| num_tokens: 12,796\n",
      "Step 00422/00701 | Training loss: 0.635484| lrm: 0.398003| num_tokens: 13,674\n",
      "Step 00423/00701 | Training loss: 0.580891| lrm: 0.396576| num_tokens: 15,302\n",
      "Step 00424/00701 | Training loss: 0.715663| lrm: 0.395150| num_tokens: 14,939\n",
      "Step 00425/00701 | Training loss: 0.857788| lrm: 0.393723| num_tokens: 7,669\n",
      "Step 00426/00701 | Training loss: 0.917257| lrm: 0.392297| num_tokens: 6,524\n",
      "Step 00427/00701 | Training loss: 0.866254| lrm: 0.390870| num_tokens: 8,753\n",
      "Step 00428/00701 | Training loss: 0.787586| lrm: 0.389444| num_tokens: 9,846\n",
      "Step 00429/00701 | Training loss: 0.687124| lrm: 0.388017| num_tokens: 10,688\n",
      "Step 00430/00701 | Training loss: 0.808204| lrm: 0.386591| num_tokens: 11,206\n",
      "Step 00431/00701 | Training loss: 1.616092| lrm: 0.385164| num_tokens: 8,919\n",
      "Step 00432/00701 | Training loss: 1.026238| lrm: 0.383738| num_tokens: 13,388\n",
      "Step 00433/00701 | Training loss: 0.595207| lrm: 0.382311| num_tokens: 10,474\n",
      "Step 00434/00701 | Training loss: 0.864085| lrm: 0.380884| num_tokens: 11,897\n",
      "Step 00435/00701 | Training loss: 0.672271| lrm: 0.379458| num_tokens: 12,165\n",
      "Step 00436/00701 | Training loss: 0.787475| lrm: 0.378031| num_tokens: 13,113\n",
      "Step 00437/00701 | Training loss: 0.947415| lrm: 0.376605| num_tokens: 12,838\n",
      "Step 00438/00701 | Training loss: 1.266877| lrm: 0.375178| num_tokens: 10,430\n",
      "Step 00439/00701 | Training loss: 0.819544| lrm: 0.373752| num_tokens: 11,832\n",
      "Step 00440/00701 | Training loss: 0.650588| lrm: 0.372325| num_tokens: 10,469\n",
      "Step 00441/00701 | Training loss: 0.781364| lrm: 0.370899| num_tokens: 11,808\n",
      "Step 00442/00701 | Training loss: 0.821427| lrm: 0.369472| num_tokens: 12,062\n",
      "Step 00443/00701 | Training loss: 0.765489| lrm: 0.368046| num_tokens: 10,750\n",
      "Step 00444/00701 | Training loss: 0.688572| lrm: 0.366619| num_tokens: 13,119\n",
      "Step 00445/00701 | Training loss: 0.799985| lrm: 0.365193| num_tokens: 9,910\n",
      "Step 00446/00701 | Training loss: 0.516151| lrm: 0.363766| num_tokens: 13,290\n",
      "Step 00447/00701 | Training loss: 1.071535| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.924231| lrm: 0.360913| num_tokens: 10,547\n",
      "Step 00449/00701 | Training loss: 0.824063| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450/00701 | Training loss: 1.066731| lrm: 0.358060| num_tokens: 15,847\n",
      "Step 00451/00701 | Training loss: 0.882711| lrm: 0.356633| num_tokens: 10,672\n",
      "Step 00452/00701 | Training loss: 0.809845| lrm: 0.355207| num_tokens: 9,304\n",
      "Step 00453/00701 | Training loss: 0.773681| lrm: 0.353780| num_tokens: 12,538\n",
      "Step 00454/00701 | Training loss: 0.933194| lrm: 0.352354| num_tokens: 7,953\n",
      "Step 00455/00701 | Training loss: 0.887097| lrm: 0.350927| num_tokens: 7,755\n",
      "Step 00456/00701 | Training loss: 1.000813| lrm: 0.349501| num_tokens: 10,549\n",
      "Step 00457/00701 | Training loss: 0.606528| lrm: 0.348074| num_tokens: 10,105\n",
      "Step 00458/00701 | Training loss: 0.839273| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.747867| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460/00701 | Training loss: 0.748654| lrm: 0.343795| num_tokens: 8,420\n",
      "Step 00461/00701 | Training loss: 0.808437| lrm: 0.342368| num_tokens: 12,615\n",
      "Step 00462/00701 | Training loss: 0.564425| lrm: 0.340942| num_tokens: 9,887\n",
      "Step 00463/00701 | Training loss: 1.128114| lrm: 0.339515| num_tokens: 7,312\n",
      "Step 00464/00701 | Training loss: 0.691591| lrm: 0.338088| num_tokens: 9,797\n",
      "Step 00465/00701 | Training loss: 1.061467| lrm: 0.336662| num_tokens: 8,338\n",
      "Step 00466/00701 | Training loss: 0.931132| lrm: 0.335235| num_tokens: 11,435\n",
      "Step 00467/00701 | Training loss: 0.654135| lrm: 0.333809| num_tokens: 10,835\n",
      "Step 00468/00701 | Training loss: 0.751212| lrm: 0.332382| num_tokens: 9,671\n",
      "Step 00469/00701 | Training loss: 0.442986| lrm: 0.330956| num_tokens: 8,338\n",
      "Step 00470/00701 | Training loss: 1.076022| lrm: 0.329529| num_tokens: 8,700\n",
      "Step 00471/00701 | Training loss: 0.941420| lrm: 0.328103| num_tokens: 15,219\n",
      "Step 00472/00701 | Training loss: 1.888883| lrm: 0.326676| num_tokens: 13,448\n",
      "Step 00473/00701 | Training loss: 0.658518| lrm: 0.325250| num_tokens: 9,585\n",
      "Step 00474/00701 | Training loss: 1.146450| lrm: 0.323823| num_tokens: 14,725\n",
      "Step 00475/00701 | Training loss: 0.921232| lrm: 0.322397| num_tokens: 12,688\n",
      "Step 00476/00701 | Training loss: 1.141563| lrm: 0.320970| num_tokens: 11,268\n",
      "Step 00477/00701 | Training loss: 1.008711| lrm: 0.319544| num_tokens: 13,478\n",
      "Step 00478/00701 | Training loss: 1.318300| lrm: 0.318117| num_tokens: 6,838\n",
      "Step 00479/00701 | Training loss: 0.756416| lrm: 0.316690| num_tokens: 13,516\n",
      "Step 00480/00701 | Training loss: 0.406127| lrm: 0.315264| num_tokens: 13,587\n",
      "Step 00481/00701 | Training loss: 1.082057| lrm: 0.313837| num_tokens: 7,015\n",
      "Step 00482/00701 | Training loss: 0.981701| lrm: 0.312411| num_tokens: 9,863\n",
      "Step 00483/00701 | Training loss: 0.688950| lrm: 0.310984| num_tokens: 11,048\n",
      "Step 00484/00701 | Training loss: 0.494940| lrm: 0.309558| num_tokens: 10,661\n",
      "Step 00485/00701 | Training loss: 1.126553| lrm: 0.308131| num_tokens: 10,617\n",
      "Step 00486/00701 | Training loss: 0.394128| lrm: 0.306705| num_tokens: 9,656\n",
      "Step 00487/00701 | Training loss: 0.664635| lrm: 0.305278| num_tokens: 8,590\n",
      "Step 00488/00701 | Training loss: 0.441090| lrm: 0.303852| num_tokens: 11,260\n",
      "Step 00489/00701 | Training loss: 0.947572| lrm: 0.302425| num_tokens: 12,388\n",
      "Step 00490/00701 | Training loss: 0.891580| lrm: 0.300999| num_tokens: 8,471\n",
      "Step 00491/00701 | Training loss: 0.927061| lrm: 0.299572| num_tokens: 8,826\n",
      "Step 00492/00701 | Training loss: 1.445371| lrm: 0.298146| num_tokens: 14,321\n",
      "Step 00493/00701 | Training loss: 0.572605| lrm: 0.296719| num_tokens: 7,284\n",
      "Step 00494/00701 | Training loss: 0.539967| lrm: 0.295292| num_tokens: 8,592\n",
      "Step 00495/00701 | Training loss: 0.869300| lrm: 0.293866| num_tokens: 10,854\n",
      "Step 00496/00701 | Training loss: 0.761099| lrm: 0.292439| num_tokens: 10,975\n",
      "Step 00497/00701 | Training loss: 0.723478| lrm: 0.291013| num_tokens: 11,925\n",
      "Step 00498/00701 | Training loss: 0.977442| lrm: 0.289586| num_tokens: 11,744\n",
      "Step 00499/00701 | Training loss: 0.295445| lrm: 0.288160| num_tokens: 14,263\n",
      "Step 00500 | Validation loss: 1.013921\n",
      "Step 00500/00701 | Training loss: 0.895405| lrm: 0.286733| num_tokens: 13,673\n",
      "Step 00501/00701 | Training loss: 0.629825| lrm: 0.285307| num_tokens: 9,954\n",
      "Step 00502/00701 | Training loss: 0.439471| lrm: 0.283880| num_tokens: 9,783\n",
      "Step 00503/00701 | Training loss: 1.390100| lrm: 0.282454| num_tokens: 13,075\n",
      "Step 00504/00701 | Training loss: 1.558269| lrm: 0.281027| num_tokens: 9,767\n",
      "Step 00505/00701 | Training loss: 0.833247| lrm: 0.279601| num_tokens: 8,895\n",
      "Step 00506/00701 | Training loss: 1.359118| lrm: 0.278174| num_tokens: 9,652\n",
      "Step 00507/00701 | Training loss: 0.593428| lrm: 0.276748| num_tokens: 6,756\n",
      "Step 00508/00701 | Training loss: 1.328463| lrm: 0.275321| num_tokens: 18,637\n",
      "Step 00509/00701 | Training loss: 0.862823| lrm: 0.273894| num_tokens: 9,678\n",
      "Step 00510/00701 | Training loss: 0.764232| lrm: 0.272468| num_tokens: 9,942\n",
      "Step 00511/00701 | Training loss: 0.578408| lrm: 0.271041| num_tokens: 5,586\n",
      "Step 00512/00701 | Training loss: 1.012794| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.790852| lrm: 0.268188| num_tokens: 8,743\n",
      "Step 00514/00701 | Training loss: 0.858147| lrm: 0.266762| num_tokens: 9,904\n",
      "Step 00515/00701 | Training loss: 1.180427| lrm: 0.265335| num_tokens: 10,734\n",
      "Step 00516/00701 | Training loss: 1.218383| lrm: 0.263909| num_tokens: 12,039\n",
      "Step 00517/00701 | Training loss: 0.549189| lrm: 0.262482| num_tokens: 10,739\n",
      "Step 00518/00701 | Training loss: 1.136770| lrm: 0.261056| num_tokens: 10,288\n",
      "Step 00519/00701 | Training loss: 0.849340| lrm: 0.259629| num_tokens: 8,447\n",
      "Step 00520/00701 | Training loss: 0.862310| lrm: 0.258203| num_tokens: 9,737\n",
      "Step 00521/00701 | Training loss: 0.969803| lrm: 0.256776| num_tokens: 14,760\n",
      "Step 00522/00701 | Training loss: 1.076958| lrm: 0.255350| num_tokens: 9,452\n",
      "Step 00523/00701 | Training loss: 0.653851| lrm: 0.253923| num_tokens: 11,752\n",
      "Step 00524/00701 | Training loss: 0.974600| lrm: 0.252496| num_tokens: 7,305\n",
      "Step 00525/00701 | Training loss: 1.539219| lrm: 0.251070| num_tokens: 10,430\n",
      "Step 00526/00701 | Training loss: 1.127294| lrm: 0.249643| num_tokens: 9,485\n",
      "Step 00527/00701 | Training loss: 1.167645| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 1.189177| lrm: 0.246790| num_tokens: 14,142\n",
      "Step 00529/00701 | Training loss: 1.138894| lrm: 0.245364| num_tokens: 14,963\n",
      "Step 00530/00701 | Training loss: 0.464818| lrm: 0.243937| num_tokens: 8,920\n",
      "Step 00531/00701 | Training loss: 0.615645| lrm: 0.242511| num_tokens: 10,497\n",
      "Step 00532/00701 | Training loss: 1.154683| lrm: 0.241084| num_tokens: 8,047\n",
      "Step 00533/00701 | Training loss: 1.049727| lrm: 0.239658| num_tokens: 11,665\n",
      "Step 00534/00701 | Training loss: 1.162984| lrm: 0.238231| num_tokens: 12,661\n",
      "Step 00535/00701 | Training loss: 0.645352| lrm: 0.236805| num_tokens: 9,332\n",
      "Step 00536/00701 | Training loss: 0.510151| lrm: 0.235378| num_tokens: 10,270\n",
      "Step 00537/00701 | Training loss: 0.695546| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.648241| lrm: 0.232525| num_tokens: 10,264\n",
      "Step 00539/00701 | Training loss: 0.809638| lrm: 0.231098| num_tokens: 13,034\n",
      "Step 00540/00701 | Training loss: 0.815312| lrm: 0.229672| num_tokens: 7,601\n",
      "Step 00541/00701 | Training loss: 1.027440| lrm: 0.228245| num_tokens: 15,932\n",
      "Step 00542/00701 | Training loss: 1.240067| lrm: 0.226819| num_tokens: 10,867\n",
      "Step 00543/00701 | Training loss: 0.368393| lrm: 0.225392| num_tokens: 11,887\n",
      "Step 00544/00701 | Training loss: 0.970098| lrm: 0.223966| num_tokens: 11,865\n",
      "Step 00545/00701 | Training loss: 0.764615| lrm: 0.222539| num_tokens: 13,438\n",
      "Step 00546/00701 | Training loss: 0.792632| lrm: 0.221113| num_tokens: 13,844\n",
      "Step 00547/00701 | Training loss: 1.262514| lrm: 0.219686| num_tokens: 6,023\n",
      "Step 00548/00701 | Training loss: 0.742160| lrm: 0.218260| num_tokens: 13,321\n",
      "Step 00549/00701 | Training loss: 1.073760| lrm: 0.216833| num_tokens: 8,358\n",
      "Step 00550/00701 | Training loss: 0.634906| lrm: 0.215407| num_tokens: 13,487\n",
      "Step 00551/00701 | Training loss: 0.763098| lrm: 0.213980| num_tokens: 11,180\n",
      "Step 00552/00701 | Training loss: 0.771086| lrm: 0.212553| num_tokens: 8,538\n",
      "Step 00553/00701 | Training loss: 0.256945| lrm: 0.211127| num_tokens: 10,572\n",
      "Step 00554/00701 | Training loss: 0.974421| lrm: 0.209700| num_tokens: 12,982\n",
      "Step 00555/00701 | Training loss: 0.892956| lrm: 0.208274| num_tokens: 11,588\n",
      "Step 00556/00701 | Training loss: 0.912250| lrm: 0.206847| num_tokens: 7,432\n",
      "Step 00557/00701 | Training loss: 0.654650| lrm: 0.205421| num_tokens: 9,354\n",
      "Step 00558/00701 | Training loss: 0.879969| lrm: 0.203994| num_tokens: 10,187\n",
      "Step 00559/00701 | Training loss: 1.005435| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560/00701 | Training loss: 0.508243| lrm: 0.201141| num_tokens: 13,597\n",
      "Step 00561/00701 | Training loss: 1.175262| lrm: 0.199715| num_tokens: 6,689\n",
      "Step 00562/00701 | Training loss: 0.243500| lrm: 0.198288| num_tokens: 8,682\n",
      "Step 00563/00701 | Training loss: 0.940824| lrm: 0.196862| num_tokens: 12,881\n",
      "Step 00564/00701 | Training loss: 0.946508| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 1.069220| lrm: 0.194009| num_tokens: 13,874\n",
      "Step 00566/00701 | Training loss: 0.996069| lrm: 0.192582| num_tokens: 12,213\n",
      "Step 00567/00701 | Training loss: 0.921644| lrm: 0.191155| num_tokens: 18,532\n",
      "Step 00568/00701 | Training loss: 0.700105| lrm: 0.189729| num_tokens: 10,003\n",
      "Step 00569/00701 | Training loss: 0.890075| lrm: 0.188302| num_tokens: 9,476\n",
      "Step 00570/00701 | Training loss: 1.257697| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.721166| lrm: 0.185449| num_tokens: 13,088\n",
      "Step 00572/00701 | Training loss: 0.691124| lrm: 0.184023| num_tokens: 7,260\n",
      "Step 00573/00701 | Training loss: 1.088569| lrm: 0.182596| num_tokens: 10,391\n",
      "Step 00574/00701 | Training loss: 0.743596| lrm: 0.181170| num_tokens: 14,540\n",
      "Step 00575/00701 | Training loss: 0.423586| lrm: 0.179743| num_tokens: 7,583\n",
      "Step 00576/00701 | Training loss: 1.753788| lrm: 0.178317| num_tokens: 8,834\n",
      "Step 00577/00701 | Training loss: 0.722965| lrm: 0.176890| num_tokens: 13,422\n",
      "Step 00578/00701 | Training loss: 0.577904| lrm: 0.175464| num_tokens: 8,190\n",
      "Step 00579/00701 | Training loss: 0.489090| lrm: 0.174037| num_tokens: 10,220\n",
      "Step 00580/00701 | Training loss: 0.993996| lrm: 0.172611| num_tokens: 9,082\n",
      "Step 00581/00701 | Training loss: 1.043881| lrm: 0.171184| num_tokens: 13,484\n",
      "Step 00582/00701 | Training loss: 1.096546| lrm: 0.169757| num_tokens: 8,858\n",
      "Step 00583/00701 | Training loss: 0.591797| lrm: 0.168331| num_tokens: 12,740\n",
      "Step 00584/00701 | Training loss: 0.823644| lrm: 0.166904| num_tokens: 13,825\n",
      "Step 00585/00701 | Training loss: 0.676409| lrm: 0.165478| num_tokens: 14,923\n",
      "Step 00586/00701 | Training loss: 0.977500| lrm: 0.164051| num_tokens: 9,923\n",
      "Step 00587/00701 | Training loss: 0.765376| lrm: 0.162625| num_tokens: 7,792\n",
      "Step 00588/00701 | Training loss: 0.931472| lrm: 0.161198| num_tokens: 12,331\n",
      "Step 00589/00701 | Training loss: 0.473920| lrm: 0.159772| num_tokens: 9,040\n",
      "Step 00590/00701 | Training loss: 0.544380| lrm: 0.158345| num_tokens: 10,334\n",
      "Step 00591/00701 | Training loss: 0.759041| lrm: 0.156919| num_tokens: 8,577\n",
      "Step 00592/00701 | Training loss: 0.980833| lrm: 0.155492| num_tokens: 11,743\n",
      "Step 00593/00701 | Training loss: 1.070893| lrm: 0.154066| num_tokens: 11,795\n",
      "Step 00594/00701 | Training loss: 0.248517| lrm: 0.152639| num_tokens: 11,224\n",
      "Step 00595/00701 | Training loss: 0.465449| lrm: 0.151213| num_tokens: 8,408\n",
      "Step 00596/00701 | Training loss: 0.745399| lrm: 0.149786| num_tokens: 5,658\n",
      "Step 00597/00701 | Training loss: 1.318594| lrm: 0.148359| num_tokens: 10,836\n",
      "Step 00598/00701 | Training loss: 0.747868| lrm: 0.146933| num_tokens: 9,759\n",
      "Step 00599/00701 | Training loss: 1.426600| lrm: 0.145506| num_tokens: 8,267\n",
      "Step 00600 | Validation loss: 1.013674\n",
      "final: 366/1024 (35.74%)\n",
      "final: 463/1024 (45.21%)\n",
      "Step 00600 | mmlu_acc: 0.357422, arc_easy_acc: 0.452148\n",
      "Step 00600/00701 | Training loss: 0.722599| lrm: 0.144080| num_tokens: 10,446\n",
      "Step 00601/00701 | Training loss: 1.065150| lrm: 0.142653| num_tokens: 11,003\n",
      "Step 00602/00701 | Training loss: 0.784023| lrm: 0.141227| num_tokens: 8,234\n",
      "Step 00603/00701 | Training loss: 1.101732| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.965729| lrm: 0.138374| num_tokens: 13,517\n",
      "Step 00605/00701 | Training loss: 0.761168| lrm: 0.136947| num_tokens: 10,453\n",
      "Step 00606/00701 | Training loss: 0.926546| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.645826| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.613300| lrm: 0.132668| num_tokens: 12,833\n",
      "Step 00609/00701 | Training loss: 1.125476| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610/00701 | Training loss: 0.498558| lrm: 0.129815| num_tokens: 14,179\n",
      "Step 00611/00701 | Training loss: 0.972412| lrm: 0.128388| num_tokens: 9,321\n",
      "Step 00612/00701 | Training loss: 0.846888| lrm: 0.126961| num_tokens: 9,523\n",
      "Step 00613/00701 | Training loss: 1.079457| lrm: 0.125535| num_tokens: 16,542\n",
      "Step 00614/00701 | Training loss: 0.550931| lrm: 0.124108| num_tokens: 15,348\n",
      "Step 00615/00701 | Training loss: 0.921405| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.619151| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.717805| lrm: 0.119829| num_tokens: 11,180\n",
      "Step 00618/00701 | Training loss: 0.566228| lrm: 0.118402| num_tokens: 8,842\n",
      "Step 00619/00701 | Training loss: 0.706082| lrm: 0.116976| num_tokens: 9,213\n",
      "Step 00620/00701 | Training loss: 0.640030| lrm: 0.115549| num_tokens: 12,340\n",
      "Step 00621/00701 | Training loss: 0.488130| lrm: 0.114123| num_tokens: 7,718\n",
      "Step 00622/00701 | Training loss: 0.963839| lrm: 0.112696| num_tokens: 7,350\n",
      "Step 00623/00701 | Training loss: 0.904255| lrm: 0.111270| num_tokens: 14,519\n",
      "Step 00624/00701 | Training loss: 1.414417| lrm: 0.109843| num_tokens: 12,368\n",
      "Step 00625/00701 | Training loss: 0.578554| lrm: 0.108417| num_tokens: 6,842\n",
      "Step 00626/00701 | Training loss: 0.732581| lrm: 0.106990| num_tokens: 11,577\n",
      "Step 00627/00701 | Training loss: 0.488141| lrm: 0.105563| num_tokens: 8,498\n",
      "Step 00628/00701 | Training loss: 0.767916| lrm: 0.104137| num_tokens: 6,965\n",
      "Step 00629/00701 | Training loss: 0.607329| lrm: 0.102710| num_tokens: 12,391\n",
      "Step 00630/00701 | Training loss: 0.560998| lrm: 0.101284| num_tokens: 10,252\n",
      "Step 00631/00701 | Training loss: 0.768184| lrm: 0.099857| num_tokens: 12,194\n",
      "Step 00632/00701 | Training loss: 0.643806| lrm: 0.098431| num_tokens: 10,964\n",
      "Step 00633/00701 | Training loss: 0.911903| lrm: 0.097004| num_tokens: 11,198\n",
      "Step 00634/00701 | Training loss: 1.005091| lrm: 0.095578| num_tokens: 7,808\n",
      "Step 00635/00701 | Training loss: 0.765731| lrm: 0.094151| num_tokens: 12,053\n",
      "Step 00636/00701 | Training loss: 0.799182| lrm: 0.092725| num_tokens: 8,138\n",
      "Step 00637/00701 | Training loss: 0.860467| lrm: 0.091298| num_tokens: 13,101\n",
      "Step 00638/00701 | Training loss: 0.560919| lrm: 0.089872| num_tokens: 8,339\n",
      "Step 00639/00701 | Training loss: 0.688962| lrm: 0.088445| num_tokens: 13,057\n",
      "Step 00640/00701 | Training loss: 0.970007| lrm: 0.087019| num_tokens: 12,703\n",
      "Step 00641/00701 | Training loss: 0.912623| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.348607| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.889083| lrm: 0.082739| num_tokens: 8,878\n",
      "Step 00644/00701 | Training loss: 0.839665| lrm: 0.081312| num_tokens: 8,861\n",
      "Step 00645/00701 | Training loss: 1.236730| lrm: 0.079886| num_tokens: 13,446\n",
      "Step 00646/00701 | Training loss: 0.757576| lrm: 0.078459| num_tokens: 12,789\n",
      "Step 00647/00701 | Training loss: 1.062623| lrm: 0.077033| num_tokens: 8,241\n",
      "Step 00648/00701 | Training loss: 0.855261| lrm: 0.075606| num_tokens: 10,330\n",
      "Step 00649/00701 | Training loss: 0.662718| lrm: 0.074180| num_tokens: 12,278\n",
      "Step 00650/00701 | Training loss: 1.330787| lrm: 0.072753| num_tokens: 7,245\n",
      "Step 00651/00701 | Training loss: 1.299613| lrm: 0.071327| num_tokens: 13,989\n",
      "Step 00652/00701 | Training loss: 1.143327| lrm: 0.069900| num_tokens: 11,516\n",
      "Step 00653/00701 | Training loss: 0.646836| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.797653| lrm: 0.067047| num_tokens: 8,702\n",
      "Step 00655/00701 | Training loss: 0.892667| lrm: 0.065621| num_tokens: 11,510\n",
      "Step 00656/00701 | Training loss: 1.253836| lrm: 0.064194| num_tokens: 11,144\n",
      "Step 00657/00701 | Training loss: 0.388510| lrm: 0.062767| num_tokens: 10,336\n",
      "Step 00658/00701 | Training loss: 1.088242| lrm: 0.061341| num_tokens: 9,779\n",
      "Step 00659/00701 | Training loss: 0.745063| lrm: 0.059914| num_tokens: 8,105\n",
      "Step 00660/00701 | Training loss: 0.795207| lrm: 0.058488| num_tokens: 10,476\n",
      "Step 00661/00701 | Training loss: 1.118041| lrm: 0.057061| num_tokens: 11,310\n",
      "Step 00662/00701 | Training loss: 0.641772| lrm: 0.055635| num_tokens: 7,871\n",
      "Step 00663/00701 | Training loss: 0.707959| lrm: 0.054208| num_tokens: 9,855\n",
      "Step 00664/00701 | Training loss: 0.667863| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.657239| lrm: 0.051355| num_tokens: 9,697\n",
      "Step 00666/00701 | Training loss: 1.035839| lrm: 0.049929| num_tokens: 10,710\n",
      "Step 00667/00701 | Training loss: 0.848609| lrm: 0.048502| num_tokens: 13,511\n",
      "Step 00668/00701 | Training loss: 0.642381| lrm: 0.047076| num_tokens: 11,917\n",
      "Step 00669/00701 | Training loss: 1.122071| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670/00701 | Training loss: 0.757337| lrm: 0.044223| num_tokens: 9,513\n",
      "Step 00671/00701 | Training loss: 0.789261| lrm: 0.042796| num_tokens: 10,895\n",
      "Step 00672/00701 | Training loss: 0.655238| lrm: 0.041369| num_tokens: 10,537\n",
      "Step 00673/00701 | Training loss: 0.695517| lrm: 0.039943| num_tokens: 9,859\n",
      "Step 00674/00701 | Training loss: 0.624888| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675/00701 | Training loss: 0.610890| lrm: 0.037090| num_tokens: 10,400\n",
      "Step 00676/00701 | Training loss: 0.858527| lrm: 0.035663| num_tokens: 10,101\n",
      "Step 00677/00701 | Training loss: 1.021652| lrm: 0.034237| num_tokens: 16,334\n",
      "Step 00678/00701 | Training loss: 0.338282| lrm: 0.032810| num_tokens: 12,035\n",
      "Step 00679/00701 | Training loss: 0.789952| lrm: 0.031384| num_tokens: 8,362\n",
      "Step 00680/00701 | Training loss: 1.207663| lrm: 0.029957| num_tokens: 12,050\n",
      "Step 00681/00701 | Training loss: 0.680025| lrm: 0.028531| num_tokens: 9,425\n",
      "Step 00682/00701 | Training loss: 0.719897| lrm: 0.027104| num_tokens: 14,504\n",
      "Step 00683/00701 | Training loss: 0.925932| lrm: 0.025678| num_tokens: 9,745\n",
      "Step 00684/00701 | Training loss: 1.594078| lrm: 0.024251| num_tokens: 12,770\n",
      "Step 00685/00701 | Training loss: 0.593876| lrm: 0.022825| num_tokens: 9,268\n",
      "Step 00686/00701 | Training loss: 0.528958| lrm: 0.021398| num_tokens: 10,168\n",
      "Step 00687/00701 | Training loss: 0.782820| lrm: 0.019971| num_tokens: 13,350\n",
      "Step 00688/00701 | Training loss: 0.704646| lrm: 0.018545| num_tokens: 10,458\n",
      "Step 00689/00701 | Training loss: 0.505537| lrm: 0.017118| num_tokens: 10,652\n",
      "Step 00690/00701 | Training loss: 0.425418| lrm: 0.015692| num_tokens: 8,627\n",
      "Step 00691/00701 | Training loss: 0.544938| lrm: 0.014265| num_tokens: 10,642\n",
      "Step 00692/00701 | Training loss: 0.909075| lrm: 0.012839| num_tokens: 11,930\n",
      "Step 00693/00701 | Training loss: 1.140424| lrm: 0.011412| num_tokens: 9,686\n",
      "Step 00694/00701 | Training loss: 1.218676| lrm: 0.009986| num_tokens: 10,109\n",
      "Step 00695/00701 | Training loss: 1.040223| lrm: 0.008559| num_tokens: 8,528\n",
      "Step 00696/00701 | Training loss: 0.528519| lrm: 0.007133| num_tokens: 12,544\n",
      "Step 00697/00701 | Training loss: 1.214770| lrm: 0.005706| num_tokens: 10,314\n",
      "Step 00698/00701 | Training loss: 0.544455| lrm: 0.004280| num_tokens: 8,859\n",
      "Step 00699/00701 | Training loss: 0.529762| lrm: 0.002853| num_tokens: 11,147\n",
      "Step 00700 | Validation loss: 1.013646\n",
      "final: 357/1024 (34.86%)\n",
      "final: 460/1024 (44.92%)\n",
      "Step 00700 | mmlu_acc: 0.348633, arc_easy_acc: 0.449219\n",
      "[W1121 20:40:56.026948191 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:40:56.227018410 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:40:56.228740849 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:40:56.304632790 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:40:57.629222174 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 710-711, summary, console lines 726-732 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading history steps 710-711, summary, console lines 726-732 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–â–ƒâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–â–„â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–„â–†â–…â–†â–…â–…â–‚â–„â–†â–…â–ƒâ–‡â–…â–…â–…â–„â–†â–†â–†â–‡â–„â–‚â–‚â–‚â–†â–†â–„â–ƒâ–ƒâ–â–„â–ƒâ–ƒâ–†â–ƒâ–ƒâ–ˆâ–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–ƒâ–„â–ƒâ–„â–â–„â–„â–ƒâ–‚â–„â–ƒâ–â–ƒâ–‚â–ƒâ–„â–ƒâ–„â–â–ƒâ–‚â–„â–ƒâ–‚â–‚â–„â–„â–ˆâ–‚â–…â–…â–‚â–â–‚â–„â–…â–‚â–ƒâ–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–ˆâ–‡â–†â–†â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.44922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.34863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 0.52976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-32-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/se8tsmc2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_203851-se8tsmc2/logs\u001b[0m\n",
      "[W1121 20:41:01.731866311 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:41:01.089393326 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:41:01.394995618 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --run=challenge-32-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96972e-571c-4570-ae8d-420fab8d3745",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb13f675-4ade-4937-9af4-7fb7f099961a",
   "metadata": {},
   "source": [
    "### Do chat eval\n",
    "\n",
    "And now repeat chat eval on the model just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66eacf-cc92-49c9-96dc-f47b5f4c2827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7d150d-e52a-464c-b3e3-6565e17f862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:41:49.732000 37868 torch/distributed/run.py:803] \n",
      "W1121 20:41:49.732000 37868 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:41:49.732000 37868 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:41:49.732000 37868 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1059/2376 (44.57%)\n",
      "ARC-Easy accuracy: 44.57%\n",
      "final: 370/1172 (31.57%)\n",
      "ARC-Challenge accuracy: 31.57%\n",
      "final: 4521/14042 (32.20%)\n",
      "MMLU accuracy: 32.20%\n",
      "\u001b[KRank 4 | 7/165 (4.24%)]]]\n",
      "\u001b[KRank 0 | 12/165 (7.27%)]\n",
      "\u001b[KRank 7 | 7/164 (4.27%)]]\n",
      "\u001b[KRank 1 | 8/165 (4.85%)]]\n",
      "\u001b[KRank 2 | 9/165 (5.45%)]]\n",
      "\u001b[KRank 6 | 8/165 (4.85%)]]\n",
      "\u001b[KRank 3 | 13/165 (7.88%)]\n",
      "\u001b[KRank 5 | 5/165 (3.03%)]\n",
      "==================================================\n",
      "final: 69/1319 (5.23%)\n",
      "GSM8K accuracy: 5.23%\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 5 | 2/20 (10.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "==================================================\n",
      "final: 10/164 (6.10%)\n",
      "HumanEval accuracy: 6.10%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 5 | 30/32 (93.75%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8d6b0-7cce-4c3d-8dfe-b24fe22577a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b5402eb-2674-4b9e-9c0a-60e54ab8fe53",
   "metadata": {},
   "source": [
    "There shouldn't be anything non-deterministic in training or eval. Can I get the same result doing a few evals in a row? Try just for ARC-Easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad66971a-0a9e-4c41-9bb9-a2d8a3438323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:49:46.581000 40804 torch/distributed/run.py:803] \n",
      "W1121 20:49:46.581000 40804 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:49:46.581000 40804 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:49:46.581000 40804 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1059/2376 (44.57%)\n",
      "ARC-Easy accuracy: 44.57%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --task-name=ARC-Easy --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cba723-b272-48bb-a1dd-b88582f86d59",
   "metadata": {},
   "source": [
    "ok, that matches the eval above, one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91cab3a-8171-4bab-9087-ab2a844cfe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:50:26.188000 41069 torch/distributed/run.py:803] \n",
      "W1121 20:50:26.188000 41069 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:50:26.188000 41069 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:50:26.188000 41069 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1059/2376 (44.57%)\n",
      "ARC-Easy accuracy: 44.57%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --task-name=ARC-Easy --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c556-1ab3-4c54-8387-d0206613d6f7",
   "metadata": {},
   "source": [
    "matches again, so the eval seems deterministic. Let's SFT train the model again with defaults, just like the last training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3416e56-9a32-4770-b1d5-07916cb8526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:52:00.461000 41423 torch/distributed/run.py:803] \n",
      "W1121 20:52:00.461000 41423 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:52:00.461000 41423 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:52:00.461000 41423 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-32-5\n",
      "user_config: {'run': 'challenge-32-5', 'source': 'mid', 'device_type': '', 'dtype': 'bfloat16', 'device_batch_size': 4, 'num_epochs': 1, 'num_iterations': -1, 'max_data_tokens': -1, 'target_examples_per_step': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'weight_decay': 0.0, 'init_lr_frac': 0.02, 'eval_every': 100, 'eval_steps': 100, 'eval_metrics_every': 200, 'eval_metrics_max_problems': 1024}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run y3aalfao (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/learn-nanochat/challenge-32-redo-chat-eval-d20/wandb/run-20251121_205212-y3aalfao\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-32-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/y3aalfao\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Target examples per step: 32\n",
      "Device batch size: 4\n",
      "Examples per step is device_batch_size * ddp_world_size: 32\n",
      " => grad accum steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "Step 00000 | Validation loss: 1.011101\n",
      "Step 00000/00701 | Training loss: 0.767198| lrm: 1.000000| num_tokens: 9,718\n",
      "Step 00001/00701 | Training loss: 0.765595| lrm: 0.998573| num_tokens: 13,979\n",
      "Step 00002/00701 | Training loss: 1.094171| lrm: 0.997147| num_tokens: 11,283\n",
      "Step 00003/00701 | Training loss: 1.217205| lrm: 0.995720| num_tokens: 12,682\n",
      "Step 00004/00701 | Training loss: 0.786474| lrm: 0.994294| num_tokens: 9,509\n",
      "Step 00005/00701 | Training loss: 0.847999| lrm: 0.992867| num_tokens: 8,344\n",
      "Step 00006/00701 | Training loss: 0.692047| lrm: 0.991441| num_tokens: 8,763\n",
      "Step 00007/00701 | Training loss: 0.417391| lrm: 0.990014| num_tokens: 10,664\n",
      "Step 00008/00701 | Training loss: 1.029425| lrm: 0.988588| num_tokens: 11,584\n",
      "Step 00009/00701 | Training loss: 0.565483| lrm: 0.987161| num_tokens: 9,920\n",
      "Step 00010/00701 | Training loss: 0.404486| lrm: 0.985735| num_tokens: 15,002\n",
      "Step 00011/00701 | Training loss: 1.073496| lrm: 0.984308| num_tokens: 10,504\n",
      "Step 00012/00701 | Training loss: 0.879631| lrm: 0.982882| num_tokens: 12,693\n",
      "Step 00013/00701 | Training loss: 0.957238| lrm: 0.981455| num_tokens: 10,802\n",
      "Step 00014/00701 | Training loss: 0.773911| lrm: 0.980029| num_tokens: 7,541\n",
      "Step 00015/00701 | Training loss: 0.880414| lrm: 0.978602| num_tokens: 10,515\n",
      "Step 00016/00701 | Training loss: 1.074604| lrm: 0.977175| num_tokens: 14,469\n",
      "Step 00017/00701 | Training loss: 0.539061| lrm: 0.975749| num_tokens: 7,715\n",
      "Step 00018/00701 | Training loss: 1.229514| lrm: 0.974322| num_tokens: 11,655\n",
      "Step 00019/00701 | Training loss: 0.591576| lrm: 0.972896| num_tokens: 11,228\n",
      "Step 00020/00701 | Training loss: 0.415431| lrm: 0.971469| num_tokens: 10,678\n",
      "Step 00021/00701 | Training loss: 0.758978| lrm: 0.970043| num_tokens: 12,324\n",
      "Step 00022/00701 | Training loss: 1.113133| lrm: 0.968616| num_tokens: 9,331\n",
      "Step 00023/00701 | Training loss: 0.691940| lrm: 0.967190| num_tokens: 9,394\n",
      "Step 00024/00701 | Training loss: 0.843727| lrm: 0.965763| num_tokens: 10,715\n",
      "Step 00025/00701 | Training loss: 0.631393| lrm: 0.964337| num_tokens: 12,383\n",
      "Step 00026/00701 | Training loss: 1.035555| lrm: 0.962910| num_tokens: 12,576\n",
      "Step 00027/00701 | Training loss: 0.919362| lrm: 0.961484| num_tokens: 8,359\n",
      "Step 00028/00701 | Training loss: 0.932282| lrm: 0.960057| num_tokens: 10,178\n",
      "Step 00029/00701 | Training loss: 0.933986| lrm: 0.958631| num_tokens: 10,629\n",
      "Step 00030/00701 | Training loss: 0.589078| lrm: 0.957204| num_tokens: 9,143\n",
      "Step 00031/00701 | Training loss: 1.143358| lrm: 0.955777| num_tokens: 7,481\n",
      "Step 00032/00701 | Training loss: 0.857919| lrm: 0.954351| num_tokens: 9,972\n",
      "Step 00033/00701 | Training loss: 0.840412| lrm: 0.952924| num_tokens: 11,270\n",
      "Step 00034/00701 | Training loss: 1.042769| lrm: 0.951498| num_tokens: 11,177\n",
      "Step 00035/00701 | Training loss: 0.847830| lrm: 0.950071| num_tokens: 11,465\n",
      "Step 00036/00701 | Training loss: 0.931172| lrm: 0.948645| num_tokens: 9,343\n",
      "Step 00037/00701 | Training loss: 1.066375| lrm: 0.947218| num_tokens: 10,954\n",
      "Step 00038/00701 | Training loss: 0.721038| lrm: 0.945792| num_tokens: 6,793\n",
      "Step 00039/00701 | Training loss: 0.609365| lrm: 0.944365| num_tokens: 5,849\n",
      "Step 00040/00701 | Training loss: 0.941639| lrm: 0.942939| num_tokens: 12,817\n",
      "Step 00041/00701 | Training loss: 0.621104| lrm: 0.941512| num_tokens: 14,344\n",
      "Step 00042/00701 | Training loss: 1.181270| lrm: 0.940086| num_tokens: 11,223\n",
      "Step 00043/00701 | Training loss: 0.998962| lrm: 0.938659| num_tokens: 11,061\n",
      "Step 00044/00701 | Training loss: 1.084778| lrm: 0.937233| num_tokens: 12,666\n",
      "Step 00045/00701 | Training loss: 1.066684| lrm: 0.935806| num_tokens: 13,359\n",
      "Step 00046/00701 | Training loss: 0.688079| lrm: 0.934379| num_tokens: 9,630\n",
      "Step 00047/00701 | Training loss: 1.077803| lrm: 0.932953| num_tokens: 12,891\n",
      "Step 00048/00701 | Training loss: 0.732450| lrm: 0.931526| num_tokens: 10,059\n",
      "Step 00049/00701 | Training loss: 0.541393| lrm: 0.930100| num_tokens: 11,237\n",
      "Step 00050/00701 | Training loss: 0.758722| lrm: 0.928673| num_tokens: 9,079\n",
      "Step 00051/00701 | Training loss: 0.276359| lrm: 0.927247| num_tokens: 9,590\n",
      "Step 00052/00701 | Training loss: 0.830589| lrm: 0.925820| num_tokens: 9,751\n",
      "Step 00053/00701 | Training loss: 1.002696| lrm: 0.924394| num_tokens: 7,924\n",
      "Step 00054/00701 | Training loss: 0.374761| lrm: 0.922967| num_tokens: 9,631\n",
      "Step 00055/00701 | Training loss: 0.898254| lrm: 0.921541| num_tokens: 6,508\n",
      "Step 00056/00701 | Training loss: 1.395862| lrm: 0.920114| num_tokens: 10,513\n",
      "Step 00057/00701 | Training loss: 1.007970| lrm: 0.918688| num_tokens: 9,782\n",
      "Step 00058/00701 | Training loss: 1.360846| lrm: 0.917261| num_tokens: 13,372\n",
      "Step 00059/00701 | Training loss: 1.060047| lrm: 0.915835| num_tokens: 7,944\n",
      "Step 00060/00701 | Training loss: 1.141083| lrm: 0.914408| num_tokens: 12,661\n",
      "Step 00061/00701 | Training loss: 1.255837| lrm: 0.912981| num_tokens: 13,711\n",
      "Step 00062/00701 | Training loss: 0.940331| lrm: 0.911555| num_tokens: 12,159\n",
      "Step 00063/00701 | Training loss: 0.569391| lrm: 0.910128| num_tokens: 10,617\n",
      "Step 00064/00701 | Training loss: 0.670108| lrm: 0.908702| num_tokens: 8,098\n",
      "Step 00065/00701 | Training loss: 0.490865| lrm: 0.907275| num_tokens: 11,033\n",
      "Step 00066/00701 | Training loss: 1.025400| lrm: 0.905849| num_tokens: 9,531\n",
      "Step 00067/00701 | Training loss: 0.555292| lrm: 0.904422| num_tokens: 11,573\n",
      "Step 00068/00701 | Training loss: 1.318824| lrm: 0.902996| num_tokens: 12,923\n",
      "Step 00069/00701 | Training loss: 0.844608| lrm: 0.901569| num_tokens: 7,943\n",
      "Step 00070/00701 | Training loss: 0.478180| lrm: 0.900143| num_tokens: 10,775\n",
      "Step 00071/00701 | Training loss: 0.732779| lrm: 0.898716| num_tokens: 9,193\n",
      "Step 00072/00701 | Training loss: 0.947657| lrm: 0.897290| num_tokens: 9,378\n",
      "Step 00073/00701 | Training loss: 1.221437| lrm: 0.895863| num_tokens: 9,060\n",
      "Step 00074/00701 | Training loss: 0.725705| lrm: 0.894437| num_tokens: 7,685\n",
      "Step 00075/00701 | Training loss: 0.699387| lrm: 0.893010| num_tokens: 6,807\n",
      "Step 00076/00701 | Training loss: 0.832687| lrm: 0.891583| num_tokens: 7,530\n",
      "Step 00077/00701 | Training loss: 0.854400| lrm: 0.890157| num_tokens: 9,319\n",
      "Step 00078/00701 | Training loss: 0.612993| lrm: 0.888730| num_tokens: 11,560\n",
      "Step 00079/00701 | Training loss: 0.583769| lrm: 0.887304| num_tokens: 9,512\n",
      "Step 00080/00701 | Training loss: 1.002966| lrm: 0.885877| num_tokens: 11,674\n",
      "Step 00081/00701 | Training loss: 0.511869| lrm: 0.884451| num_tokens: 7,783\n",
      "Step 00082/00701 | Training loss: 0.685827| lrm: 0.883024| num_tokens: 14,544\n",
      "Step 00083/00701 | Training loss: 0.540471| lrm: 0.881598| num_tokens: 11,069\n",
      "Step 00084/00701 | Training loss: 0.749707| lrm: 0.880171| num_tokens: 13,568\n",
      "Step 00085/00701 | Training loss: 1.148553| lrm: 0.878745| num_tokens: 8,788\n",
      "Step 00086/00701 | Training loss: 0.992071| lrm: 0.877318| num_tokens: 9,908\n",
      "Step 00087/00701 | Training loss: 1.020240| lrm: 0.875892| num_tokens: 9,627\n",
      "Step 00088/00701 | Training loss: 0.861813| lrm: 0.874465| num_tokens: 11,064\n",
      "Step 00089/00701 | Training loss: 1.391989| lrm: 0.873039| num_tokens: 9,523\n",
      "Step 00090/00701 | Training loss: 0.959968| lrm: 0.871612| num_tokens: 10,566\n",
      "Step 00091/00701 | Training loss: 0.751837| lrm: 0.870185| num_tokens: 10,761\n",
      "Step 00092/00701 | Training loss: 0.869232| lrm: 0.868759| num_tokens: 10,418\n",
      "Step 00093/00701 | Training loss: 0.869674| lrm: 0.867332| num_tokens: 9,591\n",
      "Step 00094/00701 | Training loss: 0.729427| lrm: 0.865906| num_tokens: 9,464\n",
      "Step 00095/00701 | Training loss: 0.957557| lrm: 0.864479| num_tokens: 9,395\n",
      "Step 00096/00701 | Training loss: 1.119869| lrm: 0.863053| num_tokens: 13,239\n",
      "Step 00097/00701 | Training loss: 1.007903| lrm: 0.861626| num_tokens: 14,318\n",
      "Step 00098/00701 | Training loss: 0.755164| lrm: 0.860200| num_tokens: 8,548\n",
      "Step 00099/00701 | Training loss: 1.138831| lrm: 0.858773| num_tokens: 8,989\n",
      "Step 00100 | Validation loss: 1.015277\n",
      "Step 00100/00701 | Training loss: 1.958074| lrm: 0.857347| num_tokens: 10,527\n",
      "Step 00101/00701 | Training loss: 0.437595| lrm: 0.855920| num_tokens: 9,838\n",
      "Step 00102/00701 | Training loss: 0.567793| lrm: 0.854494| num_tokens: 10,665\n",
      "Step 00103/00701 | Training loss: 0.717588| lrm: 0.853067| num_tokens: 11,272\n",
      "Step 00104/00701 | Training loss: 0.712284| lrm: 0.851641| num_tokens: 8,228\n",
      "Step 00105/00701 | Training loss: 0.767651| lrm: 0.850214| num_tokens: 10,153\n",
      "Step 00106/00701 | Training loss: 0.828549| lrm: 0.848787| num_tokens: 11,156\n",
      "Step 00107/00701 | Training loss: 1.044691| lrm: 0.847361| num_tokens: 10,161\n",
      "Step 00108/00701 | Training loss: 0.942736| lrm: 0.845934| num_tokens: 8,469\n",
      "Step 00109/00701 | Training loss: 0.683070| lrm: 0.844508| num_tokens: 8,740\n",
      "Step 00110/00701 | Training loss: 0.817111| lrm: 0.843081| num_tokens: 12,241\n",
      "Step 00111/00701 | Training loss: 0.705242| lrm: 0.841655| num_tokens: 10,737\n",
      "Step 00112/00701 | Training loss: 1.481700| lrm: 0.840228| num_tokens: 14,866\n",
      "Step 00113/00701 | Training loss: 0.758858| lrm: 0.838802| num_tokens: 16,098\n",
      "Step 00114/00701 | Training loss: 1.108861| lrm: 0.837375| num_tokens: 14,309\n",
      "Step 00115/00701 | Training loss: 0.318012| lrm: 0.835949| num_tokens: 11,090\n",
      "Step 00116/00701 | Training loss: 0.796798| lrm: 0.834522| num_tokens: 9,127\n",
      "Step 00117/00701 | Training loss: 0.923565| lrm: 0.833096| num_tokens: 12,217\n",
      "Step 00118/00701 | Training loss: 0.644526| lrm: 0.831669| num_tokens: 9,686\n",
      "Step 00119/00701 | Training loss: 0.644181| lrm: 0.830243| num_tokens: 14,012\n",
      "Step 00120/00701 | Training loss: 0.973895| lrm: 0.828816| num_tokens: 12,993\n",
      "Step 00121/00701 | Training loss: 1.082536| lrm: 0.827389| num_tokens: 16,514\n",
      "Step 00122/00701 | Training loss: 0.820451| lrm: 0.825963| num_tokens: 7,264\n",
      "Step 00123/00701 | Training loss: 0.875375| lrm: 0.824536| num_tokens: 6,636\n",
      "Step 00124/00701 | Training loss: 0.694316| lrm: 0.823110| num_tokens: 11,608\n",
      "Step 00125/00701 | Training loss: 0.981116| lrm: 0.821683| num_tokens: 5,773\n",
      "Step 00126/00701 | Training loss: 0.628601| lrm: 0.820257| num_tokens: 10,125\n",
      "Step 00127/00701 | Training loss: 1.057964| lrm: 0.818830| num_tokens: 10,148\n",
      "Step 00128/00701 | Training loss: 0.452897| lrm: 0.817404| num_tokens: 10,627\n",
      "Step 00129/00701 | Training loss: 0.826604| lrm: 0.815977| num_tokens: 8,873\n",
      "Step 00130/00701 | Training loss: 0.954568| lrm: 0.814551| num_tokens: 11,747\n",
      "Step 00131/00701 | Training loss: 0.938276| lrm: 0.813124| num_tokens: 11,228\n",
      "Step 00132/00701 | Training loss: 0.808405| lrm: 0.811698| num_tokens: 12,012\n",
      "Step 00133/00701 | Training loss: 0.909897| lrm: 0.810271| num_tokens: 16,435\n",
      "Step 00134/00701 | Training loss: 0.699266| lrm: 0.808845| num_tokens: 10,741\n",
      "Step 00135/00701 | Training loss: 1.373682| lrm: 0.807418| num_tokens: 13,492\n",
      "Step 00136/00701 | Training loss: 0.506856| lrm: 0.805991| num_tokens: 10,194\n",
      "Step 00137/00701 | Training loss: 0.885556| lrm: 0.804565| num_tokens: 10,271\n",
      "Step 00138/00701 | Training loss: 0.744831| lrm: 0.803138| num_tokens: 11,043\n",
      "Step 00139/00701 | Training loss: 0.959083| lrm: 0.801712| num_tokens: 12,147\n",
      "Step 00140/00701 | Training loss: 0.966126| lrm: 0.800285| num_tokens: 13,088\n",
      "Step 00141/00701 | Training loss: 0.808393| lrm: 0.798859| num_tokens: 13,647\n",
      "Step 00142/00701 | Training loss: 0.470013| lrm: 0.797432| num_tokens: 10,286\n",
      "Step 00143/00701 | Training loss: 1.172619| lrm: 0.796006| num_tokens: 8,956\n",
      "Step 00144/00701 | Training loss: 1.255478| lrm: 0.794579| num_tokens: 14,847\n",
      "Step 00145/00701 | Training loss: 0.959766| lrm: 0.793153| num_tokens: 14,649\n",
      "Step 00146/00701 | Training loss: 0.575184| lrm: 0.791726| num_tokens: 11,999\n",
      "Step 00147/00701 | Training loss: 0.642703| lrm: 0.790300| num_tokens: 8,022\n",
      "Step 00148/00701 | Training loss: 1.041429| lrm: 0.788873| num_tokens: 11,348\n",
      "Step 00149/00701 | Training loss: 1.078628| lrm: 0.787447| num_tokens: 14,182\n",
      "Step 00150/00701 | Training loss: 0.625461| lrm: 0.786020| num_tokens: 10,776\n",
      "Step 00151/00701 | Training loss: 0.677622| lrm: 0.784593| num_tokens: 14,039\n",
      "Step 00152/00701 | Training loss: 0.788899| lrm: 0.783167| num_tokens: 8,526\n",
      "Step 00153/00701 | Training loss: 0.828014| lrm: 0.781740| num_tokens: 12,782\n",
      "Step 00154/00701 | Training loss: 0.757112| lrm: 0.780314| num_tokens: 9,943\n",
      "Step 00155/00701 | Training loss: 0.834369| lrm: 0.778887| num_tokens: 10,624\n",
      "Step 00156/00701 | Training loss: 0.575250| lrm: 0.777461| num_tokens: 13,575\n",
      "Step 00157/00701 | Training loss: 1.126942| lrm: 0.776034| num_tokens: 13,319\n",
      "Step 00158/00701 | Training loss: 0.542580| lrm: 0.774608| num_tokens: 7,492\n",
      "Step 00159/00701 | Training loss: 0.720306| lrm: 0.773181| num_tokens: 9,274\n",
      "Step 00160/00701 | Training loss: 0.646261| lrm: 0.771755| num_tokens: 9,399\n",
      "Step 00161/00701 | Training loss: 0.981617| lrm: 0.770328| num_tokens: 9,537\n",
      "Step 00162/00701 | Training loss: 0.985563| lrm: 0.768902| num_tokens: 11,779\n",
      "Step 00163/00701 | Training loss: 0.492784| lrm: 0.767475| num_tokens: 7,485\n",
      "Step 00164/00701 | Training loss: 0.417839| lrm: 0.766049| num_tokens: 9,081\n",
      "Step 00165/00701 | Training loss: 0.758346| lrm: 0.764622| num_tokens: 12,046\n",
      "Step 00166/00701 | Training loss: 0.954610| lrm: 0.763195| num_tokens: 12,309\n",
      "Step 00167/00701 | Training loss: 0.903993| lrm: 0.761769| num_tokens: 10,547\n",
      "Step 00168/00701 | Training loss: 0.703074| lrm: 0.760342| num_tokens: 9,717\n",
      "Step 00169/00701 | Training loss: 0.881090| lrm: 0.758916| num_tokens: 9,577\n",
      "Step 00170/00701 | Training loss: 1.304623| lrm: 0.757489| num_tokens: 13,464\n",
      "Step 00171/00701 | Training loss: 0.682513| lrm: 0.756063| num_tokens: 10,156\n",
      "Step 00172/00701 | Training loss: 0.960784| lrm: 0.754636| num_tokens: 9,718\n",
      "Step 00173/00701 | Training loss: 0.970130| lrm: 0.753210| num_tokens: 13,931\n",
      "Step 00174/00701 | Training loss: 0.945948| lrm: 0.751783| num_tokens: 9,132\n",
      "Step 00175/00701 | Training loss: 1.047429| lrm: 0.750357| num_tokens: 9,835\n",
      "Step 00176/00701 | Training loss: 0.991048| lrm: 0.748930| num_tokens: 11,244\n",
      "Step 00177/00701 | Training loss: 0.984202| lrm: 0.747504| num_tokens: 14,670\n",
      "Step 00178/00701 | Training loss: 0.632327| lrm: 0.746077| num_tokens: 9,624\n",
      "Step 00179/00701 | Training loss: 0.952881| lrm: 0.744650| num_tokens: 11,085\n",
      "Step 00180/00701 | Training loss: 0.544345| lrm: 0.743224| num_tokens: 11,989\n",
      "Step 00181/00701 | Training loss: 0.872874| lrm: 0.741797| num_tokens: 15,749\n",
      "Step 00182/00701 | Training loss: 1.181847| lrm: 0.740371| num_tokens: 11,567\n",
      "Step 00183/00701 | Training loss: 0.486006| lrm: 0.738944| num_tokens: 12,476\n",
      "Step 00184/00701 | Training loss: 0.966270| lrm: 0.737518| num_tokens: 13,569\n",
      "Step 00185/00701 | Training loss: 0.665582| lrm: 0.736091| num_tokens: 11,265\n",
      "Step 00186/00701 | Training loss: 0.844668| lrm: 0.734665| num_tokens: 11,859\n",
      "Step 00187/00701 | Training loss: 0.481531| lrm: 0.733238| num_tokens: 13,609\n",
      "Step 00188/00701 | Training loss: 0.962799| lrm: 0.731812| num_tokens: 9,421\n",
      "Step 00189/00701 | Training loss: 0.972712| lrm: 0.730385| num_tokens: 9,300\n",
      "Step 00190/00701 | Training loss: 0.459593| lrm: 0.728959| num_tokens: 15,818\n",
      "Step 00191/00701 | Training loss: 0.907480| lrm: 0.727532| num_tokens: 12,340\n",
      "Step 00192/00701 | Training loss: 0.532666| lrm: 0.726106| num_tokens: 9,962\n",
      "Step 00193/00701 | Training loss: 1.102836| lrm: 0.724679| num_tokens: 10,650\n",
      "Step 00194/00701 | Training loss: 1.214787| lrm: 0.723252| num_tokens: 8,192\n",
      "Step 00195/00701 | Training loss: 0.546197| lrm: 0.721826| num_tokens: 14,326\n",
      "Step 00196/00701 | Training loss: 0.758482| lrm: 0.720399| num_tokens: 12,213\n",
      "Step 00197/00701 | Training loss: 1.041988| lrm: 0.718973| num_tokens: 10,264\n",
      "Step 00198/00701 | Training loss: 0.583184| lrm: 0.717546| num_tokens: 9,718\n",
      "Step 00199/00701 | Training loss: 0.616998| lrm: 0.716120| num_tokens: 10,337\n",
      "Step 00200 | Validation loss: 1.014362\n",
      "final: 335/1024 (32.71%)\n",
      "final: 422/1024 (41.21%)\n",
      "Step 00200 | mmlu_acc: 0.327148, arc_easy_acc: 0.412109\n",
      "Step 00200/00701 | Training loss: 0.844602| lrm: 0.714693| num_tokens: 17,787\n",
      "Step 00201/00701 | Training loss: 0.881369| lrm: 0.713267| num_tokens: 9,676\n",
      "Step 00202/00701 | Training loss: 1.930040| lrm: 0.711840| num_tokens: 15,694\n",
      "Step 00203/00701 | Training loss: 1.075000| lrm: 0.710414| num_tokens: 8,344\n",
      "Step 00204/00701 | Training loss: 0.989703| lrm: 0.708987| num_tokens: 11,398\n",
      "Step 00205/00701 | Training loss: 0.654068| lrm: 0.707561| num_tokens: 11,165\n",
      "Step 00206/00701 | Training loss: 0.902004| lrm: 0.706134| num_tokens: 10,515\n",
      "Step 00207/00701 | Training loss: 0.640210| lrm: 0.704708| num_tokens: 11,281\n",
      "Step 00208/00701 | Training loss: 1.289480| lrm: 0.703281| num_tokens: 13,195\n",
      "Step 00209/00701 | Training loss: 0.569468| lrm: 0.701854| num_tokens: 10,620\n",
      "Step 00210/00701 | Training loss: 0.717045| lrm: 0.700428| num_tokens: 11,855\n",
      "Step 00211/00701 | Training loss: 1.064790| lrm: 0.699001| num_tokens: 11,439\n",
      "Step 00212/00701 | Training loss: 1.011503| lrm: 0.697575| num_tokens: 12,980\n",
      "Step 00213/00701 | Training loss: 0.736380| lrm: 0.696148| num_tokens: 12,947\n",
      "Step 00214/00701 | Training loss: 1.103442| lrm: 0.694722| num_tokens: 10,888\n",
      "Step 00215/00701 | Training loss: 0.677179| lrm: 0.693295| num_tokens: 10,049\n",
      "Step 00216/00701 | Training loss: 0.755477| lrm: 0.691869| num_tokens: 8,634\n",
      "Step 00217/00701 | Training loss: 0.998066| lrm: 0.690442| num_tokens: 8,208\n",
      "Step 00218/00701 | Training loss: 0.478731| lrm: 0.689016| num_tokens: 9,537\n",
      "Step 00219/00701 | Training loss: 0.884776| lrm: 0.687589| num_tokens: 14,707\n",
      "Step 00220/00701 | Training loss: 0.930367| lrm: 0.686163| num_tokens: 8,471\n",
      "Step 00221/00701 | Training loss: 0.769709| lrm: 0.684736| num_tokens: 7,856\n",
      "Step 00222/00701 | Training loss: 1.108060| lrm: 0.683310| num_tokens: 7,401\n",
      "Step 00223/00701 | Training loss: 1.151489| lrm: 0.681883| num_tokens: 11,899\n",
      "Step 00224/00701 | Training loss: 0.711662| lrm: 0.680456| num_tokens: 10,060\n",
      "Step 00225/00701 | Training loss: 1.126064| lrm: 0.679030| num_tokens: 8,888\n",
      "Step 00226/00701 | Training loss: 0.646610| lrm: 0.677603| num_tokens: 10,747\n",
      "Step 00227/00701 | Training loss: 0.594271| lrm: 0.676177| num_tokens: 10,038\n",
      "Step 00228/00701 | Training loss: 0.704456| lrm: 0.674750| num_tokens: 12,340\n",
      "Step 00229/00701 | Training loss: 1.137769| lrm: 0.673324| num_tokens: 9,397\n",
      "Step 00230/00701 | Training loss: 1.013774| lrm: 0.671897| num_tokens: 7,844\n",
      "Step 00231/00701 | Training loss: 1.209577| lrm: 0.670471| num_tokens: 11,685\n",
      "Step 00232/00701 | Training loss: 1.650310| lrm: 0.669044| num_tokens: 10,805\n",
      "Step 00233/00701 | Training loss: 0.740570| lrm: 0.667618| num_tokens: 10,203\n",
      "Step 00234/00701 | Training loss: 0.880848| lrm: 0.666191| num_tokens: 14,225\n",
      "Step 00235/00701 | Training loss: 1.031703| lrm: 0.664765| num_tokens: 9,748\n",
      "Step 00236/00701 | Training loss: 1.011461| lrm: 0.663338| num_tokens: 7,453\n",
      "Step 00237/00701 | Training loss: 0.844491| lrm: 0.661912| num_tokens: 7,354\n",
      "Step 00238/00701 | Training loss: 1.005381| lrm: 0.660485| num_tokens: 13,482\n",
      "Step 00239/00701 | Training loss: 1.262644| lrm: 0.659058| num_tokens: 12,515\n",
      "Step 00240/00701 | Training loss: 0.559674| lrm: 0.657632| num_tokens: 12,885\n",
      "Step 00241/00701 | Training loss: 1.005554| lrm: 0.656205| num_tokens: 11,284\n",
      "Step 00242/00701 | Training loss: 0.899528| lrm: 0.654779| num_tokens: 8,141\n",
      "Step 00243/00701 | Training loss: 0.665983| lrm: 0.653352| num_tokens: 10,581\n",
      "Step 00244/00701 | Training loss: 0.468007| lrm: 0.651926| num_tokens: 6,552\n",
      "Step 00245/00701 | Training loss: 1.031619| lrm: 0.650499| num_tokens: 12,896\n",
      "Step 00246/00701 | Training loss: 0.325038| lrm: 0.649073| num_tokens: 8,779\n",
      "Step 00247/00701 | Training loss: 0.557682| lrm: 0.647646| num_tokens: 10,640\n",
      "Step 00248/00701 | Training loss: 0.807496| lrm: 0.646220| num_tokens: 14,873\n",
      "Step 00249/00701 | Training loss: 0.707668| lrm: 0.644793| num_tokens: 8,755\n",
      "Step 00250/00701 | Training loss: 1.023257| lrm: 0.643367| num_tokens: 11,347\n",
      "Step 00251/00701 | Training loss: 1.207034| lrm: 0.641940| num_tokens: 13,773\n",
      "Step 00252/00701 | Training loss: 0.692841| lrm: 0.640514| num_tokens: 10,831\n",
      "Step 00253/00701 | Training loss: 0.820586| lrm: 0.639087| num_tokens: 14,171\n",
      "Step 00254/00701 | Training loss: 0.861646| lrm: 0.637660| num_tokens: 8,633\n",
      "Step 00255/00701 | Training loss: 0.743541| lrm: 0.636234| num_tokens: 10,649\n",
      "Step 00256/00701 | Training loss: 1.410625| lrm: 0.634807| num_tokens: 10,565\n",
      "Step 00257/00701 | Training loss: 0.536024| lrm: 0.633381| num_tokens: 8,375\n",
      "Step 00258/00701 | Training loss: 1.019237| lrm: 0.631954| num_tokens: 11,893\n",
      "Step 00259/00701 | Training loss: 0.908457| lrm: 0.630528| num_tokens: 6,289\n",
      "Step 00260/00701 | Training loss: 0.454427| lrm: 0.629101| num_tokens: 12,731\n",
      "Step 00261/00701 | Training loss: 0.679124| lrm: 0.627675| num_tokens: 11,069\n",
      "Step 00262/00701 | Training loss: 0.696467| lrm: 0.626248| num_tokens: 10,292\n",
      "Step 00263/00701 | Training loss: 0.623398| lrm: 0.624822| num_tokens: 6,973\n",
      "Step 00264/00701 | Training loss: 1.046338| lrm: 0.623395| num_tokens: 9,513\n",
      "Step 00265/00701 | Training loss: 0.820461| lrm: 0.621969| num_tokens: 11,233\n",
      "Step 00266/00701 | Training loss: 0.843753| lrm: 0.620542| num_tokens: 11,266\n",
      "Step 00267/00701 | Training loss: 0.548312| lrm: 0.619116| num_tokens: 8,955\n",
      "Step 00268/00701 | Training loss: 0.921519| lrm: 0.617689| num_tokens: 12,249\n",
      "Step 00269/00701 | Training loss: 0.756443| lrm: 0.616262| num_tokens: 9,576\n",
      "Step 00270/00701 | Training loss: 1.143814| lrm: 0.614836| num_tokens: 12,109\n",
      "Step 00271/00701 | Training loss: 1.395572| lrm: 0.613409| num_tokens: 5,916\n",
      "Step 00272/00701 | Training loss: 0.856548| lrm: 0.611983| num_tokens: 13,072\n",
      "Step 00273/00701 | Training loss: 0.622386| lrm: 0.610556| num_tokens: 9,778\n",
      "Step 00274/00701 | Training loss: 0.653317| lrm: 0.609130| num_tokens: 12,238\n",
      "Step 00275/00701 | Training loss: 0.584570| lrm: 0.607703| num_tokens: 10,854\n",
      "Step 00276/00701 | Training loss: 0.706122| lrm: 0.606277| num_tokens: 10,720\n",
      "Step 00277/00701 | Training loss: 0.570767| lrm: 0.604850| num_tokens: 7,570\n",
      "Step 00278/00701 | Training loss: 0.823401| lrm: 0.603424| num_tokens: 11,628\n",
      "Step 00279/00701 | Training loss: 0.889824| lrm: 0.601997| num_tokens: 10,775\n",
      "Step 00280/00701 | Training loss: 0.777876| lrm: 0.600571| num_tokens: 8,970\n",
      "Step 00281/00701 | Training loss: 1.005596| lrm: 0.599144| num_tokens: 14,104\n",
      "Step 00282/00701 | Training loss: 0.992370| lrm: 0.597718| num_tokens: 10,950\n",
      "Step 00283/00701 | Training loss: 0.620359| lrm: 0.596291| num_tokens: 8,906\n",
      "Step 00284/00701 | Training loss: 0.430252| lrm: 0.594864| num_tokens: 12,614\n",
      "Step 00285/00701 | Training loss: 0.861989| lrm: 0.593438| num_tokens: 10,249\n",
      "Step 00286/00701 | Training loss: 0.600635| lrm: 0.592011| num_tokens: 11,914\n",
      "Step 00287/00701 | Training loss: 0.262421| lrm: 0.590585| num_tokens: 9,547\n",
      "Step 00288/00701 | Training loss: 0.678536| lrm: 0.589158| num_tokens: 5,007\n",
      "Step 00289/00701 | Training loss: 0.683181| lrm: 0.587732| num_tokens: 9,433\n",
      "Step 00290/00701 | Training loss: 1.071817| lrm: 0.586305| num_tokens: 11,035\n",
      "Step 00291/00701 | Training loss: 0.997949| lrm: 0.584879| num_tokens: 10,852\n",
      "Step 00292/00701 | Training loss: 0.947158| lrm: 0.583452| num_tokens: 11,659\n",
      "Step 00293/00701 | Training loss: 0.829433| lrm: 0.582026| num_tokens: 15,038\n",
      "Step 00294/00701 | Training loss: 0.726188| lrm: 0.580599| num_tokens: 9,439\n",
      "Step 00295/00701 | Training loss: 1.109380| lrm: 0.579173| num_tokens: 6,998\n",
      "Step 00296/00701 | Training loss: 1.221858| lrm: 0.577746| num_tokens: 12,267\n",
      "Step 00297/00701 | Training loss: 0.579922| lrm: 0.576320| num_tokens: 17,571\n",
      "Step 00298/00701 | Training loss: 1.036201| lrm: 0.574893| num_tokens: 10,103\n",
      "Step 00299/00701 | Training loss: 0.639280| lrm: 0.573466| num_tokens: 7,731\n",
      "Step 00300 | Validation loss: 1.014104\n",
      "Step 00300/00701 | Training loss: 0.750550| lrm: 0.572040| num_tokens: 8,914\n",
      "Step 00301/00701 | Training loss: 0.828903| lrm: 0.570613| num_tokens: 12,830\n",
      "Step 00302/00701 | Training loss: 0.767596| lrm: 0.569187| num_tokens: 12,209\n",
      "Step 00303/00701 | Training loss: 0.956936| lrm: 0.567760| num_tokens: 11,589\n",
      "Step 00304/00701 | Training loss: 0.613508| lrm: 0.566334| num_tokens: 8,228\n",
      "Step 00305/00701 | Training loss: 0.863405| lrm: 0.564907| num_tokens: 14,637\n",
      "Step 00306/00701 | Training loss: 0.887038| lrm: 0.563481| num_tokens: 11,570\n",
      "Step 00307/00701 | Training loss: 1.095482| lrm: 0.562054| num_tokens: 12,881\n",
      "Step 00308/00701 | Training loss: 0.665850| lrm: 0.560628| num_tokens: 6,839\n",
      "Step 00309/00701 | Training loss: 0.939714| lrm: 0.559201| num_tokens: 11,009\n",
      "Step 00310/00701 | Training loss: 0.582384| lrm: 0.557775| num_tokens: 12,118\n",
      "Step 00311/00701 | Training loss: 1.271646| lrm: 0.556348| num_tokens: 10,305\n",
      "Step 00312/00701 | Training loss: 0.748378| lrm: 0.554922| num_tokens: 11,427\n",
      "Step 00313/00701 | Training loss: 0.741506| lrm: 0.553495| num_tokens: 8,436\n",
      "Step 00314/00701 | Training loss: 1.104205| lrm: 0.552068| num_tokens: 11,858\n",
      "Step 00315/00701 | Training loss: 0.532259| lrm: 0.550642| num_tokens: 14,168\n",
      "Step 00316/00701 | Training loss: 1.128194| lrm: 0.549215| num_tokens: 6,451\n",
      "Step 00317/00701 | Training loss: 0.405990| lrm: 0.547789| num_tokens: 11,643\n",
      "Step 00318/00701 | Training loss: 0.708024| lrm: 0.546362| num_tokens: 10,043\n",
      "Step 00319/00701 | Training loss: 1.214930| lrm: 0.544936| num_tokens: 11,387\n",
      "Step 00320/00701 | Training loss: 0.615925| lrm: 0.543509| num_tokens: 6,437\n",
      "Step 00321/00701 | Training loss: 1.083768| lrm: 0.542083| num_tokens: 8,880\n",
      "Step 00322/00701 | Training loss: 0.749012| lrm: 0.540656| num_tokens: 4,147\n",
      "Step 00323/00701 | Training loss: 0.791733| lrm: 0.539230| num_tokens: 12,083\n",
      "Step 00324/00701 | Training loss: 0.978629| lrm: 0.537803| num_tokens: 11,807\n",
      "Step 00325/00701 | Training loss: 0.634895| lrm: 0.536377| num_tokens: 9,002\n",
      "Step 00326/00701 | Training loss: 0.999249| lrm: 0.534950| num_tokens: 7,345\n",
      "Step 00327/00701 | Training loss: 0.707436| lrm: 0.533524| num_tokens: 12,006\n",
      "Step 00328/00701 | Training loss: 1.043872| lrm: 0.532097| num_tokens: 12,714\n",
      "Step 00329/00701 | Training loss: 0.659550| lrm: 0.530670| num_tokens: 10,357\n",
      "Step 00330/00701 | Training loss: 1.010025| lrm: 0.529244| num_tokens: 8,086\n",
      "Step 00331/00701 | Training loss: 0.804849| lrm: 0.527817| num_tokens: 7,793\n",
      "Step 00332/00701 | Training loss: 0.728192| lrm: 0.526391| num_tokens: 10,629\n",
      "Step 00333/00701 | Training loss: 1.273137| lrm: 0.524964| num_tokens: 9,785\n",
      "Step 00334/00701 | Training loss: 0.377181| lrm: 0.523538| num_tokens: 12,100\n",
      "Step 00335/00701 | Training loss: 0.441823| lrm: 0.522111| num_tokens: 7,312\n",
      "Step 00336/00701 | Training loss: 1.149527| lrm: 0.520685| num_tokens: 12,905\n",
      "Step 00337/00701 | Training loss: 0.766595| lrm: 0.519258| num_tokens: 11,898\n",
      "Step 00338/00701 | Training loss: 0.378688| lrm: 0.517832| num_tokens: 10,202\n",
      "Step 00339/00701 | Training loss: 0.823031| lrm: 0.516405| num_tokens: 5,622\n",
      "Step 00340/00701 | Training loss: 0.775554| lrm: 0.514979| num_tokens: 11,148\n",
      "Step 00341/00701 | Training loss: 1.193097| lrm: 0.513552| num_tokens: 8,979\n",
      "Step 00342/00701 | Training loss: 0.706626| lrm: 0.512126| num_tokens: 6,240\n",
      "Step 00343/00701 | Training loss: 1.132090| lrm: 0.510699| num_tokens: 11,617\n",
      "Step 00344/00701 | Training loss: 0.562026| lrm: 0.509272| num_tokens: 7,701\n",
      "Step 00345/00701 | Training loss: 0.800108| lrm: 0.507846| num_tokens: 12,529\n",
      "Step 00346/00701 | Training loss: 0.888123| lrm: 0.506419| num_tokens: 10,846\n",
      "Step 00347/00701 | Training loss: 0.678042| lrm: 0.504993| num_tokens: 11,169\n",
      "Step 00348/00701 | Training loss: 1.321965| lrm: 0.503566| num_tokens: 9,517\n",
      "Step 00349/00701 | Training loss: 0.665583| lrm: 0.502140| num_tokens: 10,291\n",
      "Step 00350/00701 | Training loss: 0.850473| lrm: 0.500713| num_tokens: 9,018\n",
      "Step 00351/00701 | Training loss: 0.766092| lrm: 0.499287| num_tokens: 11,729\n",
      "Step 00352/00701 | Training loss: 1.034091| lrm: 0.497860| num_tokens: 12,675\n",
      "Step 00353/00701 | Training loss: 0.864504| lrm: 0.496434| num_tokens: 8,383\n",
      "Step 00354/00701 | Training loss: 0.754750| lrm: 0.495007| num_tokens: 10,059\n",
      "Step 00355/00701 | Training loss: 0.834856| lrm: 0.493581| num_tokens: 10,087\n",
      "Step 00356/00701 | Training loss: 0.658258| lrm: 0.492154| num_tokens: 8,333\n",
      "Step 00357/00701 | Training loss: 0.651293| lrm: 0.490728| num_tokens: 8,266\n",
      "Step 00358/00701 | Training loss: 0.680469| lrm: 0.489301| num_tokens: 9,973\n",
      "Step 00359/00701 | Training loss: 0.707708| lrm: 0.487874| num_tokens: 9,013\n",
      "Step 00360/00701 | Training loss: 0.534936| lrm: 0.486448| num_tokens: 8,617\n",
      "Step 00361/00701 | Training loss: 0.742337| lrm: 0.485021| num_tokens: 11,847\n",
      "Step 00362/00701 | Training loss: 1.221225| lrm: 0.483595| num_tokens: 11,199\n",
      "Step 00363/00701 | Training loss: 0.463817| lrm: 0.482168| num_tokens: 11,212\n",
      "Step 00364/00701 | Training loss: 0.675903| lrm: 0.480742| num_tokens: 13,298\n",
      "Step 00365/00701 | Training loss: 0.521799| lrm: 0.479315| num_tokens: 7,758\n",
      "Step 00366/00701 | Training loss: 0.951871| lrm: 0.477889| num_tokens: 9,322\n",
      "Step 00367/00701 | Training loss: 0.770158| lrm: 0.476462| num_tokens: 11,241\n",
      "Step 00368/00701 | Training loss: 1.135260| lrm: 0.475036| num_tokens: 13,963\n",
      "Step 00369/00701 | Training loss: 0.823812| lrm: 0.473609| num_tokens: 13,353\n",
      "Step 00370/00701 | Training loss: 0.745427| lrm: 0.472183| num_tokens: 10,639\n",
      "Step 00371/00701 | Training loss: 0.923966| lrm: 0.470756| num_tokens: 11,940\n",
      "Step 00372/00701 | Training loss: 0.933932| lrm: 0.469330| num_tokens: 11,410\n",
      "Step 00373/00701 | Training loss: 0.551063| lrm: 0.467903| num_tokens: 8,816\n",
      "Step 00374/00701 | Training loss: 0.853673| lrm: 0.466476| num_tokens: 13,098\n",
      "Step 00375/00701 | Training loss: 1.158594| lrm: 0.465050| num_tokens: 13,724\n",
      "Step 00376/00701 | Training loss: 1.183012| lrm: 0.463623| num_tokens: 9,460\n",
      "Step 00377/00701 | Training loss: 0.672329| lrm: 0.462197| num_tokens: 13,890\n",
      "Step 00378/00701 | Training loss: 0.666260| lrm: 0.460770| num_tokens: 11,856\n",
      "Step 00379/00701 | Training loss: 0.900738| lrm: 0.459344| num_tokens: 7,960\n",
      "Step 00380/00701 | Training loss: 1.287011| lrm: 0.457917| num_tokens: 8,519\n",
      "Step 00381/00701 | Training loss: 0.907605| lrm: 0.456491| num_tokens: 14,234\n",
      "Step 00382/00701 | Training loss: 0.644538| lrm: 0.455064| num_tokens: 13,353\n",
      "Step 00383/00701 | Training loss: 0.528495| lrm: 0.453638| num_tokens: 15,146\n",
      "Step 00384/00701 | Training loss: 0.642692| lrm: 0.452211| num_tokens: 12,589\n",
      "Step 00385/00701 | Training loss: 0.435140| lrm: 0.450785| num_tokens: 10,105\n",
      "Step 00386/00701 | Training loss: 1.022495| lrm: 0.449358| num_tokens: 11,158\n",
      "Step 00387/00701 | Training loss: 1.037451| lrm: 0.447932| num_tokens: 9,869\n",
      "Step 00388/00701 | Training loss: 0.915645| lrm: 0.446505| num_tokens: 11,292\n",
      "Step 00389/00701 | Training loss: 0.833902| lrm: 0.445078| num_tokens: 15,855\n",
      "Step 00390/00701 | Training loss: 0.505610| lrm: 0.443652| num_tokens: 12,768\n",
      "Step 00391/00701 | Training loss: 0.675780| lrm: 0.442225| num_tokens: 13,893\n",
      "Step 00392/00701 | Training loss: 0.355070| lrm: 0.440799| num_tokens: 10,495\n",
      "Step 00393/00701 | Training loss: 1.290185| lrm: 0.439372| num_tokens: 13,459\n",
      "Step 00394/00701 | Training loss: 0.992420| lrm: 0.437946| num_tokens: 13,515\n",
      "Step 00395/00701 | Training loss: 0.848433| lrm: 0.436519| num_tokens: 10,006\n",
      "Step 00396/00701 | Training loss: 0.962378| lrm: 0.435093| num_tokens: 11,009\n",
      "Step 00397/00701 | Training loss: 1.088284| lrm: 0.433666| num_tokens: 8,178\n",
      "Step 00398/00701 | Training loss: 0.998506| lrm: 0.432240| num_tokens: 8,070\n",
      "Step 00399/00701 | Training loss: 0.898848| lrm: 0.430813| num_tokens: 8,559\n",
      "Step 00400 | Validation loss: 1.013941\n",
      "final: 355/1024 (34.67%)\n",
      "final: 438/1024 (42.77%)\n",
      "Step 00400 | mmlu_acc: 0.346680, arc_easy_acc: 0.427734\n",
      "Step 00400/00701 | Training loss: 0.453563| lrm: 0.429387| num_tokens: 10,026\n",
      "Step 00401/00701 | Training loss: 0.742392| lrm: 0.427960| num_tokens: 6,689\n",
      "Step 00402/00701 | Training loss: 0.665463| lrm: 0.426534| num_tokens: 7,153\n",
      "Step 00403/00701 | Training loss: 1.253230| lrm: 0.425107| num_tokens: 15,333\n",
      "Step 00404/00701 | Training loss: 0.861322| lrm: 0.423680| num_tokens: 8,839\n",
      "Step 00405/00701 | Training loss: 0.694040| lrm: 0.422254| num_tokens: 10,865\n",
      "Step 00406/00701 | Training loss: 0.684957| lrm: 0.420827| num_tokens: 9,689\n",
      "Step 00407/00701 | Training loss: 0.557358| lrm: 0.419401| num_tokens: 6,294\n",
      "Step 00408/00701 | Training loss: 0.621281| lrm: 0.417974| num_tokens: 12,826\n",
      "Step 00409/00701 | Training loss: 0.598118| lrm: 0.416548| num_tokens: 8,557\n",
      "Step 00410/00701 | Training loss: 0.609293| lrm: 0.415121| num_tokens: 13,374\n",
      "Step 00411/00701 | Training loss: 0.885573| lrm: 0.413695| num_tokens: 12,917\n",
      "Step 00412/00701 | Training loss: 0.749543| lrm: 0.412268| num_tokens: 11,922\n",
      "Step 00413/00701 | Training loss: 0.855435| lrm: 0.410842| num_tokens: 7,315\n",
      "Step 00414/00701 | Training loss: 0.987598| lrm: 0.409415| num_tokens: 14,723\n",
      "Step 00415/00701 | Training loss: 0.747207| lrm: 0.407989| num_tokens: 13,804\n",
      "Step 00416/00701 | Training loss: 1.085801| lrm: 0.406562| num_tokens: 11,956\n",
      "Step 00417/00701 | Training loss: 0.659874| lrm: 0.405136| num_tokens: 12,018\n",
      "Step 00418/00701 | Training loss: 0.373618| lrm: 0.403709| num_tokens: 3,817\n",
      "Step 00419/00701 | Training loss: 0.973728| lrm: 0.402282| num_tokens: 9,950\n",
      "Step 00420/00701 | Training loss: 1.281490| lrm: 0.400856| num_tokens: 10,386\n",
      "Step 00421/00701 | Training loss: 0.826485| lrm: 0.399429| num_tokens: 12,796\n",
      "Step 00422/00701 | Training loss: 0.635116| lrm: 0.398003| num_tokens: 13,674\n",
      "Step 00423/00701 | Training loss: 0.580636| lrm: 0.396576| num_tokens: 15,302\n",
      "Step 00424/00701 | Training loss: 0.716417| lrm: 0.395150| num_tokens: 14,939\n",
      "Step 00425/00701 | Training loss: 0.855655| lrm: 0.393723| num_tokens: 7,669\n",
      "Step 00426/00701 | Training loss: 0.916377| lrm: 0.392297| num_tokens: 6,524\n",
      "Step 00427/00701 | Training loss: 0.860258| lrm: 0.390870| num_tokens: 8,753\n",
      "Step 00428/00701 | Training loss: 0.787255| lrm: 0.389444| num_tokens: 9,846\n",
      "Step 00429/00701 | Training loss: 0.685741| lrm: 0.388017| num_tokens: 10,688\n",
      "Step 00430/00701 | Training loss: 0.807024| lrm: 0.386591| num_tokens: 11,206\n",
      "Step 00431/00701 | Training loss: 1.605035| lrm: 0.385164| num_tokens: 8,919\n",
      "Step 00432/00701 | Training loss: 1.025726| lrm: 0.383738| num_tokens: 13,388\n",
      "Step 00433/00701 | Training loss: 0.595866| lrm: 0.382311| num_tokens: 10,474\n",
      "Step 00434/00701 | Training loss: 0.863688| lrm: 0.380884| num_tokens: 11,897\n",
      "Step 00435/00701 | Training loss: 0.673768| lrm: 0.379458| num_tokens: 12,165\n",
      "Step 00436/00701 | Training loss: 0.787227| lrm: 0.378031| num_tokens: 13,113\n",
      "Step 00437/00701 | Training loss: 0.946339| lrm: 0.376605| num_tokens: 12,838\n",
      "Step 00438/00701 | Training loss: 1.265500| lrm: 0.375178| num_tokens: 10,430\n",
      "Step 00439/00701 | Training loss: 0.819656| lrm: 0.373752| num_tokens: 11,832\n",
      "Step 00440/00701 | Training loss: 0.651044| lrm: 0.372325| num_tokens: 10,469\n",
      "Step 00441/00701 | Training loss: 0.781109| lrm: 0.370899| num_tokens: 11,808\n",
      "Step 00442/00701 | Training loss: 0.821147| lrm: 0.369472| num_tokens: 12,062\n",
      "Step 00443/00701 | Training loss: 0.765913| lrm: 0.368046| num_tokens: 10,750\n",
      "Step 00444/00701 | Training loss: 0.689358| lrm: 0.366619| num_tokens: 13,119\n",
      "Step 00445/00701 | Training loss: 0.799322| lrm: 0.365193| num_tokens: 9,910\n",
      "Step 00446/00701 | Training loss: 0.514687| lrm: 0.363766| num_tokens: 13,290\n",
      "Step 00447/00701 | Training loss: 1.071623| lrm: 0.362340| num_tokens: 15,029\n",
      "Step 00448/00701 | Training loss: 0.923019| lrm: 0.360913| num_tokens: 10,547\n",
      "Step 00449/00701 | Training loss: 0.823688| lrm: 0.359486| num_tokens: 8,861\n",
      "Step 00450/00701 | Training loss: 1.065481| lrm: 0.358060| num_tokens: 15,847\n",
      "Step 00451/00701 | Training loss: 0.881972| lrm: 0.356633| num_tokens: 10,672\n",
      "Step 00452/00701 | Training loss: 0.808650| lrm: 0.355207| num_tokens: 9,304\n",
      "Step 00453/00701 | Training loss: 0.774376| lrm: 0.353780| num_tokens: 12,538\n",
      "Step 00454/00701 | Training loss: 0.932954| lrm: 0.352354| num_tokens: 7,953\n",
      "Step 00455/00701 | Training loss: 0.886847| lrm: 0.350927| num_tokens: 7,755\n",
      "Step 00456/00701 | Training loss: 1.001098| lrm: 0.349501| num_tokens: 10,549\n",
      "Step 00457/00701 | Training loss: 0.605434| lrm: 0.348074| num_tokens: 10,105\n",
      "Step 00458/00701 | Training loss: 0.837433| lrm: 0.346648| num_tokens: 8,714\n",
      "Step 00459/00701 | Training loss: 0.750947| lrm: 0.345221| num_tokens: 12,122\n",
      "Step 00460/00701 | Training loss: 0.749145| lrm: 0.343795| num_tokens: 8,420\n",
      "Step 00461/00701 | Training loss: 0.808915| lrm: 0.342368| num_tokens: 12,615\n",
      "Step 00462/00701 | Training loss: 0.566443| lrm: 0.340942| num_tokens: 9,887\n",
      "Step 00463/00701 | Training loss: 1.127830| lrm: 0.339515| num_tokens: 7,312\n",
      "Step 00464/00701 | Training loss: 0.689914| lrm: 0.338088| num_tokens: 9,797\n",
      "Step 00465/00701 | Training loss: 1.062877| lrm: 0.336662| num_tokens: 8,338\n",
      "Step 00466/00701 | Training loss: 0.932549| lrm: 0.335235| num_tokens: 11,435\n",
      "Step 00467/00701 | Training loss: 0.654526| lrm: 0.333809| num_tokens: 10,835\n",
      "Step 00468/00701 | Training loss: 0.751773| lrm: 0.332382| num_tokens: 9,671\n",
      "Step 00469/00701 | Training loss: 0.443320| lrm: 0.330956| num_tokens: 8,338\n",
      "Step 00470/00701 | Training loss: 1.077650| lrm: 0.329529| num_tokens: 8,700\n",
      "Step 00471/00701 | Training loss: 0.941662| lrm: 0.328103| num_tokens: 15,219\n",
      "Step 00472/00701 | Training loss: 1.889326| lrm: 0.326676| num_tokens: 13,448\n",
      "Step 00473/00701 | Training loss: 0.658303| lrm: 0.325250| num_tokens: 9,585\n",
      "Step 00474/00701 | Training loss: 1.146401| lrm: 0.323823| num_tokens: 14,725\n",
      "Step 00475/00701 | Training loss: 0.923183| lrm: 0.322397| num_tokens: 12,688\n",
      "Step 00476/00701 | Training loss: 1.140885| lrm: 0.320970| num_tokens: 11,268\n",
      "Step 00477/00701 | Training loss: 1.007560| lrm: 0.319544| num_tokens: 13,478\n",
      "Step 00478/00701 | Training loss: 1.318058| lrm: 0.318117| num_tokens: 6,838\n",
      "Step 00479/00701 | Training loss: 0.757167| lrm: 0.316690| num_tokens: 13,516\n",
      "Step 00480/00701 | Training loss: 0.404027| lrm: 0.315264| num_tokens: 13,587\n",
      "Step 00481/00701 | Training loss: 1.082916| lrm: 0.313837| num_tokens: 7,015\n",
      "Step 00482/00701 | Training loss: 0.983449| lrm: 0.312411| num_tokens: 9,863\n",
      "Step 00483/00701 | Training loss: 0.689722| lrm: 0.310984| num_tokens: 11,048\n",
      "Step 00484/00701 | Training loss: 0.496204| lrm: 0.309558| num_tokens: 10,661\n",
      "Step 00485/00701 | Training loss: 1.126785| lrm: 0.308131| num_tokens: 10,617\n",
      "Step 00486/00701 | Training loss: 0.392832| lrm: 0.306705| num_tokens: 9,656\n",
      "Step 00487/00701 | Training loss: 0.664841| lrm: 0.305278| num_tokens: 8,590\n",
      "Step 00488/00701 | Training loss: 0.439968| lrm: 0.303852| num_tokens: 11,260\n",
      "Step 00489/00701 | Training loss: 0.947431| lrm: 0.302425| num_tokens: 12,388\n",
      "Step 00490/00701 | Training loss: 0.893356| lrm: 0.300999| num_tokens: 8,471\n",
      "Step 00491/00701 | Training loss: 0.927479| lrm: 0.299572| num_tokens: 8,826\n",
      "Step 00492/00701 | Training loss: 1.446101| lrm: 0.298146| num_tokens: 14,321\n",
      "Step 00493/00701 | Training loss: 0.573035| lrm: 0.296719| num_tokens: 7,284\n",
      "Step 00494/00701 | Training loss: 0.541175| lrm: 0.295292| num_tokens: 8,592\n",
      "Step 00495/00701 | Training loss: 0.869115| lrm: 0.293866| num_tokens: 10,854\n",
      "Step 00496/00701 | Training loss: 0.760478| lrm: 0.292439| num_tokens: 10,975\n",
      "Step 00497/00701 | Training loss: 0.724175| lrm: 0.291013| num_tokens: 11,925\n",
      "Step 00498/00701 | Training loss: 0.977134| lrm: 0.289586| num_tokens: 11,744\n",
      "Step 00499/00701 | Training loss: 0.295009| lrm: 0.288160| num_tokens: 14,263\n",
      "Step 00500 | Validation loss: 1.013912\n",
      "Step 00500/00701 | Training loss: 0.895489| lrm: 0.286733| num_tokens: 13,673\n",
      "Step 00501/00701 | Training loss: 0.630194| lrm: 0.285307| num_tokens: 9,954\n",
      "Step 00502/00701 | Training loss: 0.440492| lrm: 0.283880| num_tokens: 9,783\n",
      "Step 00503/00701 | Training loss: 1.389543| lrm: 0.282454| num_tokens: 13,075\n",
      "Step 00504/00701 | Training loss: 1.556924| lrm: 0.281027| num_tokens: 9,767\n",
      "Step 00505/00701 | Training loss: 0.832883| lrm: 0.279601| num_tokens: 8,895\n",
      "Step 00506/00701 | Training loss: 1.359255| lrm: 0.278174| num_tokens: 9,652\n",
      "Step 00507/00701 | Training loss: 0.593322| lrm: 0.276748| num_tokens: 6,756\n",
      "Step 00508/00701 | Training loss: 1.329193| lrm: 0.275321| num_tokens: 18,637\n",
      "Step 00509/00701 | Training loss: 0.861352| lrm: 0.273894| num_tokens: 9,678\n",
      "Step 00510/00701 | Training loss: 0.767312| lrm: 0.272468| num_tokens: 9,942\n",
      "Step 00511/00701 | Training loss: 0.578435| lrm: 0.271041| num_tokens: 5,586\n",
      "Step 00512/00701 | Training loss: 1.012558| lrm: 0.269615| num_tokens: 11,421\n",
      "Step 00513/00701 | Training loss: 0.790981| lrm: 0.268188| num_tokens: 8,743\n",
      "Step 00514/00701 | Training loss: 0.856970| lrm: 0.266762| num_tokens: 9,904\n",
      "Step 00515/00701 | Training loss: 1.180873| lrm: 0.265335| num_tokens: 10,734\n",
      "Step 00516/00701 | Training loss: 1.216924| lrm: 0.263909| num_tokens: 12,039\n",
      "Step 00517/00701 | Training loss: 0.549429| lrm: 0.262482| num_tokens: 10,739\n",
      "Step 00518/00701 | Training loss: 1.133999| lrm: 0.261056| num_tokens: 10,288\n",
      "Step 00519/00701 | Training loss: 0.852082| lrm: 0.259629| num_tokens: 8,447\n",
      "Step 00520/00701 | Training loss: 0.865611| lrm: 0.258203| num_tokens: 9,737\n",
      "Step 00521/00701 | Training loss: 0.971450| lrm: 0.256776| num_tokens: 14,760\n",
      "Step 00522/00701 | Training loss: 1.078223| lrm: 0.255350| num_tokens: 9,452\n",
      "Step 00523/00701 | Training loss: 0.654106| lrm: 0.253923| num_tokens: 11,752\n",
      "Step 00524/00701 | Training loss: 0.975291| lrm: 0.252496| num_tokens: 7,305\n",
      "Step 00525/00701 | Training loss: 1.535535| lrm: 0.251070| num_tokens: 10,430\n",
      "Step 00526/00701 | Training loss: 1.126095| lrm: 0.249643| num_tokens: 9,485\n",
      "Step 00527/00701 | Training loss: 1.166955| lrm: 0.248217| num_tokens: 4,665\n",
      "Step 00528/00701 | Training loss: 1.189271| lrm: 0.246790| num_tokens: 14,142\n",
      "Step 00529/00701 | Training loss: 1.139149| lrm: 0.245364| num_tokens: 14,963\n",
      "Step 00530/00701 | Training loss: 0.463851| lrm: 0.243937| num_tokens: 8,920\n",
      "Step 00531/00701 | Training loss: 0.613928| lrm: 0.242511| num_tokens: 10,497\n",
      "Step 00532/00701 | Training loss: 1.154403| lrm: 0.241084| num_tokens: 8,047\n",
      "Step 00533/00701 | Training loss: 1.048604| lrm: 0.239658| num_tokens: 11,665\n",
      "Step 00534/00701 | Training loss: 1.162921| lrm: 0.238231| num_tokens: 12,661\n",
      "Step 00535/00701 | Training loss: 0.644782| lrm: 0.236805| num_tokens: 9,332\n",
      "Step 00536/00701 | Training loss: 0.509655| lrm: 0.235378| num_tokens: 10,270\n",
      "Step 00537/00701 | Training loss: 0.696151| lrm: 0.233951| num_tokens: 6,451\n",
      "Step 00538/00701 | Training loss: 0.648227| lrm: 0.232525| num_tokens: 10,264\n",
      "Step 00539/00701 | Training loss: 0.809032| lrm: 0.231098| num_tokens: 13,034\n",
      "Step 00540/00701 | Training loss: 0.812608| lrm: 0.229672| num_tokens: 7,601\n",
      "Step 00541/00701 | Training loss: 1.028514| lrm: 0.228245| num_tokens: 15,932\n",
      "Step 00542/00701 | Training loss: 1.242935| lrm: 0.226819| num_tokens: 10,867\n",
      "Step 00543/00701 | Training loss: 0.367856| lrm: 0.225392| num_tokens: 11,887\n",
      "Step 00544/00701 | Training loss: 0.968108| lrm: 0.223966| num_tokens: 11,865\n",
      "Step 00545/00701 | Training loss: 0.764739| lrm: 0.222539| num_tokens: 13,438\n",
      "Step 00546/00701 | Training loss: 0.794188| lrm: 0.221113| num_tokens: 13,844\n",
      "Step 00547/00701 | Training loss: 1.263538| lrm: 0.219686| num_tokens: 6,023\n",
      "Step 00548/00701 | Training loss: 0.743766| lrm: 0.218260| num_tokens: 13,321\n",
      "Step 00549/00701 | Training loss: 1.073016| lrm: 0.216833| num_tokens: 8,358\n",
      "Step 00550/00701 | Training loss: 0.634866| lrm: 0.215407| num_tokens: 13,487\n",
      "Step 00551/00701 | Training loss: 0.763742| lrm: 0.213980| num_tokens: 11,180\n",
      "Step 00552/00701 | Training loss: 0.772783| lrm: 0.212553| num_tokens: 8,538\n",
      "Step 00553/00701 | Training loss: 0.257431| lrm: 0.211127| num_tokens: 10,572\n",
      "Step 00554/00701 | Training loss: 0.976420| lrm: 0.209700| num_tokens: 12,982\n",
      "Step 00555/00701 | Training loss: 0.892625| lrm: 0.208274| num_tokens: 11,588\n",
      "Step 00556/00701 | Training loss: 0.912641| lrm: 0.206847| num_tokens: 7,432\n",
      "Step 00557/00701 | Training loss: 0.652285| lrm: 0.205421| num_tokens: 9,354\n",
      "Step 00558/00701 | Training loss: 0.880655| lrm: 0.203994| num_tokens: 10,187\n",
      "Step 00559/00701 | Training loss: 1.005787| lrm: 0.202568| num_tokens: 8,765\n",
      "Step 00560/00701 | Training loss: 0.508698| lrm: 0.201141| num_tokens: 13,597\n",
      "Step 00561/00701 | Training loss: 1.178430| lrm: 0.199715| num_tokens: 6,689\n",
      "Step 00562/00701 | Training loss: 0.243654| lrm: 0.198288| num_tokens: 8,682\n",
      "Step 00563/00701 | Training loss: 0.941755| lrm: 0.196862| num_tokens: 12,881\n",
      "Step 00564/00701 | Training loss: 0.943709| lrm: 0.195435| num_tokens: 10,886\n",
      "Step 00565/00701 | Training loss: 1.069509| lrm: 0.194009| num_tokens: 13,874\n",
      "Step 00566/00701 | Training loss: 0.997312| lrm: 0.192582| num_tokens: 12,213\n",
      "Step 00567/00701 | Training loss: 0.921118| lrm: 0.191155| num_tokens: 18,532\n",
      "Step 00568/00701 | Training loss: 0.701828| lrm: 0.189729| num_tokens: 10,003\n",
      "Step 00569/00701 | Training loss: 0.889213| lrm: 0.188302| num_tokens: 9,476\n",
      "Step 00570/00701 | Training loss: 1.258298| lrm: 0.186876| num_tokens: 9,070\n",
      "Step 00571/00701 | Training loss: 0.723056| lrm: 0.185449| num_tokens: 13,088\n",
      "Step 00572/00701 | Training loss: 0.689796| lrm: 0.184023| num_tokens: 7,260\n",
      "Step 00573/00701 | Training loss: 1.087021| lrm: 0.182596| num_tokens: 10,391\n",
      "Step 00574/00701 | Training loss: 0.745345| lrm: 0.181170| num_tokens: 14,540\n",
      "Step 00575/00701 | Training loss: 0.423977| lrm: 0.179743| num_tokens: 7,583\n",
      "Step 00576/00701 | Training loss: 1.750755| lrm: 0.178317| num_tokens: 8,834\n",
      "Step 00577/00701 | Training loss: 0.722724| lrm: 0.176890| num_tokens: 13,422\n",
      "Step 00578/00701 | Training loss: 0.577875| lrm: 0.175464| num_tokens: 8,190\n",
      "Step 00579/00701 | Training loss: 0.487347| lrm: 0.174037| num_tokens: 10,220\n",
      "Step 00580/00701 | Training loss: 0.996061| lrm: 0.172611| num_tokens: 9,082\n",
      "Step 00581/00701 | Training loss: 1.045199| lrm: 0.171184| num_tokens: 13,484\n",
      "Step 00582/00701 | Training loss: 1.098183| lrm: 0.169757| num_tokens: 8,858\n",
      "Step 00583/00701 | Training loss: 0.595205| lrm: 0.168331| num_tokens: 12,740\n",
      "Step 00584/00701 | Training loss: 0.824790| lrm: 0.166904| num_tokens: 13,825\n",
      "Step 00585/00701 | Training loss: 0.676578| lrm: 0.165478| num_tokens: 14,923\n",
      "Step 00586/00701 | Training loss: 0.978156| lrm: 0.164051| num_tokens: 9,923\n",
      "Step 00587/00701 | Training loss: 0.765922| lrm: 0.162625| num_tokens: 7,792\n",
      "Step 00588/00701 | Training loss: 0.930201| lrm: 0.161198| num_tokens: 12,331\n",
      "Step 00589/00701 | Training loss: 0.472269| lrm: 0.159772| num_tokens: 9,040\n",
      "Step 00590/00701 | Training loss: 0.545802| lrm: 0.158345| num_tokens: 10,334\n",
      "Step 00591/00701 | Training loss: 0.758468| lrm: 0.156919| num_tokens: 8,577\n",
      "Step 00592/00701 | Training loss: 0.980801| lrm: 0.155492| num_tokens: 11,743\n",
      "Step 00593/00701 | Training loss: 1.069817| lrm: 0.154066| num_tokens: 11,795\n",
      "Step 00594/00701 | Training loss: 0.250703| lrm: 0.152639| num_tokens: 11,224\n",
      "Step 00595/00701 | Training loss: 0.465097| lrm: 0.151213| num_tokens: 8,408\n",
      "Step 00596/00701 | Training loss: 0.743640| lrm: 0.149786| num_tokens: 5,658\n",
      "Step 00597/00701 | Training loss: 1.319544| lrm: 0.148359| num_tokens: 10,836\n",
      "Step 00598/00701 | Training loss: 0.747209| lrm: 0.146933| num_tokens: 9,759\n",
      "Step 00599/00701 | Training loss: 1.429570| lrm: 0.145506| num_tokens: 8,267\n",
      "Step 00600 | Validation loss: 1.013701\n",
      "final: 363/1024 (35.45%)\n",
      "final: 469/1024 (45.80%)\n",
      "Step 00600 | mmlu_acc: 0.354492, arc_easy_acc: 0.458008\n",
      "Step 00600/00701 | Training loss: 0.722429| lrm: 0.144080| num_tokens: 10,446\n",
      "Step 00601/00701 | Training loss: 1.064526| lrm: 0.142653| num_tokens: 11,003\n",
      "Step 00602/00701 | Training loss: 0.785262| lrm: 0.141227| num_tokens: 8,234\n",
      "Step 00603/00701 | Training loss: 1.098925| lrm: 0.139800| num_tokens: 7,979\n",
      "Step 00604/00701 | Training loss: 0.966749| lrm: 0.138374| num_tokens: 13,517\n",
      "Step 00605/00701 | Training loss: 0.760471| lrm: 0.136947| num_tokens: 10,453\n",
      "Step 00606/00701 | Training loss: 0.926910| lrm: 0.135521| num_tokens: 7,191\n",
      "Step 00607/00701 | Training loss: 0.645994| lrm: 0.134094| num_tokens: 14,886\n",
      "Step 00608/00701 | Training loss: 0.612790| lrm: 0.132668| num_tokens: 12,833\n",
      "Step 00609/00701 | Training loss: 1.125689| lrm: 0.131241| num_tokens: 8,921\n",
      "Step 00610/00701 | Training loss: 0.497613| lrm: 0.129815| num_tokens: 14,179\n",
      "Step 00611/00701 | Training loss: 0.971267| lrm: 0.128388| num_tokens: 9,321\n",
      "Step 00612/00701 | Training loss: 0.846595| lrm: 0.126961| num_tokens: 9,523\n",
      "Step 00613/00701 | Training loss: 1.080191| lrm: 0.125535| num_tokens: 16,542\n",
      "Step 00614/00701 | Training loss: 0.548733| lrm: 0.124108| num_tokens: 15,348\n",
      "Step 00615/00701 | Training loss: 0.922564| lrm: 0.122682| num_tokens: 11,945\n",
      "Step 00616/00701 | Training loss: 0.622988| lrm: 0.121255| num_tokens: 8,214\n",
      "Step 00617/00701 | Training loss: 0.719654| lrm: 0.119829| num_tokens: 11,180\n",
      "Step 00618/00701 | Training loss: 0.566374| lrm: 0.118402| num_tokens: 8,842\n",
      "Step 00619/00701 | Training loss: 0.705625| lrm: 0.116976| num_tokens: 9,213\n",
      "Step 00620/00701 | Training loss: 0.641668| lrm: 0.115549| num_tokens: 12,340\n",
      "Step 00621/00701 | Training loss: 0.487395| lrm: 0.114123| num_tokens: 7,718\n",
      "Step 00622/00701 | Training loss: 0.964125| lrm: 0.112696| num_tokens: 7,350\n",
      "Step 00623/00701 | Training loss: 0.904731| lrm: 0.111270| num_tokens: 14,519\n",
      "Step 00624/00701 | Training loss: 1.414002| lrm: 0.109843| num_tokens: 12,368\n",
      "Step 00625/00701 | Training loss: 0.578768| lrm: 0.108417| num_tokens: 6,842\n",
      "Step 00626/00701 | Training loss: 0.732640| lrm: 0.106990| num_tokens: 11,577\n",
      "Step 00627/00701 | Training loss: 0.486015| lrm: 0.105563| num_tokens: 8,498\n",
      "Step 00628/00701 | Training loss: 0.767236| lrm: 0.104137| num_tokens: 6,965\n",
      "Step 00629/00701 | Training loss: 0.606806| lrm: 0.102710| num_tokens: 12,391\n",
      "Step 00630/00701 | Training loss: 0.561769| lrm: 0.101284| num_tokens: 10,252\n",
      "Step 00631/00701 | Training loss: 0.768829| lrm: 0.099857| num_tokens: 12,194\n",
      "Step 00632/00701 | Training loss: 0.643988| lrm: 0.098431| num_tokens: 10,964\n",
      "Step 00633/00701 | Training loss: 0.911586| lrm: 0.097004| num_tokens: 11,198\n",
      "Step 00634/00701 | Training loss: 1.005099| lrm: 0.095578| num_tokens: 7,808\n",
      "Step 00635/00701 | Training loss: 0.766220| lrm: 0.094151| num_tokens: 12,053\n",
      "Step 00636/00701 | Training loss: 0.799233| lrm: 0.092725| num_tokens: 8,138\n",
      "Step 00637/00701 | Training loss: 0.860930| lrm: 0.091298| num_tokens: 13,101\n",
      "Step 00638/00701 | Training loss: 0.562876| lrm: 0.089872| num_tokens: 8,339\n",
      "Step 00639/00701 | Training loss: 0.688190| lrm: 0.088445| num_tokens: 13,057\n",
      "Step 00640/00701 | Training loss: 0.970202| lrm: 0.087019| num_tokens: 12,703\n",
      "Step 00641/00701 | Training loss: 0.911709| lrm: 0.085592| num_tokens: 10,133\n",
      "Step 00642/00701 | Training loss: 0.347388| lrm: 0.084165| num_tokens: 11,357\n",
      "Step 00643/00701 | Training loss: 0.889022| lrm: 0.082739| num_tokens: 8,878\n",
      "Step 00644/00701 | Training loss: 0.839583| lrm: 0.081312| num_tokens: 8,861\n",
      "Step 00645/00701 | Training loss: 1.237816| lrm: 0.079886| num_tokens: 13,446\n",
      "Step 00646/00701 | Training loss: 0.756004| lrm: 0.078459| num_tokens: 12,789\n",
      "Step 00647/00701 | Training loss: 1.062626| lrm: 0.077033| num_tokens: 8,241\n",
      "Step 00648/00701 | Training loss: 0.857680| lrm: 0.075606| num_tokens: 10,330\n",
      "Step 00649/00701 | Training loss: 0.663345| lrm: 0.074180| num_tokens: 12,278\n",
      "Step 00650/00701 | Training loss: 1.331270| lrm: 0.072753| num_tokens: 7,245\n",
      "Step 00651/00701 | Training loss: 1.298990| lrm: 0.071327| num_tokens: 13,989\n",
      "Step 00652/00701 | Training loss: 1.143754| lrm: 0.069900| num_tokens: 11,516\n",
      "Step 00653/00701 | Training loss: 0.645154| lrm: 0.068474| num_tokens: 8,604\n",
      "Step 00654/00701 | Training loss: 0.796365| lrm: 0.067047| num_tokens: 8,702\n",
      "Step 00655/00701 | Training loss: 0.891467| lrm: 0.065621| num_tokens: 11,510\n",
      "Step 00656/00701 | Training loss: 1.253077| lrm: 0.064194| num_tokens: 11,144\n",
      "Step 00657/00701 | Training loss: 0.389313| lrm: 0.062767| num_tokens: 10,336\n",
      "Step 00658/00701 | Training loss: 1.090492| lrm: 0.061341| num_tokens: 9,779\n",
      "Step 00659/00701 | Training loss: 0.744262| lrm: 0.059914| num_tokens: 8,105\n",
      "Step 00660/00701 | Training loss: 0.795165| lrm: 0.058488| num_tokens: 10,476\n",
      "Step 00661/00701 | Training loss: 1.117080| lrm: 0.057061| num_tokens: 11,310\n",
      "Step 00662/00701 | Training loss: 0.641247| lrm: 0.055635| num_tokens: 7,871\n",
      "Step 00663/00701 | Training loss: 0.708129| lrm: 0.054208| num_tokens: 9,855\n",
      "Step 00664/00701 | Training loss: 0.668467| lrm: 0.052782| num_tokens: 9,648\n",
      "Step 00665/00701 | Training loss: 1.656411| lrm: 0.051355| num_tokens: 9,697\n",
      "Step 00666/00701 | Training loss: 1.037257| lrm: 0.049929| num_tokens: 10,710\n",
      "Step 00667/00701 | Training loss: 0.848064| lrm: 0.048502| num_tokens: 13,511\n",
      "Step 00668/00701 | Training loss: 0.642115| lrm: 0.047076| num_tokens: 11,917\n",
      "Step 00669/00701 | Training loss: 1.120740| lrm: 0.045649| num_tokens: 10,199\n",
      "Step 00670/00701 | Training loss: 0.755740| lrm: 0.044223| num_tokens: 9,513\n",
      "Step 00671/00701 | Training loss: 0.788346| lrm: 0.042796| num_tokens: 10,895\n",
      "Step 00672/00701 | Training loss: 0.653566| lrm: 0.041369| num_tokens: 10,537\n",
      "Step 00673/00701 | Training loss: 0.693878| lrm: 0.039943| num_tokens: 9,859\n",
      "Step 00674/00701 | Training loss: 0.622678| lrm: 0.038516| num_tokens: 10,435\n",
      "Step 00675/00701 | Training loss: 0.611801| lrm: 0.037090| num_tokens: 10,400\n",
      "Step 00676/00701 | Training loss: 0.857989| lrm: 0.035663| num_tokens: 10,101\n",
      "Step 00677/00701 | Training loss: 1.020631| lrm: 0.034237| num_tokens: 16,334\n",
      "Step 00678/00701 | Training loss: 0.339401| lrm: 0.032810| num_tokens: 12,035\n",
      "Step 00679/00701 | Training loss: 0.790428| lrm: 0.031384| num_tokens: 8,362\n",
      "Step 00680/00701 | Training loss: 1.205950| lrm: 0.029957| num_tokens: 12,050\n",
      "Step 00681/00701 | Training loss: 0.678960| lrm: 0.028531| num_tokens: 9,425\n",
      "Step 00682/00701 | Training loss: 0.720210| lrm: 0.027104| num_tokens: 14,504\n",
      "Step 00683/00701 | Training loss: 0.925645| lrm: 0.025678| num_tokens: 9,745\n",
      "Step 00684/00701 | Training loss: 1.595933| lrm: 0.024251| num_tokens: 12,770\n",
      "Step 00685/00701 | Training loss: 0.592232| lrm: 0.022825| num_tokens: 9,268\n",
      "Step 00686/00701 | Training loss: 0.530214| lrm: 0.021398| num_tokens: 10,168\n",
      "Step 00687/00701 | Training loss: 0.782744| lrm: 0.019971| num_tokens: 13,350\n",
      "Step 00688/00701 | Training loss: 0.705351| lrm: 0.018545| num_tokens: 10,458\n",
      "Step 00689/00701 | Training loss: 0.505679| lrm: 0.017118| num_tokens: 10,652\n",
      "Step 00690/00701 | Training loss: 0.424970| lrm: 0.015692| num_tokens: 8,627\n",
      "Step 00691/00701 | Training loss: 0.545320| lrm: 0.014265| num_tokens: 10,642\n",
      "Step 00692/00701 | Training loss: 0.909035| lrm: 0.012839| num_tokens: 11,930\n",
      "Step 00693/00701 | Training loss: 1.139468| lrm: 0.011412| num_tokens: 9,686\n",
      "Step 00694/00701 | Training loss: 1.218256| lrm: 0.009986| num_tokens: 10,109\n",
      "Step 00695/00701 | Training loss: 1.039710| lrm: 0.008559| num_tokens: 8,528\n",
      "Step 00696/00701 | Training loss: 0.528845| lrm: 0.007133| num_tokens: 12,544\n",
      "Step 00697/00701 | Training loss: 1.214723| lrm: 0.005706| num_tokens: 10,314\n",
      "Step 00698/00701 | Training loss: 0.545528| lrm: 0.004280| num_tokens: 8,859\n",
      "Step 00699/00701 | Training loss: 0.530193| lrm: 0.002853| num_tokens: 11,147\n",
      "Step 00700 | Validation loss: 1.013710\n",
      "final: 360/1024 (35.16%)\n",
      "final: 461/1024 (45.02%)\n",
      "Step 00700 | mmlu_acc: 0.351562, arc_easy_acc: 0.450195\n",
      "[W1121 20:54:18.609631541 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:18.610378596 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:18.631047251 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:18.647408771 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:18.016202291 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/model_000700.pt\n",
      "saved metadata to /home/ubuntu/mynanochat/chatsft_checkpoints/d20/meta_000700.json\n",
      "Saved model checkpoint to /home/ubuntu/mynanochat/chatsft_checkpoints/d20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 711-711, summary, console lines 727-732 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc â–â–ƒâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc â–â–†â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens â–‡â–†â–…â–ƒâ–„â–†â–ˆâ–…â–‡â–ˆâ–…â–…â–†â–‡â–ƒâ–…â–â–…â–†â–…â–ƒâ–‚â–„â–‡â–‡â–‡â–„â–…â–†â–„â–„â–â–‚â–„â–ƒâ–†â–…â–ƒâ–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss â–…â–…â–ƒâ–†â–ƒâ–„â–ƒâ–ƒâ–†â–…â–†â–ƒâ–†â–†â–ƒâ–‚â–‚â–†â–‚â–…â–†â–â–ˆâ–‡â–‚â–„â–â–„â–„â–„â–‚â–„â–ƒâ–ƒâ–‡â–â–„â–†â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–ˆâ–†â–†â–†â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_easy_acc 0.4502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lrm 0.00285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     mmlu_acc 0.35156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   num_tokens 11147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss 0.53019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss 1.01371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-32-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft/runs/y3aalfao\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_205212-y3aalfao/logs\u001b[0m\n",
      "[W1121 20:54:22.157985251 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:23.760718577 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1121 20:54:23.834550163 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_sft -- \\\n",
    "--model_tag=d20 --run=challenge-32-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c54a53-3f86-420d-afb8-a96f1b41d019",
   "metadata": {},
   "source": [
    "Now repeat chat eval for ARC-Easy, looking for 44.57% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1264e39d-67e0-43b1-927a-45ab7827671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1121 20:54:45.271000 42973 torch/distributed/run.py:803] \n",
      "W1121 20:54:45.271000 42973 torch/distributed/run.py:803] *****************************************\n",
      "W1121 20:54:45.271000 42973 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1121 20:54:45.271000 42973 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1054/2376 (44.36%)\n",
      "ARC-Easy accuracy: 44.36%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- \\\n",
    "--source=sft --task-name=ARC-Easy --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ece82-c956-4d2a-81e1-8e4cc534cb81",
   "metadata": {},
   "source": [
    "No. So the training is not deterministic? Also can see that final val loss is not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473111f-971f-490b-afb5-2c5522f20ce6",
   "metadata": {},
   "source": [
    "Let me get off of the GPU machine and think about this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01deed1-ef4c-4139-ada4-5fad437c586d",
   "metadata": {},
   "source": [
    "### Back on mac\n",
    "\n",
    "Well, that didn't go as planned!\n",
    "\n",
    "Was on the machine for 1.51 hours at a cost of $36.13\n",
    "\n",
    "Lots of stuff to look into now. Will do that in `investigate-runs.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054aeef-4116-4f4e-babd-3dce7f6746b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
