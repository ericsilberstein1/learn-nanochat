{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139add61-2875-45d4-a3e6-b98c84963c58",
   "metadata": {},
   "source": [
    "### add flop/mfu stuff\n",
    "\n",
    "I never added the flops / MFU stuff. Add that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cda4663-ed09-4ad1-a133-63cc527b7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_checkpoint_manager import load_model\n",
    "from my_nanochat.my_common import compute_init, autodetect_device_type, get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01190051-54fa-4098-86ad-ee4b2dfd18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type() \n",
    "_, _, _, _, device = compute_init(device_type)\n",
    "model, tokenizer, meta_data = load_model('base', model_tag='d20', device=device, phase='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0452957-c82f-45dd-a321-94c7f73430a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,491,758,080'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{model.estimate_flops():,d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c172234c-d9f3-4c0d-88e2-2eaf523f66a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4 with step 500\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65536, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, meta_data = load_model('base', model_tag='d4', device=device, phase='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd27899c-fc19-4232-8d4e-fc058716ab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'121,110,528'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{model.estimate_flops():,d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74df0ded-0c18-410b-a84d-fb2f12f93052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdiff --git a/my_nanochat/scripts/my_base_train.py b/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[1mindex 38c5bc6..fd5c966 100644\u001b[m\n",
      "\u001b[1m--- a/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[1m+++ b/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[36m@@ -121,8 +121,8 @@\u001b[m \u001b[morig_model = model\u001b[m\n",
      " model = torch.compile(model, dynamic=False)\u001b[m\n",
      " num_params = sum(p.numel() for p in model.parameters())\u001b[m\n",
      " print0(f\"Number of parameters: {num_params:,}\")\u001b[m\n",
      "\u001b[31m-# TODO num_flops_per_token = model.estimate_flops()\u001b[m\n",
      "\u001b[31m-# print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mnum_flops_per_token = model.estimate_flops()\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\u001b[m\n",
      " \u001b[m\n",
      " if num_iterations > 0:\u001b[m\n",
      "     print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\u001b[m\n",
      "\u001b[36m@@ -135,7 +135,7 @@\u001b[m \u001b[melif target_param_data_ratio > 0:\u001b[m\n",
      " total_tokens = total_batch_size * num_iterations\u001b[m\n",
      " print0(f\"Total number of training tokens: {total_tokens:,}\")\u001b[m\n",
      " print0(f\"tokens : param ratio: {total_tokens / num_params:.2f} (he has note that Chinchilla is ~20)\")\u001b[m\n",
      "\u001b[31m-# TODO print total training FLOPs estimate\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")\u001b[m\n",
      " \u001b[m\n",
      " # initialize optimizer\u001b[m\n",
      " optimizers = model.setup_optimizers(\u001b[m\n",
      "\u001b[36m@@ -175,10 +175,11 @@\u001b[m \u001b[mmin_val_bpb = float(\"inf\")\u001b[m\n",
      " smooth_train_loss = 0 # EMA of training loss\u001b[m\n",
      " ema_beta = 0.9 # EMA decay factor\u001b[m\n",
      " total_training_time = 0 # wall-clock time\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mresults = {} # CORE eval\u001b[m\n",
      " # run +1 steps to eval and save at end\u001b[m\n",
      " for step in range(num_iterations+1):\u001b[m\n",
      "     last_step = step == num_iterations\u001b[m\n",
      "\u001b[31m-    # TODO flops_so_far = num_flops_per_token * total_batch_size * step\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    flops_so_far = num_flops_per_token * total_batch_size * step\u001b[m\n",
      "     \u001b[m\n",
      "     if last_step or step % eval_every == 0:\u001b[m\n",
      "         # once in a while evaluate the val bpb\u001b[m\n",
      "\u001b[36m@@ -192,10 +193,10 @@\u001b[m \u001b[mfor step in range(num_iterations+1):\u001b[m\n",
      "             min_val_bpb = val_bpb\u001b[m\n",
      "         wandb_run.log({\u001b[m\n",
      "             'step': step,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'total_training_flops': flops_so_far,\u001b[m\n",
      "             'total_training_time': total_training_time,\u001b[m\n",
      "             'val/bpb': val_bpb,\u001b[m\n",
      "         })\u001b[m\n",
      "\u001b[31m-        # TODO log flops once have\u001b[m\n",
      "         model.train()\u001b[m\n",
      " \u001b[m\n",
      "     if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\u001b[m\n",
      "\u001b[36m@@ -205,10 +206,10 @@\u001b[m \u001b[mfor step in range(num_iterations+1):\u001b[m\n",
      "         print0(f\"Step {step:05d}: CORE metric: {results['core_metric']:.4f}\")\u001b[m\n",
      "         wandb_run.log({\u001b[m\n",
      "             'step': step,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'total_training_flops': flops_so_far,\u001b[m\n",
      "             'core_metric': results['core_metric'],\u001b[m\n",
      "             'centered_results': results['centered_results'],\u001b[m\n",
      "         })\u001b[m\n",
      "\u001b[31m-        # TODO log flops once have\u001b[m\n",
      "         model.train()\u001b[m\n",
      " \u001b[m\n",
      "     if master_process and (last_step or (step > 0 and step % sample_every == 0)):\u001b[m\n",
      "\u001b[36m@@ -289,9 +290,9 @@\u001b[m \u001b[mfor step in range(num_iterations+1):\u001b[m\n",
      "     debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))\u001b[m\n",
      "     pct_done = 100 * step / num_iterations\u001b[m\n",
      "     tok_per_sec = int(total_batch_size / dt)\u001b[m\n",
      "\u001b[31m-    # TODO flops_per_sec\u001b[m\n",
      "\u001b[31m-    # TODO promised_flops_per_sec\u001b[m\n",
      "\u001b[31m-    mfu = -1 # TODO mfu\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    flops_per_sec = num_flops_per_token * total_batch_size / dt\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # h100 xsm bfloat16 without sparsity (spec sheet says to halve without)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100\u001b[m\n",
      "     if step > 10:\u001b[m\n",
      "         total_training_time += dt\u001b[m\n",
      "     print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\u001b[m\n",
      "\u001b[36m@@ -299,13 +300,14 @@\u001b[m \u001b[mfor step in range(num_iterations+1):\u001b[m\n",
      "     if step % 100 == 0:\u001b[m\n",
      "         log_data = {\u001b[m\n",
      "             'step': step,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'total_training_flops': flops_so_far,\u001b[m\n",
      "             'total_training_time': total_training_time,\u001b[m\n",
      "             'train/loss': debiased_smooth_loss,\u001b[m\n",
      "             'train/lrm': lrm,\u001b[m\n",
      "             'train/dt': dt,\u001b[m\n",
      "             'train/tok_per_sec': tok_per_sec,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'train/mfu': mfu,\u001b[m\n",
      "         }\u001b[m\n",
      "\u001b[31m-        # TODO add total_training_flops and mfu once have\u001b[m\n",
      "         if grad_clip_enabled:\u001b[m\n",
      "             log_data['train/grad_norm'] = grad_norm\u001b[m\n",
      "         wandb_run.log(log_data)\u001b[m\n",
      "\u001b[36m@@ -320,7 +322,7 @@\u001b[m \u001b[mget_report().log(section='Base model training', data=[\u001b[m\n",
      "     user_config,\u001b[m\n",
      "     { # stats about the training setup\u001b[m\n",
      "         \"Number of parameters\": num_params,\u001b[m\n",
      "\u001b[31m-        # TODO \"Number of FLOPs per token\": f\"{num_flops_per_token:e}\",\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        \"Number of FLOPs per token\": f\"{num_flops_per_token:e}\",\u001b[m\n",
      "         \"Calculated number of iterations\": num_iterations,\u001b[m\n",
      "         \"Number of training tokens\": total_tokens,\u001b[m\n",
      "         \"Tokens : Params ratio\": total_batch_size * num_iterations / num_params,\u001b[m\n",
      "\u001b[36m@@ -333,8 +335,8 @@\u001b[m \u001b[mget_report().log(section='Base model training', data=[\u001b[m\n",
      "         \"Minimum validation bpb\": min_val_bpb,\u001b[m\n",
      "         \"Final validation bpb\": val_bpb,\u001b[m\n",
      "         \"CORE metric estimate\": results.get(\"core_metric\", None),\u001b[m\n",
      "\u001b[31m-        # TODO \"MFU %\": f\"{mfu:.2f}%\",\u001b[m\n",
      "\u001b[31m-        # TODO \"Total training flops\": f\"{flops_so_far:e}\",\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        \"MFU %\": f\"{mfu:.2f}%\",\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        \"Total training flops\": f\"{flops_so_far:e}\",\u001b[m\n",
      "         \"Total training time\": f\"{total_training_time/60:.2f}m\",\u001b[m\n",
      "         \"Peak memory usage\": f\"{get_max_memory() / 1024 / 1024:.2f}MiB\",\u001b[m\n",
      "     }])\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# !git diff ../my_nanochat/scripts/my_base_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaeefb0-ab7d-4521-bea8-a13d36d010cf",
   "metadata": {},
   "source": [
    "#### try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46efe669-1bc6-450d-9953-e0735f7c7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d9b379-4d2b-4fc4-84fd-ee3dec98d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding core_metric_every = 0\n",
      "overriding eval_tokens = 256\n",
      "overriding run = challenge-37-1\n",
      "user_config: {'run': 'challenge-37-1', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 10, 'target_param_data_ratio': 20, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 250, 'eval_tokens': 256, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: mps\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m setting up run hdni7nqh (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run hdni7nqh (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run hdni7nqh (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/challenge-37-prepare-to-train-d32/wandb/run-20251222_070823-hdni7nqh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-37-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hdni7nqh\u001b[0m\n",
      "Vocab size: 65,536\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65536, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65536, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,160\n",
      "Estimated FLOPs per token: 1.211105e+08\n",
      "Using user-provided number of iterations: 10\n",
      "Total number of training tokens: 1,280\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Total training FLOPs estimate: 1.550215e+11\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "step 00000 | Validation bpb: 3.1950\n",
      "step 00000/00010 (0.00%) | loss: 11.090355 | grad norm: 1.5950 | lrm: 1.00 | dt: 740.54ms | tok/sec: 172 | mfu: 0.00 | total time: 0.00m\n",
      "step 00001/00010 (10.00%) | loss: 11.071288 | grad norm: 2.1864 | lrm: 1.00 | dt: 63.36ms | tok/sec: 2,020 | mfu: 0.02 | total time: 0.00m\n",
      "step 00002/00010 (20.00%) | loss: 10.978713 | grad norm: 3.1352 | lrm: 1.00 | dt: 64.85ms | tok/sec: 1,973 | mfu: 0.02 | total time: 0.00m\n",
      "step 00003/00010 (30.00%) | loss: 10.824392 | grad norm: 4.8507 | lrm: 1.00 | dt: 69.88ms | tok/sec: 1,831 | mfu: 0.02 | total time: 0.00m\n",
      "step 00004/00010 (40.00%) | loss: 10.681846 | grad norm: 4.8060 | lrm: 1.00 | dt: 121.15ms | tok/sec: 1,056 | mfu: 0.01 | total time: 0.00m\n",
      "step 00005/00010 (50.00%) | loss: 10.549461 | grad norm: 4.4196 | lrm: 1.00 | dt: 85.48ms | tok/sec: 1,497 | mfu: 0.02 | total time: 0.00m\n",
      "step 00006/00010 (60.00%) | loss: 10.092110 | grad norm: 5.9742 | lrm: 1.00 | dt: 66.48ms | tok/sec: 1,925 | mfu: 0.02 | total time: 0.00m\n",
      "step 00007/00010 (70.00%) | loss: 9.797204 | grad norm: 4.9418 | lrm: 1.00 | dt: 68.25ms | tok/sec: 1,875 | mfu: 0.02 | total time: 0.00m\n",
      "step 00008/00010 (80.00%) | loss: 9.534563 | grad norm: 3.6029 | lrm: 1.00 | dt: 68.30ms | tok/sec: 1,874 | mfu: 0.02 | total time: 0.00m\n",
      "step 00009/00010 (90.00%) | loss: 9.223165 | grad norm: 3.9064 | lrm: 0.50 | dt: 69.13ms | tok/sec: 1,851 | mfu: 0.02 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.7599\n",
      "<|bos|>The capital of France is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<|bos|>The chemical symbol of gold is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<|bos|>The opposite of hot is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<|bos|>The planets of the solar system are: to be a less expensive by air freight.\n",
      "Ocean\n",
      "<|bos|>My favorite color is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "<|bos|>If 5*x + 3 = 13, then x is a less expensive by air freight.\n",
      "Ocean freight.\n",
      "\n",
      "saved model to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/model_000010.pt\n",
      "saved metadata to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/meta_000010.json\n",
      "saved optimizer to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/optim_000010_rank0.pt\n",
      "Peak memory usage: 0.00MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 2.7599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 0-2, summary, console lines 1-73 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading history steps 0-2, summary, console lines 1-73 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 step â–â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_flops â–â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  total_training_time â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/dt â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/grad_norm â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/lrm â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/mfu â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train/tok_per_sec â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/bpb â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 step 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_flops 155021475840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  total_training_time 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/dt 0.74054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/grad_norm 1.595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss 11.09035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/lrm 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/mfu 0.00212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train/tok_per_sec 172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/bpb 2.75995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-37-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hdni7nqh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251222_070823-hdni7nqh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --core_metric_every=0 \\\n",
    "    --eval_tokens=256 \\\n",
    "    --run=challenge-37-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a7fbfb-a6eb-4c8e-b873-7300db64c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of FLOPs per token: 1.211105e+08\n",
      "- Total training flops: 1.550215e+11\n"
     ]
    }
   ],
   "source": [
    "!grep -i flop {get_base_dir()}/report/base-model-training.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0042ea-10d2-4f3a-b9e7-ec2bd64c3d51",
   "metadata": {},
   "source": [
    "### mid_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c83ff93-bd9b-403a-81a9-84cdabf23ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdiff --git a/my_nanochat/scripts/my_mid_train.py b/my_nanochat/scripts/my_mid_train.py\u001b[m\n",
      "\u001b[1mindex d60a77d..48e03ca 100644\u001b[m\n",
      "\u001b[1m--- a/my_nanochat/scripts/my_mid_train.py\u001b[m\n",
      "\u001b[1m+++ b/my_nanochat/scripts/my_mid_train.py\u001b[m\n",
      "\u001b[36m@@ -67,7 +67,7 @@\u001b[m \u001b[mif pretrain_batch_size is not None and device_batch_size > pretrain_batch_size:\u001b[m\n",
      " orig_model = model\u001b[m\n",
      " model = torch.compile(model, dynamic=False)\u001b[m\n",
      " depth = model.config.n_layer\u001b[m\n",
      "\u001b[31m-# TODO num_flops_per_token\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mnum_flops_per_token = model.estimate_flops()\u001b[m\n",
      " tokens_per_fwdbwd = device_batch_size * max_seq_len # for a single rank\u001b[m\n",
      " world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size\u001b[m\n",
      " assert total_batch_size % world_tokens_per_fwdbwd == 0\u001b[m\n",
      "\u001b[36m@@ -167,7 +167,7 @@\u001b[m \u001b[mema_beta = 0.9\u001b[m\n",
      " total_training_time = 0\u001b[m\n",
      " step = 0\u001b[m\n",
      " while True:\u001b[m\n",
      "\u001b[31m-    # TODO flops_so_far\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    flops_so_far = num_flops_per_token * total_batch_size * step\u001b[m\n",
      " \u001b[m\n",
      "     if ddp:\u001b[m\n",
      "         last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)\u001b[m\n",
      "\u001b[36m@@ -186,10 +186,10 @@\u001b[m \u001b[mwhile True:\u001b[m\n",
      "             min_val_bpb = val_bpb\u001b[m\n",
      "         wandb_run.log({\u001b[m\n",
      "             'step': step,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            \"total_training_flops\": flops_so_far,\u001b[m\n",
      "             'total_training_time': total_training_time,\u001b[m\n",
      "             'val/bpb': val_bpb,\u001b[m\n",
      "         })\u001b[m\n",
      "\u001b[31m-        # TODO log flops once have\u001b[m\n",
      "         model.train()\u001b[m\n",
      "     \u001b[m\n",
      "     if master_process and last_step and not dry_run:\u001b[m\n",
      "\u001b[36m@@ -254,10 +254,9 @@\u001b[m \u001b[mwhile True:\u001b[m\n",
      "     debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))\u001b[m\n",
      "     pct_done = 100 * progress\u001b[m\n",
      "     tok_per_sec = int(total_batch_size / dt)\u001b[m\n",
      "\u001b[31m-    # TODO flops_per_sec\u001b[m\n",
      "\u001b[31m-    # TODO promised_flops_per_sec\u001b[m\n",
      "\u001b[31m-    # TODO mfu\u001b[m\n",
      "\u001b[31m-    mfu = -1 # TODO\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    flops_per_sec = num_flops_per_token * total_batch_size / dt\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # h100 xsm bfloat16 without sparsity (spec sheet says to halve without)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100\u001b[m\n",
      "     if step > 10:\u001b[m\n",
      "         total_training_time += dt\u001b[m\n",
      " \u001b[m\n",
      "\u001b[36m@@ -265,13 +264,14 @@\u001b[m \u001b[mwhile True:\u001b[m\n",
      "     if step % 10 == 0:\u001b[m\n",
      "         wandb_run.log({\u001b[m\n",
      "             'step': step,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'total_training_flops': flops_so_far,\u001b[m\n",
      "             'total_training_time': total_training_time,\u001b[m\n",
      "             'train/loss': debiased_smooth_loss,\u001b[m\n",
      "             'train/lrm': lrm,\u001b[m\n",
      "             'train/dt': dt,\u001b[m\n",
      "             'train/tok_per_sec': tok_per_sec,\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            'train/mfu': mfu,\u001b[m\n",
      "         })\u001b[m\n",
      "\u001b[31m-        # TODO log flops and mfu once have\u001b[m\n",
      " \u001b[m\n",
      " # print a few more stats\u001b[m\n",
      " print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# !git diff ../my_nanochat/scripts/my_mid_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c847db7a-95b6-4dc1-80b3-49382b993516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding model_tag = d4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 10\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_tokens = 256\n",
      "overriding run = challenge-37-2\n",
      "user_config: {'run': 'challenge-37-2', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': 10, 'max_seq_len': 128, 'device_batch_size': 1, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 256, 'total_batch_size': 128, 'dry_run': 0}\n",
      "Autodetected device type: mps\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/ericsilberstein/Documents/ericsilberstein1-repos/learn-nanochat/challenge-37-prepare-to-train-d32/wandb/run-20251222_072258-5nt4a6nm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-37-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/5nt4a6nm\u001b[0m\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4 with step 500\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65536, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "README.md: 7.93kB [00:00, 8.68MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 180446.13 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 266577.70 examples/s]\n",
      "step 00000 | Validation bpb: 1.9028\n",
      "step 00001 (20.00%) | loss: 4.377713 | lrm: 1.00 | dt: 727.16ms | tok/sec: 176 | mfu: 0.00 | total time: 0.00m\n",
      "step 00002 (30.00%) | loss: 5.784154 | lrm: 1.00 | dt: 27.25ms | tok/sec: 4,696 | mfu: 0.06 | total time: 0.00m\n",
      "step 00003 (40.00%) | loss: 6.181037 | lrm: 1.00 | dt: 21.63ms | tok/sec: 5,917 | mfu: 0.07 | total time: 0.00m\n",
      "step 00004 (50.00%) | loss: 6.355887 | lrm: 1.00 | dt: 24.37ms | tok/sec: 5,252 | mfu: 0.06 | total time: 0.00m\n",
      "step 00005 (60.00%) | loss: 6.507420 | lrm: 1.00 | dt: 25.55ms | tok/sec: 5,009 | mfu: 0.06 | total time: 0.00m\n",
      "step 00006 (70.00%) | loss: 6.567737 | lrm: 1.00 | dt: 26.65ms | tok/sec: 4,803 | mfu: 0.06 | total time: 0.00m\n",
      "step 00007 (80.00%) | loss: 6.642848 | lrm: 1.00 | dt: 25.98ms | tok/sec: 4,926 | mfu: 0.06 | total time: 0.00m\n",
      "step 00008 (90.00%) | loss: 6.806648 | lrm: 0.50 | dt: 27.36ms | tok/sec: 4,677 | mfu: 0.06 | total time: 0.00m\n",
      "step 00009 (100.00%) | loss: 6.824322 | lrm: 0.00 | dt: 25.74ms | tok/sec: 4,973 | mfu: 0.06 | total time: 0.00m\n",
      "step 00009 | Validation bpb: 1.9305\n",
      "saved model to /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4/model_000009.pt\n",
      "saved metadata to /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4/meta_000009.json\n",
      "saved optimizer to /Users/ericsilberstein/.cache/my_nanochat/mid_checkpoints/d4/optim_000009_rank0.pt\n",
      "Peak memory usage: 0.00MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 1.9028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 0-1, summary, console lines 0-25 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading history steps 0-1, summary, console lines 0-25 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading history steps 0-1, summary, console lines 0-25 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 step â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_flops â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  total_training_time â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/bpb â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 step 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_flops 139519328256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  total_training_time 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/bpb 1.93049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-37-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid/runs/5nt4a6nm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat-mid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251222_072258-5nt4a6nm/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_mid_train \\\n",
    "    --model_tag=d4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=10 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_tokens=256 \\\n",
    "    --run=challenge-37-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ee57-2cc8-4fef-ac43-c4001a4203c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a39e21-3285-4e5e-a7a8-c07804303338",
   "metadata": {},
   "source": [
    "### code updated in this challenge\n",
    "\n",
    "- my_gpt.py\n",
    "- my_base_train.py\n",
    "- my_mid_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad355fb3-c90d-47c0-a74a-74346bddaa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
