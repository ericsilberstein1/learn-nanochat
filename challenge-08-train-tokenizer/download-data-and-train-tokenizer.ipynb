{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149cd104-a408-4997-b68d-d2b91f59c6d3",
   "metadata": {},
   "source": [
    "Start with downloading 10 files.\n",
    "\n",
    "The code in [dataset.py](https://github.com/karpathy/nanochat/blob/master/nanochat/dataset.py) and [scripts/tok_train.py](https://github.com/karpathy/nanochat/blob/master/scripts/tok_train.py) seems straightforward enough. Rather than recreating all of it at this point, \"copy\" just enough to `my_dataset.py` to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe934f7-ba83-4733-8c2f-bb7e5ca16436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import download_single_file, parquets_iter_batched, text_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eaf9e63-9393-459f-a311-bd0f93ba1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c057784-3ed7-4fa0-be8e-885e894f48b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try downloading 10 files with 2 workers (in the style of dataset.py)\n",
    "\n",
    "ids_to_download = list(range(10))\n",
    "with Pool(processes=2) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f34b232-9509-4662-b8fd-1c06339c67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 ericsilberstein  staff    90M Oct 26 08:26 shard_00000.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    90M Oct 26 08:27 shard_00001.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:26 shard_00002.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00003.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    91M Oct 26 08:27 shard_00004.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00005.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00006.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00007.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00008.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 26 08:27 shard_00009.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh | grep parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2120aa7-f64c-436b-8a8d-d20c6fe8b2c5",
   "metadata": {},
   "source": [
    "### Look at one of the parquet files with pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ec69e5-70cc-4468-ac95-f00c9b503b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651bdf22-a96c-4a64-8255-744b120b3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pq.ParquetFile(\"shard_00000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ebd9af-ed65-42e5-a327-d3617a12cc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x103f03240>\n",
       "required group field_id=-1 schema {\n",
       "  optional binary field_id=-1 text (String);\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095e7cf7-8d1f-4f36-840c-af5f2e4e220d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745540e9-c33b-4c50-a481-3f351b3ace47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "text: string\n",
       "----\n",
       "text: [[\"Shipment & Transport-Sea, Air, Rail, Road, Pipeline\n",
       "The mode of transportation is an important con (... 8601 chars omitted)\",\"12. Definition — In this Part, unless the context otherwise requires, “the State” includes t (... 9050 chars omitted)\",\"Gúthwinë was the sword that belonged to Éomer.\n",
       "It was borne by him at the Battle of the Hornbur (... 713 chars omitted)\",\"The robot in the picture above is called YOLO, which stands for “your own living object.” It� (... 14639 chars omitted)\",\"Metal additive manufacturing (AM) is growing at a fast-paced spurring the world’s current econom (... 1730 chars omitted)\",...,\"Coconut oil is often touted as a wonder oil. It speeds up the metabolism, improves calcium and mag (... 5327 chars omitted)\",\"Posted on Apr 01, 2017, 6 a.m.\n",
       "WHO (World Health Organization) report confirms that air pollution  (... 3686 chars omitted)\",\"Their suffering under racist Italian colonialism pushed Eritreans to retreat into their millennia- (... 9876 chars omitted)\",\"Space-based solar power (SBSP) (or historically space solar power- SSP) is a system for the collec (... 23576 chars omitted)\",\"“Human population is growing like never before. We are now adding one billion people to the plan (... 3068 chars omitted)\"]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg = pf.read_row_group(0); rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2cfa65-d937-4ac3-a5e0-f63331b23030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f20d2dc-d21e-4918-95f4-89323637f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipment & Transport-Sea, Air, Rail, Road, Pipeline\n",
      "The mode of transportation is an important consideration when planning the shipment process. Besides the costs, the urgency of the shipment, the value of the goods being shipped as well as the size and weight of the goods need to be evaluated when determining the form of transportation.\n",
      "Seaborne trade accounts for about 90% of the global trade, and as per UNCTAD, 1687 million tons (2015 estimate) were carried in around 177.6 million containers (2015 estimate) covering 998 billion ton-miles (2016 estimate).\n",
      "Because of size or volume, there are several types of cargoes that cannot be or is economically unviable to move by other modes of transport than the sea.\n",
      "Ocean freight is a less expensive method of shipping goods, but the drawback is a longer transit time. Another benefit for ocean freight is while size and weight may be an issue for air; it is not for ocean freight.\n",
      "Ocean freight is used quite extensively for the movement of bulk \n"
     ]
    }
   ],
   "source": [
    "print(rg.column('text').to_pylist()[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39cf38e-17fc-4f86-be13-1e665a81af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do we mean by disability?\n",
      "Under the Disability Discrimination Act 2005 (updated by Equality Act 2010) a disabled student may be a student with:\n",
      "- specific Learning Difficulties (SpLD) including dyslexia, dyspraxia, dyscalculia, Attention Deficit Disorder (ADD)\n",
      "- mental Health difficulties (including anxiety and depressive disorders, psychological and psychiatric illness)\n",
      "- long term medical conditions such as arthritis, epilepsy, diabetes, asthma, chronic fatigue syndrome\n",
      "- autistic Spectrum Disorders such as Asperger’s’ Syndrome\n",
      "- sensory impairments\n",
      "- neurological conditions such as Multiple Sclerosis, Cerebral Palsy\n",
      "- mobility difficulties\n",
      "A person can be defined as disabled if their physical or mental impairment:\n",
      "- has a substantial effect on them\n",
      "- is long term and has lasted or is expected to last 12 months or more\n",
      "- has an adverse effect on his/her ability to carry out normal day to day activities\n",
      "Sharing information and confidentiality\n",
      "Many disabled people have impairments\n"
     ]
    }
   ],
   "source": [
    "print(rg.column('text').to_pylist()[50][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fc8d7-5d4e-4a29-b384-7df9ebe6e8d0",
   "metadata": {},
   "source": [
    "### Now try our functions for iterating through the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cc5289-c744-4eb8-b793-a05e7f3008d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0: Shipment & Transport-Sea, Air,\n",
      "0,100: The Center for Integrative Sci\n",
      "0,200: - Special Sections\\n- Public No\n",
      "0,300: Regarded as one of the greates\n",
      "0,400: 2-D or 3-D plot of output from\n",
      "0,500: We love nurses. Their dedicati\n",
      "0,600: In China, bamboo is a symbol o\n",
      "0,700: Students will become actively \n",
      "0,800: Letter S Tracing\\nThis workshee\n",
      "0,900: Many scientists have done focu\n",
      "0,1000: A dental filling is a dental r\n",
      "100,0: H.323 is a standard that speci\n",
      "100,100: Table of Contents\\nWhy People w\n",
      "100,200: Siyavush and Afrasiab\\nThe Lege\n",
      "100,300: One of the most effective ways\n",
      "100,400: Most students don’t want to mi\n",
      "100,500: In the existing world, the INT\n",
      "100,600: The treatment of crop residues\n",
      "100,700: During this trying time, UC Sa\n",
      "100,800: Buckthorn is a tree, from whic\n",
      "100,900: Energy security has received s\n",
      "100,1000: Students in Perrysburg High Sc\n",
      "200,0: As much as social media platfo\n",
      "200,100: Teaching Effective Classroom R\n",
      "200,200: “Everything was trial and erro\n",
      "200,300: Did you think Groundhog Day on\n",
      "200,400: Have you ever wondered where t\n",
      "200,500: Philosophy of Mind & Cognition\n",
      "200,600: I love creating resources and \n",
      "200,700: Sophie's World (pp.162-184)\\nSo\n",
      "200,800: The Defamation Bill\\nThe defama\n",
      "200,900: CMPT 115 Lecture Notes - Lectu\n",
      "200,1000: One year after Solomon Islands\n",
      "300,0: Presentation on theme: \"When a\n",
      "300,100: In 1933 Mr Norbury, an amateur\n",
      "300,200: - Confetti Garden\\n- Creeping J\n",
      "300,300: IT ESTIMATED THAT THE TOTAL NU\n",
      "300,400: What Is The Definition Of Busi\n",
      "300,500: CONNECT WITH US:\\nFind us on Fa\n",
      "300,600: If you want to venture out of \n",
      "300,700: Lyme disease is transmitted th\n",
      "300,800: Choose 1 U.S. environmental la\n",
      "300,900: Do all religions approach beli\n",
      "300,1000: Scoop a cup of water out of th\n",
      "400,0: by Che Guevara\\nChapter I: Gene\n",
      "400,100: According to the functionalist\n",
      "400,200: Proving that some people have \n",
      "400,300: Solving the race problem in Am\n",
      "400,400: The distinct sound of wild rin\n",
      "400,500: Even though the point of this \n",
      "400,600: It found that only 43 per cent\n",
      "400,700: A crowd of people are gatherin\n",
      "400,800: 85 miles W of Spokane, 92 mile\n",
      "400,900: Masonry in Moldova has a long,\n",
      "400,1000: Pursuing a college education c\n"
     ]
    }
   ],
   "source": [
    "for i, texts in enumerate(parquets_iter_batched('train')):\n",
    "    for j, text in enumerate(texts):\n",
    "        if i % 100 == 0 and j % 100 == 0:\n",
    "            excerpt = text[:30].replace('\\n','\\\\n')\n",
    "            print(f\"{i},{j}: {excerpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8076201-6cba-49f6-b027-150a016891d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Shipment & Transport-Sea, Air, Rail, Road, Pipelin\n",
      "1: 12. Definition — In this Part, unless the context \n",
      "2: Gúthwinë was the sword that belonged to Éomer.\\nIt \n",
      "3: The robot in the picture above is called YOLO, whi\n",
      "4: Metal additive manufacturing (AM) is growing at a \n",
      "5: The region Bergisches Land is located in southern \n",
      "6: The investigation of past cultures of the modern n\n",
      "7: Agreement On Food Safety\\nInternational trade rules\n",
      "8: Many good novels in the past have had films produc\n",
      "9: In January we began a survey of the history of Ame\n",
      "10: Wednesday, October 13, 2010\\nSEEDS OF CHANGE- JEN C\n",
      "11: Japan is set to be nuclear power-free, for just th\n",
      "12: Modern humans crowded out Europes Neanderthals\\nSci\n",
      "13: USGS Groundwater Information\\nGroundwater Resources\n",
      "14: We all know the human heart helps pump blood throu\n",
      "15: It is frequently cited that more than half of us n\n",
      "16: At the start of the 20th century:\\nLawrence produce\n",
      "17: The ease of doing business rankings for 2017 was r\n",
      "18: They may be small but these mighty berries has a l\n",
      "19: This is ‘What If’\\nand here’s how you can survive a\n"
     ]
    }
   ],
   "source": [
    "# this is the text_iterator intended for training the tokenizer\n",
    "for i, doc in enumerate(text_iterator(max_chars=1000, doc_cap=50)):\n",
    "    doc = doc.replace('\\n','\\\\n')\n",
    "    print(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930f658-8b71-409a-a0f1-52b6bb15ab9e",
   "metadata": {},
   "source": [
    "### Now train our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c51a6e-822b-478e-acbc-bed2c8f9942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice, count\n",
    "import sys\n",
    "sys.path.append('../challenge-07-rust-and-python-simplified-tokenizer')\n",
    "from my_tokenizer import MyTokenizer, SPLIT_PATTERN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434bd95-2d89-46d0-8647-06afa064d607",
   "metadata": {},
   "source": [
    "#### first with very little text (1000 chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacc1458-8722-4783-8d44-17938b1593a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator(max_chars=1000, doc_cap=50), vocab_size = 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee89ea0-07de-4ae0-89b1-5427cecf6ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb46c55e-0da6-4109-8f05-229a0b5b422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(ids):\n",
    "    for id in ids:\n",
    "        print(f\"{id} -> {tokenizer.decode([id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888b73ef-7869-4f31-a5f5-f0f070f07014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699 -> <bos>\n",
      "698 -> Wednesday\n",
      "697 -> Lawrence\n",
      "696 -> International\n",
      "695 -> Gúthwinë\n",
      "600 -> less\n",
      "599 -> bot\n",
      "598 ->  ‘\n",
      "597 ->  —\n",
      "596 ->  locat\n",
      "300 ->  R\n",
      "299 ->  O\n",
      "298 ->  I\n",
      "297 ->  be\n",
      "296 -> ing\n",
      "70 -> F\n",
      "69 -> E\n",
      "68 -> D\n",
      "67 -> C\n",
      "66 -> B\n"
     ]
    }
   ],
   "source": [
    "print_tokens(chain(\n",
    "    islice(count(699, -1), 5),\n",
    "    islice(count(600, -1), 5),\n",
    "    islice(count(300, -1), 5),\n",
    "    islice(count(70, -1), 5),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea93ee7-43e4-45dc-92b0-ce63ea8aff36",
   "metadata": {},
   "source": [
    "#### now with more text (a million chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2db3b20-c9f7-4542-9160-aa88bcee184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator(max_chars=1_000_000), vocab_size = 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cfa8444-fa28-4536-b931-87f7cc29970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34288"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.enc.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3879e-e18b-421c-be12-4e1763a1faf0",
   "metadata": {},
   "source": [
    "34288 < vocab_size means that every word was fully merged (turned into a token), does that seem right?\n",
    "\n",
    "Let's count total and unique words to sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d419af-a8d0-476f-97ba-0ea254976962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 193,066\n",
      "unique_words: 22,386\n",
      "chars per word: 5.18\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "total_words = 0\n",
    "unique_words = set()\n",
    "for doc in text_iterator(max_chars=1_000_000):\n",
    "    for word in regex.findall(SPLIT_PATTERN, doc):\n",
    "        total_words += 1\n",
    "        unique_words.add(word)\n",
    "print(f\"total words: {total_words:,}\\nunique_words: {len(unique_words):,}\\nchars per word: {(1_000_000 / total_words):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39ec9-01d2-49e2-a8d8-b37f55707ff2",
   "metadata": {},
   "source": [
    "5.2 seems high for chars per word, but maybe not because many \"words\" including a leading space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8ee1872-1138-4493-ab38-ef9e85483769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Programs -> [16968]\n",
      " anthems -> [17959]\n",
      "extra -> [27169]\n",
      " cameras -> [29648]\n",
      "-HIV -> [28586]\n",
      "sebaceous -> [14906]\n",
      " extract -> [4616]\n",
      " thrusts -> [18482]\n",
      "Intervention -> [24531]\n",
      " productive -> [12497]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for word in random.sample(list(unique_words), 10):\n",
    "    print(f\"{word} -> {tokenizer.encode(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e4c15b-c9c5-4799-9aed-7e318b7a8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34287 -> <bos>\n",
      "34286 ->  чрезвычайной\n",
      "34285 ->  人类的交往可建立信任\n",
      "34284 ->  водонагревателя\n",
      "34283 ->  Русский\n",
      "34282 ->  θερμος\n",
      "34281 ->  KINDERGARTEN\n",
      "34280 ->  RELATIVES\n",
      "34279 ->  riboflavin\n",
      "34278 ->  BANKRUPTCY\n",
      "32000 -> -coloured\n",
      "31999 ->  delightful\n",
      "31998 ->  retrieval\n",
      "31997 ->  retriever\n",
      "31996 ->  immunomod\n",
      "25000 ->  –\n",
      "\n",
      "24999 ->  estuarine\n",
      "24998 ->  estates\n",
      "24997 -> urns\n",
      "24996 ->  heralded\n",
      "5000 -> Carbon\n",
      "4999 ->  juice\n",
      "4998 ->  Physical\n",
      "4997 ->  moderate\n",
      "4996 ->  precise\n",
      "1000 -> vern\n",
      "999 ->  law\n",
      "998 ->  dr\n",
      "997 ->  school\n",
      "996 -> 17\n",
      "500 ->  all\n",
      "499 -> ally\n",
      "498 ->  J\n",
      "497 ->  which\n",
      "496 ->  whe\n",
      "70 -> F\n",
      "69 -> E\n",
      "68 -> D\n",
      "67 -> C\n",
      "66 -> B\n"
     ]
    }
   ],
   "source": [
    "print_tokens(chain(\n",
    "    islice(count(34287, -1), 10),\n",
    "    islice(count(32000, -1), 5),\n",
    "    islice(count(25000, -1), 5),\n",
    "    islice(count(5000, -1), 5),\n",
    "    islice(count(1000, -1), 5),\n",
    "    islice(count(500, -1), 5),\n",
    "    islice(count(70, -1), 5),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f6023ca-a3c4-41ad-993a-8bf99e73c633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1db902-9ee7-4f54-9e18-389febfbec56",
   "metadata": {},
   "source": [
    "#### now with even more text, ten million chars\n",
    "\n",
    "This is still 200x smaller than the 2B he trains on in `speedrun.sh` and 400x smaller than the 4B in `run1000.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "463b6c57-e94b-4157-9d7e-79c92cdf5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator(max_chars=10_000_000), vocab_size = 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8820f412-2ee9-41b8-8b3a-985c73f553f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "129213f8-4741-4d47-bae5-0e585220b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536 -> <bos>\n",
      "65535 ->  HIPC\n",
      "65534 ->  Hammon\n",
      "65533 ->  Wanless\n",
      "65532 ->  Wanat\n",
      "65531 ->  BSEs\n",
      "65530 ->  Breedlove\n",
      "65529 ->  Breeders\n",
      "65528 ->  BETA\n",
      "65527 ->  Boudnath\n",
      "50000 ->  discernment\n",
      "49999 ->  obliv\n",
      "49998 ->  Yamamoto\n",
      "49997 ->  Yamuna\n",
      "49996 ->  Vilayph\n",
      "49995 ->  unrestrained\n",
      "49994 ->  unlawful\n",
      "49993 ->  unlucky\n",
      "49992 ->  Jonathan\n",
      "49991 ->  Laubach\n",
      "40000 -> -selected\n",
      "39999 -> -seat\n",
      "39998 ->  acquies\n",
      "39997 ->  atmospheres\n",
      "39996 ->  Serving\n",
      "39995 ->  Sanctuary\n",
      "39994 ->  dedicating\n",
      "39993 ->  vulnerabilities\n",
      "39992 ->  electrically\n",
      "39991 ->  Remedies\n",
      "32000 ->  Osborne\n",
      "31999 ->  stupas\n",
      "31998 ->  Tibbits\n",
      "31997 -> iliary\n",
      "31996 ->  erectus\n",
      "25000 ->  Behavioral\n",
      "24999 ->  acknowledging\n",
      "24998 ->  instrumentalist\n",
      "24997 ->  Showers\n",
      "24996 ->  Affordable\n",
      "5000 ->  Cath\n",
      "4999 -> iration\n",
      "4998 ->  Sal\n",
      "4997 -> igure\n",
      "4996 ->  northern\n",
      "1000 ->  If\n",
      "999 ->  good\n",
      "998 ->  proble\n",
      "997 ->  def\n",
      "996 -> ices\n",
      "500 -> end\n",
      "499 ->  O\n",
      "498 ->  ad\n",
      "497 ->  will\n",
      "496 ->  which\n",
      "70 -> F\n",
      "69 -> E\n",
      "68 -> D\n",
      "67 -> C\n",
      "66 -> B\n"
     ]
    }
   ],
   "source": [
    "print_tokens(chain(\n",
    "    islice(count(65536, -1), 10),\n",
    "    islice(count(50000, -1), 10),\n",
    "    islice(count(40000, -1), 10),\n",
    "    islice(count(32000, -1), 5),\n",
    "    islice(count(25000, -1), 5),\n",
    "    islice(count(5000, -1), 5),\n",
    "    islice(count(1000, -1), 5),\n",
    "    islice(count(500, -1), 5),\n",
    "    islice(count(70, -1), 5),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d572d93-f066-4ea6-b2e0-e9201c2dddb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5187, 310, 257, 2483, 46]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"That is a cat.\"); ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55794327-310a-4bcb-be11-cd458885a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5187 -> That\n",
      "310 ->  is\n",
      "257 ->  a\n",
      "2483 ->  cat\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids) # as expected with these common words, each has its own token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639dfe5a-4019-4415-9583-61fb22cf8a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37027, 609, 262, 488, 288, 9034, 33135, 7346, 345, 3066, 2085, 46]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"Griffonage and proxinosini are rare words.\"); ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6212e532-ee98-43bb-9102-36cecf772a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37027 -> Gr\n",
      "609 -> iff\n",
      "262 -> on\n",
      "488 -> age\n",
      "288 ->  and\n",
      "9034 ->  prox\n",
      "33135 -> inos\n",
      "7346 -> ini\n",
      "345 ->  are\n",
      "3066 ->  rare\n",
      "2085 ->  words\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f095a46-5259-4134-9703-99c3ed168827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"一会儿去看电影\"); len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "150c3423-3116-4571-9632-a6051a20c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 -> �\n",
      "184 -> �\n",
      "128 -> �\n",
      "228 -> �\n",
      "188 -> �\n",
      "154 -> �\n",
      "229 -> �\n",
      "132 -> �\n",
      "191 -> �\n",
      "229 -> �\n",
      "142 -> �\n",
      "187 -> �\n",
      "231 -> �\n",
      "156 -> �\n",
      "139 -> �\n",
      "231 -> �\n",
      "148 -> �\n",
      "181 -> �\n",
      "229 -> �\n",
      "189 -> �\n",
      "177 -> �\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids)\n",
    "# there must not be much Chinese text in what we're training on because none of the bytes got merged into common chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8683708b-86c3-4e87-81de-327b8450261e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"Я говорю по-русски\"); len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50d368d6-2c88-4e44-86b9-abeea9cb2f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 -> �\n",
      "175 -> �\n",
      "13875 ->  �\n",
      "179 -> �\n",
      "64329 -> ов\n",
      "12288 -> о\n",
      "14761 -> р\n",
      "209 -> �\n",
      "142 -> �\n",
      "63808 ->  п\n",
      "12288 -> о\n",
      "45 -> -\n",
      "14761 -> р\n",
      "23850 -> у\n",
      "16975 -> с\n",
      "29353 -> ск\n",
      "10608 -> и\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids)\n",
    "# same for Russian, maybe there's a bit since \"ов\" and \"ск\", both very common in Russian words and names, got merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f11aba95-ffb9-4ecb-be68-312812cd7f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"El gato compró un mapa.\"); len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3606cb21-67d7-45f2-b918-3cf00650f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15747 -> El\n",
      "313 ->  g\n",
      "7442 -> ato\n",
      "522 ->  comp\n",
      "114 -> r\n",
      "8073 -> ó\n",
      "572 ->  un\n",
      "4179 ->  map\n",
      "97 -> a\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02ecf3e6-cdc8-469f-aed6-0d979e5a5893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"10 11 12 13 14 15 16 17 18 19 20 90 91 92 93 94 95 96 97 98 99\"); len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04f9822d-c54c-4358-9932-18e68223c5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734 -> 10\n",
      "32 ->  \n",
      "1306 -> 11\n",
      "32 ->  \n",
      "1029 -> 12\n",
      "32 ->  \n",
      "1301 -> 13\n",
      "32 ->  \n",
      "1312 -> 14\n",
      "32 ->  \n",
      "1105 -> 15\n",
      "32 ->  \n",
      "1321 -> 16\n",
      "32 ->  \n",
      "1167 -> 17\n",
      "32 ->  \n",
      "832 -> 18\n",
      "32 ->  \n",
      "559 -> 19\n",
      "32 ->  \n",
      "503 -> 20\n",
      "32 ->  \n",
      "2166 -> 90\n",
      "32 ->  \n",
      "5101 -> 91\n",
      "32 ->  \n",
      "4373 -> 92\n",
      "32 ->  \n",
      "5241 -> 93\n",
      "32 ->  \n",
      "4565 -> 94\n",
      "32 ->  \n",
      "3358 -> 95\n",
      "32 ->  \n",
      "4191 -> 96\n",
      "32 ->  \n",
      "3910 -> 97\n",
      "32 ->  \n",
      "3843 -> 98\n",
      "32 ->  \n",
      "4037 -> 99\n"
     ]
    }
   ],
   "source": [
    "print_tokens(ids)\n",
    "# as expected at least these 20 2 digit numbers all got their own token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
