{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b24a9d-756d-4ca4-a846-7f693bbff141",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenge. See `understand-reporting.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d39cf-f36b-4865-b693-c95df5fe9f94",
   "metadata": {},
   "source": [
    "### Run evals on d20 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b99e49-d4a3-464e-b996-2329b3347da0",
   "metadata": {},
   "source": [
    "Now that reporting is in place, get on the GPU machine and run the evals again. (But at least for now don't train again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c9f13-8cf5-4bd5-aa24-ca0c7bc55857",
   "metadata": {},
   "source": [
    "Follow the instructions here to get the machine ready: `challenge-28-midtrain-d20/midtrain-d20.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6194b5d3-5d2f-4faa-86bf-10495a845900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 01:35:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   47C    P0             75W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   47C    P0             77W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             74W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ac0fea-6713-4273-866f-1d5323ed8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27f49c6-077f-4755-8cbe-a37768a3b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset report and wrote header to /home/ubuntu/mynanochat/report/header.md\n",
      "W1125 01:35:35.711000 174883 torch/distributed/run.py:803] \n",
      "W1125 01:35:35.711000 174883 torch/distributed/run.py:803] *****************************************\n",
      "W1125 01:35:35.711000 174883 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1125 01:35:35.711000 174883 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "overriding model_tag = d20\n",
      "user_config: {'device_batch_size': 32, 'split_tokens': 10485760, 'device_type': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "train bpb: 0.8164\n",
      "val bpb: 0.8136\n",
      "<|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile, and ductilely reactive metal\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will be Tuesday. If tomorrow is\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,\n",
      "<|bos|>My favorite color is red. It is the color of the blood of the martyrs. It is the\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*x + 3 = \n",
      "W1125 01:36:09.952000 175841 torch/distributed/run.py:803] \n",
      "W1125 01:36:09.952000 175841 torch/distributed/run.py:803] *****************************************\n",
      "W1125 01:36:09.952000 175841 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1125 01:36:09.952000 175841 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4425 | centered: 0.2567 | time: 16.03s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1186 | centered: 0.1186 | time: 3.64s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5366 | centered: 0.5366 | time: 31.53s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6469 | centered: 0.5292 | time: 4.77s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3515 | centered: 0.1354 | time: 2.16s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.22s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2301 | centered: 0.0377 | time: 2.29s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6795 | centered: 0.3591 | time: 3.74s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.82s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3745 | centered: 0.3745 | time: 7.85s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4471 | centered: 0.2628 | time: 26.53s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.44s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5280 | centered: 0.0560 | time: 1.87s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1020 | centered: 0.1020 | time: 2.09s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.59s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3583 | centered: 0.3583 | time: 2.41s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.40s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.06s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2345 | centered: 0.2345 | time: 22.97s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1813 | centered: 0.1813 | time: 13.31s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5070 | centered: -0.2973 | time: 8.35s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2530 | centered: 0.1782 | time: 42.08s\n",
      "================================================================================\n",
      "Model: base_model (step 21400)\n",
      "================================================================================\n",
      "Task                               , Accuracy  , Centered  \n",
      "hellaswag_zeroshot                 , 0.442541  , 0.256722  \n",
      "jeopardy                           , 0.118564  , 0.118564  \n",
      "bigbench_qa_wikidata               , 0.536588  , 0.536588  \n",
      "arc_easy                           , 0.646886  , 0.529181  \n",
      "arc_challenge                      , 0.351536  , 0.135381  \n",
      "copa                               , 0.690000  , 0.380000  \n",
      "commonsense_qa                     , 0.230139  , 0.037674  \n",
      "piqa                               , 0.679543  , 0.359086  \n",
      "openbook_qa                        , 0.360000  , 0.146667  \n",
      "lambada_openai                     , 0.374539  , 0.374539  \n",
      "hellaswag                          , 0.447122  , 0.262829  \n",
      "winograd                           , 0.641026  , 0.282051  \n",
      "winogrande                         , 0.528019  , 0.056038  \n",
      "bigbench_dyck_languages            , 0.102000  , 0.102000  \n",
      "agi_eval_lsat_ar                   , 0.221739  , 0.027174  \n",
      "bigbench_cs_algorithms             , 0.358333  , 0.358333  \n",
      "bigbench_operators                 , 0.166667  , 0.166667  \n",
      "bigbench_repeat_copy_logic         , 0.000000  , 0.000000  \n",
      "squad                              , 0.234532  , 0.234532  \n",
      "coqa                               , 0.181260  , 0.181260  \n",
      "boolq                              , 0.507034  , -0.297280 \n",
      "bigbench_language_identification   , 0.253000  , 0.178218  \n",
      "CORE                               ,           , 0.201192  \n",
      "\n",
      "W1125 01:39:44.087000 417707 torch/distributed/run.py:803] \n",
      "W1125 01:39:44.087000 417707 torch/distributed/run.py:803] *****************************************\n",
      "W1125 01:39:44.087000 417707 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1125 01:39:44.087000 417707 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cudaAutodetected device type: cuda\n",
      "\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 9.00kB [00:00, 26.1MB/s]\n",
      "ARC-Easy/train-00000-of-00001.parquet: 100%|█| 331k/331k [00:00<00:00, 1.40MB/s]\n",
      "ARC-Easy/test-00000-of-00001.parquet: 100%|██| 346k/346k [00:00<00:00, 1.55MB/s]\n",
      "ARC-Easy/validation-00000-of-00001.parqu(…): 100%|█| 86.1k/86.1k [00:00<00:00, 7\n",
      "Generating train split: 100%|████| 2251/2251 [00:00<00:00, 357804.16 examples/s]\n",
      "Generating test split: 100%|█████| 2376/2376 [00:00<00:00, 548468.15 examples/s]\n",
      "Generating validation split: 100%|█| 570/570 [00:00<00:00, 256024.13 examples/s]\n",
      "final: 1026/2376 (43.18%)\n",
      "ARC-Easy accuracy: 43.18%\n",
      "ARC-Challenge/train-00000-of-00001.parqu(…): 100%|█| 190k/190k [00:00<00:00, 819\n",
      "ARC-Challenge/test-00000-of-00001.parque(…): 100%|█| 204k/204k [00:00<00:00, 1.7\n",
      "ARC-Challenge/validation-00000-of-00001.(…): 100%|█| 55.7k/55.7k [00:00<00:00, 4\n",
      "Generating train split: 100%|████| 1119/1119 [00:00<00:00, 204854.71 examples/s]\n",
      "Generating test split: 100%|█████| 1172/1172 [00:00<00:00, 320785.98 examples/s]\n",
      "Generating validation split: 100%|█| 299/299 [00:00<00:00, 140373.51 examples/s]\n",
      "final: 389/1172 (33.19%)\n",
      "ARC-Challenge accuracy: 33.19%\n",
      "README.md: 53.2kB [00:00, 112MB/s]\n",
      "dataset_infos.json: 138kB [00:00, 212MB/s]\n",
      "all/test-00000-of-00001.parquet: 100%|█████| 3.50M/3.50M [00:00<00:00, 9.02MB/s]\n",
      "all/validation-00000-of-00001.parquet: 100%|█| 408k/408k [00:00<00:00, 2.01MB/s]\n",
      "all/dev-00000-of-00001.parquet: 100%|███████| 76.5k/76.5k [00:00<00:00, 541kB/s]\n",
      "all/auxiliary_train-00000-of-00001.parqu(…): 100%|█| 47.5M/47.5M [00:00<00:00, 6\n",
      "Generating test split: 100%|███| 14042/14042 [00:00<00:00, 563629.04 examples/s]\n",
      "Generating validation split: 100%|█| 1531/1531 [00:00<00:00, 372843.26 examples/\n",
      "Generating dev split: 100%|████████| 285/285 [00:00<00:00, 146924.37 examples/s]\n",
      "Generating auxiliary_train split: 100%|█| 99842/99842 [00:00<00:00, 394443.92 ex\n",
      "final: 4644/14042 (33.07%)\n",
      "MMLU accuracy: 33.07%\n",
      "README.md: 7.94kB [00:00, 23.5MB/s]\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 12.6MB/s]\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 3.93MB/s]\n",
      "Generating train split: 100%|████| 7473/7473 [00:00<00:00, 521305.82 examples/s]\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 347986.35 examples/s]\n",
      "\u001b[KRank 0 | 4/165 (2.42%)]\n",
      "\u001b[KRank 4 | 8/165 (4.85%)]\n",
      "\u001b[KRank 6 | 6/165 (3.64%)]\n",
      "\u001b[KRank 5 | 2/165 (1.21%)]\n",
      "\u001b[KRank 3 | 11/165 (6.67%)]\n",
      "\u001b[KRank 2 | 3/165 (1.82%)]\n",
      "\u001b[KRank 7 | 6/164 (3.66%)]\n",
      "\u001b[KRank 1 | 5/165 (3.03%)]\n",
      "==================================================\n",
      "final: 45/1319 (3.41%)\n",
      "GSM8K accuracy: 3.41%\n",
      "README.md: 6.52kB [00:00, 16.8MB/s]\n",
      "openai_humaneval/test-00000-of-00001.par(…): 100%|█| 83.9k/83.9k [00:00<00:00, 3\n",
      "Generating test split: 100%|████████| 164/164 [00:00<00:00, 43123.68 examples/s]\n",
      "\u001b[KRank 6 | 2/20 (10.00%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]]\n",
      "\u001b[KRank 2 | 3/21 (14.29%)]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "\u001b[KRank 7 | 1/20 (5.00%)]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "==================================================\n",
      "final: 11/164 (6.71%)\n",
      "HumanEval accuracy: 6.71%\n",
      "\u001b[KRank 5 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 1 | 31/32 (96.88%)]\n",
      "==================================================\n",
      "final: 251/256 (98.05%)\n",
      "SpellingBee accuracy: 98.05%\n",
      "W1125 01:48:27.568000 420975 torch/distributed/run.py:803] \n",
      "W1125 01:48:27.568000 420975 torch/distributed/run.py:803] *****************************************\n",
      "W1125 01:48:27.568000 420975 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1125 01:48:27.568000 420975 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/learn-nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/chatsft_checkpoints/d20 with step 700\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 1054/2376 (44.36%)\n",
      "ARC-Easy accuracy: 44.36%\n",
      "final: 373/1172 (31.83%)\n",
      "ARC-Challenge accuracy: 31.83%\n",
      "final: 4530/14042 (32.26%)\n",
      "MMLU accuracy: 32.26%\n",
      "\u001b[KRank 4 | 7/165 (4.24%)]]]\n",
      "\u001b[KRank 7 | 5/164 (3.05%)]]\n",
      "\u001b[KRank 0 | 14/165 (8.48%)]\n",
      "\u001b[KRank 5 | 5/165 (3.03%)]\n",
      "\u001b[KRank 6 | 6/165 (3.64%)]]\n",
      "\u001b[KRank 1 | 10/165 (6.06%)]\n",
      "\u001b[KRank 2 | 6/165 (3.64%)]]\n",
      "\u001b[KRank 3 | 13/165 (7.88%)]\n",
      "==================================================\n",
      "final: 66/1319 (5.00%)\n",
      "GSM8K accuracy: 5.00%\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 7 | 2/20 (10.00%)]\n",
      "\u001b[KRank 5 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 0 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 2 | 2/21 (9.52%)]]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]\n",
      "\u001b[KRank 4 | 0/20 (0.00%)]\n",
      "==================================================\n",
      "final: 10/164 (6.10%)\n",
      "HumanEval accuracy: 6.10%\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 5 | 30/32 (93.75%)]\n",
      "\u001b[KRank 1 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 4 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 30/32 (93.75%)]\n",
      "\u001b[KRank 3 | 31/32 (96.88%)]\n",
      "==================================================\n",
      "final: 249/256 (97.27%)\n",
      "SpellingBee accuracy: 97.27%\n",
      "Generating report to /home/ubuntu/mynanochat/report/report.md\n",
      "Warning: /home/ubuntu/mynanochat/report/tokenizer-training.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/tokenizer-evaluation.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/base-model-training.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/midtraining.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/chat-sft.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/chat-rl.md does not exist, skipping\n",
      "Warning: /home/ubuntu/mynanochat/report/chat-evaluation-rl.md does not exist, skipping\n",
      "Copying report.md to current directory for convenience\n"
     ]
    }
   ],
   "source": [
    "!python -m my_nanochat.my_report reset\n",
    "\n",
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_loss -- --model_tag=d20\n",
    "\n",
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --model-tag=d20\n",
    "\n",
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --model-tag=d20 --source=mid\n",
    "\n",
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --model-tag=d20 --source=sft\n",
    "\n",
    "!python -m my_nanochat.my_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173edce9-a552-4211-9d58-1ad0c9409001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9c4f75-0cd5-4812-875e-e2ef09597da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nanochat training report\n",
      "\n",
      "Generated: 2025-11-25 01:35:30\n",
      "\n",
      "## Environment\n",
      "\n",
      "### Git Information\n",
      "- Branch: master\n",
      "- Commit: 2cf49d1 (dirty)\n",
      "- Message: challenge 34: understand and add reporting - preparing to run evaluations on gpu\n",
      "\n",
      "### Hardware\n",
      "- Platform: Linux\n",
      "- CPUs: 104 cores (208 logical)\n",
      "- Memory: 1771.7 GB\n",
      "- GPUs: 8x NVIDIA H100 80GB HBM3\n",
      "- GPU Memory: 633.5 GB total\n",
      "- CUDA Version: 12.8\n",
      "\n",
      "### Software\n",
      "- Python: 3.10.12\n",
      "- PyTorch: 2.9.0+cu128\n",
      "\n",
      "Run started: 2025-11-25 01:35:33\n",
      "\n",
      "--\n",
      "\n",
      "## Base model loss\n",
      "timestamp: 2025-11-25 01:36:04\n",
      "\n",
      "- train bpb: 0.8164\n",
      "- val bpb: 0.8136\n",
      "- sample 0: <|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe\n",
      "- sample 1: <|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile, and ductilely reactive metal\n",
      "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will be Tuesday. If tomorrow is\n",
      "- sample 3: <|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.\n",
      "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,\n",
      "- sample 5: <|bos|>My favorite color is red. It is the color of the blood of the martyrs. It is the\n",
      "- sample 6: <|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*x + 3 = \n",
      "\n",
      "\n",
      "## Base model evaluation\n",
      "timestamp: 2025-11-25 01:39:38\n",
      "\n",
      "- Model: base_model (step 21400)\n",
      "- CORE metric: 0.2012\n",
      "- hellaswag_zeroshot: 0.2567\n",
      "- jeopardy: 0.1186\n",
      "- bigbench_qa_wikidata: 0.5366\n",
      "- arc_easy: 0.5292\n",
      "- arc_challenge: 0.1354\n",
      "- copa: 0.3800\n",
      "- commonsense_qa: 0.0377\n",
      "- piqa: 0.3591\n",
      "- openbook_qa: 0.1467\n",
      "- lambada_openai: 0.3745\n",
      "- hellaswag: 0.2628\n",
      "- winograd: 0.2821\n",
      "- winogrande: 0.0560\n",
      "- bigbench_dyck_languages: 0.1020\n",
      "- agi_eval_lsat_ar: 0.0272\n",
      "- bigbench_cs_algorithms: 0.3583\n",
      "- bigbench_operators: 0.1667\n",
      "- bigbench_repeat_copy_logic: 0.0000\n",
      "- squad: 0.2345\n",
      "- coqa: 0.1813\n",
      "- boolq: -0.2973\n",
      "- bigbench_language_identification: 0.1782\n",
      "\n",
      "\n",
      "## Chat evaluation mid\n",
      "timestamp: 2025-11-25 01:48:21\n",
      "\n",
      "- source: mid\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d20\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.4318\n",
      "- ARC-Challenge: 0.3319\n",
      "- MMLU: 0.3307\n",
      "- GSM8K: 0.0341\n",
      "- HumanEval: 0.0671\n",
      "- SpellingBee: 0.9805\n",
      "- ChatCORE metric: 0.2568\n",
      "\n",
      "\n",
      "## Chat evaluation sft\n",
      "timestamp: 2025-11-25 01:55:32\n",
      "\n",
      "- source: sft\n",
      "- task_name: None\n",
      "- dtype: bfloat16\n",
      "- temperature: 0.0000\n",
      "- max_new_tokens: 512\n",
      "- num_samples: 1\n",
      "- top_k: 50\n",
      "- batch_size: 8\n",
      "- model_tag: d20\n",
      "- step: None\n",
      "- max_problems: None\n",
      "- print_failed: False\n",
      "- device_type: \n",
      "- ARC-Easy: 0.4436\n",
      "- ARC-Challenge: 0.3183\n",
      "- MMLU: 0.3226\n",
      "- GSM8K: 0.0500\n",
      "- HumanEval: 0.0610\n",
      "- SpellingBee: 0.9727\n",
      "- ChatCORE metric: 0.2549\n",
      "\n",
      "\n",
      "## Summary\n",
      "\n",
      "| Metric          | BASE     | MID      | SFT      | RL       |\n",
      "|-----------------|----------|----------|----------|----------|\n",
      "| CORE            | 0.2012   | -        | -        | -        |\n",
      "| ARC-Challenge   | -        | 0.3319   | 0.3183   | -        |\n",
      "| ARC-Easy        | -        | 0.4318   | 0.4436   | -        |\n",
      "| GSM8K           | -        | 0.0341   | 0.0500   | -        |\n",
      "| HumanEval       | -        | 0.0671   | 0.0610   | -        |\n",
      "| MMLU            | -        | 0.3307   | 0.3226   | -        |\n",
      "| ChatCORE        | -        | 0.2568   | 0.2549   | -        |\n",
      "\n",
      "Total wall clock time: 0h19m\n"
     ]
    }
   ],
   "source": [
    "!cat report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756ae53a-c42c-46d2-8604-10b1dff5b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c8ab68-9b0b-4a43-ad27-b042993186f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# nanochat training report\n",
       "\n",
       "Generated: 2025-11-25 01:35:30\n",
       "\n",
       "## Environment\n",
       "\n",
       "### Git Information\n",
       "- Branch: master\n",
       "- Commit: 2cf49d1 (dirty)\n",
       "- Message: challenge 34: understand and add reporting - preparing to run evaluations on gpu\n",
       "\n",
       "### Hardware\n",
       "- Platform: Linux\n",
       "- CPUs: 104 cores (208 logical)\n",
       "- Memory: 1771.7 GB\n",
       "- GPUs: 8x NVIDIA H100 80GB HBM3\n",
       "- GPU Memory: 633.5 GB total\n",
       "- CUDA Version: 12.8\n",
       "\n",
       "### Software\n",
       "- Python: 3.10.12\n",
       "- PyTorch: 2.9.0+cu128\n",
       "\n",
       "Run started: 2025-11-25 01:35:33\n",
       "\n",
       "--\n",
       "\n",
       "## Base model loss\n",
       "timestamp: 2025-11-25 01:36:04\n",
       "\n",
       "- train bpb: 0.8164\n",
       "- val bpb: 0.8136\n",
       "- sample 0: <|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe\n",
       "- sample 1: <|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile, and ductilely reactive metal\n",
       "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will be Tuesday. If tomorrow is\n",
       "- sample 3: <|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.\n",
       "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,\n",
       "- sample 5: <|bos|>My favorite color is red. It is the color of the blood of the martyrs. It is the\n",
       "- sample 6: <|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*x + 3 = \n",
       "\n",
       "\n",
       "## Base model evaluation\n",
       "timestamp: 2025-11-25 01:39:38\n",
       "\n",
       "- Model: base_model (step 21400)\n",
       "- CORE metric: 0.2012\n",
       "- hellaswag_zeroshot: 0.2567\n",
       "- jeopardy: 0.1186\n",
       "- bigbench_qa_wikidata: 0.5366\n",
       "- arc_easy: 0.5292\n",
       "- arc_challenge: 0.1354\n",
       "- copa: 0.3800\n",
       "- commonsense_qa: 0.0377\n",
       "- piqa: 0.3591\n",
       "- openbook_qa: 0.1467\n",
       "- lambada_openai: 0.3745\n",
       "- hellaswag: 0.2628\n",
       "- winograd: 0.2821\n",
       "- winogrande: 0.0560\n",
       "- bigbench_dyck_languages: 0.1020\n",
       "- agi_eval_lsat_ar: 0.0272\n",
       "- bigbench_cs_algorithms: 0.3583\n",
       "- bigbench_operators: 0.1667\n",
       "- bigbench_repeat_copy_logic: 0.0000\n",
       "- squad: 0.2345\n",
       "- coqa: 0.1813\n",
       "- boolq: -0.2973\n",
       "- bigbench_language_identification: 0.1782\n",
       "\n",
       "\n",
       "## Chat evaluation mid\n",
       "timestamp: 2025-11-25 01:48:21\n",
       "\n",
       "- source: mid\n",
       "- task_name: None\n",
       "- dtype: bfloat16\n",
       "- temperature: 0.0000\n",
       "- max_new_tokens: 512\n",
       "- num_samples: 1\n",
       "- top_k: 50\n",
       "- batch_size: 8\n",
       "- model_tag: d20\n",
       "- step: None\n",
       "- max_problems: None\n",
       "- print_failed: False\n",
       "- device_type: \n",
       "- ARC-Easy: 0.4318\n",
       "- ARC-Challenge: 0.3319\n",
       "- MMLU: 0.3307\n",
       "- GSM8K: 0.0341\n",
       "- HumanEval: 0.0671\n",
       "- SpellingBee: 0.9805\n",
       "- ChatCORE metric: 0.2568\n",
       "\n",
       "\n",
       "## Chat evaluation sft\n",
       "timestamp: 2025-11-25 01:55:32\n",
       "\n",
       "- source: sft\n",
       "- task_name: None\n",
       "- dtype: bfloat16\n",
       "- temperature: 0.0000\n",
       "- max_new_tokens: 512\n",
       "- num_samples: 1\n",
       "- top_k: 50\n",
       "- batch_size: 8\n",
       "- model_tag: d20\n",
       "- step: None\n",
       "- max_problems: None\n",
       "- print_failed: False\n",
       "- device_type: \n",
       "- ARC-Easy: 0.4436\n",
       "- ARC-Challenge: 0.3183\n",
       "- MMLU: 0.3226\n",
       "- GSM8K: 0.0500\n",
       "- HumanEval: 0.0610\n",
       "- SpellingBee: 0.9727\n",
       "- ChatCORE metric: 0.2549\n",
       "\n",
       "\n",
       "## Summary\n",
       "\n",
       "| Metric          | BASE     | MID      | SFT      | RL       |\n",
       "|-----------------|----------|----------|----------|----------|\n",
       "| CORE            | 0.2012   | -        | -        | -        |\n",
       "| ARC-Challenge   | -        | 0.3319   | 0.3183   | -        |\n",
       "| ARC-Easy        | -        | 0.4318   | 0.4436   | -        |\n",
       "| GSM8K           | -        | 0.0341   | 0.0500   | -        |\n",
       "| HumanEval       | -        | 0.0671   | 0.0610   | -        |\n",
       "| MMLU            | -        | 0.3307   | 0.3226   | -        |\n",
       "| ChatCORE        | -        | 0.2568   | 0.2549   | -        |\n",
       "\n",
       "Total wall clock time: 0h19m\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(open('report.md').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4af006-cf2d-44e7-b05c-4e58a63694e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ea0f3-a5bc-4753-92ea-1bfd6d3392ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1abd907-e936-4cfc-98fd-23de4e949c81",
   "metadata": {},
   "source": [
    "Noticed while running:\n",
    "\n",
    "On base eval, why is 50% on boolq centered at negative 30%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c7832-a649-4855-99db-032ffd621157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
