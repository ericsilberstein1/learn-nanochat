{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fa6821-8185-41dd-b336-ae9188c5a6c9",
   "metadata": {},
   "source": [
    "This is not the main notebook in his challenge. See `understand-reporting.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36262e3-42ad-4ba2-83ca-ce444d3f8a2c",
   "metadata": {},
   "source": [
    "## Review d20 evals\n",
    "\n",
    "Look through `review-d20-evals.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ffc0a-8d98-46b5-a182-3b86adef8c61",
   "metadata": {},
   "source": [
    "1) Sanity check against data collected during prior runs.\n",
    "\n",
    "2) Why the -30% boolq result?\n",
    "\n",
    "3) Compare ChatCORE mid to sft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d138-e9da-49a6-be71-8efdff59408e",
   "metadata": {},
   "source": [
    "### #1 Sanity check against prior runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2f834-d255-42b9-a531-2807ceab6fde",
   "metadata": {},
   "source": [
    "For base eval, I should compare with the base eval I ran at the top of this notebook: `challenge-28-midtrain-d20/midtrain-d20.ipynb`\n",
    "\n",
    "For base loss, I never ran that before, but I can somewhat sanity check against the loss from the d20 base training run: `challenge-25-pretrain-d20/full-run.ipynb`\n",
    "\n",
    "For chat eval mid, it should match with the mid chat eval I summarized here: `challenge-32-redo-chat-eval-d20/investigate-runs.ipynb`?\n",
    "\n",
    "For chat eval sft, it should be \"similar\" to the sft eval in that same notebook, but should not match exactly, because I kept repeating the sft train but never repeated the full chat eval.\n",
    "\n",
    "#### base eval\n",
    "\n",
    "From `challenge-28-midtrain-d20/midtrain-d20.ipynb`:\n",
    "```\n",
    "CORE metric: 0.2012\n",
    "centered results:\n",
    "{\n",
    "    \"hellaswag_zeroshot\": 0.25672173500061035,\n",
    "    \"jeopardy\": 0.11856400221586227,\n",
    "    \"bigbench_qa_wikidata\": 0.5365877747535706,\n",
    "    \"arc_easy\": 0.5291806856791178,\n",
    "    \"arc_challenge\": 0.13538110256195068,\n",
    "    \"copa\": 0.3799999952316284,\n",
    "    \"commonsense_qa\": 0.03767403215169905,\n",
    "    \"piqa\": 0.35908591747283936,\n",
    "    \"openbook_qa\": 0.14666668574015299,\n",
    "    \"lambada_openai\": 0.3745391070842743,\n",
    "    \"hellaswag\": 0.26282942295074463,\n",
    "    \"winograd\": 0.28205132484436035,\n",
    "    \"winogrande\": 0.05603790283203125,\n",
    "    \"bigbench_dyck_languages\": 0.10200000554323196,\n",
    "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
    "    \"bigbench_cs_algorithms\": 0.3583333194255829,\n",
    "    \"bigbench_operators\": 0.1666666716337204,\n",
    "    \"bigbench_repeat_copy_logic\": 0.0,\n",
    "    \"squad\": 0.2345316857099533,\n",
    "    \"coqa\": 0.18126018345355988,\n",
    "    \"boolq\": -0.2972798786665264,\n",
    "    \"bigbench_language_identification\": 0.1782178120775716\n",
    "}\n",
    "```\n",
    "\n",
    "Yes CORE 0.2012 matches exactly and I compared a few others and they also match exactly.\n",
    "\n",
    "#### base loss\n",
    "\n",
    "In `challenge-25-pretrain-d20/full-run.ipynb`, final validation bpb was 0.8135. In the report, val bpb: 0.8136. Seems good. Train bpb is 0.8164. Also seems to pass a sanity check.\n",
    "\n",
    "#### chat eval mid\n",
    "\n",
    "In `challenge-32-redo-chat-eval-d20/investigate-runs.ipynb` I hand created (really vim magic) this table:\n",
    "\n",
    "```\n",
    "                   mid        sft         increase (from mid to sft)\n",
    "\n",
    "ARC-Easy           43.18      44.07       2.1%\n",
    "ARC-Challenge      33.19      31.74       -4.4%\n",
    "MMLU               33.07      32.34       -2.2% \n",
    "GSM8K              3.41       5.38        57.8%\n",
    "HumanEval          6.71       6.10        -9%\n",
    "SpellingBee        98.05      96.88       -1.2%\n",
    "```\n",
    "\n",
    "The numbers in the mid column match perfectly.\n",
    "\n",
    "### chat eval sft\n",
    "\n",
    "The numbers in the sft column are \"similar\" at least enough for a sanity check on the reporting code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade35a5-62b5-41d0-9b0e-e748dd322be1",
   "metadata": {},
   "source": [
    "### #2 Odd boolq results\n",
    "\n",
    "Why in CORE eval did boolq come out as centered -0.297?\n",
    "\n",
    "From `challenge-21-understand-core-metric/core-evaluation-data-examples.ipynb`, looks like boolq is yes/no multiple choice questions like this one:\n",
    "\n",
    "```\n",
    "Query: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
    "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
    "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
    "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
    "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
    "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
    "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
    "Correct prompt: 1\n",
    "\n",
    "prompt 0: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
    "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
    "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
    "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
    "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
    "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
    "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
    "Answer: no\n",
    "\n",
    "prompt 1: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
    "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
    "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
    "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
    "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
    "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
    "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
    "Answer: yes\n",
    "```\n",
    "\n",
    "What's the random baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dba69a2d-8306-4292-a752-2d9caee052f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eval Task</th>\n",
       "      <th>Task Category</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>#shots</th>\n",
       "      <th>#datapoints</th>\n",
       "      <th>Random baseline</th>\n",
       "      <th>Centered Metric?</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>boolq</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>3270</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BoolQ consists of 3,270 short passages on a d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Eval Task          Task Category        Task Type  #shots  #datapoints  \\\n",
       "45     boolq  reading comprehension  multiple choice      10         3270   \n",
       "\n",
       "    Random baseline  Centered Metric?  \\\n",
       "45             62.0               NaN   \n",
       "\n",
       "                                          Description  \n",
       "45   BoolQ consists of 3,270 short passages on a d...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "import pandas as pd\n",
    "eval_meta_data = pd.read_csv(f\"{get_base_dir()}/eval_bundle/eval_meta_data.csv\")\n",
    "eval_meta_data[eval_meta_data['Eval Task'] == 'boolq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c16490b-9973-418b-a60e-c34b2b0d09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my_base_eval.py\n",
    "def centered_result(accuracy, random_baseline):\n",
    "    return (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a639a8ed-4edb-4cc8-9145-79ba69d4cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29736842105263156"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_result(0.5070, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631cc9d-3277-4c55-a70c-42a97d384454",
   "metadata": {},
   "source": [
    "But why is the random baseline 62%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "990f4d67-9259-46cf-a5cf-2396578f15ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BoolQ consists of 3,270 short passages on a diverse range of subjects followed by a yes/no questions. The model is expected to answer in multiple-choice format.\n"
     ]
    }
   ],
   "source": [
    "print(eval_meta_data.iloc[45]['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d5966-ec40-492d-b9fc-2fdb0754acc0",
   "metadata": {},
   "source": [
    "I'm sure they all have 2 answers, but just to be sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "240cf367-38b2-4bb8-a8ea-1a162eb55461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agi_eval_lsat_lr.jsonl              coqa.jsonl\n",
      "agi_eval_lsat_rc.jsonl              narrative_qa.jsonl\n",
      "agi_eval_sat_en.jsonl               pubmed_qa_labeled.jsonl\n",
      "bigbench_understanding_fables.jsonl squad.jsonl\n",
      "boolq.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls {get_base_dir()}/eval_bundle/eval_data/reading_comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7364cb98-bf59-4a75-83bf-1e4a79ffe33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"{get_base_dir()}/eval_bundle/eval_data/reading_comprehension/boolq.jsonl\"\n",
    "import json\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bac26a73-7a7b-48fb-a84b-ebf2b94406f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91c25b99-a33b-449d-b99f-52e5c478533e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\\nQuestion: does ethanol take more energy make that produces?\",\n",
       " 'choices': ['no', 'yes'],\n",
       " 'gold': 0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7dc287b2-7fb0-4639-bd8a-837366e84590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3270, 2, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_choices = [len(item['choices']) for item in data]\n",
    "len(num_choices), min(num_choices), max(num_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacdb53-6786-4007-888d-59ba1ffdd9c1",
   "metadata": {},
   "source": [
    "Where did `eval_meta_data.csv` come from?\n",
    "\n",
    "We download from:\n",
    "\n",
    "```\n",
    "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\"\n",
    "```\n",
    "\n",
    "Download again now. Maybe it's been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae4e70be-ac07-45fc-b228-6bee749b1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 ericsilberstein  staff  26060410 Nov 13 07:30 /Users/ericsilberstein/.cache/my_nanochat/eval_bundle.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -l {get_base_dir()}/eval_bundle.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93ca938b-2b6e-4cfd-a615-68c9f0e6ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {get_base_dir()}/eval_bundle.zip {get_base_dir()}/eval_bundle-downloaded-on-2025-Nov-13.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a72022b-a886-4fd1-ad0a-041f37419736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {get_base_dir()}/eval_bundle.zip.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad31fb01-eacc-49ee-a719-7b485163e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {get_base_dir()}/eval_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7d3998a-219d-43e4-8cc2-104ea7a93611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d696c6cd-c24d-40be-a19d-96580c1b4870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "loading the model from /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d1 with step 10\n",
      "Building model with config: {'sequence_len': 256, 'vocab_size': 65536, 'n_layer': 1, 'n_head': 1, 'n_kv_head': 1, 'n_embd': 64}\n",
      "downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "downloaded to /Users/ericsilberstein/.cache/my_nanochat/eval_bundle.zip\n",
      "placed eval_bundle dir at /Users/ericsilberstein/.cache/my_nanochat/eval_bundle\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.64s\n",
      "================================================================================\n",
      "Model: base_model (step 10)\n",
      "================================================================================\n",
      "Task                               , Accuracy  , Centered  \n",
      "jeopardy                           , 0.000000  , 0.000000  \n",
      "CORE                               ,           , 0.000000  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just to force it to download again\n",
    "!python -m scripts.my_base_eval \\\n",
    "    --model-tag=d1 \\\n",
    "    --source=base \\\n",
    "    --max-per-task=11 \\\n",
    "    --tasks-to-run=jeopardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7da9b5a-352e-4e35-9ec6-188dd638d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eval Task</th>\n",
       "      <th>Task Category</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>#shots</th>\n",
       "      <th>#datapoints</th>\n",
       "      <th>Random baseline</th>\n",
       "      <th>Centered Metric?</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>boolq</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>3270</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BoolQ consists of 3,270 short passages on a d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Eval Task          Task Category        Task Type  #shots  #datapoints  \\\n",
       "45     boolq  reading comprehension  multiple choice      10         3270   \n",
       "\n",
       "    Random baseline  Centered Metric?  \\\n",
       "45             62.0               NaN   \n",
       "\n",
       "                                          Description  \n",
       "45   BoolQ consists of 3,270 short passages on a d...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_meta_data = pd.read_csv(f\"{get_base_dir()}/eval_bundle/eval_meta_data.csv\")\n",
    "eval_meta_data[eval_meta_data['Eval Task'] == 'boolq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39afac3-b9dc-47fc-9344-010c3b9acb96",
   "metadata": {},
   "source": [
    "Look on HuggingFace: https://huggingface.co/datasets/google/boolq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b54ab6-c4b3-407d-9ade-873a916c29a3",
   "metadata": {},
   "source": [
    "Ah! Google AI reponse said: This compared to a 62% majority-class baseline (predicting the most frequent answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537adc7-7b81-40dd-8896-b10c9c4945b4",
   "metadata": {},
   "source": [
    "### #3 Compare ChatCORE mid to sft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64c4f4-4272-48ef-b168-8254cb618922",
   "metadata": {},
   "source": [
    "I spent a lot of time in `challenge-32-redo-chat-eval-d20/investigate-runs.ipynb` trying to decide if sft training worked and did anything good and that was one of the motivations to add reporting. One of a few good things about adding the reporting was computing the ChatCORE metric which is the average of the centered chat core task results (ARC-Easy, Arc-Challenge, etc.)\n",
    "\n",
    "ChatCORE declines 0.2568 to 0.2549 from mid to sft. Doesn't really add anything to what was trying to figure out in the other notebook, but does seem like in this comment in speedrun.sh:\n",
    "\n",
    "```# train sft and re-eval right away (should see a small bump)```\n",
    "\n",
    "he might have been talking about ChatCORE, and I didn't see a small bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e81bc7-9d48-4885-a894-89a38d8e61a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
