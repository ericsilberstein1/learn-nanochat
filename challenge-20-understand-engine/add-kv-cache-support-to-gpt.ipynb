{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c927ec-8ac6-4962-85aa-79518207a21a",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenge. Start with `understand-engine.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf24496-d705-4c70-a98d-abd894b9243e",
   "metadata": {},
   "source": [
    "### Understand KV cache and add support to `my_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acdf74-e93d-4510-b0f5-c8c5f37acd9f",
   "metadata": {},
   "source": [
    "I can see I won't get very far with the engine without kv cache support in GPT. I left out that code earlier. I'm not sure if it's better to first understand the `KVCache` class in [engine.py](https://github.com/karpathy/nanochat/blob/master/nanochat/engine.py) or first add kv cache support in `my_gpt.py`. There's a lot of subtle stuff going on in both places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb1e94-5560-40e1-98ab-c97de5499670",
   "metadata": {},
   "source": [
    "Actually let me first just think about the concept. In the naive generate, we put in say 3 tokens and we get out 3 distributions over our vocab, we look only at the last one, take say the highest probability one, and that gives us our new token.\n",
    "\n",
    "Now we put those 4 tokens in, etc.\n",
    "\n",
    "But when working on those 4 tokens, all the calculations for the first 3 will be exactly the same as before.\n",
    "\n",
    "We want to avoid repeating the calculations, but we do need all that information to be available during the \"mixing\" part in self-attention because the value in that 4th position is going to be a mix of values from the first 3. (Being a little loose with language here.)\n",
    "\n",
    "So maybe what we do is stick only that new 4th token into the machine but pass a cache so that scaled_dot_product_attention() does the right thing. Let's see if I can construct a simple example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e27c98-75b2-476f-a9f6-81ec25a713ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1555,  0.3554,  0.4374, -1.2441, -1.1757],\n",
       "          [ 0.1047,  0.3228,  0.1903, -1.0041, -0.8421],\n",
       "          [ 0.4303,  0.9916,  0.2892, -0.8870, -0.5080],\n",
       "          [ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "B = 1 # batch size\n",
    "T = 4 # sequence length\n",
    "H = 1 # heads\n",
    "C = 5 # channels per head\n",
    "q = torch.randn((B, H, T, C)) # keep it like in the real thing even though doing B = 1 and H = 1 here\n",
    "k = torch.randn((B, H, T, C))\n",
    "v = torch.randn((B, H, T, C))\n",
    "output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bf5ce7-aea9-40ad-8d5c-b9557138ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's say the only thing we really need is that last row\n",
    "output[:,:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf5bc5b-33fd-42da-9114-fb8b781c1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5312, -0.4794,  0.2350,  0.0690, -0.6563]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can we get that from k, v and q[:,:,-1:,:]?\n",
    "q_last_token_only = q[:,:,-1:,:]\n",
    "q_last_token_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82541cdf-8697-411c-a357-db465b147369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1555,  0.3554,  0.4374, -1.2441, -1.1757]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.scaled_dot_product_attention(q_last_token_only, k, v, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ba598b-7b38-40d7-9422-77cade4ff761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.scaled_dot_product_attention(q_last_token_only, k, v, is_causal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddcef1-2d81-41a0-9c5d-04a3fea20a5b",
   "metadata": {},
   "source": [
    "Yes? But why only if we pass is_causal=False?\n",
    "\n",
    "I bet the attention mask that gets built in F.scaled_dot_product_attention isn't right for this situation. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df360ae-d875-489e-b63b-2d050286b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# copying from https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias = attn_mask + attn_bias\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value, attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2083a0a-35d3-44ed-9231-5535b39bbe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1555,  0.3554,  0.4374, -1.2441, -1.1757],\n",
       "          [ 0.1047,  0.3228,  0.1903, -1.0041, -0.8421],\n",
       "          [ 0.4303,  0.9916,  0.2892, -0.8870, -0.5080],\n",
       "          [ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first repeat what we did above but using this version, should match\n",
    "output, attn_bias = scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2dc735-ed98-4e83-9cae-19823fc97cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1c3698-f9cc-4087-a255-d51a7c024f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1555,  0.3554,  0.4374, -1.2441, -1.1757]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attn_bias = scaled_dot_product_attention(q_last_token_only, k, v, is_causal=True)\n",
    "output # expect NOT to match the last row, just like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ce862a-05e1-4299-8bb3-801b7c16b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d7a1f-a96a-466d-999d-e26293b85f03",
   "metadata": {},
   "source": [
    "Yes, it assumes we're only allowed to look at the first \"token.\"\n",
    "\n",
    "So for a single token in q, passing is_causal=False works because it's the same as no mask which is right for the last position. However, it looks like the code in `CausalSelfAttention` constructs its own mask, and that's probably to support a q with multiple tokens.\n",
    "\n",
    "When would that come up in inference? When you first start you likely have a prompt which means many tokens, but why would you have a KV cache? And later I'm starting to think it goes one token at a time.\n",
    "\n",
    "Come back to that. Let's see how the mask is constructed and make sure it's the same as no mask when q has length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4879fcd-478a-4730-ba68-83528007bb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False],\n",
       "        [ True,  True, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with q having the two final \"tokens\"\n",
    "q_last_two_tokens = q[:,:,-2:,:]\n",
    "# copying code from CausalSelfAttention.forward\n",
    "Tq = q_last_two_tokens.size(2) # 2\n",
    "Tk = k.size(2) # 4\n",
    "attn_mask = torch.zeros((Tq,Tk), dtype=torch.bool)\n",
    "prefix_len = Tk - Tq # 2\n",
    "attn_mask[:, :prefix_len] = True    # so whatever is in the prefix we're always allowed to see\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a30ef3-9653-4ce0-9592-297fbde76c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True, False],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we need a triangle for the remaining Tq x Tq\n",
    "attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool))\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34918ffc-adaa-431a-a526-622228e15c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.4303,  0.9916,  0.2892, -0.8870, -0.5080],\n",
       "           [ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]]),\n",
       " tensor([[0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q_last_two_tokens, k, v, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786c3a5-2695-4da3-b95f-ddc3f0a9fecd",
   "metadata": {},
   "source": [
    "^ Yes, this matches the last two rows computed the \"normal\" way above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4fe253c-fc3a-4722-9e32-ec3c34c31824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and just to be sure, compute the attn_mask when prefix is 3\n",
    "Tq = q_last_token_only.size(2) # 1\n",
    "Tk = k.size(2) # 4\n",
    "attn_mask = torch.zeros((Tq,Tk), dtype=torch.bool)\n",
    "prefix_len = Tk - Tq # 3\n",
    "attn_mask[:, :prefix_len] = True\n",
    "attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool))\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98375a-ecf5-4ff3-868c-944a35735c21",
   "metadata": {},
   "source": [
    "^ Yes, that seems right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98153405-488d-4290-b1dd-b6cc839c577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.1842,  0.5891, -0.1506, -0.2712,  0.0022]]]]),\n",
       " tensor([[0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q_last_token_only, k, v, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6ecd6-880e-443d-983d-f31bb0172b16",
   "metadata": {},
   "source": [
    "^ and yes, that matches the last row\n",
    "\n",
    "(Later as I started to copy the code I saw that when Tq is 1 it does in fact skip forming a mask and call sdpa with causal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8778b83-04d9-4bc5-b8ad-1c928a37936d",
   "metadata": {},
   "source": [
    "So if this is the right idea, then I should see somewhere in engine generate() where it passes only the latest token rather than all previous tokens. (Or maybe latest token(s) if it's constructing multiple sequences in parallel say to do a beam search like I talk about [here](https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c/) and was used in the original 2017 paper.) Let's see...\n",
    "\n",
    "In the main generation loop in `engine.py` it does this:\n",
    "\n",
    "```\n",
    "logits = self.model.forward(ids, kv_cache=kv_cache_decode)\n",
    "```\n",
    "\n",
    "But what is `ids`?\n",
    "\n",
    "```\n",
    "...some code...\n",
    "\n",
    "token_column = [] # contains the next token id along each row\n",
    "\n",
    "...some code...\n",
    "\n",
    "ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)\n",
    "```\n",
    "\n",
    "So it does seem like that's what's happening. Each time we send in sequences of length 1. I can come back and confirm  / understand better once I have the engine code implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b419f4f-f229-4307-b1d0-016fbec9a3e2",
   "metadata": {},
   "source": [
    "### Add KV cache support to GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f8a97-90be-4b7e-bdf8-49e92506043b",
   "metadata": {},
   "source": [
    "I get the concept enough that I should be able to follow and hand copy the kv cache support stuff to `my_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c2be2-3f7e-4220-aaab-50986c50aec3",
   "metadata": {},
   "source": [
    "```\n",
    "@@ -48,7 +48,6 @@ class CausalSelfAttention(nn.Module):\n",
    "         self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    " \n",
    "     def forward(self, x, cos_sin, kv_cache=None):\n",
    "-        assert kv_cache is None # add support for this later\n",
    "         B, T, C = x.size()\n",
    " \n",
    "         q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "@@ -63,13 +62,36 @@ class CausalSelfAttention(nn.Module):\n",
    " \n",
    "         q, k, v = q.transpose(2,1), k.transpose(2,1), v.transpose(2,1) # (B,T,H,D) -> (B,H,T,D)\n",
    " \n",
    "+        # apply KV cache: insert current k,v into cache and get the full view so far\n",
    "+        if kv_cache is not None:\n",
    "+            k, v = kv_cache.insert_kv(self.layer_idx, k, v)\n",
    "+        Tq = q.size(2) # number of queries in this forward pass (I think will usually be 1)\n",
    "+        Tk = k.size(2) # number of keys/values in total (in the cache + in this forward pass)\n",
    "+\n",
    "         # code related to KV cache goes here\n",
    " \n",
    "         # will understand and add code for GQA later\n",
    "         assert self.n_head == self.n_kv_head\n",
    "         enable_gqa = self.n_head != self.n_kv_head # always false for now\n",
    " \n",
    "-        y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "+        # read notes in challenge-20-understand-engine/add-kv-cache-support-to-gpt.ipynb\n",
    "+        if kv_cache is None or Tq == Tk:\n",
    "+            y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "+        elif Tq == 1:\n",
    "+            # believe this is the common case during inference after the initial prompt is processed\n",
    "+            y = F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n",
    "+        else:\n",
    "+            attn_mask = torch.zeros((Tq,Tk), dtype=torch.bool, device=q.device) # True = keep\n",
    "+            prefix_len = Tk - Tq # 2\n",
    "+            if prefix_len > 0: # he says can't be negative but could be zero but don't think can be 0 due to above\n",
    "+                attn_mask[:, :prefix_len] = True    # so whatever is in the prefix is allowed\n",
    "+            attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool, device=q.device))\n",
    "+            # A \"square\" of Trues on the left and a \"triangle\" of Trues on the right, like\n",
    "+            # if Tk = 4 and Tq = 2 we'll end up with\n",
    "+            # True True True False\n",
    "+            # True True True True\n",
    "+            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)\n",
    "+\n",
    "         y = y.transpose(1,2).contiguous().view(B, T, -1)\n",
    "         y = self.c_proj(y)\n",
    "         return y\n",
    "@@ -193,7 +215,6 @@ class GPT(nn.Module):\n",
    " \n",
    " \n",
    "     def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n",
    "-        assert kv_cache is None # for now\n",
    "         \n",
    "         B, T = idx.size()\n",
    " \n",
    "@@ -201,7 +222,8 @@ class GPT(nn.Module):\n",
    "         assert idx.device == self.cos.device\n",
    "         assert self.cos.dtype == torch.bfloat16\n",
    " \n",
    "-        T0 = 0 # TODO T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "+        # if kv cache exists, we need to offset the rotary embeddings to the current position in the cache\n",
    "+        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "         cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb569-22d4-4543-8559-1c5e75d5fb82",
   "metadata": {},
   "source": [
    "Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796daae8-6fc3-4ab0-a527-0b64f84c925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "from my_nanochat.my_checkpoint_manager import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6969c4ae-9cd4-4600-bfc1-e49976c82c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=torch.get_default_device(), phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57210bb-a64e-4ed9-97dc-d7673a0cba1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536, 28466]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = tokenizer.encode('Hello', prepend=tokenizer.get_bos_token_id())\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9e03c9-0336-4528-a990-1a1bd05d1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StupidKVCache:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.pos = 0\n",
    "    \n",
    "    def insert_kv(self, layer_idx, k, v):\n",
    "        print(f\"about to cache layer: {layer_idx}, k.shape: {k.shape}\")\n",
    "        k_key = f\"layer_{layer_idx}_k\"\n",
    "        v_key = f\"layer_{layer_idx}_v\"\n",
    "        cached_k = self.cache.get(k_key)\n",
    "        cached_v = self.cache.get(v_key)\n",
    "        if cached_k is not None:\n",
    "            k = torch.concat((cached_k, k), dim=2)\n",
    "            v = torch.concat((cached_v, v), dim=2)\n",
    "        self.cache[k_key] = k\n",
    "        self.cache[v_key] = v\n",
    "        self.pos = k.shape[2]\n",
    "        print(f\"about to return k with shape {k.shape}\")\n",
    "        return k, v\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "\n",
    "kv_cache = StupidKVCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45b9e4a-b0b9-42b9-a5f0-baa7700d98e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to cache layer: 0, k.shape: torch.Size([1, 2, 2, 128])\n",
      "about to return k with shape torch.Size([1, 2, 2, 128])\n",
      "about to cache layer: 1, k.shape: torch.Size([1, 2, 2, 128])\n",
      "about to return k with shape torch.Size([1, 2, 2, 128])\n",
      "about to cache layer: 2, k.shape: torch.Size([1, 2, 2, 128])\n",
      "about to return k with shape torch.Size([1, 2, 2, 128])\n",
      "about to cache layer: 3, k.shape: torch.Size([1, 2, 2, 128])\n",
      "about to return k with shape torch.Size([1, 2, 2, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4920, -1.4920, -1.4920,  ..., -1.4920, -1.4920, -1.4920],\n",
       "         [-1.6174, -1.6174, -1.6174,  ..., -1.6174, -1.6174, -1.6174]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([prompt_tokens]), kv_cache=kv_cache)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258bc168-7a3c-4fa1-89be-a06a837bdd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits[0,-1]).item(); next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4214dfa6-8943-43d5-be9b-3a69514f6086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to cache layer: 0, k.shape: torch.Size([1, 2, 1, 128])\n",
      "about to return k with shape torch.Size([1, 2, 3, 128])\n",
      "about to cache layer: 1, k.shape: torch.Size([1, 2, 1, 128])\n",
      "about to return k with shape torch.Size([1, 2, 3, 128])\n",
      "about to cache layer: 2, k.shape: torch.Size([1, 2, 1, 128])\n",
      "about to return k with shape torch.Size([1, 2, 3, 128])\n",
      "about to cache layer: 3, k.shape: torch.Size([1, 2, 1, 128])\n",
      "about to return k with shape torch.Size([1, 2, 3, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4710, -2.4710, -2.4710,  ..., -2.4710, -2.4710, -2.4710]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([[next_token]]), kv_cache=kv_cache)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f75a91-40f1-424e-83d2-718efe952e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4920, -1.4920, -1.4920,  ..., -1.4920, -1.4920, -1.4920],\n",
       "         [-1.6174, -1.6174, -1.6174,  ..., -1.6174, -1.6174, -1.6174],\n",
       "         [-2.4710, -2.4710, -2.4710,  ..., -2.4710, -2.4710, -2.4710]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([prompt_tokens + [next_token]]), kv_cache=None)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e1a60-b69f-4f20-9373-d1a8d7cf8fbb",
   "metadata": {},
   "source": [
    "### \"Copy\" actual `KVCache` class\n",
    "\n",
    "I'll hand copy the KVCache class from [engine.py](https://github.com/karpathy/nanochat/blob/master/nanochat/engine.py) to  `my_engine.py` (will be first thing in the file)\n",
    "\n",
    "I'm guessing KVCache will be similar to the StupidKVCache I made above but more efficient, but there could be other things going on too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01f8cb5-806d-4ba6-920f-c0d095631bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# He uses this bit operation \"trick\" to round up to nearest multiple of 1024\n",
    "(4234 + 1023) & ~1023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51106639-2bf3-409c-90aa-d231c7cd0836",
   "metadata": {},
   "source": [
    "Try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19cef6a0-1310-4098-8876-a52a7d8a66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_engine import KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac880c2-994a-4278-a102-df2d46914f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_cache = KVCache(\n",
    "    batch_size=1,\n",
    "    num_heads=meta_data['model_config']['n_head'],\n",
    "    seq_len=100,\n",
    "    head_dim=meta_data['model_config']['n_embd'] // meta_data['model_config']['n_head'],\n",
    "    num_layers=meta_data['model_config']['n_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4350f016-2e0c-4b20-a5a1-4be49c66afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 1, 2, 100, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache.kv_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f578de2c-965e-47c8-8914-ec36bd6d3bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4920, -1.4920, -1.4920,  ..., -1.4920, -1.4920, -1.4920],\n",
       "         [-1.6174, -1.6174, -1.6174,  ..., -1.6174, -1.6174, -1.6174]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([prompt_tokens]), kv_cache=kv_cache)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a388c41f-9664-4fb5-82c9-1065229a4ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits[0,-1]).item(); next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdc3f47-095d-478f-8022-c0c8752ea9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4710, -2.4710, -2.4710,  ..., -2.4710, -2.4710, -2.4710]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([[next_token]]), kv_cache=kv_cache)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce505ae-1eed-49db-a1fb-a4e4bd64b003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4920, -1.4920, -1.4920,  ..., -1.4920, -1.4920, -1.4920],\n",
       "         [-1.6174, -1.6174, -1.6174,  ..., -1.6174, -1.6174, -1.6174],\n",
       "         [-2.4710, -2.4710, -2.4710,  ..., -2.4710, -2.4710, -2.4710]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(torch.tensor([prompt_tokens + [next_token]]), kv_cache=None)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9aa539-829d-40ec-9e09-fa50e5d22671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
