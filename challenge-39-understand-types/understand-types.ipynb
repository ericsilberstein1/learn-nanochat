{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc45ede-32cf-4b15-b858-44e54849a247",
   "metadata": {},
   "source": [
    "## Understand types\n",
    "\n",
    "On my mac I'm pretty sure that everything other than the inputs and targets are float32. On the GPU we're sometimes working with bfloat16 but I'm unsure with the autocasting if the interim tensors / activations are float32 or bfloat16.\n",
    "\n",
    "To figure this out without going crazy I'm hacking in some debug prints into `my_gpt.py` and `my_base_train.py` which I'll show here but won't commit. I'll then run a tiny training and summarize what I learn from the print statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d981d-ed19-43c0-8496-4deecd1efea5",
   "metadata": {},
   "source": [
    "### patch with the debug prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d31707cb-685a-4816-a821-a96b6ebcbbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdiff --git a/my_nanochat/my_nanochat/my_gpt.py b/my_nanochat/my_nanochat/my_gpt.py\u001b[m\n",
      "\u001b[1mindex 1cfb8bf..6b26ab8 100644\u001b[m\n",
      "\u001b[1m--- a/my_nanochat/my_nanochat/my_gpt.py\u001b[m\n",
      "\u001b[1m+++ b/my_nanochat/my_nanochat/my_gpt.py\u001b[m\n",
      "\u001b[36m@@ -116,8 +116,10 @@\u001b[m \u001b[mclass Block(nn.Module):\u001b[m\n",
      "         self.mlp = MLP(config)\u001b[m\n",
      " \u001b[m\n",
      "     def forward(self, x, cos_sin, kv_cache):\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"input to transformer block layer {self.attn.layer_idx} type is {x.dtype}\")\u001b[m\n",
      "         x = x + self.attn(norm(x), cos_sin, kv_cache)\u001b[m\n",
      "         x = x + self.mlp(norm(x))\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"output of transformer block layer {self.attn.layer_idx} type is {x.dtype}\")\u001b[m\n",
      "         return x\u001b[m\n",
      " \u001b[m\n",
      " class GPT(nn.Module):\u001b[m\n",
      "\u001b[36m@@ -233,18 +235,26 @@\u001b[m \u001b[mclass GPT(nn.Module):\u001b[m\n",
      "         T0 = 0 if kv_cache is None else kv_cache.get_pos()\u001b[m\n",
      "         cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]\u001b[m\n",
      " \u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"input to model type is {idx.dtype}\")\u001b[m\n",
      "         x = self.transformer.wte(idx)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"output of wte type is {x.dtype}\")\u001b[m\n",
      "         x = norm(x)\u001b[m\n",
      "         for block in self.transformer.h:\u001b[m\n",
      "             x = block(x, cos_sin, kv_cache)\u001b[m\n",
      "         x = norm(x)\u001b[m\n",
      " \u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"input to lm_head type is {x.dtype}\")\u001b[m\n",
      "         logits = self.lm_head(x)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"output of lm_head type is {logits.dtype}\")\u001b[m\n",
      "         softcap = 15\u001b[m\n",
      "         logits = softcap * torch.tanh(logits / softcap)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(f\"output of softcap type is {logits.dtype}\")\u001b[m\n",
      "         if targets is not None:\u001b[m\n",
      "             logits = logits.float()\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            print(f\"logits input to F.cross_entropy type is {logits.dtype}\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            print(f\"targets input to F.cross_entropy type is {targets.dtype}\")\u001b[m\n",
      "             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m            print(f\"loss type is {loss.dtype}\")\u001b[m\n",
      "             return loss\u001b[m\n",
      "         else:\u001b[m\n",
      "             return logits\u001b[m\n",
      "\u001b[1mdiff --git a/my_nanochat/scripts/my_base_train.py b/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[1mindex fd5c966..21beb50 100644\u001b[m\n",
      "\u001b[1m--- a/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[1m+++ b/my_nanochat/scripts/my_base_train.py\u001b[m\n",
      "\u001b[36m@@ -12,7 +12,7 @@\u001b[m \u001b[mfrom pathlib import Path\u001b[m\n",
      " \u001b[m\n",
      " import torch\u001b[m\n",
      " \u001b[m\n",
      "\u001b[31m-from my_nanochat.my_common import autodetect_device_type, compute_init, compute_cleanup, print0, get_base_dir, DummyWandb\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mfrom my_nanochat.my_common import autodetect_device_type, compute_init, compute_cleanup, print0, get_base_dir, DummyWandb, print_param_dtypes\u001b[m\n",
      " from my_nanochat.my_tokenizer import get_tokenizer, get_token_bytes\u001b[m\n",
      " from my_nanochat.my_gpt import GPTConfig, GPT\u001b[m\n",
      " from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\u001b[m\n",
      "\u001b[36m@@ -117,6 +117,14 @@\u001b[m \u001b[mwith torch.device(\"meta\"):\u001b[m\n",
      " print0(model)\u001b[m\n",
      " model.to_empty(device=device)\u001b[m\n",
      " model.init_weights()\u001b[m\n",
      "\u001b[32m+\u001b[m\n",
      "\u001b[32m+\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint(\"====param types====\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint_param_dtypes(model)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint(f\"model.cos type is {model.cos.dtype}\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint(f\"model.sin type is {model.sin.dtype}\")\u001b[m\n",
      "\u001b[32m+\u001b[m\n",
      "\u001b[32m+\u001b[m\n",
      " orig_model = model\u001b[m\n",
      " model = torch.compile(model, dynamic=False)\u001b[m\n",
      " num_params = sum(p.numel() for p in model.parameters())\u001b[m\n",
      "\u001b[36m@@ -214,21 +222,16 @@\u001b[m \u001b[mfor step in range(num_iterations+1):\u001b[m\n",
      " \u001b[m\n",
      "     if master_process and (last_step or (step > 0 and step % sample_every == 0)):\u001b[m\n",
      "         # once in a while sample from the model\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m        print(\"about to sample\")\u001b[m\n",
      "         model.eval()\u001b[m\n",
      "         prompts = [\u001b[m\n",
      "             \"The capital of France is\",\u001b[m\n",
      "\u001b[31m-            \"The chemical symbol of gold is\",\u001b[m\n",
      "\u001b[31m-            \"If yesterday was Friday, then tomorrow will be\",\u001b[m\n",
      "\u001b[31m-            \"The opposite of hot is\",\u001b[m\n",
      "\u001b[31m-            \"The planets of the solar system are:\",\u001b[m\n",
      "\u001b[31m-            \"My favorite color is\",\u001b[m\n",
      "\u001b[31m-            \"If 5*x + 3 = 13, then x is\",\u001b[m\n",
      "         ]\u001b[m\n",
      "         engine = Engine(orig_model, tokenizer)\u001b[m\n",
      "         for prompt in prompts:\u001b[m\n",
      "             tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\u001b[m\n",
      "             with autocast_ctx:\u001b[m\n",
      "\u001b[31m-                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=10, temperature=0)\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=1, temperature=0)\u001b[m\n",
      "             print0(tokenizer.decode(sample[0]))\u001b[m\n",
      "         model.train()\u001b[m\n",
      " \u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git diff ../my_nanochat/my_nanochat/my_gpt.py ../my_nanochat/scripts/my_base_train.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd214a0e-e05d-4ed4-8fc8-7e22cf1489c1",
   "metadata": {},
   "source": [
    "### first on my mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8634bb9-8bb4-4da2-a829-cad7b0193d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "710150e2-e4c1-4b10-b4d8-79fa4581ed18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 1\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_tokens = 128\n",
      "overriding core_metric_every = 0\n",
      "user_config: {'run': 'dummy', 'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 1, 'target_param_data_ratio': 20, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 250, 'eval_tokens': 128, 'core_metric_every': 0, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: mps\n",
      "This process is ddp_rank: 0, ddp_local_rank: 0, ddp_world_size: 1\n",
      "Vocab size: 65,536\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65536, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65536, bias=False)\n",
      ")\n",
      "====param types====\n",
      "transformer.wte.weight: dtype=torch.float32, shape=(65536, 256), device=mps:0\n",
      "transformer.h.0.attn.c_q.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.0.attn.c_k.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.0.attn.c_v.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.0.attn.c_proj.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.0.mlp.c_fc.weight: dtype=torch.float32, shape=(1024, 256), device=mps:0\n",
      "transformer.h.0.mlp.c_proj.weight: dtype=torch.float32, shape=(256, 1024), device=mps:0\n",
      "transformer.h.1.attn.c_q.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.1.attn.c_k.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.1.attn.c_v.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.1.attn.c_proj.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.1.mlp.c_fc.weight: dtype=torch.float32, shape=(1024, 256), device=mps:0\n",
      "transformer.h.1.mlp.c_proj.weight: dtype=torch.float32, shape=(256, 1024), device=mps:0\n",
      "transformer.h.2.attn.c_q.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.2.attn.c_k.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.2.attn.c_v.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.2.attn.c_proj.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.2.mlp.c_fc.weight: dtype=torch.float32, shape=(1024, 256), device=mps:0\n",
      "transformer.h.2.mlp.c_proj.weight: dtype=torch.float32, shape=(256, 1024), device=mps:0\n",
      "transformer.h.3.attn.c_q.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.3.attn.c_k.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.3.attn.c_v.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.3.attn.c_proj.weight: dtype=torch.float32, shape=(256, 256), device=mps:0\n",
      "transformer.h.3.mlp.c_fc.weight: dtype=torch.float32, shape=(1024, 256), device=mps:0\n",
      "transformer.h.3.mlp.c_proj.weight: dtype=torch.float32, shape=(256, 1024), device=mps:0\n",
      "lm_head.weight: dtype=torch.float32, shape=(65536, 256), device=mps:0\n",
      "model.cos type is torch.bfloat16\n",
      "model.sin type is torch.bfloat16\n",
      "Number of parameters: 36,700,160\n",
      "Estimated FLOPs per token: 1.211105e+08\n",
      "Using user-provided number of iterations: 1\n",
      "Total number of training tokens: 128\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Total training FLOPs estimate: 1.550215e+10\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "input to model type is torch.int32\n",
      "output of wte type is torch.float32\n",
      "input to transformer block layer 0 type is torch.float32\n",
      "output of transformer block layer 0 type is torch.float32\n",
      "input to transformer block layer 1 type is torch.float32\n",
      "output of transformer block layer 1 type is torch.float32\n",
      "input to transformer block layer 2 type is torch.float32\n",
      "output of transformer block layer 2 type is torch.float32\n",
      "input to transformer block layer 3 type is torch.float32\n",
      "output of transformer block layer 3 type is torch.float32\n",
      "input to lm_head type is torch.float32\n",
      "output of lm_head type is torch.float32\n",
      "output of softcap type is torch.float32\n",
      "logits input to F.cross_entropy type is torch.float32\n",
      "targets input to F.cross_entropy type is torch.int64\n",
      "loss type is torch.float32\n",
      "step 00000 | Validation bpb: 3.2303\n",
      "input to model type is torch.int32\n",
      "output of wte type is torch.float32\n",
      "input to transformer block layer 0 type is torch.float32\n",
      "output of transformer block layer 0 type is torch.float32\n",
      "input to transformer block layer 1 type is torch.float32\n",
      "output of transformer block layer 1 type is torch.float32\n",
      "input to transformer block layer 2 type is torch.float32\n",
      "output of transformer block layer 2 type is torch.float32\n",
      "input to transformer block layer 3 type is torch.float32\n",
      "output of transformer block layer 3 type is torch.float32\n",
      "input to lm_head type is torch.float32\n",
      "output of lm_head type is torch.float32\n",
      "output of softcap type is torch.float32\n",
      "logits input to F.cross_entropy type is torch.float32\n",
      "targets input to F.cross_entropy type is torch.int64\n",
      "loss type is torch.float32\n",
      "step 00000/00001 (0.00%) | loss: 11.090355 | grad norm: 1.5950 | lrm: 1.00 | dt: 1052.32ms | tok/sec: 121 | mfu: 0.00 | total time: 0.00m\n",
      "input to model type is torch.int32\n",
      "output of wte type is torch.float32\n",
      "input to transformer block layer 0 type is torch.float32\n",
      "output of transformer block layer 0 type is torch.float32\n",
      "input to transformer block layer 1 type is torch.float32\n",
      "output of transformer block layer 1 type is torch.float32\n",
      "input to transformer block layer 2 type is torch.float32\n",
      "output of transformer block layer 2 type is torch.float32\n",
      "input to transformer block layer 3 type is torch.float32\n",
      "output of transformer block layer 3 type is torch.float32\n",
      "input to lm_head type is torch.float32\n",
      "output of lm_head type is torch.float32\n",
      "output of softcap type is torch.float32\n",
      "logits input to F.cross_entropy type is torch.float32\n",
      "targets input to F.cross_entropy type is torch.int64\n",
      "loss type is torch.float32\n",
      "step 00001 | Validation bpb: 3.2238\n",
      "about to sample\n",
      "input to model type is torch.int64\n",
      "output of wte type is torch.float32\n",
      "input to transformer block layer 0 type is torch.float32\n",
      "output of transformer block layer 0 type is torch.float32\n",
      "input to transformer block layer 1 type is torch.float32\n",
      "output of transformer block layer 1 type is torch.float32\n",
      "input to transformer block layer 2 type is torch.float32\n",
      "output of transformer block layer 2 type is torch.float32\n",
      "input to transformer block layer 3 type is torch.float32\n",
      "output of transformer block layer 3 type is torch.float32\n",
      "input to lm_head type is torch.float32\n",
      "output of lm_head type is torch.float32\n",
      "output of softcap type is torch.float32\n",
      "<|bos|>The capital of France is an\n",
      "saved model to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/model_000001.pt\n",
      "saved metadata to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/meta_000001.json\n",
      "saved optimizer to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/optim_000001_rank0.pt\n",
      "Peak memory usage: 0.00MiB\n",
      "Total training time: 0.00m\n",
      "Minimum validation bpb: 3.2238\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=1 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_tokens=128 \\\n",
    "    --core_metric_every=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf4717-8213-4fe4-87ef-e45cf772ebca",
   "metadata": {},
   "source": [
    "#### what I see (on my mac / MPS)\n",
    "\n",
    "- all parameters are float32\n",
    "- precomputed cos and sin are bfloat16\n",
    "- model input (token ids) is int32 during training but int64 during sampling (noticed this earlier and don't understand it but can't imagine it matters and int32 is more than enough for our vocab size)\n",
    "- model targets (token ids) is int64\n",
    "- all other interim tensors appear to be float32 (output of wte, input to each layer, output of lm_head, loss)\n",
    "\n",
    "All is exactly as expected except I forgot that cos and sin are bfloat16 even on MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f2709-d9df-4b71-909b-daaece2efd73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
