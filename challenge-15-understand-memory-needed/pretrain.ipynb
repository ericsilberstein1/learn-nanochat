{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1393b7b-957b-41b6-ae94-b43b64a98dde",
   "metadata": {},
   "source": [
    "This is starting as a copy of the `baby-pretrain` notebook from challenge 14. I want to try it with bigger dimensions mostly just for fun before moving to the next thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84c42a9-3a3b-4eb7-ad9c-7efb24ec124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "import my_nanochat.my_tokenizer\n",
    "from my_nanochat.my_dataset import text_iterator\n",
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\n",
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3860e927-8740-4d7f-b988-2e3ab274e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac495a3-e877-495d-b066-1af8a5054f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39795aa5-fde4-4a1c-91ae-b2adac24cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type()\n",
    "device = device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e093329f-3b67-4eb6-a77a-d7661572bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "depth = 10\n",
    "max_seq_len = 400\n",
    "\n",
    "# training horizon\n",
    "num_iterations = 10_000\n",
    "\n",
    "# optimization (not sure why this section is called that yet)\n",
    "device_batch_size = 2\n",
    "total_batch_size = device_batch_size * max_seq_len\n",
    "\n",
    "# these next 4 are for the optimizers and we already saw them in setup_optimizers()\n",
    "embedding_lr = 0.2\n",
    "unembedding_lr = 0.004\n",
    "weight_decay = 0.0\n",
    "matrix_lr = 0.02\n",
    "\n",
    "grad_clip = 1.0\n",
    "\n",
    "# LR scheduler\n",
    "warmup_ratio = 0.0\n",
    "warmdown_ratio = 0.2\n",
    "final_lr_fraction = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa574717-4624-4113-9e7b-09ba15dbd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054930e0-0064-4716-87a0-47dbad221dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = my_nanochat.my_tokenizer.get_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6cdadd-1ac2-4f06-9a80-9e537af37cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 640, 5, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model kwargs are derived from desired depth of model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # so for example in the default in GPTConfig it's 12 * 64 = 768)\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads\n",
    "num_layers, model_dim, num_heads, num_kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c32fb3-7e8c-41aa-9c6d-5752184af4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "tokens_per_fwdbwd, grad_accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05ad7ee-b60a-4ff3-b211-75589efdc510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65537, 640)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x Block(\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (c_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (c_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (c_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=640, out_features=2560, bias=False)\n",
       "          (c_proj): Linear(in_features=2560, out_features=640, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=65537, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config_kwargs = dict(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=vocab_size, \n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads,\n",
    "    n_kv_head=num_kv_heads,\n",
    "    n_embd=model_dim,\n",
    ")\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8e26d9-80cd-416b-8019-0ee441ad0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807f534c-c7d3-4c6a-a2f7-9fdd8245e139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b533ab91-21f6-47cf-a866-581122e84184",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = model # original, uncompiled model -- looks like even in this minimal notebook we might use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504dd9a9-33eb-4443-ae6d-912297bf29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(65537, 640)\n",
       "      (h): ModuleList(\n",
       "        (0-9): 10 x Block(\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (c_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (c_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (c_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "          )\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=640, out_features=2560, bias=False)\n",
       "            (c_proj): Linear(in_features=2560, out_features=640, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=640, out_features=65537, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.compile(model, dynamic=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eefdb34-e1b4-429e-8422-a6d2fd68af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133039360"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = sum([param.numel() for param in model.parameters()])\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15caa09-d893-4c76-a052-b76d11ca5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = total_batch_size * num_iterations\n",
    "total_tokens # total number of training tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add089d9-3c14-44ac-a643-a24362dd75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(640/768) = 1.0954451150103321\n"
     ]
    }
   ],
   "source": [
    "# initialize optimizer\n",
    "optimizers = model.setup_optimizers(\n",
    "    unembedding_lr=unembedding_lr,\n",
    "    embedding_lr=embedding_lr,\n",
    "    matrix_lr=matrix_lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb4aa47-d3a1-449c-bbd0-275c46f01fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 400]), torch.Size([2, 400]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize DataLoader\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "x, y = next(train_loader)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a240c2a-40be-4dbe-83fc-fef60e87e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter scheulders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ad43dab-0257-453c-9844-03c6eea6f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_fraction\n",
    "\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85  + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff1f7-393e-4605-aef4-7c366c9ba390",
   "metadata": {},
   "source": [
    "### the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3866ca86-9232-4e6b-9d77-75acb9c5e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "W1106 20:06:12.325000 15362 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.090370178222656\n",
      "The person lead|CT| speed\n",
      "He went to ensure| place| grow\n",
      "1 + 2 = 90|19|4\n",
      "first of total| car| high\n",
      "3 cats and 2 percent|87|IT\n",
      "mom and heavy| weight| some\n",
      "the redOver|The|-large\n",
      "She |U|,\n",
      "step: 10, loss: 8.238045692443848\n",
      "step: 20, loss: 8.747520446777344\n",
      "step: 30, loss: 9.347489356994629\n",
      "step: 40, loss: 7.792864799499512\n",
      "step: 50, loss: 7.742763042449951\n",
      "step: 60, loss: 7.471697807312012\n",
      "step: 70, loss: 7.594133377075195\n",
      "step: 80, loss: 7.461619853973389\n",
      "step: 90, loss: 7.201691150665283\n",
      "step: 100, loss: 7.920388221740723\n",
      "step: 110, loss: 7.754680156707764\n",
      "step: 120, loss: 7.253090858459473\n",
      "step: 130, loss: 7.736426830291748\n",
      "step: 140, loss: 7.707849502563477\n",
      "step: 150, loss: 7.38935661315918\n",
      "step: 160, loss: 7.353476524353027\n",
      "step: 170, loss: 7.105140209197998\n",
      "step: 180, loss: 7.331638336181641\n",
      "step: 190, loss: 8.088733673095703\n",
      "step: 200, loss: 7.534080982208252\n",
      "step: 210, loss: 7.7223124504089355\n",
      "step: 220, loss: 7.705325126647949\n",
      "step: 230, loss: 7.352940559387207\n",
      "step: 240, loss: 7.023370742797852\n",
      "step: 250, loss: 8.161596298217773\n",
      "step: 260, loss: 7.242773056030273\n",
      "step: 270, loss: 7.207427978515625\n",
      "step: 280, loss: 7.62063455581665\n",
      "step: 290, loss: 6.905625820159912\n",
      "step: 300, loss: 7.285768032073975\n",
      "step: 310, loss: 7.113317966461182\n",
      "step: 320, loss: 7.2554850578308105\n",
      "step: 330, loss: 7.2983808517456055\n",
      "step: 340, loss: 6.795318126678467\n",
      "step: 350, loss: 6.631024360656738\n",
      "step: 360, loss: 7.285219192504883\n",
      "step: 370, loss: 7.054129123687744\n",
      "step: 380, loss: 7.449472427368164\n",
      "step: 390, loss: 7.111602783203125\n",
      "step: 400, loss: 7.150217056274414\n",
      "step: 410, loss: 7.173227310180664\n",
      "step: 420, loss: 6.479082107543945\n",
      "step: 430, loss: 7.763514518737793\n",
      "step: 440, loss: 7.083402633666992\n",
      "step: 450, loss: 7.099524021148682\n",
      "step: 460, loss: 7.001985549926758\n",
      "step: 470, loss: 6.638964653015137\n",
      "step: 480, loss: 7.189963340759277\n",
      "step: 490, loss: 6.941430568695068\n",
      "step: 500, loss: 7.431949615478516\n",
      "The person’s| of|,\n",
      "He went to the| their| a\n",
      "1 + 2 = 20|18|19\n",
      "first of the| a| \n",
      "3 cats and 2.|,| (\n",
      "mom and the| a| \n",
      "the red and|,| of\n",
      "She,|’s| to\n",
      "step: 510, loss: 7.019834041595459\n",
      "step: 520, loss: 6.911706447601318\n",
      "step: 530, loss: 6.769010543823242\n",
      "step: 540, loss: 6.94656229019165\n",
      "step: 550, loss: 6.32377815246582\n",
      "step: 560, loss: 6.9938459396362305\n",
      "step: 570, loss: 6.875359058380127\n",
      "step: 580, loss: 7.36468505859375\n",
      "step: 590, loss: 6.905540943145752\n",
      "step: 600, loss: 7.085590839385986\n",
      "step: 610, loss: 6.466914653778076\n",
      "step: 620, loss: 6.987246513366699\n",
      "step: 630, loss: 7.314425468444824\n",
      "step: 640, loss: 7.872539043426514\n",
      "step: 650, loss: 7.330202579498291\n",
      "step: 660, loss: 7.138403415679932\n",
      "step: 670, loss: 7.44938850402832\n",
      "step: 680, loss: 7.148147106170654\n",
      "step: 690, loss: 6.705793380737305\n",
      "step: 700, loss: 6.735728740692139\n",
      "step: 710, loss: 7.302958965301514\n",
      "step: 720, loss: 7.2974748611450195\n",
      "step: 730, loss: 7.017247200012207\n",
      "step: 740, loss: 6.940717697143555\n",
      "step: 750, loss: 7.151787281036377\n",
      "step: 760, loss: 7.751806735992432\n",
      "step: 770, loss: 6.99259090423584\n",
      "step: 780, loss: 7.640732288360596\n",
      "step: 790, loss: 6.9853739738464355\n",
      "step: 800, loss: 7.056346416473389\n",
      "step: 810, loss: 6.780733585357666\n",
      "step: 820, loss: 6.7773823738098145\n",
      "step: 830, loss: 6.953546047210693\n",
      "step: 840, loss: 7.431139945983887\n",
      "step: 850, loss: 6.703366756439209\n",
      "step: 860, loss: 6.916421890258789\n",
      "step: 870, loss: 6.886395454406738\n",
      "step: 880, loss: 6.3280744552612305\n",
      "step: 890, loss: 7.58072566986084\n",
      "step: 900, loss: 6.960451602935791\n",
      "step: 910, loss: 7.0532073974609375\n",
      "step: 920, loss: 6.859178066253662\n",
      "step: 930, loss: 7.275877475738525\n",
      "step: 940, loss: 7.962792873382568\n",
      "step: 950, loss: 6.938685417175293\n",
      "step: 960, loss: 6.890487194061279\n",
      "step: 970, loss: 6.742497444152832\n",
      "step: 980, loss: 6.557067394256592\n",
      "step: 990, loss: 7.111570835113525\n",
      "step: 1000, loss: 7.536683559417725\n",
      "The person is|,| will\n",
      "He went to the| a| to\n",
      "1 + 2 = 20|19|18\n",
      "first of the| |.\n",
      "3 cats and 2.|0|,\n",
      "mom and.|,| in\n",
      "the red of| and|,\n",
      "She is| will| says\n",
      "step: 1010, loss: 6.5286431312561035\n",
      "step: 1020, loss: 7.43983268737793\n",
      "step: 1030, loss: 6.67662239074707\n",
      "step: 1040, loss: 6.6354079246521\n",
      "step: 1050, loss: 6.984402656555176\n",
      "step: 1060, loss: 7.570146560668945\n",
      "step: 1070, loss: 6.447459697723389\n",
      "step: 1080, loss: 6.715646266937256\n",
      "step: 1090, loss: 6.251867294311523\n",
      "step: 1100, loss: 7.017857551574707\n",
      "step: 1110, loss: 6.2283148765563965\n",
      "step: 1120, loss: 6.67137336730957\n",
      "step: 1130, loss: 7.373276233673096\n",
      "step: 1140, loss: 7.1816582679748535\n",
      "step: 1150, loss: 6.852657318115234\n",
      "step: 1160, loss: 6.118610382080078\n",
      "step: 1170, loss: 5.977781772613525\n",
      "step: 1180, loss: 5.3972954750061035\n",
      "step: 1190, loss: 5.626321792602539\n",
      "step: 1200, loss: 6.456863880157471\n",
      "step: 1210, loss: 4.196139812469482\n",
      "step: 1220, loss: 7.170250415802002\n",
      "step: 1230, loss: 7.083829402923584\n",
      "step: 1240, loss: 7.5278401374816895\n",
      "step: 1250, loss: 7.230982780456543\n",
      "step: 1260, loss: 6.7856364250183105\n",
      "step: 1270, loss: 6.7527995109558105\n",
      "step: 1280, loss: 7.155144214630127\n",
      "step: 1290, loss: 6.507819652557373\n",
      "step: 1300, loss: 6.876920223236084\n",
      "step: 1310, loss: 6.943631649017334\n",
      "step: 1320, loss: 6.657540321350098\n",
      "step: 1330, loss: 6.997354507446289\n",
      "step: 1340, loss: 6.806988716125488\n",
      "step: 1350, loss: 8.607219696044922\n",
      "step: 1360, loss: 6.439420700073242\n",
      "step: 1370, loss: 8.52959156036377\n",
      "step: 1380, loss: 6.745211601257324\n",
      "step: 1390, loss: 7.173436164855957\n",
      "step: 1400, loss: 7.2950897216796875\n",
      "step: 1410, loss: 6.8176422119140625\n",
      "step: 1420, loss: 7.253201484680176\n",
      "step: 1430, loss: 6.802973747253418\n",
      "step: 1440, loss: 6.968948841094971\n",
      "step: 1450, loss: 6.9026994705200195\n",
      "step: 1460, loss: 8.604819297790527\n",
      "step: 1470, loss: 7.478945255279541\n",
      "step: 1480, loss: 7.785901069641113\n",
      "step: 1490, loss: 7.498899459838867\n",
      "step: 1500, loss: 6.972707748413086\n",
      "The person was|’s|'s\n",
      "He went to the| a| his\n",
      "1 + 2 = 3|19|20\n",
      "first of the| |.\n",
      "3 cats and 2.|,| to\n",
      "mom and the| | by\n",
      "the red and| |,\n",
      "She was| had| became\n",
      "step: 1510, loss: 6.469581127166748\n",
      "step: 1520, loss: 7.736010551452637\n",
      "step: 1530, loss: 6.997460842132568\n",
      "step: 1540, loss: 6.318746566772461\n",
      "step: 1550, loss: 6.636544704437256\n",
      "step: 1560, loss: 6.7099714279174805\n",
      "step: 1570, loss: 6.386928558349609\n",
      "step: 1580, loss: 7.0218305587768555\n",
      "step: 1590, loss: 6.140329360961914\n",
      "step: 1600, loss: 4.9458088874816895\n",
      "step: 1610, loss: 7.081854343414307\n",
      "step: 1620, loss: 6.955338954925537\n",
      "step: 1630, loss: 6.956641674041748\n",
      "step: 1640, loss: 6.384449005126953\n",
      "step: 1650, loss: 6.40544319152832\n",
      "step: 1660, loss: 6.534079074859619\n",
      "step: 1670, loss: 5.951745510101318\n",
      "step: 1680, loss: 7.348547458648682\n",
      "step: 1690, loss: 6.9171319007873535\n",
      "step: 1700, loss: 6.183746337890625\n",
      "step: 1710, loss: 7.374696731567383\n",
      "step: 1720, loss: 6.698504447937012\n",
      "step: 1730, loss: 6.8590569496154785\n",
      "step: 1740, loss: 7.193913459777832\n",
      "step: 1750, loss: 6.594862461090088\n",
      "step: 1760, loss: 7.182109355926514\n",
      "step: 1770, loss: 6.414287090301514\n",
      "step: 1780, loss: 6.858199596405029\n",
      "step: 1790, loss: 6.590997219085693\n",
      "step: 1800, loss: 7.094358921051025\n",
      "step: 1810, loss: 7.682513236999512\n",
      "step: 1820, loss: 6.378385066986084\n",
      "step: 1830, loss: 6.689882278442383\n",
      "step: 1840, loss: 6.779489040374756\n",
      "step: 1850, loss: 8.169963836669922\n",
      "step: 1860, loss: 7.067011833190918\n",
      "step: 1870, loss: 7.227401256561279\n",
      "step: 1880, loss: 7.304328441619873\n",
      "step: 1890, loss: 6.981115818023682\n",
      "step: 1900, loss: 6.898069858551025\n",
      "step: 1910, loss: 6.450550556182861\n",
      "step: 1920, loss: 7.980555534362793\n",
      "step: 1930, loss: 7.152492046356201\n",
      "step: 1940, loss: 6.7691192626953125\n",
      "step: 1950, loss: 7.501183986663818\n",
      "step: 1960, loss: 5.991297721862793\n",
      "step: 1970, loss: 6.430424690246582\n",
      "step: 1980, loss: 5.28939962387085\n",
      "step: 1990, loss: 7.070586681365967\n",
      "step: 2000, loss: 7.033204555511475\n",
      "The person is| was|.\n",
      "He went to the|.| to\n",
      "1 + 2 = 19|20|0\n",
      "first of the|.|,\n",
      "3 cats and 2.|,| million\n",
      "mom and the|.|,\n",
      "the red.|,| the\n",
      "She says| is| was\n",
      "step: 2010, loss: 6.759738922119141\n",
      "step: 2020, loss: 7.902956485748291\n",
      "step: 2030, loss: 6.419534683227539\n",
      "step: 2040, loss: 7.547116756439209\n",
      "step: 2050, loss: 6.599480152130127\n",
      "step: 2060, loss: 6.346796989440918\n",
      "step: 2070, loss: 6.742011547088623\n",
      "step: 2080, loss: 6.961316108703613\n",
      "step: 2090, loss: 6.943603515625\n",
      "step: 2100, loss: 6.535406112670898\n",
      "step: 2110, loss: 7.094972133636475\n",
      "step: 2120, loss: 7.057878494262695\n",
      "step: 2130, loss: 6.611396312713623\n",
      "step: 2140, loss: 7.081957817077637\n",
      "step: 2150, loss: 6.730347156524658\n",
      "step: 2160, loss: 6.743890762329102\n",
      "step: 2170, loss: 6.919010639190674\n",
      "step: 2180, loss: 7.423965930938721\n",
      "step: 2190, loss: 7.012004375457764\n",
      "step: 2200, loss: 6.917738437652588\n",
      "step: 2210, loss: 6.363839626312256\n",
      "step: 2220, loss: 6.279476165771484\n",
      "step: 2230, loss: 6.778206825256348\n",
      "step: 2240, loss: 6.474222183227539\n",
      "step: 2250, loss: 6.7631120681762695\n",
      "step: 2260, loss: 6.901297569274902\n",
      "step: 2270, loss: 8.39892292022705\n",
      "step: 2280, loss: 6.6057353019714355\n",
      "step: 2290, loss: 6.655195236206055\n",
      "step: 2300, loss: 6.191534996032715\n",
      "step: 2310, loss: 5.876229763031006\n",
      "step: 2320, loss: 6.202495098114014\n",
      "step: 2330, loss: 6.884979248046875\n",
      "step: 2340, loss: 6.611632823944092\n",
      "step: 2350, loss: 6.509990692138672\n",
      "step: 2360, loss: 6.701921463012695\n",
      "step: 2370, loss: 6.50385856628418\n",
      "step: 2380, loss: 6.517055034637451\n",
      "step: 2390, loss: 6.528860092163086\n",
      "step: 2400, loss: 6.179884433746338\n",
      "step: 2410, loss: 6.575358867645264\n",
      "step: 2420, loss: 7.0025634765625\n",
      "step: 2430, loss: 6.652090072631836\n",
      "step: 2440, loss: 6.933389663696289\n",
      "step: 2450, loss: 6.720522880554199\n",
      "step: 2460, loss: 6.771252632141113\n",
      "step: 2470, loss: 6.913381576538086\n",
      "step: 2480, loss: 6.510829925537109\n",
      "step: 2490, loss: 6.7281599044799805\n",
      "step: 2500, loss: 6.892333984375\n",
      "The person was| is|’s\n",
      "He went to the| be| a\n",
      "1 + 2 = 20|1|2\n",
      "first of the| a| this\n",
      "3 cats and 2%|.|,\n",
      "mom and\n",
      "|,| the\n",
      "the red was|,|.\n",
      "She said| wrote| was\n",
      "step: 2510, loss: 6.800779819488525\n",
      "step: 2520, loss: 6.613411903381348\n",
      "step: 2530, loss: 7.101185321807861\n",
      "step: 2540, loss: 6.30415153503418\n",
      "step: 2550, loss: 6.341692924499512\n",
      "step: 2560, loss: 7.015518665313721\n",
      "step: 2570, loss: 7.277250289916992\n",
      "step: 2580, loss: 6.441181659698486\n",
      "step: 2590, loss: 6.910006046295166\n",
      "step: 2600, loss: 6.493342399597168\n",
      "step: 2610, loss: 6.186025619506836\n",
      "step: 2620, loss: 6.812401294708252\n",
      "step: 2630, loss: 7.119007587432861\n",
      "step: 2640, loss: 6.501363754272461\n",
      "step: 2650, loss: 6.461391448974609\n",
      "step: 2660, loss: 6.3480095863342285\n",
      "step: 2670, loss: 5.938236236572266\n",
      "step: 2680, loss: 6.047064304351807\n",
      "step: 2690, loss: 7.362529277801514\n",
      "step: 2700, loss: 6.466344833374023\n",
      "step: 2710, loss: 6.967179775238037\n",
      "step: 2720, loss: 6.411195755004883\n",
      "step: 2730, loss: 6.299183368682861\n",
      "step: 2740, loss: 6.849347114562988\n",
      "step: 2750, loss: 6.494578838348389\n",
      "step: 2760, loss: 6.731197357177734\n",
      "step: 2770, loss: 7.059373378753662\n",
      "step: 2780, loss: 6.262274265289307\n",
      "step: 2790, loss: 6.4946608543396\n",
      "step: 2800, loss: 7.630767822265625\n",
      "step: 2810, loss: 6.481259822845459\n",
      "step: 2820, loss: 6.607702255249023\n",
      "step: 2830, loss: 7.085178375244141\n",
      "step: 2840, loss: 6.914583683013916\n",
      "step: 2850, loss: 6.476967811584473\n",
      "step: 2860, loss: 6.467357158660889\n",
      "step: 2870, loss: 6.798385143280029\n",
      "step: 2880, loss: 6.314630031585693\n",
      "step: 2890, loss: 6.448221206665039\n",
      "step: 2900, loss: 7.261946201324463\n",
      "step: 2910, loss: 5.935266494750977\n",
      "step: 2920, loss: 7.184854507446289\n",
      "step: 2930, loss: 6.831311225891113\n",
      "step: 2940, loss: 6.710700035095215\n",
      "step: 2950, loss: 6.52058219909668\n",
      "step: 2960, loss: 6.992745399475098\n",
      "step: 2970, loss: 5.741575241088867\n",
      "step: 2980, loss: 5.249528408050537\n",
      "step: 2990, loss: 6.215974807739258\n",
      "step: 3000, loss: 7.081501483917236\n",
      "The person is| | with\n",
      "He went to his| him| the\n",
      "1 + 2 = 10|20|5\n",
      "first of the| his| a\n",
      "3 cats and 2.|.\n",
      "| of\n",
      "mom and the| his| a\n",
      "the red is|,|.\n",
      "She was| is| might\n",
      "step: 3010, loss: 7.051990509033203\n",
      "step: 3020, loss: 6.442159652709961\n",
      "step: 3030, loss: 6.155858039855957\n",
      "step: 3040, loss: 6.759181022644043\n",
      "step: 3050, loss: 6.662711143493652\n",
      "step: 3060, loss: 6.996664047241211\n",
      "step: 3070, loss: 6.304541110992432\n",
      "step: 3080, loss: 7.231873035430908\n",
      "step: 3090, loss: 7.077314853668213\n",
      "step: 3100, loss: 6.603762149810791\n",
      "step: 3110, loss: 6.215988636016846\n",
      "step: 3120, loss: 6.816476345062256\n",
      "step: 3130, loss: 6.941955089569092\n",
      "step: 3140, loss: 6.1950788497924805\n",
      "step: 3150, loss: 6.9559502601623535\n",
      "step: 3160, loss: 6.592402458190918\n",
      "step: 3170, loss: 7.073987007141113\n",
      "step: 3180, loss: 6.665097713470459\n",
      "step: 3190, loss: 6.6156182289123535\n",
      "step: 3200, loss: 6.505573749542236\n",
      "step: 3210, loss: 6.887036323547363\n",
      "step: 3220, loss: 6.534107685089111\n",
      "step: 3230, loss: 6.517974853515625\n",
      "step: 3240, loss: 7.200578689575195\n",
      "step: 3250, loss: 6.1456074714660645\n",
      "step: 3260, loss: 5.891292572021484\n",
      "step: 3270, loss: 6.283183574676514\n",
      "step: 3280, loss: 6.5288543701171875\n",
      "step: 3290, loss: 6.585960865020752\n",
      "step: 3300, loss: 6.565916538238525\n",
      "step: 3310, loss: 6.811216831207275\n",
      "step: 3320, loss: 6.446301460266113\n",
      "step: 3330, loss: 6.129921913146973\n",
      "step: 3340, loss: 6.470482349395752\n",
      "step: 3350, loss: 6.651689529418945\n",
      "step: 3360, loss: 6.617358207702637\n",
      "step: 3370, loss: 6.774747848510742\n",
      "step: 3380, loss: 6.224668979644775\n",
      "step: 3390, loss: 6.7122673988342285\n",
      "step: 3400, loss: 7.028109550476074\n",
      "step: 3410, loss: 6.487430572509766\n",
      "step: 3420, loss: 6.520622730255127\n",
      "step: 3430, loss: 6.637959003448486\n",
      "step: 3440, loss: 6.735495567321777\n",
      "step: 3450, loss: 6.210594654083252\n",
      "step: 3460, loss: 6.362893104553223\n",
      "step: 3470, loss: 6.327869892120361\n",
      "step: 3480, loss: 6.332888126373291\n",
      "step: 3490, loss: 6.283808708190918\n",
      "step: 3500, loss: 6.319145679473877\n",
      "The person is| was|'s\n",
      "He went to his| the| him\n",
      "1 + 2 = 20|5|2\n",
      "first of the| a| which\n",
      "3 cats and 2.|.\n",
      "|,\n",
      "mom and the| other| a\n",
      "the red,|.| meat\n",
      "She said| was| had\n",
      "step: 3510, loss: 6.081087589263916\n",
      "step: 3520, loss: 6.283233642578125\n",
      "step: 3530, loss: 6.275558948516846\n",
      "step: 3540, loss: 6.348175525665283\n",
      "step: 3550, loss: 6.482809066772461\n",
      "step: 3560, loss: 6.187803745269775\n",
      "step: 3570, loss: 6.60118293762207\n",
      "step: 3580, loss: 6.506081581115723\n",
      "step: 3590, loss: 6.412652492523193\n",
      "step: 3600, loss: 6.170406341552734\n",
      "step: 3610, loss: 6.382744312286377\n",
      "step: 3620, loss: 6.138068675994873\n",
      "step: 3630, loss: 6.505395889282227\n",
      "step: 3640, loss: 6.074650764465332\n",
      "step: 3650, loss: 6.244598388671875\n",
      "step: 3660, loss: 6.363702297210693\n",
      "step: 3670, loss: 6.601797103881836\n",
      "step: 3680, loss: 6.333322525024414\n",
      "step: 3690, loss: 6.207552909851074\n",
      "step: 3700, loss: 6.242650032043457\n",
      "step: 3710, loss: 6.543163299560547\n",
      "step: 3720, loss: 5.998953819274902\n",
      "step: 3730, loss: 6.7797441482543945\n",
      "step: 3740, loss: 6.615053653717041\n",
      "step: 3750, loss: 6.397519588470459\n",
      "step: 3760, loss: 6.385591983795166\n",
      "step: 3770, loss: 5.738852024078369\n",
      "step: 3780, loss: 6.97308349609375\n",
      "step: 3790, loss: 6.658819675445557\n",
      "step: 3800, loss: 7.057702541351318\n",
      "step: 3810, loss: 6.61428689956665\n",
      "step: 3820, loss: 6.85297966003418\n",
      "step: 3830, loss: 6.591394424438477\n",
      "step: 3840, loss: 6.393270492553711\n",
      "step: 3850, loss: 6.311346530914307\n",
      "step: 3860, loss: 6.249274253845215\n",
      "step: 3870, loss: 6.238996505737305\n",
      "step: 3880, loss: 6.453248500823975\n",
      "step: 3890, loss: 6.615419864654541\n",
      "step: 3900, loss: 6.54550313949585\n",
      "step: 3910, loss: 6.306285381317139\n",
      "step: 3920, loss: 6.190316200256348\n",
      "step: 3930, loss: 6.663390636444092\n",
      "step: 3940, loss: 5.936773777008057\n",
      "step: 3950, loss: 7.060154438018799\n",
      "step: 3960, loss: 5.737141132354736\n",
      "step: 3970, loss: 6.243354320526123\n",
      "step: 3980, loss: 6.984797954559326\n",
      "step: 3990, loss: 6.611153602600098\n",
      "step: 4000, loss: 6.279111385345459\n",
      "The person is|’s| has\n",
      "He went to a| the| him\n",
      "1 + 2 = 20|1|3\n",
      "first of the| | a\n",
      "3 cats and 2.|,|.\n",
      "\n",
      "mom and the| a| in\n",
      "the red,| and|.\n",
      "She said| did| has\n",
      "step: 4010, loss: 6.590152740478516\n",
      "step: 4020, loss: 5.485510349273682\n",
      "step: 4030, loss: 6.773175239562988\n",
      "step: 4040, loss: 6.6157546043396\n",
      "step: 4050, loss: 7.165738582611084\n",
      "step: 4060, loss: 7.3656744956970215\n",
      "step: 4070, loss: 6.947969913482666\n",
      "step: 4080, loss: 6.357465744018555\n",
      "step: 4090, loss: 6.346376895904541\n",
      "step: 4100, loss: 7.162847995758057\n",
      "step: 4110, loss: 6.433159351348877\n",
      "step: 4120, loss: 6.110918998718262\n",
      "step: 4130, loss: 6.874795436859131\n",
      "step: 4140, loss: 7.264831066131592\n",
      "step: 4150, loss: 6.419824600219727\n",
      "step: 4160, loss: 6.630372524261475\n",
      "step: 4170, loss: 6.251309394836426\n",
      "step: 4180, loss: 6.243597507476807\n",
      "step: 4190, loss: 5.4413580894470215\n",
      "step: 4200, loss: 6.414487361907959\n",
      "step: 4210, loss: 6.140714168548584\n",
      "step: 4220, loss: 6.17185115814209\n",
      "step: 4230, loss: 6.191765785217285\n",
      "step: 4240, loss: 6.680423736572266\n",
      "step: 4250, loss: 6.610326766967773\n",
      "step: 4260, loss: 6.404397964477539\n",
      "step: 4270, loss: 6.741041660308838\n",
      "step: 4280, loss: 7.046918869018555\n",
      "step: 4290, loss: 6.561845779418945\n",
      "step: 4300, loss: 6.865779399871826\n",
      "step: 4310, loss: 5.662898063659668\n",
      "step: 4320, loss: 6.434635639190674\n",
      "step: 4330, loss: 5.951862812042236\n",
      "step: 4340, loss: 5.8862175941467285\n",
      "step: 4350, loss: 6.321217060089111\n",
      "step: 4360, loss: 6.617128849029541\n",
      "step: 4370, loss: 6.223506927490234\n",
      "step: 4380, loss: 6.167901992797852\n",
      "step: 4390, loss: 6.154508590698242\n",
      "step: 4400, loss: 6.197930812835693\n",
      "step: 4410, loss: 6.272543907165527\n",
      "step: 4420, loss: 6.0566487312316895\n",
      "step: 4430, loss: 4.939113140106201\n",
      "step: 4440, loss: 6.408599853515625\n",
      "step: 4450, loss: 6.059179782867432\n",
      "step: 4460, loss: 6.769647121429443\n",
      "step: 4470, loss: 5.868524074554443\n",
      "step: 4480, loss: 7.032283782958984\n",
      "step: 4490, loss: 6.688446521759033\n",
      "step: 4500, loss: 7.46115255355835\n",
      "The person’s| is|,\n",
      "He went to the| a| be\n",
      "1 + 2 = 20|19|1\n",
      "first of the| Australia| Northern\n",
      "3 cats and 2,| (|.\n",
      "mom and a| the| in\n",
      "the red,| season| town\n",
      "She said|.| is\n",
      "step: 4510, loss: 6.586301326751709\n",
      "step: 4520, loss: 6.187237739562988\n",
      "step: 4530, loss: 6.37291145324707\n",
      "step: 4540, loss: 6.740514755249023\n",
      "step: 4550, loss: 6.564528942108154\n",
      "step: 4560, loss: 5.988970756530762\n",
      "step: 4570, loss: 7.060153961181641\n",
      "step: 4580, loss: 5.962202072143555\n",
      "step: 4590, loss: 7.748015880584717\n",
      "step: 4600, loss: 6.576617240905762\n",
      "step: 4610, loss: 5.819216728210449\n",
      "step: 4620, loss: 6.129471302032471\n",
      "step: 4630, loss: 6.92491340637207\n",
      "step: 4640, loss: 6.220639705657959\n",
      "step: 4650, loss: 5.782556056976318\n",
      "step: 4660, loss: 5.712136268615723\n",
      "step: 4670, loss: 4.891283988952637\n",
      "step: 4680, loss: 6.587591171264648\n",
      "step: 4690, loss: 6.368072509765625\n",
      "step: 4700, loss: 6.144697189331055\n",
      "step: 4710, loss: 6.841662406921387\n",
      "step: 4720, loss: 6.376854419708252\n",
      "step: 4730, loss: 6.259788990020752\n",
      "step: 4740, loss: 6.440023422241211\n",
      "step: 4750, loss: 6.332332611083984\n",
      "step: 4760, loss: 6.653730392456055\n",
      "step: 4770, loss: 6.564924716949463\n",
      "step: 4780, loss: 5.886598110198975\n",
      "step: 4790, loss: 6.251510143280029\n",
      "step: 4800, loss: 6.490942478179932\n",
      "step: 4810, loss: 5.935208797454834\n",
      "step: 4820, loss: 7.799414157867432\n",
      "step: 4830, loss: 6.189965724945068\n",
      "step: 4840, loss: 6.528347015380859\n",
      "step: 4850, loss: 6.03875732421875\n",
      "step: 4860, loss: 5.646137714385986\n",
      "step: 4870, loss: 5.049544811248779\n",
      "step: 4880, loss: 5.50213623046875\n",
      "step: 4890, loss: 6.296547889709473\n",
      "step: 4900, loss: 6.394354820251465\n",
      "step: 4910, loss: 6.387364387512207\n",
      "step: 4920, loss: 6.459814548492432\n",
      "step: 4930, loss: 5.984115123748779\n",
      "step: 4940, loss: 6.382558822631836\n",
      "step: 4950, loss: 6.505377292633057\n",
      "step: 4960, loss: 6.252291202545166\n",
      "step: 4970, loss: 6.358851432800293\n",
      "step: 4980, loss: 6.108692646026611\n",
      "step: 4990, loss: 6.785531044006348\n",
      "step: 5000, loss: 6.77926778793335\n",
      "The person of|'s|,\n",
      "He went to the| his| a\n",
      "1 + 2 = 1|5|2\n",
      "first of the| their| \n",
      "3 cats and 2.|,|.\n",
      "\n",
      "mom and the| a| their\n",
      "the red,| of| and\n",
      "She said|,| He\n",
      "step: 5010, loss: 6.14890193939209\n",
      "step: 5020, loss: 6.96943473815918\n",
      "step: 5030, loss: 6.492530345916748\n",
      "step: 5040, loss: 6.899307727813721\n",
      "step: 5050, loss: 6.572416305541992\n",
      "step: 5060, loss: 6.303421497344971\n",
      "step: 5070, loss: 6.269832134246826\n",
      "step: 5080, loss: 5.7080159187316895\n",
      "step: 5090, loss: 6.091613292694092\n",
      "step: 5100, loss: 5.853611946105957\n",
      "step: 5110, loss: 6.346268177032471\n",
      "step: 5120, loss: 6.32682991027832\n",
      "step: 5130, loss: 6.583498477935791\n",
      "step: 5140, loss: 6.277001857757568\n",
      "step: 5150, loss: 6.784821033477783\n",
      "step: 5160, loss: 6.405205249786377\n",
      "step: 5170, loss: 6.214461803436279\n",
      "step: 5180, loss: 6.580617904663086\n",
      "step: 5190, loss: 6.522598743438721\n",
      "step: 5200, loss: 6.107051849365234\n",
      "step: 5210, loss: 5.495396614074707\n",
      "step: 5220, loss: 5.702785491943359\n",
      "step: 5230, loss: 6.471396446228027\n",
      "step: 5240, loss: 6.193789005279541\n",
      "step: 5250, loss: 5.98583984375\n",
      "step: 5260, loss: 6.443621635437012\n",
      "step: 5270, loss: 6.420274257659912\n",
      "step: 5280, loss: 5.9275617599487305\n",
      "step: 5290, loss: 5.9998860359191895\n",
      "step: 5300, loss: 7.141304969787598\n",
      "step: 5310, loss: 5.93710994720459\n",
      "step: 5320, loss: 5.4772491455078125\n",
      "step: 5330, loss: 7.033823013305664\n",
      "step: 5340, loss: 5.876170635223389\n",
      "step: 5350, loss: 6.206387519836426\n",
      "step: 5360, loss: 6.171142578125\n",
      "step: 5370, loss: 6.590980052947998\n",
      "step: 5380, loss: 6.644920825958252\n",
      "step: 5390, loss: 6.494053363800049\n",
      "step: 5400, loss: 6.121843338012695\n",
      "step: 5410, loss: 7.274600982666016\n",
      "step: 5420, loss: 6.511553764343262\n",
      "step: 5430, loss: 6.2042083740234375\n",
      "step: 5440, loss: 6.56037712097168\n",
      "step: 5450, loss: 6.288514614105225\n",
      "step: 5460, loss: 6.224774360656738\n",
      "step: 5470, loss: 6.3246750831604\n",
      "step: 5480, loss: 6.551959037780762\n",
      "step: 5490, loss: 6.134404182434082\n",
      "step: 5500, loss: 5.229564189910889\n",
      "The person was| who|'s\n",
      "He went to the| a| his\n",
      "1 + 2 = 1|3|2\n",
      "first of the| a| \n",
      "3 cats and 2.|,|nd\n",
      "mom and the| a| some\n",
      "the red,| and| to\n",
      "She,| was| says\n",
      "step: 5510, loss: 6.060755729675293\n",
      "step: 5520, loss: 6.171226978302002\n",
      "step: 5530, loss: 6.244674205780029\n",
      "step: 5540, loss: 5.524652481079102\n",
      "step: 5550, loss: 6.77336311340332\n",
      "step: 5560, loss: 5.707638740539551\n",
      "step: 5570, loss: 6.806185722351074\n",
      "step: 5580, loss: 5.918823719024658\n",
      "step: 5590, loss: 7.296141147613525\n",
      "step: 5600, loss: 6.337982177734375\n",
      "step: 5610, loss: 5.8225998878479\n",
      "step: 5620, loss: 6.858964920043945\n",
      "step: 5630, loss: 6.529562473297119\n",
      "step: 5640, loss: 6.118185997009277\n",
      "step: 5650, loss: 7.264410495758057\n",
      "step: 5660, loss: 6.771417140960693\n",
      "step: 5670, loss: 6.742471694946289\n",
      "step: 5680, loss: 5.981689453125\n",
      "step: 5690, loss: 6.363890171051025\n",
      "step: 5700, loss: 6.844715595245361\n",
      "step: 5710, loss: 6.255076885223389\n",
      "step: 5720, loss: 6.237515926361084\n",
      "step: 5730, loss: 6.157139301300049\n",
      "step: 5740, loss: 6.64249324798584\n",
      "step: 5750, loss: 6.926455020904541\n",
      "step: 5760, loss: 6.214749813079834\n",
      "step: 5770, loss: 6.3932085037231445\n",
      "step: 5780, loss: 5.851362705230713\n",
      "step: 5790, loss: 6.197526931762695\n",
      "step: 5800, loss: 5.693558216094971\n",
      "step: 5810, loss: 6.08647346496582\n",
      "step: 5820, loss: 6.25631856918335\n",
      "step: 5830, loss: 6.3688459396362305\n",
      "step: 5840, loss: 6.087679386138916\n",
      "step: 5850, loss: 6.833268642425537\n",
      "step: 5860, loss: 6.997274398803711\n",
      "step: 5870, loss: 7.047886848449707\n",
      "step: 5880, loss: 6.559511661529541\n",
      "step: 5890, loss: 6.101459980010986\n",
      "step: 5900, loss: 6.627591133117676\n",
      "step: 5910, loss: 6.290881156921387\n",
      "step: 5920, loss: 6.537010669708252\n",
      "step: 5930, loss: 5.897956371307373\n",
      "step: 5940, loss: 5.745241641998291\n",
      "step: 5950, loss: 6.343601703643799\n",
      "step: 5960, loss: 5.853635311126709\n",
      "step: 5970, loss: 6.857323169708252\n",
      "step: 5980, loss: 6.379326343536377\n",
      "step: 5990, loss: 6.1618194580078125\n",
      "step: 6000, loss: 5.654857158660889\n",
      "The person of|,| and\n",
      "He went to meet| give| the\n",
      "1 + 2 = 2|1|5\n",
      "first of the| a| their\n",
      "3 cats and 2.|,|%\n",
      "mom and the| P| C\n",
      "the red,| and| of\n",
      "She,|'s|.\n",
      "step: 6010, loss: 5.101396083831787\n",
      "step: 6020, loss: 7.385014533996582\n",
      "step: 6030, loss: 6.172546863555908\n",
      "step: 6040, loss: 5.9115681648254395\n",
      "step: 6050, loss: 6.744725227355957\n",
      "step: 6060, loss: 6.437579154968262\n",
      "step: 6070, loss: 6.079747200012207\n",
      "step: 6080, loss: 5.6631760597229\n",
      "step: 6090, loss: 7.773399829864502\n",
      "step: 6100, loss: 5.448749542236328\n",
      "step: 6110, loss: 5.87554931640625\n",
      "step: 6120, loss: 6.1743855476379395\n",
      "step: 6130, loss: 5.989626407623291\n",
      "step: 6140, loss: 6.190647125244141\n",
      "step: 6150, loss: 6.2430739402771\n",
      "step: 6160, loss: 5.954221248626709\n",
      "step: 6170, loss: 6.408779144287109\n",
      "step: 6180, loss: 6.953769683837891\n",
      "step: 6190, loss: 6.206864833831787\n",
      "step: 6200, loss: 6.456010818481445\n",
      "step: 6210, loss: 6.085141658782959\n",
      "step: 6220, loss: 5.542790412902832\n",
      "step: 6230, loss: 6.708118915557861\n",
      "step: 6240, loss: 6.616300582885742\n",
      "step: 6250, loss: 6.410829067230225\n",
      "step: 6260, loss: 6.405697822570801\n",
      "step: 6270, loss: 6.204495906829834\n",
      "step: 6280, loss: 6.268565654754639\n",
      "step: 6290, loss: 7.0127105712890625\n",
      "step: 6300, loss: 6.250597953796387\n",
      "step: 6310, loss: 6.533022403717041\n",
      "step: 6320, loss: 5.874768257141113\n",
      "step: 6330, loss: 6.422008037567139\n",
      "step: 6340, loss: 6.566408157348633\n",
      "step: 6350, loss: 6.579561710357666\n",
      "step: 6360, loss: 6.542735576629639\n",
      "step: 6370, loss: 5.990510940551758\n",
      "step: 6380, loss: 6.814655780792236\n",
      "step: 6390, loss: 6.590089321136475\n",
      "step: 6400, loss: 6.245069980621338\n",
      "step: 6410, loss: 6.806368827819824\n",
      "step: 6420, loss: 6.665450572967529\n",
      "step: 6430, loss: 6.1559553146362305\n",
      "step: 6440, loss: 5.872319221496582\n",
      "step: 6450, loss: 6.280318737030029\n",
      "step: 6460, loss: 6.466655731201172\n",
      "step: 6470, loss: 5.192293643951416\n",
      "step: 6480, loss: 5.987575531005859\n",
      "step: 6490, loss: 6.588062763214111\n",
      "step: 6500, loss: 6.053374767303467\n",
      "The person is|’s| to\n",
      "He went to walk| him| be\n",
      "1 + 2 = 20|2|1\n",
      "first of the| | a\n",
      "3 cats and 2%|.| \n",
      "mom and the| other| others\n",
      "the red,|.| and\n",
      "She said| says| and\n",
      "step: 6510, loss: 6.211964130401611\n",
      "step: 6520, loss: 6.194730281829834\n",
      "step: 6530, loss: 6.245092391967773\n",
      "step: 6540, loss: 5.817843437194824\n",
      "step: 6550, loss: 6.143757343292236\n",
      "step: 6560, loss: 5.856037616729736\n",
      "step: 6570, loss: 5.6134819984436035\n",
      "step: 6580, loss: 6.457419395446777\n",
      "step: 6590, loss: 6.352911472320557\n",
      "step: 6600, loss: 6.068780899047852\n",
      "step: 6610, loss: 6.305073261260986\n",
      "step: 6620, loss: 6.425844669342041\n",
      "step: 6630, loss: 7.013432502746582\n",
      "step: 6640, loss: 7.773536205291748\n",
      "step: 6650, loss: 6.304223537445068\n",
      "step: 6660, loss: 6.014689922332764\n",
      "step: 6670, loss: 6.663385391235352\n",
      "step: 6680, loss: 6.263545989990234\n",
      "step: 6690, loss: 6.426175594329834\n",
      "step: 6700, loss: 6.208621978759766\n",
      "step: 6710, loss: 6.172134876251221\n",
      "step: 6720, loss: 6.265191555023193\n",
      "step: 6730, loss: 6.327381610870361\n",
      "step: 6740, loss: 6.458749771118164\n",
      "step: 6750, loss: 6.319028377532959\n",
      "step: 6760, loss: 6.406468391418457\n",
      "step: 6770, loss: 6.030271053314209\n",
      "step: 6780, loss: 6.481781482696533\n",
      "step: 6790, loss: 6.102492809295654\n",
      "step: 6800, loss: 7.5965423583984375\n",
      "step: 6810, loss: 6.697292327880859\n",
      "step: 6820, loss: 6.289701461791992\n",
      "step: 6830, loss: 6.703853607177734\n",
      "step: 6840, loss: 6.476152420043945\n",
      "step: 6850, loss: 6.567410945892334\n",
      "step: 6860, loss: 5.879504203796387\n",
      "step: 6870, loss: 5.633986949920654\n",
      "step: 6880, loss: 6.197684288024902\n",
      "step: 6890, loss: 6.1225905418396\n",
      "step: 6900, loss: 6.0908050537109375\n",
      "step: 6910, loss: 6.683159351348877\n",
      "step: 6920, loss: 6.330845832824707\n",
      "step: 6930, loss: 7.543088436126709\n",
      "step: 6940, loss: 6.9898786544799805\n",
      "step: 6950, loss: 6.325071811676025\n",
      "step: 6960, loss: 6.288593769073486\n",
      "step: 6970, loss: 6.290884017944336\n",
      "step: 6980, loss: 6.167809963226318\n",
      "step: 6990, loss: 5.955313205718994\n",
      "step: 7000, loss: 6.302385330200195\n",
      "The person's|,| who\n",
      "He went to the| his| Rome\n",
      "1 + 2 = 1|5|3\n",
      "first of the| his| war\n",
      "3 cats and 2.|,|)\n",
      "mom and the| his| their\n",
      "the red;|\n",
      "| of\n",
      "She seeks| said| says\n",
      "step: 7010, loss: 5.8584442138671875\n",
      "step: 7020, loss: 5.500056266784668\n",
      "step: 7030, loss: 5.5329203605651855\n",
      "step: 7040, loss: 5.321234226226807\n",
      "step: 7050, loss: 5.730969429016113\n",
      "step: 7060, loss: 5.568149566650391\n",
      "step: 7070, loss: 7.006973743438721\n",
      "step: 7080, loss: 6.60246467590332\n",
      "step: 7090, loss: 6.600862979888916\n",
      "step: 7100, loss: 6.630195140838623\n",
      "step: 7110, loss: 7.668385028839111\n",
      "step: 7120, loss: 6.6362810134887695\n",
      "step: 7130, loss: 6.463233470916748\n",
      "step: 7140, loss: 6.144062519073486\n",
      "step: 7150, loss: 6.530844688415527\n",
      "step: 7160, loss: 6.596997261047363\n",
      "step: 7170, loss: 6.103000640869141\n",
      "step: 7180, loss: 6.556558609008789\n",
      "step: 7190, loss: 5.851281642913818\n",
      "step: 7200, loss: 5.439467906951904\n",
      "step: 7210, loss: 6.299307823181152\n",
      "step: 7220, loss: 6.815516471862793\n",
      "step: 7230, loss: 6.283708095550537\n",
      "step: 7240, loss: 6.223986625671387\n",
      "step: 7250, loss: 6.268136024475098\n",
      "step: 7260, loss: 6.064572811126709\n",
      "step: 7270, loss: 6.256712436676025\n",
      "step: 7280, loss: 6.102194309234619\n",
      "step: 7290, loss: 6.58683967590332\n",
      "step: 7300, loss: 5.857813835144043\n",
      "step: 7310, loss: 7.259209156036377\n",
      "step: 7320, loss: 6.555233001708984\n",
      "step: 7330, loss: 7.180349826812744\n",
      "step: 7340, loss: 6.5419206619262695\n",
      "step: 7350, loss: 6.70741081237793\n",
      "step: 7360, loss: 6.628233432769775\n",
      "step: 7370, loss: 6.52512264251709\n",
      "step: 7380, loss: 6.176734447479248\n",
      "step: 7390, loss: 6.369095325469971\n",
      "step: 7400, loss: 6.318322658538818\n",
      "step: 7410, loss: 5.983310699462891\n",
      "step: 7420, loss: 6.057361602783203\n",
      "step: 7430, loss: 6.916393280029297\n",
      "step: 7440, loss: 6.366574764251709\n",
      "step: 7450, loss: 5.853733062744141\n",
      "step: 7460, loss: 6.621477127075195\n",
      "step: 7470, loss: 5.658524036407471\n",
      "step: 7480, loss: 6.039590358734131\n",
      "step: 7490, loss: 6.471498012542725\n",
      "step: 7500, loss: 6.263942718505859\n",
      "The person was| is| would\n",
      "He went to the| join| a\n",
      "1 + 2 = 2|3|1\n",
      "first of the| a| its\n",
      "3 cats and 2.|,| diabetes\n",
      "mom and the| \"| L\n",
      "the red,| and|.\n",
      "She was| would| had\n",
      "step: 7510, loss: 6.434451103210449\n",
      "step: 7520, loss: 6.151761531829834\n",
      "step: 7530, loss: 6.733938694000244\n",
      "step: 7540, loss: 5.874265193939209\n",
      "step: 7550, loss: 6.119019985198975\n",
      "step: 7560, loss: 6.487719535827637\n",
      "step: 7570, loss: 5.843751907348633\n",
      "step: 7580, loss: 6.924582004547119\n",
      "step: 7590, loss: 6.145339488983154\n",
      "step: 7600, loss: 5.71999979019165\n",
      "step: 7610, loss: 5.981700897216797\n",
      "step: 7620, loss: 5.9944167137146\n",
      "step: 7630, loss: 6.303018569946289\n",
      "step: 7640, loss: 6.479785919189453\n",
      "step: 7650, loss: 6.669331073760986\n",
      "step: 7660, loss: 6.234431266784668\n",
      "step: 7670, loss: 6.344351768493652\n",
      "step: 7680, loss: 6.4117326736450195\n",
      "step: 7690, loss: 6.217928886413574\n",
      "step: 7700, loss: 5.653529167175293\n",
      "step: 7710, loss: 6.785449981689453\n",
      "step: 7720, loss: 6.722537040710449\n",
      "step: 7730, loss: 6.761855602264404\n",
      "step: 7740, loss: 6.50602912902832\n",
      "step: 7750, loss: 6.237573146820068\n",
      "step: 7760, loss: 5.790163993835449\n",
      "step: 7770, loss: 5.938812255859375\n",
      "step: 7780, loss: 6.415933609008789\n",
      "step: 7790, loss: 6.078472137451172\n",
      "step: 7800, loss: 6.244308948516846\n",
      "step: 7810, loss: 6.1258721351623535\n",
      "step: 7820, loss: 6.190317153930664\n",
      "step: 7830, loss: 6.423868179321289\n",
      "step: 7840, loss: 5.991708278656006\n",
      "step: 7850, loss: 6.089592456817627\n",
      "step: 7860, loss: 6.631709098815918\n",
      "step: 7870, loss: 5.741334438323975\n",
      "step: 7880, loss: 6.381761074066162\n",
      "step: 7890, loss: 6.048589706420898\n",
      "step: 7900, loss: 5.746026515960693\n",
      "step: 7910, loss: 6.811279296875\n",
      "step: 7920, loss: 6.063600063323975\n",
      "step: 7930, loss: 6.490106105804443\n",
      "step: 7940, loss: 6.187737941741943\n",
      "step: 7950, loss: 6.320256233215332\n",
      "step: 7960, loss: 6.122881889343262\n",
      "step: 7970, loss: 5.669852256774902\n",
      "step: 7980, loss: 5.748740196228027\n",
      "step: 7990, loss: 6.675664901733398\n",
      "step: 8000, loss: 5.5228705406188965\n",
      "The person was| were|’s\n",
      "He went to the| a| his\n",
      "1 + 2 = 1|2|10\n",
      "first of the| | a\n",
      "3 cats and 2.|D|,\n",
      "mom and the| a| are\n",
      "the red,| and|.\n",
      "She was| said| asked\n",
      "step: 8010, loss: 6.262775897979736\n",
      "step: 8020, loss: 6.364058971405029\n",
      "step: 8030, loss: 6.4491801261901855\n",
      "step: 8040, loss: 5.556100368499756\n",
      "step: 8050, loss: 5.918299674987793\n",
      "step: 8060, loss: 6.361286640167236\n",
      "step: 8070, loss: 6.409750938415527\n",
      "step: 8080, loss: 6.197757720947266\n",
      "step: 8090, loss: 6.802425384521484\n",
      "step: 8100, loss: 6.70356559753418\n",
      "step: 8110, loss: 6.392614841461182\n",
      "step: 8120, loss: 6.632070541381836\n",
      "step: 8130, loss: 6.380774021148682\n",
      "step: 8140, loss: 6.197108268737793\n",
      "step: 8150, loss: 6.8569865226745605\n",
      "step: 8160, loss: 6.456734657287598\n",
      "step: 8170, loss: 5.981231689453125\n",
      "step: 8180, loss: 6.307473182678223\n",
      "step: 8190, loss: 6.268465518951416\n",
      "step: 8200, loss: 5.695019721984863\n",
      "step: 8210, loss: 5.809241771697998\n",
      "step: 8220, loss: 6.674981594085693\n",
      "step: 8230, loss: 6.299350738525391\n",
      "step: 8240, loss: 6.511812686920166\n",
      "step: 8250, loss: 5.338277816772461\n",
      "step: 8260, loss: 6.488947868347168\n",
      "step: 8270, loss: 6.147849082946777\n",
      "step: 8280, loss: 6.039717197418213\n",
      "step: 8290, loss: 5.915186882019043\n",
      "step: 8300, loss: 6.463499546051025\n",
      "step: 8310, loss: 5.841330528259277\n",
      "step: 8320, loss: 6.628098011016846\n",
      "step: 8330, loss: 6.874012470245361\n",
      "step: 8340, loss: 6.7555155754089355\n",
      "step: 8350, loss: 6.345151424407959\n",
      "step: 8360, loss: 5.4327521324157715\n",
      "step: 8370, loss: 5.701366424560547\n",
      "step: 8380, loss: 5.1899094581604\n",
      "step: 8390, loss: 4.878377914428711\n",
      "step: 8400, loss: 5.838085174560547\n",
      "step: 8410, loss: 5.9027099609375\n",
      "step: 8420, loss: 6.293293476104736\n",
      "step: 8430, loss: 5.361882209777832\n",
      "step: 8440, loss: 6.420678615570068\n",
      "step: 8450, loss: 5.888243198394775\n",
      "step: 8460, loss: 5.6723809242248535\n",
      "step: 8470, loss: 5.972028732299805\n",
      "step: 8480, loss: 6.397586822509766\n",
      "step: 8490, loss: 6.016396522521973\n",
      "step: 8500, loss: 5.9233317375183105\n",
      "The person of| is|,\n",
      "He went to the| his| a\n",
      "1 + 2 = 3|2|1\n",
      "first of the| a| water\n",
      "3 cats and 2.|,|)\n",
      "mom and Sp| C| the\n",
      "the red,| tree| leaves\n",
      "She lived| taught| had\n",
      "step: 8510, loss: 5.78515625\n",
      "step: 8520, loss: 5.747006893157959\n",
      "step: 8530, loss: 5.998483657836914\n",
      "step: 8540, loss: 6.318039417266846\n",
      "step: 8550, loss: 6.157699108123779\n",
      "step: 8560, loss: 5.519579887390137\n",
      "step: 8570, loss: 5.928748607635498\n",
      "step: 8580, loss: 6.268292427062988\n",
      "step: 8590, loss: 6.23260498046875\n",
      "step: 8600, loss: 6.108307361602783\n",
      "step: 8610, loss: 5.912979602813721\n",
      "step: 8620, loss: 5.663547992706299\n",
      "step: 8630, loss: 6.443864822387695\n",
      "step: 8640, loss: 6.178878307342529\n",
      "step: 8650, loss: 6.126370906829834\n",
      "step: 8660, loss: 6.5547614097595215\n",
      "step: 8670, loss: 5.881602764129639\n",
      "step: 8680, loss: 5.9868483543396\n",
      "step: 8690, loss: 5.494456768035889\n",
      "step: 8700, loss: 6.051191806793213\n",
      "step: 8710, loss: 6.289709568023682\n",
      "step: 8720, loss: 7.034286022186279\n",
      "step: 8730, loss: 5.838111400604248\n",
      "step: 8740, loss: 5.1388163566589355\n",
      "step: 8750, loss: 6.415056228637695\n",
      "step: 8760, loss: 6.18421745300293\n",
      "step: 8770, loss: 6.012941837310791\n",
      "step: 8780, loss: 6.345691680908203\n",
      "step: 8790, loss: 6.383692741394043\n",
      "step: 8800, loss: 6.334030628204346\n",
      "step: 8810, loss: 6.3698506355285645\n",
      "step: 8820, loss: 6.170788764953613\n",
      "step: 8830, loss: 5.831472396850586\n",
      "step: 8840, loss: 5.587955474853516\n",
      "step: 8850, loss: 5.8681206703186035\n",
      "step: 8860, loss: 5.805668830871582\n",
      "step: 8870, loss: 5.681960582733154\n",
      "step: 8880, loss: 6.659901142120361\n",
      "step: 8890, loss: 6.199028491973877\n",
      "step: 8900, loss: 5.18874454498291\n",
      "step: 8910, loss: 6.600771427154541\n",
      "step: 8920, loss: 6.762432098388672\n",
      "step: 8930, loss: 5.776325225830078\n",
      "step: 8940, loss: 6.366766452789307\n",
      "step: 8950, loss: 5.893214225769043\n",
      "step: 8960, loss: 7.184298515319824\n",
      "step: 8970, loss: 7.272843837738037\n",
      "step: 8980, loss: 5.994153022766113\n",
      "step: 8990, loss: 6.816022872924805\n",
      "step: 9000, loss: 6.194919586181641\n",
      "The person is| of| who\n",
      "He went to the| him| a\n",
      "1 + 2 = 3|2|10\n",
      "first of the| | a\n",
      "3 cats and 2x|.|/\n",
      "mom and S| the| E\n",
      "the red and|,|.\n",
      "She said| studied| had\n",
      "step: 9010, loss: 5.493963718414307\n",
      "step: 9020, loss: 5.887723445892334\n",
      "step: 9030, loss: 5.813817024230957\n",
      "step: 9040, loss: 6.300230503082275\n",
      "step: 9050, loss: 5.98539924621582\n",
      "step: 9060, loss: 6.033312797546387\n",
      "step: 9070, loss: 5.845426559448242\n",
      "step: 9080, loss: 6.099786281585693\n",
      "step: 9090, loss: 6.1437482833862305\n",
      "step: 9100, loss: 5.0051589012146\n",
      "step: 9110, loss: 6.647381782531738\n",
      "step: 9120, loss: 6.136875629425049\n",
      "step: 9130, loss: 5.831146240234375\n",
      "step: 9140, loss: 5.810327053070068\n",
      "step: 9150, loss: 5.893999099731445\n",
      "step: 9160, loss: 5.774311542510986\n",
      "step: 9170, loss: 5.260236740112305\n",
      "step: 9180, loss: 6.003476619720459\n",
      "step: 9190, loss: 6.109283447265625\n",
      "step: 9200, loss: 5.202493190765381\n",
      "step: 9210, loss: 5.591891288757324\n",
      "step: 9220, loss: 5.816292762756348\n",
      "step: 9230, loss: 5.846601486206055\n",
      "step: 9240, loss: 6.062243461608887\n",
      "step: 9250, loss: 6.653026580810547\n",
      "step: 9260, loss: 6.870762348175049\n",
      "step: 9270, loss: 5.684655666351318\n",
      "step: 9280, loss: 6.648613452911377\n",
      "step: 9290, loss: 6.245298862457275\n",
      "step: 9300, loss: 6.355291843414307\n",
      "step: 9310, loss: 5.713588237762451\n",
      "step: 9320, loss: 6.437129974365234\n",
      "step: 9330, loss: 5.49390983581543\n",
      "step: 9340, loss: 6.325448036193848\n",
      "step: 9350, loss: 6.030130386352539\n",
      "step: 9360, loss: 5.216389179229736\n",
      "step: 9370, loss: 6.215430736541748\n",
      "step: 9380, loss: 6.975646495819092\n",
      "step: 9390, loss: 5.957281589508057\n",
      "step: 9400, loss: 5.68632698059082\n",
      "step: 9410, loss: 5.3120551109313965\n",
      "step: 9420, loss: 6.677303314208984\n",
      "step: 9430, loss: 5.684649467468262\n",
      "step: 9440, loss: 5.894984245300293\n",
      "step: 9450, loss: 6.103772163391113\n",
      "step: 9460, loss: 6.019598007202148\n",
      "step: 9470, loss: 5.95123291015625\n",
      "step: 9480, loss: 6.041707992553711\n",
      "step: 9490, loss: 5.922647476196289\n",
      "step: 9500, loss: 5.852575778961182\n",
      "The person is| of| who\n",
      "He went to the| his| a\n",
      "1 + 2 = 2|3|1\n",
      "first of the| | its\n",
      "3 cats and 2.|,|%\n",
      "mom and the| P| other\n",
      "the red,| and|.\n",
      "She had| was| said\n",
      "step: 9510, loss: 6.107908248901367\n",
      "step: 9520, loss: 6.209283828735352\n",
      "step: 9530, loss: 5.681917667388916\n",
      "step: 9540, loss: 5.364933013916016\n",
      "step: 9550, loss: 6.205655574798584\n",
      "step: 9560, loss: 6.691696643829346\n",
      "step: 9570, loss: 5.9004807472229\n",
      "step: 9580, loss: 6.083993434906006\n",
      "step: 9590, loss: 6.706113815307617\n",
      "step: 9600, loss: 5.7145490646362305\n",
      "step: 9610, loss: 6.0811381340026855\n",
      "step: 9620, loss: 5.361020565032959\n",
      "step: 9630, loss: 5.7084221839904785\n",
      "step: 9640, loss: 6.9115095138549805\n",
      "step: 9650, loss: 4.932055473327637\n",
      "step: 9660, loss: 5.456042289733887\n",
      "step: 9670, loss: 6.052630424499512\n",
      "step: 9680, loss: 7.418153285980225\n",
      "step: 9690, loss: 6.026203632354736\n",
      "step: 9700, loss: 6.1830668449401855\n",
      "step: 9710, loss: 5.812899112701416\n",
      "step: 9720, loss: 6.003468036651611\n",
      "step: 9730, loss: 5.869581699371338\n",
      "step: 9740, loss: 5.467957973480225\n",
      "step: 9750, loss: 6.072481632232666\n",
      "step: 9760, loss: 5.951353549957275\n",
      "step: 9770, loss: 5.508622646331787\n",
      "step: 9780, loss: 5.636514663696289\n",
      "step: 9790, loss: 6.690202713012695\n",
      "step: 9800, loss: 6.5029296875\n",
      "step: 9810, loss: 5.241329193115234\n",
      "step: 9820, loss: 6.068118095397949\n",
      "step: 9830, loss: 6.097824573516846\n",
      "step: 9840, loss: 6.095147609710693\n",
      "step: 9850, loss: 5.706711292266846\n",
      "step: 9860, loss: 6.926546573638916\n",
      "step: 9870, loss: 5.733124256134033\n",
      "step: 9880, loss: 5.571690559387207\n",
      "step: 9890, loss: 6.117183208465576\n",
      "step: 9900, loss: 5.898735523223877\n",
      "step: 9910, loss: 6.606832504272461\n",
      "step: 9920, loss: 6.086887836456299\n",
      "step: 9930, loss: 5.681250095367432\n",
      "step: 9940, loss: 5.5038042068481445\n",
      "step: 9950, loss: 5.42498779296875\n",
      "step: 9960, loss: 5.610793590545654\n",
      "step: 9970, loss: 5.0696892738342285\n",
      "step: 9980, loss: 5.939274787902832\n",
      "step: 9990, loss: 5.868901252746582\n"
     ]
    }
   ],
   "source": [
    "for step in range(num_iterations):\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx: # before I added this in was getting BackendCompilerFailed: backend='inductor' raised: RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach()\n",
    "        loss = loss / grad_accum_steps # seems import to understand, but n/a here since grad_accum_steps is 1, see his comment\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader)\n",
    "    # gradient clipping\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip) # check exactly what this does, it's not a simple cip\n",
    "    # step optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step: {step}, loss: {train_loss}\")\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "            with autocast_ctx:\n",
    "                logits = orig_model(torch.tensor([tokenizer.encode(prompt)], device=device)).detach()\n",
    "                top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "                print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dfc1227-d8e2-4de0-9928-2bd46c7ba24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person is| of| who\n",
      "He went to the| his| a\n",
      "1 + 2 = 3|2|1\n",
      "first of the| | a\n",
      "3 cats and 2.|,|%\n",
      "mom and P| S| the\n",
      "the red,| and|.\n",
      "She had| said| was\n"
     ]
    }
   ],
   "source": [
    "# show top 3 next tokens for a few prompts\n",
    "for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "    with autocast_ctx:\n",
    "        logits = orig_model(torch.tensor([tokenizer.encode(prompt)], device=device)).detach()\n",
    "    top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "    print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ea365f9-f8a6-4fa0-9473-d5d1039194a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(orig_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c6752-c61a-4cc5-8369-46c8f0354566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
