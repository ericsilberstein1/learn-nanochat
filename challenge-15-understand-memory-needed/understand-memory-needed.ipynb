{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444814c1-78fa-4f53-881b-1d4f228abab2",
   "metadata": {},
   "source": [
    "I want to learn how to know how big (depth, max_seq_len) a model I can create and have it still fit on my GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eddb700-d7a9-446e-9f95-f700bd70ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815eae6-b765-469a-8266-546a465215bd",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "Let me start by seeing if I can understand it for a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f139415b-e4da-4c19-95ba-f667c1b6650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dda1a13-77f3-4c9f-9121-9b2ed4aad566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8249999360, 8354660352)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "free, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00ced97-4492-4824-bf9a-1858ca6e71e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 7.68 GB\n",
      "Total memory: 7.78 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Free memory: {free / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadc3c98-b5c6-4741-b65e-58c83e48264a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated()\n",
    "allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55839c59-996f-4fac-b011-a4e02b82776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_before = free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9ca892-cd9b-4d3b-b78e-d80f00e76081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=10, out_features=10, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f642e8bd-f3f2-4158-b29d-f50a617378d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should have 100 params, each 32 bits = 4 bytes, so guessing consumes this many bytes:\n",
    "100 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f22d0eb-8602-4cd8-a699-08173822ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194304"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "free_before - free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cff6a59-27aa-4127-8ccd-1e0593901382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdda18-7e32-4736-ba3b-6602e56d35c0",
   "metadata": {},
   "source": [
    "Is the 512 because it allocates in certain minimum units? Let's try a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81d3c2e-3da7-4eae-bbf4-a2dffa728fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb44f3d1-3d66-44d9-ad85-e2d1dbd61975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first confirm repeatable\n",
    "model = torch.nn.Linear(in_features=10, out_features=10, bias=False, device=\"cuda\", dtype=torch.float32)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5007652-75fa-4aab-b956-d12329adb05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af276f5-472c-4f8f-91b6-8cd395c0af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 x 11 x 4 = 484, so guessing we'll still see 512\n",
    "model = torch.nn.Linear(in_features=11, out_features=11, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42214989-67dd-4751-b11c-bbf4d36acdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0a5994c-4088-4b32-91df-446a828c8673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a488842a-c825-4c6d-8d9d-c06b83b6c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 x 12 x 4 = 576, so guessing we'll still see 1024\n",
    "model = torch.nn.Linear(in_features=12, out_features=12, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b92c64c-0ce3-4b0a-81a4-d1f9cd261e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe1289-54ff-452b-90fe-86e7fb40053f",
   "metadata": {},
   "source": [
    "What about the gradient? Does that space get allocated only as needed? Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54833ec6-bc3f-4dba-a7ae-7e901e45b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(12, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5388eae9-9e0d-4113-b089-0124e34edf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()\n",
    "# 1024 + 512 = 1636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8103cb4c-9720-4240-a611-35dc9d3f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb651c27-c6f1-4d7c-a5d2-0d93441cb693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684ac17-ecb3-4e37-a055-a514d4758dfe",
   "metadata": {},
   "source": [
    "What's that all about? Is that memory that gets allocated to do the forward pass? Maybe we should start even simpler and see what happens if we allocte two tensors and multiply them. But first see what happens when we call backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7e8052e-3ac8-4547-a759-1ba1c031d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9bae810-2a69-4ff8-bb16-282bc27ee2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17042432"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac7aa28d-2130-43fa-97d5-d2703bb36385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be307f75-c4e1-47a6-a576-f33498b61da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "246346bd-38d0-4d07-a421-11e9c91dd4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.grad.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1efe240c-9079-46c6-adf8-c7849480345d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "804e866b-d882-404d-aa0c-4e2f0e61fd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23b51e-9a37-4427-9780-b08be83b3952",
   "metadata": {},
   "source": [
    "Maybe it's not such a good idea to try to line things up at this level with `torch.cuda.memory_allocated()` because torch (or lower-level stuff?) could be allocating caches, etc. Let's see how things work with bigger numbers. But first, let's see what happens with allocating and multiplying raw tensors. My hope is there the numbers will match up.\n",
    "\n",
    "Also, could jupyter be holding onto things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431cb804-1d14-48d8-8d44-10c75dda82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35675bea-1145-4d6f-a068-3193236feb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad2ea98-0d29-4adc-97e1-bedec8110f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a5a326-227d-4f76-bf81-6d187129c005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b68db92-5032-442f-a13e-bdbe2c4a1d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e7bc2d-9269-44c1-82e3-939b0c1e77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb3fd65-f1d3-4959-9970-494ca252010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() # expecting 1024 + 512 = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2622a4a-cf39-4cc0-bc55-03b0dee90e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a602ed4-3413-433a-8107-83c879df2174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() # expecting 1024 + 512 + 512 = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237ee48-35f0-40fb-8cd8-fdcfe6019163",
   "metadata": {},
   "source": [
    "No. Maybe as soon as you do matrix math it allocates memory for something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d899f5c-b503-4e01-bc12-47aafe865fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03b8cc7-bebd-4585-a74c-ab82c889b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c188d-28f9-47a7-b1ef-9708e830d947",
   "metadata": {},
   "source": [
    "Does torch.no_grad() matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316666c2-af1b-4046-9125-c8d6fe20e8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a11e7a2-2130-4fee-b322-2cd2dd72c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7edbb0c-e394-46b8-9fe7-64218c7252ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc48da71-250d-4dfa-a402-550e5a471189",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2fb81c-6d54-4bc3-b600-0820c28672db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2728c59-a34e-4fc4-9bd0-b2f9d65548fb",
   "metadata": {},
   "source": [
    "How about preallocating space for the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb25ba13-a658-470a-9ec1-e9b2f5d38fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233c3d94-893a-4ad2-bf6b-946057be08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)\n",
    "y = torch.empty_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1d016e-5ef8-4bc5-92d6-099007e483eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848ad82a-c56e-4436-b1ac-c3f1d2a1f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = torch.matmul(m, x, out=y)\n",
    "del foo\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d36631-1c19-45d4-8d2a-84b936cadae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f96b0-3b93-4393-8b1b-af6019b6a3f9",
   "metadata": {},
   "source": [
    "Not sure, let's try with much bigger numbers and see. Maybe the unaccounted for stuff is fixed stuff that will become a rounding error at bigger sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b9d58c-1db9-4022-b0cd-1fb694b8a5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef746e4-6467-4b4e-b417-60d0cca96697",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(1000, 1000, device=\"cuda\", dtype=torch.float32) # 4,000,000 bytes\n",
    "x = torch.randn(1000, device=\"cuda\", dtype=torch.float32) # 4000 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67dfb785-d666-4a80-9c29-8f15d88e35fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4004352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cff9a67-de02-48a6-8631-c20356d254e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000000, 4000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes(), x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146f56dd-8767-4066-9354-8ba85d5be831",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1780b3-b509-47f7-abf6-2b6a4d6f5c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed86df9-d767-4d8e-a360-5ade25b5232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12528128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4995e1e-55e1-4554-b278-bde663d2bf13",
   "metadata": {},
   "source": [
    "No. It jumps from ~4M to ~12M. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d48fff-7f43-418f-8c11-50450ead1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12528128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549f5fd-1eb3-439d-a9d1-bb894d90b4b9",
   "metadata": {},
   "source": [
    "What if the tensors large enough that they and the result take up all the space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272bdf2b-e8f4-4d51-b4b4-b4b7826fc307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2369cc4d-9fb4-4013-91bb-055e829f9e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8,249,999,360'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "f\"{free:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0baa8cf5-0ded-416b-a6ab-635590a52bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45414.753549920315"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(free / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a97d92b-0701-4a06-a221-8e321c6b189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(40_000, 40_000, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(40_000, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe84600-c959-4a8f-8e70-1fbb98c0a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400160000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes() + x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76efc666-55fa-4677-98d8-589e39ebabc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,668,160'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1877d89-4992-4d54-a943-81f983f84673",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01f0513-a493-4a68-8c0f-9001c1b8d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67b5305-9d1c-45db-bda7-1fbb1686a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,409,348,096'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b677448-1367-43f8-863b-f979d27400a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,813,839,872'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "f\"{free:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657c73c-3c02-49dd-af2b-7b182787b793",
   "metadata": {},
   "source": [
    "well, that worked, and the \"extra\" increase after the multiplication was \"only\" ~9M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db73c325-86d9-4068-a0a5-ab4b6daad27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9028096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() - (m.untyped_storage().nbytes() + x.untyped_storage().nbytes() + y.untyped_storage().nbytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042988b-e0f4-4b4b-bfc9-423500d40888",
   "metadata": {},
   "source": [
    "so maybe it's not worth worrying about exactly how this works, but what happens if we go back to the simple linear model with the same large number of params. Will the forward pass work but it will fail with an out of memory problem when call backward() because it doesn't have room to store the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e0f20f-dfc1-47b5-b4ad-e527fad954a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfe45e8-92f7-4a9a-983a-8cd690ed63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=40_000, out_features=40_000, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bf2abf-ca12-4336-9bec-f0cbd4355cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,507,904'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba76c22-84b3-4dd1-9265-9dd64f1081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(40_000, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291c0b2c-f6d8-4a90-bdcd-cddd4b0a8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,668,160'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e49019-2f56-45a0-9a18-ef05facb01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f88aeb6-a626-4202-8bb1-03d16de21865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,409,188,352'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6616e3bc-ff8b-4f48-a846-9e21c6e2c719",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 7.78 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 7.78 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b31bfb-0479-473f-9e1d-4f163cf04805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
