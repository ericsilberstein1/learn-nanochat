{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444814c1-78fa-4f53-881b-1d4f228abab2",
   "metadata": {},
   "source": [
    "I want to learn how to know how big (depth, max_seq_len) a model I can create and have it still fit on my GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eddb700-d7a9-446e-9f95-f700bd70ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815eae6-b765-469a-8266-546a465215bd",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "Let me start by seeing if I can understand it for a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f139415b-e4da-4c19-95ba-f667c1b6650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dda1a13-77f3-4c9f-9121-9b2ed4aad566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8249999360, 8354660352)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "free, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00ced97-4492-4824-bf9a-1858ca6e71e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 7.68 GB\n",
      "Total memory: 7.78 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Free memory: {free / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadc3c98-b5c6-4741-b65e-58c83e48264a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated()\n",
    "allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55839c59-996f-4fac-b011-a4e02b82776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_before = free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9ca892-cd9b-4d3b-b78e-d80f00e76081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=10, out_features=10, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f642e8bd-f3f2-4158-b29d-f50a617378d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should have 100 params, each 32 bits = 4 bytes, so guessing consumes this many bytes:\n",
    "100 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f22d0eb-8602-4cd8-a699-08173822ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194304"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "free_before - free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cff6a59-27aa-4127-8ccd-1e0593901382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdda18-7e32-4736-ba3b-6602e56d35c0",
   "metadata": {},
   "source": [
    "Is the 512 because it allocates in certain minimum units? Let's try a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81d3c2e-3da7-4eae-bbf4-a2dffa728fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb44f3d1-3d66-44d9-ad85-e2d1dbd61975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first confirm repeatable\n",
    "model = torch.nn.Linear(in_features=10, out_features=10, bias=False, device=\"cuda\", dtype=torch.float32)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5007652-75fa-4aab-b956-d12329adb05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af276f5-472c-4f8f-91b6-8cd395c0af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 x 11 x 4 = 484, so guessing we'll still see 512\n",
    "model = torch.nn.Linear(in_features=11, out_features=11, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42214989-67dd-4751-b11c-bbf4d36acdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0a5994c-4088-4b32-91df-446a828c8673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a488842a-c825-4c6d-8d9d-c06b83b6c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 x 12 x 4 = 576, so guessing we'll still see 1024\n",
    "model = torch.nn.Linear(in_features=12, out_features=12, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b92c64c-0ce3-4b0a-81a4-d1f9cd261e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe1289-54ff-452b-90fe-86e7fb40053f",
   "metadata": {},
   "source": [
    "What about the gradient? Does that space get allocated only as needed? Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54833ec6-bc3f-4dba-a7ae-7e901e45b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(12, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5388eae9-9e0d-4113-b089-0124e34edf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()\n",
    "# 1024 + 512 = 1636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8103cb4c-9720-4240-a611-35dc9d3f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb651c27-c6f1-4d7c-a5d2-0d93441cb693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684ac17-ecb3-4e37-a055-a514d4758dfe",
   "metadata": {},
   "source": [
    "What's that all about? Is that memory that gets allocated to do the forward pass? Maybe we should start even simpler and see what happens if we allocte two tensors and multiply them. But first see what happens when we call backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7e8052e-3ac8-4547-a759-1ba1c031d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9bae810-2a69-4ff8-bb16-282bc27ee2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17042432"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac7aa28d-2130-43fa-97d5-d2703bb36385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997],\n",
       "        [ 0.6030, -1.4976,  0.3691, -1.1289,  0.7871, -1.1535,  0.2922,  0.8758,\n",
       "         -0.8272, -0.8537,  0.0650,  0.6997]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be307f75-c4e1-47a6-a576-f33498b61da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "246346bd-38d0-4d07-a421-11e9c91dd4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.grad.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1efe240c-9079-46c6-adf8-c7849480345d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "804e866b-d882-404d-aa0c-4e2f0e61fd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23b51e-9a37-4427-9780-b08be83b3952",
   "metadata": {},
   "source": [
    "Maybe it's not such a good idea to try to line things up at this level with `torch.cuda.memory_allocated()` because torch (or lower-level stuff?) could be allocating caches, etc. Let's see how things work with bigger numbers. But first, let's see what happens with allocating and multiplying raw tensors. My hope is there the numbers will match up.\n",
    "\n",
    "Also, could jupyter be holding onto things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431cb804-1d14-48d8-8d44-10c75dda82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35675bea-1145-4d6f-a068-3193236feb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad2ea98-0d29-4adc-97e1-bedec8110f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a5a326-227d-4f76-bf81-6d187129c005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b68db92-5032-442f-a13e-bdbe2c4a1d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e7bc2d-9269-44c1-82e3-939b0c1e77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb3fd65-f1d3-4959-9970-494ca252010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() # expecting 1024 + 512 = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2622a4a-cf39-4cc0-bc55-03b0dee90e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a602ed4-3413-433a-8107-83c879df2174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() # expecting 1024 + 512 + 512 = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237ee48-35f0-40fb-8cd8-fdcfe6019163",
   "metadata": {},
   "source": [
    "No. Maybe as soon as you do matrix math it allocates memory for something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d899f5c-b503-4e01-bc12-47aafe865fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03b8cc7-bebd-4585-a74c-ab82c889b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c188d-28f9-47a7-b1ef-9708e830d947",
   "metadata": {},
   "source": [
    "Does torch.no_grad() matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316666c2-af1b-4046-9125-c8d6fe20e8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a11e7a2-2130-4fee-b322-2cd2dd72c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7edbb0c-e394-46b8-9fe7-64218c7252ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc48da71-250d-4dfa-a402-550e5a471189",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2fb81c-6d54-4bc3-b600-0820c28672db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2728c59-a34e-4fc4-9bd0-b2f9d65548fb",
   "metadata": {},
   "source": [
    "How about preallocating space for the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb25ba13-a658-470a-9ec1-e9b2f5d38fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233c3d94-893a-4ad2-bf6b-946057be08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(12, 12, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(12, device=\"cuda\", dtype=torch.float32)\n",
    "y = torch.empty_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1d016e-5ef8-4bc5-92d6-099007e483eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848ad82a-c56e-4436-b1ac-c3f1d2a1f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = torch.matmul(m, x, out=y)\n",
    "del foo\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d36631-1c19-45d4-8d2a-84b936cadae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f96b0-3b93-4393-8b1b-af6019b6a3f9",
   "metadata": {},
   "source": [
    "Not sure, let's try with much bigger numbers and see. Maybe the unaccounted for stuff is fixed stuff that will become a rounding error at bigger sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b9d58c-1db9-4022-b0cd-1fb694b8a5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef746e4-6467-4b4e-b417-60d0cca96697",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(1000, 1000, device=\"cuda\", dtype=torch.float32) # 4,000,000 bytes\n",
    "x = torch.randn(1000, device=\"cuda\", dtype=torch.float32) # 4000 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67dfb785-d666-4a80-9c29-8f15d88e35fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4004352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cff9a67-de02-48a6-8631-c20356d254e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000000, 4000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes(), x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146f56dd-8767-4066-9354-8ba85d5be831",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1780b3-b509-47f7-abf6-2b6a4d6f5c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed86df9-d767-4d8e-a360-5ade25b5232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12528128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4995e1e-55e1-4554-b278-bde663d2bf13",
   "metadata": {},
   "source": [
    "No. It jumps from ~4M to ~12M. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d48fff-7f43-418f-8c11-50450ead1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12528128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549f5fd-1eb3-439d-a9d1-bb894d90b4b9",
   "metadata": {},
   "source": [
    "What if the tensors large enough that they and the result take up all the space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272bdf2b-e8f4-4d51-b4b4-b4b7826fc307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2369cc4d-9fb4-4013-91bb-055e829f9e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8,249,999,360'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "f\"{free:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0baa8cf5-0ded-416b-a6ab-635590a52bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45414.753549920315"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(free / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a97d92b-0701-4a06-a221-8e321c6b189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(40_000, 40_000, device=\"cuda\", dtype=torch.float32)\n",
    "x = torch.randn(40_000, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe84600-c959-4a8f-8e70-1fbb98c0a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400160000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.untyped_storage().nbytes() + x.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76efc666-55fa-4677-98d8-589e39ebabc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,668,160'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1877d89-4992-4d54-a943-81f983f84673",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01f0513-a493-4a68-8c0f-9001c1b8d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.untyped_storage().nbytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67b5305-9d1c-45db-bda7-1fbb1686a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,409,348,096'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b677448-1367-43f8-863b-f979d27400a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,813,839,872'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "f\"{free:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657c73c-3c02-49dd-af2b-7b182787b793",
   "metadata": {},
   "source": [
    "well, that worked, and the \"extra\" increase after the multiplication was \"only\" ~9M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db73c325-86d9-4068-a0a5-ab4b6daad27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9028096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() - (m.untyped_storage().nbytes() + x.untyped_storage().nbytes() + y.untyped_storage().nbytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042988b-e0f4-4b4b-bfc9-423500d40888",
   "metadata": {},
   "source": [
    "so maybe it's not worth worrying about exactly how this works, but what happens if we go back to the simple linear model with the same large number of params. Will the forward pass work but it will fail with an out of memory problem when call backward() because it doesn't have room to store the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e0f20f-dfc1-47b5-b4ad-e527fad954a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfe45e8-92f7-4a9a-983a-8cd690ed63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=40_000, out_features=40_000, bias=False, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bf2abf-ca12-4336-9bec-f0cbd4355cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,507,904'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba76c22-84b3-4dd1-9265-9dd64f1081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(40_000, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291c0b2c-f6d8-4a90-bdcd-cddd4b0a8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,400,668,160'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e49019-2f56-45a0-9a18-ef05facb01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f88aeb6-a626-4202-8bb1-03d16de21865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,409,188,352'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{torch.cuda.memory_allocated():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6616e3bc-ff8b-4f48-a846-9e21c6e2c719",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 7.78 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 7.78 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a57e1-5804-4058-8e72-3002ad035649",
   "metadata": {},
   "source": [
    "### bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a870f41-cc87-4136-8739-28d0e79bc4c2",
   "metadata": {},
   "source": [
    "Another thing I want to understand before digging into the real model is bfloat16.\n",
    "\n",
    "I ended up spending way more time than the few minutes I thought it would take to see how torch.float32 works and moved the explortation to `understand-torch-float32.ipynb`. If nothing else I'll be more cautious about numerical precision and understand that float are not magical, for example, why the way the encoding works causes the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "f6352a12-78f8-40fd-8f31-294c2393e48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12345678848.0"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(12345678912, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "1bded46f-b29e-453f-bcff-bb3486e8b54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2345000173693373e-20"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1.2345e-20, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "a52b980f-987e-4258-8d40-923f96be5bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2345678442216013e-05"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1.2345678e-5, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "f354d52e-c7ca-4401-890f-23d4f8abdb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123456.78125"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1.2345678e5, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72686d19-2aa9-4b2a-971b-76c17a02157c",
   "metadata": {},
   "source": [
    "### \"Hand\" calculate memory for the gpt model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9819e-4083-421a-8d17-4472da88939a",
   "metadata": {},
   "source": [
    "Let's use the sizes from the baby pretrain notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db36ff7c-faf4-43a6-8c69-6ff9ade66c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e877ae00-8ed6-4dc1-91d1-90d94c4988f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example config\n",
    "\n",
    "depth = 4\n",
    "max_seq_len = 128\n",
    "\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # 256\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads # 2\n",
    "\n",
    "vocab_size = 65537\n",
    "\n",
    "device_batch_size = 1\n",
    "total_batch_size = device_batch_size * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d741b983-57d5-4ef8-91e1-4c9e25fabf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777472, 1048576, 2097152, 16777472, 163840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_wte_params = vocab_size * model_dim\n",
    "n_attn_params = num_layers * 4 * model_dim * model_dim # (c_q, c_k, c_v, c_proj)\n",
    "n_mlp_params = num_layers * 2 * model_dim * 4 * model_dim # (c_fc, c_proj)\n",
    "n_lm_head_params = model_dim * vocab_size\n",
    "n_precomputed_cos_sin_params = 10 * max_seq_len * (model_dim // num_heads)\n",
    "n_total_params = n_wte_params + n_attn_params + n_mlp_params + n_lm_head_params + n_precomputed_cos_sin_params\n",
    "n_wte_params, n_attn_params, n_mlp_params, n_lm_head_params, n_precomputed_cos_sin_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921085fb-6fd1-454e-aeaf-8ffa7bda3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36,864,512\n"
     ]
    }
   ],
   "source": [
    "print(f\"{n_total_params:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093c1ed1-6ea6-4744-bf15-af066a3d6815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(\n",
    "    [n_wte_params, n_attn_params, n_mlp_params, n_lm_head_params, n_precomputed_cos_sin_params],\n",
    "    labels=['wte', 'attn', 'mlp', 'lm_head', 'cos_sin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649cd2e4-63b5-4834-b2ff-6e683be150be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33554944, 4194304, 8388608, 67109888, 327680)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU\n",
    "wte_bytes = n_wte_params * 2 # on GPU, wte is set to bfloat16\n",
    "attn_bytes = n_attn_params * 4\n",
    "mlp_bytes = n_mlp_params * 4\n",
    "lm_head_bytes = n_lm_head_params * 4\n",
    "precomputed_cos_sin_bytes = n_precomputed_cos_sin_params * 2 # uses bfloat16 on CPU and GPU\n",
    "wte_bytes, attn_bytes, mlp_bytes, lm_head_bytes, precomputed_cos_sin_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "384f6b09-300f-4f45-b698-b457b11df347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(\n",
    "    [wte_bytes, attn_bytes, mlp_bytes, lm_head_bytes, precomputed_cos_sin_bytes],\n",
    "    labels=['wte', 'attn', 'mlp', 'lm_head', 'cos_sin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6698496-61bc-468a-87f8-a255ca88003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in gradients and tensors the optimizers must use\n",
    "wte_bytes += wte_bytes # gradient (not sure what muon is yet and if it needs to maintain other large tensors)\n",
    "attn_bytes += attn_bytes * 3 # gradient, m and v in AdamW optimizer (moving averages)\n",
    "mlp_bytes += mlp_bytes * 3 # gradient, m and v in AdamW optimizer \n",
    "lm_head_bytes += lm_head_bytes * 3 # gradient, m and v in AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e37b88d-ac70-414e-a730-789901440415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264192"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Really rough...sure none of this is meaningful...more just to think about...also not sure when\n",
    "# memory can be freed\n",
    "other_bytes_needed = 0\n",
    "other_bytes_needed += total_batch_size * 4 # x, the input batch (int32)\n",
    "other_bytes_needed += total_batch_size * 8 # y, the targets (int64)\n",
    "other_bytes_needed += total_batch_size * model_dim * 4 # the output of hidden layers\n",
    "other_bytes_needed += total_batch_size * 4 # overall output (the logits)\n",
    "other_bytes_needed += total_batch_size * model_dim * 4 # one more interim tensor (?)\n",
    "other_bytes_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a71588-8f37-420c-b6d2-90b640ad9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_needed = wte_bytes + attn_bytes + mlp_bytes + lm_head_bytes + precomputed_cos_sin_bytes + other_bytes_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "763b09f4-c78a-4fb4-8443-049ee9c8ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386,472,960 bytes = 0.36 GiB\n"
     ]
    }
   ],
   "source": [
    "# So for this configuration on GPU estimating need about this many bytes\n",
    "print(f\"{bytes_needed:,d} bytes = {bytes_needed / 1024 ** 3:2.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acad3473-168b-45e7-b229-d6634a5fe501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pie chart of approx bytes needed including for gradients\n",
    "# (of course I could be missing other big chunks that end up being needed)\n",
    "plt.pie(\n",
    "    [wte_bytes * 2, attn_bytes * 2, mlp_bytes * 2, lm_head_bytes * 2, precomputed_cos_sin_bytes, other_bytes_needed],\n",
    "    labels=['wte w/ g+o', 'attn w/ g+o', 'mlp w/ g+o', 'lm_head w/ g+0', 'cos_sin', 'other']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6b902-774d-4925-88fc-95ae7d0a61d9",
   "metadata": {},
   "source": [
    "Check with actual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7781cb-c158-4cb3-883e-063763cd23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3da61f-ec5c-40bf-a232-810c53d717a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device = autodetect_device_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35060fe-7745-46df-9b07-bccaeb598b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3c2320-2b0b-4d6f-84ca-52a53f7c5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65537, 256]) has 16,777,472 elements using 33,554,944 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([1024, 256]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 1024]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([1024, 256]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 1024]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([1024, 256]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 1024]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([256, 256]) has 65,536 elements using 262,144 bytes\n",
      "torch.Size([1024, 256]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([256, 1024]) has 262,144 elements using 1,048,576 bytes\n",
      "torch.Size([65537, 256]) has 16,777,472 elements using 67,109,888 bytes\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "total_bytes = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "    total_bytes += param.untyped_storage().nbytes()\n",
    "    print(f\"{param.shape} has {param.numel():,d} elements using {param.untyped_storage().nbytes():,d} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786500be-e503-43fb-ae33-b80582b7c876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36700672, 113247744)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params, total_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2554ea2-95f8-4eaf-b7de-0a8b2263d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos torch.Size([1, 1280, 1, 64]) 81920 163840\n",
      "sin torch.Size([1, 1280, 1, 64]) 81920 163840\n"
     ]
    }
   ],
   "source": [
    "for name, buf in model.named_buffers():\n",
    "    total_params += buf.numel()\n",
    "    total_bytes += buf.untyped_storage().nbytes()\n",
    "    print(name, buf.shape, buf.numel(), buf.untyped_storage().nbytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed6fd2e-f3cd-4179-950b-c33da28cf34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: 36,864,512\n",
      "total bytes: 113,575,424 bytes = 0.11 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"total params: {total_params:,d}\")\n",
    "print(f\"total bytes: {total_bytes:,d} bytes = {total_bytes / 1024 ** 3:2.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f4b36-ea1a-4018-92d0-fe590027843b",
   "metadata": {},
   "source": [
    "^ But that leaves out a huge amount (gradients, optimizers, other tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43a455-94fb-415f-b57d-6de19cf361a1",
   "metadata": {},
   "source": [
    "### Now with bigger dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6ca94-5d89-4e2c-81bd-068acd6447df",
   "metadata": {},
   "source": [
    "Repeat the cells above but with more realistic dimensions. Want to see how the pie chart looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5161df80-f656-4651-8b1a-976f4976b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config from his d20 in speedrun.sh / base_train.py\n",
    "\n",
    "depth = 20\n",
    "max_seq_len = 2048\n",
    "\n",
    "num_layers = depth\n",
    "model_dim = depth * 64\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads\n",
    "\n",
    "vocab_size = 65537\n",
    "\n",
    "device_batch_size = 32\n",
    "total_batch_size = device_batch_size * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c77bab38-dae3-4f70-8fdc-6b0281173e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83887360, 131072000, 262144000, 83887360, 2621440)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_wte_params = vocab_size * model_dim\n",
    "n_attn_params = num_layers * 4 * model_dim * model_dim # (c_q, c_k, c_v, c_proj)\n",
    "n_mlp_params = num_layers * 2 * model_dim * 4 * model_dim # (c_fc, c_proj)\n",
    "n_lm_head_params = model_dim * vocab_size\n",
    "n_precomputed_cos_sin_params = 10 * max_seq_len * (model_dim // num_heads)\n",
    "n_total_params = n_wte_params + n_attn_params + n_mlp_params + n_lm_head_params + n_precomputed_cos_sin_params\n",
    "n_wte_params, n_attn_params, n_mlp_params, n_lm_head_params, n_precomputed_cos_sin_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ca0eb8a-64f0-4318-a6e9-cbfa2e3bea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563,612,160\n"
     ]
    }
   ],
   "source": [
    "print(f\"{n_total_params:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35f4eee-c22f-49e6-ad71-855070157196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(\n",
    "    [n_wte_params, n_attn_params, n_mlp_params, n_lm_head_params, n_precomputed_cos_sin_params],\n",
    "    labels=['wte', 'attn', 'mlp', 'lm_head', 'cos_sin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8e11873-7d6a-4a90-9789-5d344e101aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167774720, 524288000, 1048576000, 335549440, 5242880)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU\n",
    "wte_bytes = n_wte_params * 2 # on GPU, wte is set to bfloat16\n",
    "attn_bytes = n_attn_params * 4\n",
    "mlp_bytes = n_mlp_params * 4\n",
    "lm_head_bytes = n_lm_head_params * 4\n",
    "precomputed_cos_sin_bytes = n_precomputed_cos_sin_params * 2 # uses bfloat16 on CPU and GPU\n",
    "wte_bytes, attn_bytes, mlp_bytes, lm_head_bytes, precomputed_cos_sin_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93aa4f0-52fb-4a24-853a-7a892076d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(\n",
    "    [wte_bytes, attn_bytes, mlp_bytes, lm_head_bytes, precomputed_cos_sin_bytes],\n",
    "    labels=['wte', 'attn', 'mlp', 'lm_head', 'cos_sin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dc2074a-67c9-4172-818b-7d799c620f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in gradients and tensors the optimizers must use\n",
    "wte_bytes += wte_bytes # gradient (not sure what muon is yet and if it needs to maintain other large tensors)\n",
    "attn_bytes += attn_bytes * 3 # gradient, m and v in AdamW optimizer (moving averages)\n",
    "mlp_bytes += mlp_bytes * 3 # gradient, m and v in AdamW optimizer \n",
    "lm_head_bytes += lm_head_bytes * 3 # gradient, m and v in AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20671edc-0cc0-4897-a8e6-69fd80495597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672137216"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Really rough...sure none of this is meaningful...more just to think about...also not sure when\n",
    "# memory can be freed\n",
    "other_bytes_needed = 0\n",
    "other_bytes_needed += total_batch_size * 4 # x, the input batch (int32)\n",
    "other_bytes_needed += total_batch_size * 8 # y, the targets (int64)\n",
    "other_bytes_needed += total_batch_size * model_dim * 4 # the output of hidden layers\n",
    "other_bytes_needed += total_batch_size * 4 # overall output (the logits)\n",
    "other_bytes_needed += total_batch_size * model_dim * 4 # one more interim tensor (?)\n",
    "other_bytes_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8cf40ab-a333-44b8-9ee0-02e55b54128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_needed = wte_bytes + attn_bytes + mlp_bytes + lm_head_bytes + precomputed_cos_sin_bytes + other_bytes_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0b70968-e809-4ab0-8aad-9b8d611306d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,646,583,296 bytes = 8.05 GiB\n"
     ]
    }
   ],
   "source": [
    "# So for this configuration on GPU estimating need about this many bytes\n",
    "print(f\"{bytes_needed:,d} bytes = {bytes_needed / 1024 ** 3:2.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1961aa39-f335-4ffd-ba88-238cdc81d4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pie chart of approx bytes needed including for gradients\n",
    "# (of course I could be missing other big chunks that end up being needed)\n",
    "plt.pie(\n",
    "    [wte_bytes * 2, attn_bytes * 2, mlp_bytes * 2, lm_head_bytes * 2, precomputed_cos_sin_bytes, other_bytes_needed],\n",
    "    labels=['wte w/ g+o', 'attn w/ g+o', 'mlp w/ g+o', 'lm_head w/ g+0', 'cos_sin', 'other']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07273dec-1a10-4a70-b5be-2a177efa29c6",
   "metadata": {},
   "source": [
    "^ So even if I'm in the right ballpark for \"other\" it's not insigificant. Changing batch size will adjust other while leaving everything else, so maybe the idea is to make batch size as large as possible before running out of memory to maximize GPU during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172827d-3b75-46b8-9552-9dad3814a6f8",
   "metadata": {},
   "source": [
    "Check with actual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "744c603c-29b8-47ca-905b-404bf6abc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faf47af3-5b39-46c1-815d-05404da21158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device = autodetect_device_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62f6b7b0-1731-4962-bfb5-33fe127cbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6560434-d478-4000-8eb1-737e29792f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65537, 1280]) has 83,887,360 elements using 167,774,720 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([1280, 1280]) has 1,638,400 elements using 6,553,600 bytes\n",
      "torch.Size([5120, 1280]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([1280, 5120]) has 6,553,600 elements using 26,214,400 bytes\n",
      "torch.Size([65537, 1280]) has 83,887,360 elements using 335,549,440 bytes\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "total_bytes = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "    total_bytes += param.untyped_storage().nbytes()\n",
    "    print(f\"{param.shape} has {param.numel():,d} elements using {param.untyped_storage().nbytes():,d} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70b6ca1a-1aaf-48b4-a98e-83fc67990053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560990720, 2076188160)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params, total_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da6f30c-30dd-4467-aaa6-b807b954a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos torch.Size([1, 20480, 1, 64]) 1310720 2621440\n",
      "sin torch.Size([1, 20480, 1, 64]) 1310720 2621440\n"
     ]
    }
   ],
   "source": [
    "for name, buf in model.named_buffers():\n",
    "    total_params += buf.numel()\n",
    "    total_bytes += buf.untyped_storage().nbytes()\n",
    "    print(name, buf.shape, buf.numel(), buf.untyped_storage().nbytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4d434df-a887-42ed-9e00-62b53c804668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: 563,612,160\n",
      "total bytes: 2,081,431,040 bytes = 1.94 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"total params: {total_params:,d}\")\n",
    "print(f\"total bytes: {total_bytes:,d} bytes = {total_bytes / 1024 ** 3:2.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cad820-0bc3-470c-b253-9b000d1bd2e9",
   "metadata": {},
   "source": [
    "^ But that leaves out a huge amount (gradients, optimizers, other tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffcf5b-1bac-4ea9-b180-c3f1a30c1bac",
   "metadata": {},
   "source": [
    "### Put estimate code in a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227736f5-9cca-4c20-b2a6-c9ccd5e598a6",
   "metadata": {},
   "source": [
    "I want a function that will spit out my estimate so that I can try various sizes and \"search\" for where I run out of memory on the GPU. Also have function return model config kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a3c14d-763d-42d7-8d4c-b118a1788745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bytes_needed(depth=20, max_seq_len=2048, device_batch_size=32):\n",
    "    num_layers = depth\n",
    "    model_dim = depth * 64\n",
    "    num_heads = max(1, (model_dim + 127) // 128)\n",
    "    num_kv_heads = num_heads\n",
    "    vocab_size = 65537\n",
    "    total_batch_size = device_batch_size * max_seq_len\n",
    "\n",
    "    n_wte_params = vocab_size * model_dim\n",
    "    n_attn_params = num_layers * 4 * model_dim * model_dim # (c_q, c_k, c_v, c_proj)\n",
    "    n_mlp_params = num_layers * 2 * model_dim * 4 * model_dim # (c_fc, c_proj)\n",
    "    n_lm_head_params = model_dim * vocab_size\n",
    "    n_precomputed_cos_sin_params = 10 * max_seq_len * (model_dim // num_heads)\n",
    "\n",
    "    wte_bytes = n_wte_params * 2 # on GPU, wte is set to bfloat16\n",
    "    attn_bytes = n_attn_params * 4\n",
    "    mlp_bytes = n_mlp_params * 4\n",
    "    lm_head_bytes = n_lm_head_params * 4\n",
    "    precomputed_cos_sin_bytes = n_precomputed_cos_sin_params * 2 # uses bfloat16 on CPU and GPU\n",
    "\n",
    "    # adding in gradients and tensors the optimizers must use\n",
    "    wte_bytes += wte_bytes # gradient (not sure what muon is yet and if it needs to maintain other large tensors)\n",
    "    attn_bytes += attn_bytes * 3 # gradient, m and v in AdamW optimizer (moving averages)\n",
    "    mlp_bytes += mlp_bytes * 3 # gradient, m and v in AdamW optimizer \n",
    "    lm_head_bytes += lm_head_bytes * 3 # gradient, m and v in AdamW optimizer\n",
    "\n",
    "    # Really rough...sure none of this is meaningful...more just to think about...also not sure when\n",
    "    # memory can be freed\n",
    "    other_bytes_needed = 0\n",
    "    other_bytes_needed += total_batch_size * 4 # x, the input batch (int32)\n",
    "    other_bytes_needed += total_batch_size * 8 # y, the targets (int64)\n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # the output of hidden layers\n",
    "    other_bytes_needed += total_batch_size * 4 # overall output (the logits)\n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # one more interim tensor (?)\n",
    "\n",
    "    bytes_needed = wte_bytes + attn_bytes + mlp_bytes + lm_head_bytes + precomputed_cos_sin_bytes + other_bytes_needed\n",
    "    print(f\"My (wrong) estimate for bytes needed: {bytes_needed:,d} bytes = {bytes_needed / 1024 ** 3:2.2f} GiB\")\n",
    "\n",
    "    model_config_kwargs = dict(\n",
    "        sequence_len=max_seq_len,\n",
    "        vocab_size=vocab_size,\n",
    "        n_layer=num_layers,\n",
    "        n_head=num_heads,\n",
    "        n_kv_head=num_kv_heads,\n",
    "        n_embd=model_dim\n",
    "    )\n",
    "    \n",
    "    return bytes_needed, model_config_kwargs, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ca32c8-f047-4fa6-9404-fd3ad949f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (wrong) estimate for bytes needed: 8,646,583,296 bytes = 8.05 GiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence_len': 2048,\n",
       " 'vocab_size': 65537,\n",
       " 'n_layer': 20,\n",
       " 'n_head': 10,\n",
       " 'n_kv_head': 10,\n",
       " 'n_embd': 1280}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_needed, model_config_kwargs, vocab_size = estimate_bytes_needed(depth=20, max_seq_len=2048, device_batch_size=32)\n",
    "model_config_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af17bd0e-51d4-4e2c-9eb1-7e55dbb3a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_do_one_training_step(depth, max_seq_len, device_batch_size):\n",
    "\n",
    "    free, _ = torch.cuda.mem_get_info()\n",
    "    print(f\"Free memory before: {free / 1024**3:.2f} GB\")\n",
    "    \n",
    "    device = autodetect_device_type()\n",
    "    autocast_ctx = torch.amp.autocast(device_type=device, dtype=torch.bfloat16) if device == \"cuda\" else nullcontext()\n",
    "    _, model_config_kwargs, vocab_size = estimate_bytes_needed(depth, max_seq_len, device_batch_size)\n",
    "    with torch.device(\"meta\"):\n",
    "        model_config = GPTConfig(**model_config_kwargs)\n",
    "        model = GPT(model_config)\n",
    "    model.to_empty(device=device)\n",
    "    model.init_weights()\n",
    "    optimizers = model.setup_optimizers()\n",
    "    x = torch.randint(0, vocab_size, (device_batch_size, max_seq_len), dtype=torch.int32, device=device)\n",
    "    y = torch.randint(0, vocab_size, (device_batch_size, max_seq_len), dtype=torch.int64, device=device)\n",
    "    with autocast_ctx:\n",
    "        loss = model(x,y)\n",
    "    loss.backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "\n",
    "    free, _ = torch.cuda.mem_get_info()\n",
    "    print(f\"Free memory after: {free / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3371e-6560-4c65-bfef-c99a4b11e98f",
   "metadata": {},
   "source": [
    "### Figure out when we run out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb6c2bf-d949-4df9-9854-d092056f8b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3910d93c-167c-46db-9f58-e670de02cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  4 16:52:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                Off |   00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   29C    P8             11W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f842749a-eee5-483f-b827-d416b06f5c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 7.68 GB\n",
      "Total memory: 7.78 GB\n"
     ]
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"Free memory: {free / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a5a4fd-c722-4013-98fc-bec7ac88939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My estimate for bytes needed: 386,472,960 bytes = 0.36 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 16:53:05.607000 2435 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory after: 7.12 GB\n"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=4, max_seq_len=128, device_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3ebcd-449d-453f-9344-2f5b54dabec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4f88a7-05a2-4c5f-8477-a5717f648ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 7.57 GB\n",
      "Total memory: 7.78 GB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"Free memory: {free / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620af543-137f-413c-962c-989858c176d4",
   "metadata": {},
   "source": [
    "^ Why do we not return to the previous free memory? Won't worry about it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97855e-8c5b-4d13-b1b6-06f5be55d460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e115ba0e-abe8-44b0-af82-d23dc0575413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.57 GB\n",
      "Autodetected device type: cuda\n",
      "My estimate for bytes needed: 8,646,583,296 bytes = 8.05 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 2.08 GiB is free. Including non-PyTorch memory, this process has 5.70 GiB memory in use. Of the allocated memory 5.46 GiB is allocated by PyTorch, and 113.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# guessing this will fail\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 2.08 GiB is free. Including non-PyTorch memory, this process has 5.70 GiB memory in use. Of the allocated memory 5.46 GiB is allocated by PyTorch, and 113.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# guessing this will fail\n",
    "create_model_and_do_one_training_step(depth=20, max_seq_len=2048, device_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464da6dc-c347-48e5-9105-ced10f9eb17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3afadca-7af5-46ac-8621-998ea53869f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd8c801-a7ef-438d-8be4-62f9ba10481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My estimate for bytes needed: 5,106,585,600 bytes = 4.76 GiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5106585600,\n",
       " {'sequence_len': 2048,\n",
       "  'vocab_size': 65537,\n",
       "  'n_layer': 16,\n",
       "  'n_head': 8,\n",
       "  'n_kv_head': 8,\n",
       "  'n_embd': 1024},\n",
       " 65537)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=2048, device_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8268dad5-e61d-4c7f-b739-d3c5f4c836c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My estimate for bytes needed: 5,106,585,600 bytes = 4.76 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 3.68 GiB is free. Including non-PyTorch memory, this process has 4.10 GiB memory in use. Of the allocated memory 3.92 GiB is allocated by PyTorch, and 70.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# guessing this will work\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 3.68 GiB is free. Including non-PyTorch memory, this process has 4.10 GiB memory in use. Of the allocated memory 3.92 GiB is allocated by PyTorch, and 70.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# guessing this will work\n",
    "create_model_and_do_one_training_step(depth=16, max_seq_len=2048, device_batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe412e74-51e8-4eee-9181-78a575f0e2f1",
   "metadata": {},
   "source": [
    "No, didn't work. Maybe I'm grossly underestimating the memory needed for \"interim tensors\" such as to computte scaled dot product attention. Come back to that. Let's try other sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256538-0cc3-42be-901d-514189c380e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d669ba71-6fcc-4021-b864-978fd29420f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2230e27f-67e9-4c53-b337-c4b0d445c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (wrong) estimate for bytes needed: 4,837,625,856 bytes = 4.51 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 591.81 MiB is free. Including non-PyTorch memory, this process has 7.20 GiB memory in use. Of the allocated memory 6.38 GiB is allocated by PyTorch, and 722.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 7.78 GiB of which 591.81 MiB is free. Including non-PyTorch memory, this process has 7.20 GiB memory in use. Of the allocated memory 6.38 GiB is allocated by PyTorch, and 722.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=2048, device_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c26764-8c07-47f0-ab0e-28db3f7cd1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9965938a-7205-4a29-aa55-7305150a14d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c282b06-9890-4f58-a6a3-287e8ab9e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (wrong) estimate for bytes needed: 4,700,524,544 bytes = 4.38 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 77.81 MiB is free. Including non-PyTorch memory, this process has 7.70 GiB memory in use. Of the allocated memory 7.56 GiB is allocated by PyTorch, and 26.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 77.81 MiB is free. Including non-PyTorch memory, this process has 7.70 GiB memory in use. Of the allocated memory 7.56 GiB is allocated by PyTorch, and 26.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=1024, device_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b80c3-42e3-46f6-ae76-c8e80dfaacdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7849ac-00e4-48fc-b4d5-802f168d8b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba884dc-0c4d-4515-96b5-28668d109154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (wrong) estimate for bytes needed: 4,631,973,888 bytes = 4.31 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 125.81 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.51 GiB is allocated by PyTorch, and 29.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 125.81 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.51 GiB is allocated by PyTorch, and 29.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6612b8d-f193-49ba-ab50-0da7c5cc195f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1ba866-7d89-48b0-998c-230ca423d109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b39f2ab-6489-49f7-9847-43d0749f399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (wrong) estimate for bytes needed: 4,598,353,920 bytes = 4.28 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 331.81 MiB is free. Including non-PyTorch memory, this process has 7.45 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 48.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:215\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m    214\u001b[0m softcap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m--> 215\u001b[0m logits \u001b[38;5;241m=\u001b[39m softcap \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 331.81 MiB is free. Including non-PyTorch memory, this process has 7.45 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 48.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d88a9-a9e0-4abb-bbf8-1de72cc18b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8caf1ac-4657-415b-a21f-8f3e087f6cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5e11f1-abec-413a-94a1-ca99cce94874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (wrong) estimate for bytes needed: 4,580,888,576 bytes = 4.27 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 17:09:29.880000 3150 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory after: 1.63 GB\n"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=256, device_batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be993a-9a73-4774-b7b5-f6be4ba123e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01a7f7c5-e3ba-43e9-a7f0-cba1ae22107c",
   "metadata": {},
   "source": [
    "So the above shows I'm completely wrong about how much memory is needed. Now why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05243cf-d6e7-4361-92b5-1ffaa438d843",
   "metadata": {},
   "source": [
    "Well, first, my thinking that the \"other\" category was not meaningful seems totally wrong. These interim tensors take up a lot of space.\n",
    "\n",
    "Also, that last error where it runs out while allocating space for that logits calculation makes me realize that this part of my estimate: `other_bytes_needed += total_batch_size * 4 # overall output (the logits)` is missing vocab_size! It's not one float32 per output token (total_batch_size), it's one per vocab! So it should be `other_bytes_needed += total_batch_size * vocab_size * 4`, 65537 times greater in this case.\n",
    "\n",
    "For the MLP, we go from model_dim to 4 x model_dim and then back to model_dim. I didn't really think about that before. That interim is going live in a tensor with this many bytes: `total_batch_size * 4 * model_dim * 4`.\n",
    "\n",
    "Feel like I need to understand more about how memory freeing works. Come back to that.\n",
    "\n",
    "In the errors above the last one it hits a problem here: `F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)`. Looking at where I tried the documentation version of that function in challenge 10, it looks like we'll need to allocate a `total_batch_size * model_dim * 4` byte tensor for the result and a `total_batch_size * max_seq_length * 4` byte tensor for attn_weight. (Not very confident in that. Only looked quickly. And also that's not necessarily how it's implemented.)\n",
    "\n",
    "Let's write a new estimate_bytes_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c632a44-39e6-4d25-a4c9-af660cdfc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bytes_needed(depth=20, max_seq_len=2048, device_batch_size=32):\n",
    "    num_layers = depth\n",
    "    model_dim = depth * 64\n",
    "    num_heads = max(1, (model_dim + 127) // 128)\n",
    "    num_kv_heads = num_heads\n",
    "    vocab_size = 65537\n",
    "    total_batch_size = device_batch_size * max_seq_len\n",
    "\n",
    "    n_wte_params = vocab_size * model_dim\n",
    "    n_attn_params = num_layers * 4 * model_dim * model_dim # (c_q, c_k, c_v, c_proj)\n",
    "    n_mlp_params = num_layers * 2 * model_dim * 4 * model_dim # (c_fc, c_proj)\n",
    "    n_lm_head_params = model_dim * vocab_size\n",
    "    n_precomputed_cos_sin_params = 10 * max_seq_len * (model_dim // num_heads)\n",
    "\n",
    "    wte_bytes = n_wte_params * 2 # on GPU, wte is set to bfloat16\n",
    "    attn_bytes = n_attn_params * 4\n",
    "    mlp_bytes = n_mlp_params * 4\n",
    "    lm_head_bytes = n_lm_head_params * 4\n",
    "    precomputed_cos_sin_bytes = n_precomputed_cos_sin_params * 2 # uses bfloat16 on CPU and GPU\n",
    "\n",
    "    # adding in gradients and tensors the optimizers must use\n",
    "    wte_bytes += wte_bytes # gradient (not sure what muon is yet and if it needs to maintain other large tensors)\n",
    "    attn_bytes += attn_bytes * 3 # gradient, m and v in AdamW optimizer (moving averages)\n",
    "    mlp_bytes += mlp_bytes * 3 # gradient, m and v in AdamW optimizer \n",
    "    lm_head_bytes += lm_head_bytes * 3 # gradient, m and v in AdamW optimizer\n",
    "\n",
    "    # Really rough...sure none of this is meaningful...more just to think about...also not sure when\n",
    "    # memory can be freed\n",
    "    other_bytes_needed = 0\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * 4 # x, the input batch (int32)\n",
    "    other_bytes_needed += total_batch_size * 8 # y, the targets (int64)\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # the input/output of blocks\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # c_q result inside attn\n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # c_k result inside attn\n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # c_v result inside attn\n",
    "    other_bytes_needed += total_batch_size * max_seq_len * 4 # attn_weight inside F.scaled_dot_product_attention\n",
    "    other_bytes_needed += total_batch_size * model_dim * 4 # for result of F.scaled_dot_product_attention\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * 4 * model_dim * 4 # the interim inside the MLP\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * vocab_size * 4 # overall output (the logits), fixed from above\n",
    "\n",
    "    bytes_needed = wte_bytes + attn_bytes + mlp_bytes + lm_head_bytes + precomputed_cos_sin_bytes + other_bytes_needed\n",
    "    print(f\"My (prob still wrong) estimate for bytes needed: {bytes_needed:,d} bytes = {bytes_needed / 1024 ** 3:2.2f} GiB\")\n",
    "\n",
    "    model_config_kwargs = dict(\n",
    "        sequence_len=max_seq_len,\n",
    "        vocab_size=vocab_size,\n",
    "        n_layer=num_layers,\n",
    "        n_head=num_heads,\n",
    "        n_kv_head=num_kv_heads,\n",
    "        n_embd=model_dim\n",
    "    )\n",
    "    \n",
    "    return bytes_needed, model_config_kwargs, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c7180c-2d55-4cf4-9f91-7ead5d9f7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 28,712,133,632 bytes = 26.74 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=20, max_seq_len=2048, device_batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a07bf6-43f1-4fb0-b4ea-47a78289772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 5,178,576,896 bytes = 4.82 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=256, device_batch_size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "698b195f-b0a5-4537-9894-ea58b3cc5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 5,797,924,864 bytes = 5.40 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=512, device_batch_size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b467e16-92e4-4002-9d58-035abe7f6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 7,031,115,776 bytes = 6.55 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=512, device_batch_size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "340fe909-6427-4a31-817e-2aaccd8bdff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 9,497,497,600 bytes = 8.85 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=512, device_batch_size=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c12b89-5784-484c-9907-c89a5bf5fa15",
   "metadata": {},
   "source": [
    "Let's try a few and see if we're closer to when we actually run out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7618d2a8-d1f5-498d-bc8c-ca39e2c54da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from above\n",
    "\n",
    "def create_model_and_do_one_training_step(depth, max_seq_len, device_batch_size):\n",
    "\n",
    "    free, _ = torch.cuda.mem_get_info()\n",
    "    print(f\"Free memory before: {free / 1024**3:.2f} GB\")\n",
    "    \n",
    "    device = autodetect_device_type()\n",
    "    autocast_ctx = torch.amp.autocast(device_type=device, dtype=torch.bfloat16) if device == \"cuda\" else nullcontext()\n",
    "    _, model_config_kwargs, vocab_size = estimate_bytes_needed(depth, max_seq_len, device_batch_size)\n",
    "    with torch.device(\"meta\"):\n",
    "        model_config = GPTConfig(**model_config_kwargs)\n",
    "        model = GPT(model_config)\n",
    "    model.to_empty(device=device)\n",
    "    model.init_weights()\n",
    "    optimizers = model.setup_optimizers()\n",
    "    x = torch.randint(0, vocab_size, (device_batch_size, max_seq_len), dtype=torch.int32, device=device)\n",
    "    y = torch.randint(0, vocab_size, (device_batch_size, max_seq_len), dtype=torch.int64, device=device)\n",
    "    with autocast_ctx:\n",
    "        loss = model(x,y)\n",
    "    loss.backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "\n",
    "    free, _ = torch.cuda.mem_get_info()\n",
    "    print(f\"Free memory after: {free / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0c1c1e-21f5-44c7-9fd7-5b484deb8eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae434700-1293-44f2-b83c-703c4b002986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (prob still wrong) estimate for bytes needed: 7,031,115,776 bytes = 6.55 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 125.81 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.51 GiB is allocated by PyTorch, and 29.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:210\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:96\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_sin, kv_cache):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:72\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, cos_sin, kv_cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head\n\u001b[1;32m     70\u001b[0m enable_gqa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_head \u001b[38;5;66;03m# always false for now\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_gqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 125.81 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.51 GiB is allocated by PyTorch, and 29.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e7032-500e-43f0-bb32-ed60720edd20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81285db-e204-4a97-955c-0a81b2156ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1627605-6676-4185-a3cc-700734badaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (prob still wrong) estimate for bytes needed: 5,797,924,864 bytes = 5.40 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 331.81 MiB is free. Including non-PyTorch memory, this process has 7.45 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 48.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (device_batch_size, max_seq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py:215\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m    214\u001b[0m softcap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m--> 215\u001b[0m logits \u001b[38;5;241m=\u001b[39m softcap \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 331.81 MiB is free. Including non-PyTorch memory, this process has 7.45 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 48.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585835a-15f7-499b-b1a4-e928ed641713",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4658a794-e28f-488a-9e0a-310c329e0107",
   "metadata": {},
   "source": [
    "One thing I'm not doing, but the actual training does, is compile the model first. From some quick reading it looks like that does not reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23583ed7-2db8-427b-89b4-f1115c7902c4",
   "metadata": {},
   "source": [
    "Ooh, from asking ChatGPT \"When running the forward of a model, will python/pytorch re-use / free up memory as needed?,\" it pointed out that \"In training with autograd, intermediates needed for backprop must be kept until backward() finishes.\" Won't all of them be needed then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0f338c-b17d-44d3-94bc-1544afffdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bytes_needed(depth=20, max_seq_len=2048, device_batch_size=32):\n",
    "    num_layers = depth\n",
    "    model_dim = depth * 64\n",
    "    num_heads = max(1, (model_dim + 127) // 128)\n",
    "    num_kv_heads = num_heads\n",
    "    vocab_size = 65537\n",
    "    total_batch_size = device_batch_size * max_seq_len\n",
    "\n",
    "    n_wte_params = vocab_size * model_dim\n",
    "    n_attn_params = num_layers * 4 * model_dim * model_dim # (c_q, c_k, c_v, c_proj)\n",
    "    n_mlp_params = num_layers * 2 * model_dim * 4 * model_dim # (c_fc, c_proj)\n",
    "    n_lm_head_params = model_dim * vocab_size\n",
    "    n_precomputed_cos_sin_params = 10 * max_seq_len * (model_dim // num_heads)\n",
    "\n",
    "    wte_bytes = n_wte_params * 2 # on GPU, wte is set to bfloat16\n",
    "    attn_bytes = n_attn_params * 4\n",
    "    mlp_bytes = n_mlp_params * 4\n",
    "    lm_head_bytes = n_lm_head_params * 4\n",
    "    precomputed_cos_sin_bytes = n_precomputed_cos_sin_params * 2 # uses bfloat16 on CPU and GPU\n",
    "\n",
    "    # adding in gradients and tensors the optimizers must use\n",
    "    wte_bytes += wte_bytes # gradient (not sure what muon is yet and if it needs to maintain other large tensors)\n",
    "    attn_bytes += attn_bytes * 3 # gradient, m and v in AdamW optimizer (moving averages)\n",
    "    mlp_bytes += mlp_bytes * 3 # gradient, m and v in AdamW optimizer \n",
    "    lm_head_bytes += lm_head_bytes * 3 # gradient, m and v in AdamW optimizer\n",
    "\n",
    "    # Really rough...sure none of this is meaningful...more just to think about...also not sure when\n",
    "    # memory can be freed\n",
    "    other_bytes_needed = 0\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * 4 # x, the input batch (int32)\n",
    "    other_bytes_needed += total_batch_size * 8 # y, the targets (int64)\n",
    "\n",
    "    bytes_needed_to_compute_block = 0\n",
    "    bytes_needed_to_compute_block += total_batch_size * model_dim * 4 # the input/output of blocks\n",
    "    bytes_needed_to_compute_block += total_batch_size * model_dim * 4 # c_q result inside attn\n",
    "    bytes_needed_to_compute_block += total_batch_size * model_dim * 4 # c_k result inside attn\n",
    "    bytes_needed_to_compute_block += total_batch_size * model_dim * 4 # c_v result inside attn\n",
    "    bytes_needed_to_compute_block += total_batch_size * max_seq_len * 4 # attn_weight inside F.scaled_dot_product_attention\n",
    "    bytes_needed_to_compute_block += model_dim * 4 # for result of F.scaled_dot_product_attention\n",
    "    bytes_needed_to_compute_block += total_batch_size * 4 * model_dim * 4 # the interim inside the MLP\n",
    "\n",
    "    other_bytes_needed += num_layers * bytes_needed_to_compute_block\n",
    "    \n",
    "    other_bytes_needed += total_batch_size * vocab_size * 4 # overall output (the logits), fixed from above\n",
    "\n",
    "    bytes_needed = wte_bytes + attn_bytes + mlp_bytes + lm_head_bytes + precomputed_cos_sin_bytes + other_bytes_needed\n",
    "    print(f\"My (prob still wrong) estimate for bytes needed: {bytes_needed:,d} bytes = {bytes_needed / 1024 ** 3:2.2f} GiB\")\n",
    "\n",
    "    model_config_kwargs = dict(\n",
    "        sequence_len=max_seq_len,\n",
    "        vocab_size=vocab_size,\n",
    "        n_layer=num_layers,\n",
    "        n_head=num_heads,\n",
    "        n_kv_head=num_kv_heads,\n",
    "        n_embd=model_dim\n",
    "    )\n",
    "    \n",
    "    return bytes_needed, model_config_kwargs, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378b38d7-94ef-4dc5-8014-ca3ab7a66f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 6,208,344,064 bytes = 5.78 GiB\n"
     ]
    }
   ],
   "source": [
    "# look at the config that worked above\n",
    "estimate_bytes_needed(depth=16, max_seq_len=256, device_batch_size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89590430-53a6-4c9a-8c96-19a070674190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 11,275,816,960 bytes = 10.50 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=512, device_batch_size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c694b10-912c-44fe-a50e-f22371bfefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My (prob still wrong) estimate for bytes needed: 7,081,431,040 bytes = 6.60 GiB\n"
     ]
    }
   ],
   "source": [
    "estimate_bytes_needed(depth=16, max_seq_len=512, device_batch_size=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6cb8-1cdf-4764-98cd-72485a682bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62d517f-bb6e-401e-9c28-8e009e034fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527488a0-b3a2-41ab-a328-3a010979768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (prob still wrong) estimate for bytes needed: 7,081,431,040 bytes = 6.60 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 625.81 MiB is free. Including non-PyTorch memory, this process has 7.17 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 33.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model_and_do_one_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mcreate_model_and_do_one_training_step\u001b[0;34m(depth, max_seq_len, device_batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(x,y)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n\u001b[1;32m     23\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 625.81 MiB is free. Including non-PyTorch memory, this process has 7.17 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 33.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f41783-ae48-4296-af26-65f8ee467db9",
   "metadata": {},
   "source": [
    "^ Ran out of memory on backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea52a38-9e14-4d2f-94c1-f7d74c9b2214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206b9a3f-3491-41e6-9bef-51dfbed61e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restart kernel ##\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "from my_nanochat.my_common import autodetect_device_type\n",
    "torch.cuda.memory_allocated() # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5783544-63ca-4d00-a8d8-42b69be9da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "My (prob still wrong) estimate for bytes needed: 6,242,553,856 bytes = 5.81 GiB\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 22:48:25.677000 2824 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory after: 1.37 GB\n"
     ]
    }
   ],
   "source": [
    "create_model_and_do_one_training_step(depth=16, max_seq_len=512, device_batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40294ad2-1e99-4bcf-9726-39845fc79388",
   "metadata": {},
   "source": [
    "### A few thoughts on what to do next...\n",
    "\n",
    "1) It's annoying to keep restarting kernel. Make a create_model_and_do_one_training_step script.\n",
    "2) Instrument the model to show torch.cuda.memory_allocated() along the way (even though as we saw above you can't perfectly judge by that)\n",
    "3) See if it's possible to get a complete list of allocated tensors from torch or the python garbage collector so I can see what they are each point\n",
    "4) Create a toy model that does have intermediate tensors to understand memory requirements for forward passes with autograd and when there is an optimier so I can build an intuition for a simpler case\n",
    "5) Read some stuff / ask ChatGPT about estimating total memory requirements\n",
    "6) Just move on\n",
    "\n",
    "Let me at least do 1, then might skip to 5 and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323577a-fa77-4c52-8c81-42ba99373164",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803d79b-c5ef-4961-9884-908ba9d75b22",
   "metadata": {},
   "source": [
    "Wrote `create_model_and_do_one_training_step.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6beea9e5-68b8-4be7-80b8-041e3ec590ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: create_model_and_do_one_training_step.py [-h] --depth DEPTH\n",
      "                                                --max-seq-len MAX_SEQ_LEN\n",
      "                                                --device-batch-size\n",
      "                                                DEVICE_BATCH_SIZE\n",
      "create_model_and_do_one_training_step.py: error: the following arguments are required: --depth, --max-seq-len, --device-batch-size\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11afcdd8-30bf-4038-a216-53d2e011fb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 512, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "W1105 18:41:27.016000 2616 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "Free memory after: 1.37 GB\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 512 --device-batch-size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b515f3cb-1c36-4499-876c-53964adf183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 512, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "W1105 18:42:31.130000 2693 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "Free memory after: 0.21 GB\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 512 --device-batch-size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86188ff7-9dc3-45b5-a8c0-c470fd418316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 1024, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 59, in <module>\n",
      "    create_model_and_do_one_training_step(args.depth, args.max_seq_len, args.device_batch_size)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 43, in create_model_and_do_one_training_step\n",
      "    loss = model(x,y)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py\", line 213, in forward\n",
      "    logits = self.lm_head(x)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 297.81 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 78.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 1024 --device-batch-size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50da59fc-0d74-4dbf-8387-db339a658131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 512, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "W1105 18:45:35.030000 2844 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "Free memory after: 3.58 GB\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 512 --device-batch-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9ceb616-b229-4be8-ab37-2fcbd3feda34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory before: 7.68 GB\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 1024, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "W1105 18:46:18.751000 2915 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "Free memory after: 3.42 GB\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 1024 --device-batch-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d02a607-819a-4fea-94be-89acaf47fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Torch memory allocated: 0.00 GiB (free = 7.68 GiB)\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 1024, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "After creating model: Torch memory allocated: 1.25 GiB (free = 6.43 GiB)\n",
      "After initializing weights: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "After setting up optimizers: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "After creating x and y: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "After forward: Torch memory allocated: 5.22 GiB (free = 1.37 GiB)\n",
      "After backward: Torch memory allocated: 2.27 GiB (free = 0.87 GiB)\n",
      "After optimizer 0 step: Torch memory allocated: 3.02 GiB (free = 0.87 GiB)\n",
      "W1105 19:03:25.518000 3859 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "After optimizer 1 step: Torch memory allocated: 3.77 GiB (free = 0.87 GiB)\n",
      "After: Torch memory allocated: 3.77 GiB (free = 0.87 GiB)\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 1024 --device-batch-size=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a9882-cdea-44b6-ada9-04b0b4a2b578",
   "metadata": {},
   "source": [
    "^ does look like the muon optimizer used up a bunch of memory and I ignored that in my esimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3e91d98-338f-48ff-8246-0a29f6b8ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Torch memory allocated: 0.00 GiB (free = 7.68 GiB)\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 1024, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "After creating model: Torch memory allocated: 1.25 GiB (free = 6.43 GiB)\n",
      "After initializing weights: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "After setting up optimizers: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "After creating x and y: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 64, in <module>\n",
      "    create_model_and_do_one_training_step(args.depth, args.max_seq_len, args.device_batch_size)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 46, in create_model_and_do_one_training_step\n",
      "    loss = model(x,y)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py\", line 213, in forward\n",
      "    logits = self.lm_head(x)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 297.81 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 78.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 1024 --device-batch-size=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeac0fd-1cc0-415e-9948-87c66de0b2bc",
   "metadata": {},
   "source": [
    "^ as expected with the same depth and max seq length as the previous try that worked, memory allocated up to the forward step is the same (or prob around the same since x and y should be bigger). But with the bigger batch size the forward fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a7190-0deb-422b-b6ad-bfcdf62a9e99",
   "metadata": {},
   "source": [
    "For now, add some quick and dirty memory logging to my_gpt.py which I'll undo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91b81be7-44e0-411a-8abb-c7bd15f6eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Torch memory allocated: 0.00 GiB (free = 7.68 GiB)\n",
      "Autodetected device type: cuda\n",
      "{'sequence_len': 1024, 'vocab_size': 65537, 'n_layer': 16, 'n_head': 8, 'n_kv_head': 8, 'n_embd': 1024}\n",
      "After creating model: Torch memory allocated: 1.25 GiB (free = 6.43 GiB)\n",
      "After initializing weights: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1024/768) = 0.8660254037844387\n",
      "After setting up optimizers: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "After creating x and y: Torch memory allocated: 1.13 GiB (free = 6.27 GiB)\n",
      "After wte: Torch memory allocated: 1.14 GiB (free = 6.27 GiB)\n",
      "After norm: Torch memory allocated: 1.14 GiB (free = 6.27 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 1.42 GiB (free = 6.01 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 1.54 GiB (free = 6.01 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 1.53 GiB (free = 6.01 GiB)\n",
      "After block: Torch memory allocated: 1.53 GiB (free = 6.01 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 1.79 GiB (free = 5.66 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 1.92 GiB (free = 5.66 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 1.90 GiB (free = 5.66 GiB)\n",
      "After block: Torch memory allocated: 1.90 GiB (free = 5.66 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 2.17 GiB (free = 5.30 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 2.29 GiB (free = 5.24 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 2.28 GiB (free = 5.24 GiB)\n",
      "After block: Torch memory allocated: 2.28 GiB (free = 5.24 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 2.54 GiB (free = 4.91 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 2.67 GiB (free = 4.91 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 2.65 GiB (free = 4.91 GiB)\n",
      "After block: Torch memory allocated: 2.65 GiB (free = 4.91 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 2.92 GiB (free = 4.53 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 3.04 GiB (free = 4.47 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 3.03 GiB (free = 4.47 GiB)\n",
      "After block: Torch memory allocated: 3.03 GiB (free = 4.47 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 3.29 GiB (free = 4.17 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 3.42 GiB (free = 4.17 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 3.40 GiB (free = 4.17 GiB)\n",
      "After block: Torch memory allocated: 3.40 GiB (free = 4.17 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 3.67 GiB (free = 3.80 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 3.80 GiB (free = 3.73 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 3.78 GiB (free = 3.73 GiB)\n",
      "After block: Torch memory allocated: 3.78 GiB (free = 3.73 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 4.05 GiB (free = 3.42 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 4.17 GiB (free = 3.36 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 4.16 GiB (free = 3.36 GiB)\n",
      "After block: Torch memory allocated: 4.16 GiB (free = 3.36 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 4.42 GiB (free = 3.04 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 4.55 GiB (free = 2.98 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 4.53 GiB (free = 2.98 GiB)\n",
      "After block: Torch memory allocated: 4.53 GiB (free = 2.98 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 4.80 GiB (free = 2.67 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 4.92 GiB (free = 2.61 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 4.91 GiB (free = 2.61 GiB)\n",
      "After block: Torch memory allocated: 4.91 GiB (free = 2.61 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 5.17 GiB (free = 2.29 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 5.30 GiB (free = 2.23 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 5.28 GiB (free = 2.23 GiB)\n",
      "After block: Torch memory allocated: 5.28 GiB (free = 2.23 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 5.55 GiB (free = 1.92 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 5.67 GiB (free = 1.86 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 5.66 GiB (free = 1.86 GiB)\n",
      "After block: Torch memory allocated: 5.66 GiB (free = 1.86 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 5.92 GiB (free = 1.54 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 6.05 GiB (free = 1.48 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 6.03 GiB (free = 1.48 GiB)\n",
      "After block: Torch memory allocated: 6.03 GiB (free = 1.48 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 6.30 GiB (free = 1.17 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 6.42 GiB (free = 1.11 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 6.41 GiB (free = 1.11 GiB)\n",
      "After block: Torch memory allocated: 6.41 GiB (free = 1.11 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 6.67 GiB (free = 0.79 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 6.80 GiB (free = 0.73 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 6.78 GiB (free = 0.73 GiB)\n",
      "After block: Torch memory allocated: 6.78 GiB (free = 0.73 GiB)\n",
      "   After MLP c_fc: Torch memory allocated: 7.05 GiB (free = 0.42 GiB)\n",
      "   After MLP RELU squared: Torch memory allocated: 7.17 GiB (free = 0.35 GiB)\n",
      "   After MLP c_proj: Torch memory allocated: 7.16 GiB (free = 0.35 GiB)\n",
      "After block: Torch memory allocated: 7.16 GiB (free = 0.35 GiB)\n",
      "After norm: Torch memory allocated: 7.17 GiB (free = 0.35 GiB)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 64, in <module>\n",
      "    create_model_and_do_one_training_step(args.depth, args.max_seq_len, args.device_batch_size)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/create_model_and_do_one_training_step.py\", line 46, in create_model_and_do_one_training_step\n",
      "    loss = model(x,y)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/challenge-15-understand-memory-needed/../my_nanochat/my_nanochat/my_gpt.py\", line 220, in forward\n",
      "    logits = self.lm_head(x)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 297.81 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 78.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python create_model_and_do_one_training_step.py --depth 16 --max-seq-len 1024 --device-batch-size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbed0b5-25cf-48fe-9d3d-84a8e2de88d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
