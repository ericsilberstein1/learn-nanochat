{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771b8558-92f8-48c9-9171-2d17b252414a",
   "metadata": {},
   "source": [
    "Contents:\n",
    "\n",
    "A) Trying my rust_tokenizer\n",
    "\n",
    "B) Trying my Tokenizer python class\n",
    "\n",
    "C) Trying other misc rust functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22af509-13c1-462b-9a53-e1186f887034",
   "metadata": {},
   "source": [
    "To build the rust library:\n",
    "\n",
    "```\n",
    "cd rust_tokenizer\n",
    "maturin develop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef43668-97fd-4283-8760-06469711621b",
   "metadata": {},
   "source": [
    "## A) Trying my rust_tokenizer\n",
    "\n",
    "`rust_tokenizer/src/lib.rs` is my partial copy of nanochat's [rustbpe](https://github.com/karpathy/nanochat/blob/master/rustbpe/src/lib.rs). It copies all the same techniques for doing things efficiently, for example, counting in parallel and careful bookkeeping to avoid recomputing, unlike my play examples in challenges 1 and 3. I added extremely verbose output to help understand exactly how the whole thing works. This is only useful if training on a few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51335480-ebe8-49a4-8161-6584b9e307a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/karpathy/nanochat/blob/master/nanochat/tokenizer.py\n",
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "881cb497-190f-4b25-b60f-50d979b557f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rust_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9b34cba-8352-4cb1-9c41-297a79a83d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = rust_tokenizer.Tokenizer(debug_print = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be1360cf-e680-4539-b348-58c60eea99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start of iterating through text passed in, splitting it into words, and counting----\n",
      "\n",
      "just filled one buffer under GIL, buffer contains 1 strings\n",
      "will now split into words and count in parallel without holding GIL\n",
      "finished splitting and counting for this buffer, 'local' counts map: {\" the\": 1, \" cat\": 2, \"the\": 1}\n",
      "\n",
      "just filled one buffer under GIL, buffer contains 1 strings\n",
      "will now split into words and count in parallel without holding GIL\n",
      "finished splitting and counting for this buffer, 'local' counts map: {\"hi\": 1, \" cat\": 1}\n",
      "\n",
      "----end of iterating through text passed in, splitting it into words, and counting----\n",
      "\n",
      "counts map: {\"hi\": 1, \" the\": 1, \"the\": 1, \" cat\": 3}\n",
      "\n",
      "words: [Word { ids: [104, 105] }, Word { ids: [32, 116, 104, 101] }, Word { ids: [116, 104, 101] }, Word { ids: [32, 99, 97, 116] }]\n",
      "\n",
      "cvec: [1, 1, 1, 3]\n",
      "\n",
      "as an example, here are the pairs from the first word: [(104, 105)]\n",
      "\n",
      "Will now form and count all pairs, keeping track of which word(s) each pair comes from\n",
      "\n",
      "pair -> count: {(32, 99): 3, (99, 97): 3, (32, 116): 1, (104, 105): 1, (104, 101): 2, (116, 104): 2, (97, 116): 3}\n",
      "\n",
      "pair -> indices: {(32, 99): {3}, (99, 97): {3}, (116, 104): {1, 2}, (32, 116): {1}, (104, 105): {0}, (104, 101): {1, 2}, (97, 116): {3}}\n",
      "\n",
      "Heap after adding initial pairs, shown sorted: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }, MergeJob { pair: (116, 104), count: 2, pos: {1, 2} }, MergeJob { pair: (104, 101), count: 2, pos: {1, 2} }, MergeJob { pair: (99, 97), count: 3, pos: {3} }, MergeJob { pair: (97, 116), count: 3, pos: {3} }, MergeJob { pair: (32, 99), count: 3, pos: {3} }]\n",
      "\n",
      "Will now merge until we have 744 merges for a total vocab size of 1000 or there is nothing left to merge.\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (32, 99) with new id 256\n",
      "\n",
      "About to merge pair (32, 99) into word Word { ids: [32, 99, 97, 116] } with new id 256\n",
      "After merge, word is Word { ids: [256, 97, 116] }\n",
      "and changes/deltas vector is [((32, 99), -1), ((99, 97), -1), ((256, 97), 1)]\n",
      "changing global count for pair (32, 99) by -3 to 0\n",
      "changing global count for pair (99, 97) by -3 to 0\n",
      "changing global count for pair (256, 97) by 3 to 3\n",
      "\n",
      "----- end of merge in of pair (32, 99) with id 256 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [32, 116, 104, 101] }, Word { ids: [116, 104, 101] }, Word { ids: [256, 97, 116] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 2, (104, 105): 1, (97, 116): 3, (256, 97): 3, (32, 116): 1, (116, 104): 2}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }, MergeJob { pair: (116, 104), count: 2, pos: {1, 2} }, MergeJob { pair: (104, 101), count: 2, pos: {1, 2} }, MergeJob { pair: (256, 97), count: 3, pos: {3} }, MergeJob { pair: (99, 97), count: 3, pos: {3} }, MergeJob { pair: (97, 116), count: 3, pos: {3} }]\n",
      "\n",
      "Merges: {(32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (97, 116) with new id 257\n",
      "\n",
      "About to merge pair (97, 116) into word Word { ids: [256, 97, 116] } with new id 257\n",
      "After merge, word is Word { ids: [256, 257] }\n",
      "and changes/deltas vector is [((256, 97), -1), ((256, 257), 1), ((97, 116), -1)]\n",
      "changing global count for pair (256, 97) by -3 to 0\n",
      "changing global count for pair (256, 257) by 3 to 3\n",
      "changing global count for pair (97, 116) by -3 to 0\n",
      "\n",
      "----- end of merge in of pair (97, 116) with id 257 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [32, 116, 104, 101] }, Word { ids: [116, 104, 101] }, Word { ids: [256, 257] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 2, (104, 105): 1, (97, 116): 0, (256, 97): 0, (32, 116): 1, (256, 257): 3, (116, 104): 2}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }, MergeJob { pair: (116, 104), count: 2, pos: {1, 2} }, MergeJob { pair: (104, 101), count: 2, pos: {1, 2} }, MergeJob { pair: (256, 257), count: 3, pos: {3} }, MergeJob { pair: (256, 97), count: 3, pos: {3} }, MergeJob { pair: (99, 97), count: 3, pos: {3} }]\n",
      "\n",
      "Merges: {(97, 116): 257, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (256, 257) with new id 258\n",
      "\n",
      "About to merge pair (256, 257) into word Word { ids: [256, 257] } with new id 258\n",
      "After merge, word is Word { ids: [258] }\n",
      "and changes/deltas vector is [((256, 257), -1)]\n",
      "changing global count for pair (256, 257) by -3 to 0\n",
      "\n",
      "----- end of merge in of pair (256, 257) with id 258 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [32, 116, 104, 101] }, Word { ids: [116, 104, 101] }, Word { ids: [258] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 2, (104, 105): 1, (97, 116): 0, (256, 97): 0, (32, 116): 1, (256, 257): 0, (116, 104): 2}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }, MergeJob { pair: (116, 104), count: 2, pos: {1, 2} }, MergeJob { pair: (104, 101), count: 2, pos: {1, 2} }]\n",
      "\n",
      "Merges: {(97, 116): 257, (256, 257): 258, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (104, 101) with new id 259\n",
      "\n",
      "About to merge pair (104, 101) into word Word { ids: [32, 116, 104, 101] } with new id 259\n",
      "After merge, word is Word { ids: [32, 116, 259] }\n",
      "and changes/deltas vector is [((116, 104), -1), ((116, 259), 1), ((104, 101), -1)]\n",
      "changing global count for pair (116, 104) by -1 to 1\n",
      "changing global count for pair (116, 259) by 1 to 1\n",
      "changing global count for pair (104, 101) by -1 to 1\n",
      "\n",
      "About to merge pair (104, 101) into word Word { ids: [116, 104, 101] } with new id 259\n",
      "After merge, word is Word { ids: [116, 259] }\n",
      "and changes/deltas vector is [((116, 104), -1), ((116, 259), 1), ((104, 101), -1)]\n",
      "changing global count for pair (116, 104) by -1 to 0\n",
      "changing global count for pair (116, 259) by 1 to 2\n",
      "changing global count for pair (104, 101) by -1 to 0\n",
      "\n",
      "----- end of merge in of pair (104, 101) with id 259 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [32, 116, 259] }, Word { ids: [116, 259] }, Word { ids: [258] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 0, (104, 105): 1, (97, 116): 0, (256, 97): 0, (116, 259): 2, (32, 116): 1, (256, 257): 0, (116, 104): 0}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }, MergeJob { pair: (116, 259), count: 2, pos: {2, 1} }, MergeJob { pair: (116, 104), count: 2, pos: {1, 2} }]\n",
      "\n",
      "Merges: {(104, 101): 259, (97, 116): 257, (256, 257): 258, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (116, 259) with new id 260\n",
      "\n",
      "About to merge pair (116, 259) into word Word { ids: [116, 259] } with new id 260\n",
      "After merge, word is Word { ids: [260] }\n",
      "and changes/deltas vector is [((116, 259), -1)]\n",
      "changing global count for pair (116, 259) by -1 to 1\n",
      "\n",
      "About to merge pair (116, 259) into word Word { ids: [32, 116, 259] } with new id 260\n",
      "After merge, word is Word { ids: [32, 260] }\n",
      "and changes/deltas vector is [((32, 116), -1), ((32, 260), 1), ((116, 259), -1)]\n",
      "changing global count for pair (32, 116) by -1 to 0\n",
      "changing global count for pair (32, 260) by 1 to 1\n",
      "changing global count for pair (116, 259) by -1 to 0\n",
      "\n",
      "----- end of merge in of pair (116, 259) with id 260 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [32, 260] }, Word { ids: [260] }, Word { ids: [258] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 0, (104, 105): 1, (97, 116): 0, (256, 97): 0, (32, 260): 1, (116, 259): 0, (32, 116): 0, (256, 257): 0, (116, 104): 0}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }, MergeJob { pair: (32, 260), count: 1, pos: {1} }, MergeJob { pair: (32, 116), count: 1, pos: {1} }]\n",
      "\n",
      "Merges: {(104, 101): 259, (116, 259): 260, (97, 116): 257, (256, 257): 258, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (32, 260) with new id 261\n",
      "\n",
      "About to merge pair (32, 260) into word Word { ids: [32, 260] } with new id 261\n",
      "After merge, word is Word { ids: [261] }\n",
      "and changes/deltas vector is [((32, 260), -1)]\n",
      "changing global count for pair (32, 260) by -1 to 0\n",
      "\n",
      "----- end of merge in of pair (32, 260) with id 261 ------\n",
      "state now:\n",
      "Words: [Word { ids: [104, 105] }, Word { ids: [261] }, Word { ids: [260] }, Word { ids: [258] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 0, (104, 105): 1, (97, 116): 0, (256, 97): 0, (32, 260): 0, (116, 259): 0, (32, 116): 0, (256, 257): 0, (116, 104): 0}\n",
      "\n",
      "Heap: [MergeJob { pair: (104, 105), count: 1, pos: {0} }]\n",
      "\n",
      "Merges: {(104, 101): 259, (116, 259): 260, (32, 260): 261, (97, 116): 257, (256, 257): 258, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "----- start of merge ------\n",
      "This merge is for (104, 105) with new id 262\n",
      "\n",
      "About to merge pair (104, 105) into word Word { ids: [104, 105] } with new id 262\n",
      "After merge, word is Word { ids: [262] }\n",
      "and changes/deltas vector is [((104, 105), -1)]\n",
      "changing global count for pair (104, 105) by -1 to 0\n",
      "\n",
      "----- end of merge in of pair (104, 105) with id 262 ------\n",
      "state now:\n",
      "Words: [Word { ids: [262] }, Word { ids: [261] }, Word { ids: [260] }, Word { ids: [258] }]\n",
      "\n",
      "Pair counts: {(32, 99): 0, (99, 97): 0, (104, 101): 0, (104, 105): 0, (97, 116): 0, (256, 97): 0, (32, 260): 0, (116, 259): 0, (32, 116): 0, (256, 257): 0, (116, 104): 0}\n",
      "\n",
      "Heap: []\n",
      "\n",
      "Merges: {(104, 101): 259, (116, 259): 260, (32, 260): 261, (104, 105): 262, (97, 116): 257, (256, 257): 258, (32, 99): 256}\n",
      "---------------------------------------------------\n",
      "\n",
      "Done merging! Total merges: 7\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    iterator = ['the cat the cat', 'hi cat'],\n",
    "    vocab_size = 1000,\n",
    "    buffer_size = 1,\n",
    "    pattern = SPLIT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2153bb3c-6ad7-449e-a2b0-c658f07d8fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<', 60),\n",
       " (b'=', 61),\n",
       " (b'>', 62),\n",
       " (b'?', 63),\n",
       " (b'@', 64),\n",
       " (b'A', 65),\n",
       " (b'B', 66),\n",
       " (b'C', 67),\n",
       " (b'D', 68),\n",
       " (b'E', 69)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergeable_ranks = tokenizer.get_mergeable_ranks()\n",
    "mergeable_ranks[60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f40a5b7-48d0-4483-b6b7-8f4e1dfa7fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'\\xe9', 233),\n",
       " (b'\\xea', 234),\n",
       " (b'\\xeb', 235),\n",
       " (b'\\xec', 236),\n",
       " (b'\\xed', 237),\n",
       " (b'\\xee', 238),\n",
       " (b'\\xef', 239),\n",
       " (b'\\xf0', 240),\n",
       " (b'\\xf1', 241),\n",
       " (b'\\xf2', 242),\n",
       " (b'\\xf3', 243),\n",
       " (b'\\xf4', 244),\n",
       " (b'\\xf5', 245),\n",
       " (b'\\xf6', 246),\n",
       " (b'\\xf7', 247),\n",
       " (b'\\xf8', 248),\n",
       " (b'\\xf9', 249),\n",
       " (b'\\xfa', 250),\n",
       " (b'\\xfb', 251),\n",
       " (b'\\xfc', 252),\n",
       " (b'\\xfd', 253),\n",
       " (b'\\xfe', 254),\n",
       " (b'\\xff', 255),\n",
       " (b' c', 256),\n",
       " (b'at', 257),\n",
       " (b' cat', 258),\n",
       " (b'he', 259),\n",
       " (b'the', 260),\n",
       " (b' the', 261),\n",
       " (b'hi', 262)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergeable_ranks[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a0c3e04-b7da-4564-a9b9-1da5aecb65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = rust_tokenizer.Tokenizer(debug_print = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4da2ff69-5e94-44cd-9154-e01493f2035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    iterator = ['The cat looked for a dog.', 'The dog looked for a cat.', 'Bicycles are great.', 'Cats are good.'],\n",
    "    vocab_size = 1000,\n",
    "    buffer_size = 50,\n",
    "    pattern = SPLIT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a74341a1-b582-46f9-a6ac-00fc5bac0092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'\\xfa', 250),\n",
       " (b'\\xfb', 251),\n",
       " (b'\\xfc', 252),\n",
       " (b'\\xfd', 253),\n",
       " (b'\\xfe', 254),\n",
       " (b'\\xff', 255),\n",
       " (b' a', 256),\n",
       " (b'at', 257),\n",
       " (b'oo', 258),\n",
       " (b're', 259),\n",
       " (b' c', 260),\n",
       " (b' d', 261),\n",
       " (b' f', 262),\n",
       " (b' g', 263),\n",
       " (b' l', 264),\n",
       " (b'Th', 265),\n",
       " (b'ed', 266),\n",
       " (b'ked', 267),\n",
       " (b'og', 268),\n",
       " (b'or', 269),\n",
       " (b' are', 270),\n",
       " (b'ooked', 271),\n",
       " (b' cat', 272),\n",
       " (b' dog', 273),\n",
       " (b' for', 274),\n",
       " (b' looked', 275),\n",
       " (b'The', 276),\n",
       " (b'Bi', 277),\n",
       " (b'Cat', 278),\n",
       " (b'cl', 279),\n",
       " (b'cy', 280),\n",
       " (b'es', 281),\n",
       " (b'ood', 282),\n",
       " (b'reat', 283),\n",
       " (b' good', 284),\n",
       " (b' great', 285),\n",
       " (b'Bicy', 286),\n",
       " (b'Cats', 287),\n",
       " (b'cles', 288),\n",
       " (b'Bicycles', 289)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_mergeable_ranks()[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e34ca6-dbbc-4665-a5e8-f793251ae90b",
   "metadata": {},
   "source": [
    "## B) Trying my Tokenizer python class\n",
    "\n",
    "`my_tokenizer.py` is my very partial copy of nanochat's [RustBPETokenizer class](https://github.com/karpathy/nanochat/blob/master/nanochat/tokenizer.py). It uses the rust tokenizer and tiktoken. It doesn't do anything with special tokens like `<|user_start|>`that I assume will be needed later once we start workign with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9e52023-e7c2-4e6e-97c8-786ec38dd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f21b9ae8-1b42-4f3e-a571-5c01af357b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator = [\"the cat\", \"the bat\"], vocab_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0298e0b9-feee-4b7c-83b6-eb2e95027024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 262]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"the cat\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6280ba1b-4c1a-45a3-adbf-7d20cf33a8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "152914fc-fa33-4604-9e7d-a53013a75edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[122, 101, 98, 114, 97]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"zebra\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dd27c40-b5b2-4869-8b57-2fc18443dc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zebra'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba93197-7bc0-49b1-ac4c-925a7a6353b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "971b4273-54fd-4ab0-8d23-0cafbe356e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator = [\"the cat\", \"the bat\", \"hello ðŸ‘‹\", \"ä½ å¥½\"], vocab_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e5a985b-8e6b-4088-9de4-008965df838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[272]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\" ðŸ‘‹\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd36ec57-3a68-4d15-9871-b3e993a4f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[273]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"ä½ å¥½\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14df368f-2b82-472f-b4ba-44f33485d466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cb745-2e63-42d7-8a9a-dee2fd7f09e6",
   "metadata": {},
   "source": [
    "try saving / loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94f23543-fb7f-4a5c-8de2-0ddd210456c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 260, 97, 114]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator = [\"the cat\", \"the bat\"], vocab_size = 1000)\n",
    "tokenizer.encode(\"the car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd8d9144-16d3-4bcf-889c-130dc64c4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to my-tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save('my-tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a02cf1fb-c70b-46e8-94a6-fc37d78c2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer.load_from_file('my-tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed9f3045-9f07-46c4-ad86-280ab5a4334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 260, 97, 114]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"the car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941f3c8-2e05-4841-b9e1-06c8e66284fc",
   "metadata": {},
   "source": [
    "## C) Trying other misc rust functions\n",
    "\n",
    "Since I'm still so new to rust and how you use it from python, I exposed a second class called \"Play\" to try things from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1552c1a1-1e97-4712-8032-44a905a11808",
   "metadata": {},
   "outputs": [],
   "source": [
    "play = rust_tokenizer.Play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b1570dc-72a0-4467-8e7a-fd217133cd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello Ernie'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.hello(\"Ernie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9417e9a-b49d-4798-99df-cd312cedb8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'str'>\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.get_type(\"Ernie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f270e3b2-ff73-4cc5-9669-c4aeb2abc184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'list'>\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.get_type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1af2c3db-7d47-4112-b627-7f581cb981d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'list_iterator'>\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.get_type(iter([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4708a357-2cd6-477c-94d8-ccc7ff299cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thecatflew'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.concat_from_iterator([\"the\", \"cat\", \"flew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19d20975-af0b-408a-a6e5-090168086c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# expect error\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "play.concat_from_iterator(123) # expect error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b553e9bc-ffda-408b-b69a-11b3226d6cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thecatflew'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.fancy_concat_from_iterator([\"the\", \"cat\", \"flew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9e71ec3-9457-4707-a81d-650c62cabbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', ' went', ' to', ' buy', ' ', '12', '34', ' candies', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play.find_matches(SPLIT_PATTERN, \"They went to buy 1234 candies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96004605-faf6-455a-8715-d6e80ba7aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair_a.cmp(pair_b): Less\n",
      "pair_b.cmp(pair_a): Greater\n",
      "pair_a.cmp(pair_c): Equal\n",
      "pair_a.cmp(pair_d): Less\n"
     ]
    }
   ],
   "source": [
    "play.understand_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf9b1a9f-d4f3-484e-936f-3acba0555129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word after merge: Word { ids: [4, 3, 4] }\n",
      "detals: [((1, 2), -1), ((2, 3), -1), ((4, 3), 1), ((3, 1), -1), ((3, 4), 1), ((1, 2), -1)]\n"
     ]
    }
   ],
   "source": [
    "play.merge_pair_into_word(word_ids = [1,2,3,1,2], pair = (1,2), new_id = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ea07c-8445-4bd9-b0ef-1cefeccbe7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
