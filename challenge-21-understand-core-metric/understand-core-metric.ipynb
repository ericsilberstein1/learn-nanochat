{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97448724-4c56-4237-be17-6d3c541d20de",
   "metadata": {},
   "source": [
    "### CORE Metric Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d460a8-73e6-4f0d-bb66-166ac3982cb4",
   "metadata": {},
   "source": [
    "Another thing he does every so many steps in [base_train.py](https://github.com/karpathy/nanochat/blob/master/scripts/base_train.py) is evalute the CORE metric. It looks like there's a lot involved. I want to understand it better and then I'll either hand copy that code now or move on and circle back later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac13b09-a409-4c93-91a1-832c9b645ef9",
   "metadata": {},
   "source": [
    "[base_eval.py](https://github.com/karpathy/nanochat/blob/master/scripts/base_eval.py) is the starting pont. He calls it from `base_train.py` and it also can be run by itself to evaluate a model. The first thing it does is download an \"eval bundle\" zip file. Let me see what that is. \n",
    "\n",
    "```\n",
    "# ~162MB of data needed to evaluate the CORE metric\n",
    "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1cb43-c472-4440-8e07-4b9ce67a4844",
   "metadata": {},
   "source": [
    "might as well start copying straightforward functions, like the one to unzip the file, to `my_base_eval.py` rather than doing that inline in this notebook, because will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275b3f0e-86dc-40dd-864d-74f1e03f01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from scripts.my_base_eval import EVAL_BUNDLE_URL, place_eval_bundle\n",
    "from my_nanochat.my_common import get_base_dir, download_file_with_lock\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6bb1e-512d-4cab-b906-818b12750b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bundle_dir = os.path.join(get_base_dir(), 'eval_bundle')\n",
    "if not os.path.exists(eval_bundle_dir):\n",
    "    download_file_with_lock(EVAL_BUNDLE_URL, 'eval_bundle.zip', postprocess_fn=place_eval_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e22515-6db5-4247-a7b2-138b7bd7b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 120\n",
      "-rw-r--r--  1 ericsilberstein  staff   3.1K Nov 13 07:30 core.yaml\n",
      "drwxr-xr-x  9 ericsilberstein  staff   288B Nov 13 07:30 \u001b[34meval_data\u001b[m\u001b[m\n",
      "-rw-r--r--  1 ericsilberstein  staff    18K Nov 13 07:30 EVAL_GAUNTLET.md\n",
      "-rw-r--r--  1 ericsilberstein  staff    16K Nov 13 07:30 eval_meta_data.csv\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.4K Nov 13 07:30 openai-community-gpt2-large.csv\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.4K Nov 13 07:30 openai-community-gpt2-medium.csv\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.4K Nov 13 07:30 openai-community-gpt2-xl.csv\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.4K Nov 13 07:30 openai-community-gpt2.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {eval_bundle_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d47bc91-db22-428b-89bd-a888924087b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icl_tasks:\n",
      "-\n",
      "  label: hellaswag_zeroshot\n",
      "  dataset_uri: language_understanding/hellaswag.jsonl\n",
      "  num_fewshot: [0]\n",
      "  icl_task_type: multiple_choice\n",
      "-\n",
      "  label: jeopardy\n",
      "  dataset_uri: world_knowledge/jeopardy_all.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: language_modeling\n",
      "  continuation_delimiter: \"\\nAnswer: \"\n",
      "  has_categories: true\n",
      "-\n",
      "  label: bigbench_qa_wikidata\n"
     ]
    }
   ],
   "source": [
    "!head -15 {eval_bundle_dir}/core.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509bcc77-e191-4020-a061-02c32c16d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  10 ericsilberstein  staff   320B Nov 13 07:30 \u001b[34mcommonsense_reasoning\u001b[m\u001b[m\n",
      "drwxr-xr-x  10 ericsilberstein  staff   320B Nov 13 07:30 \u001b[34mlanguage_understanding\u001b[m\u001b[m\n",
      "drwxr-xr-x  10 ericsilberstein  staff   320B Nov 13 07:30 \u001b[34mprogramming\u001b[m\u001b[m\n",
      "drwxr-xr-x  11 ericsilberstein  staff   352B Nov 13 07:30 \u001b[34mreading_comprehension\u001b[m\u001b[m\n",
      "drwxr-xr-x   6 ericsilberstein  staff   192B Nov 13 07:30 \u001b[34msafety\u001b[m\u001b[m\n",
      "drwxr-xr-x  22 ericsilberstein  staff   704B Nov 13 07:30 \u001b[34msymbolic_problem_solving\u001b[m\u001b[m\n",
      "drwxr-xr-x  15 ericsilberstein  staff   480B Nov 13 07:30 \u001b[34mworld_knowledge\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {eval_bundle_dir}/eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3174e6a-f7d4-4922-ac62-6d8161c6021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 63832\n",
      "-rw-r--r--  1 ericsilberstein  staff   357K Nov 13 07:30 arc_challenge.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   622K Nov 13 07:30 arc_easy.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff    46K Nov 13 07:30 bigbench_misconceptions.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   123K Nov 13 07:30 bigbench_movie_recommendation.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.6M Nov 13 07:30 bigbench_qa_wikidata.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   356K Nov 13 07:30 jeopardy_all.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   2.8K Nov 13 07:30 jeopardy_small.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   9.9M Nov 13 07:30 mmlu_expand.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   7.7M Nov 13 07:30 mmlu.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.2M Nov 13 07:30 triviaqa_sm_sub.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   4.6M Nov 13 07:30 triviaqa_sm.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff    12K Nov 13 07:30 triviaqa_small.jsonl\n",
      "-rw-r--r--  1 ericsilberstein  staff   4.7M Nov 13 07:30 triviaqa.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {eval_bundle_dir}/eval_data/world_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd29e3de-e992-4bfa-8657-2553d1811f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"context\": \"WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\", \"continuation\": \"Admiral Richard Byrd\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: Accused of accepting bribes, Francis Bacon was imprisoned in this forbidding complex in 1621\", \"continuation\": \"Tower of London\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: More than 250,000 died in fighting before France granted this African nation independence July 3, 1962\", \"continuation\": \"Algeria\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: In 1784 she founded the city of Sevastopol in her new domain of the Crimea\", \"continuation\": \"Catherine the Great\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: This Portuguese Admiral of the Indian Seas discovered & named the Amirante Islands\", \"continuation\": \"Vasco da Gama\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: This dominion was created by the British North America Act on July 1, 1867\", \"continuation\": \"Canada\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: On May 9, 1946 this countrys King Victor Emmanuel abdicated in favor of his son Umberto\", \"continuation\": \"Italy\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: In 1763, as a result of this numerical war, Florida became a British possession\", \"continuation\": \"Seven Years War\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORD ORIGINS: From Yiddish, for a wooden beam, it describes a person so clumsy he might bump into one\", \"continuation\": \"Klutz\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: The name of this tent fabric comes from a Dutch word for cloth, not from a web-footed bird\", \"continuation\": \"Duck\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: When they begin this dance, they should know its name is from the French for a flirtation\", \"continuation\": \"Beguine\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: The name of this xylophone that originated in Africa may be from the Kimbundu language\", \"continuation\": \"Marimba\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: This other term for a pastor is derived from the Middle Latin word persona\", \"continuation\": \"parson\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: The name of this fine, soft leather is the French word for Sweden; Sweden was famous for gloves made of it\", \"continuation\": \"suede\", \"category\": \"word_origins\"}\n",
      "{\"context\": \"WORD ORIGINS: This word for a blob or lump, perhaps of whipped cream, may come from dolpur, Icelandic for fat man\", \"continuation\": \"dollop\", \"category\": \"word_origins\"}\n"
     ]
    }
   ],
   "source": [
    "!head -15 {eval_bundle_dir}/eval_data/world_knowledge/jeopardy_small.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f0eca-8a69-43ca-8531-00ca22bf75ef",
   "metadata": {},
   "source": [
    "So for those seems like give the model context and see if comes up with the continuation. But what about for say those multiple choice ones? Is it the same? It seems like there's too much code to support CORE metric if in every case it's just about seeing if a continuation matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4488559e-89ba-4220-977a-af76a0cde7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icl_tasks:\n",
      "-\n",
      "  label: hellaswag_zeroshot\n",
      "  dataset_uri: language_understanding/hellaswag.jsonl\n",
      "  num_fewshot: [0]\n",
      "  icl_task_type: multiple_choice\n",
      "-\n",
      "  label: jeopardy\n",
      "  dataset_uri: world_knowledge/jeopardy_all.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: language_modeling\n",
      "  continuation_delimiter: \"\\nAnswer: \"\n",
      "  has_categories: true\n",
      "-\n",
      "  label: bigbench_qa_wikidata\n",
      "  dataset_uri: world_knowledge/bigbench_qa_wikidata.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: language_modeling\n",
      "-\n",
      "  label: arc_easy\n",
      "  dataset_uri: world_knowledge/arc_easy.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: multiple_choice\n",
      "  continuation_delimiter: \"\\nAnswer: \"\n",
      "-\n",
      "  label: arc_challenge\n",
      "  dataset_uri: world_knowledge/arc_challenge.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: multiple_choice\n",
      "  continuation_delimiter: \"\\nAnswer: \"\n"
     ]
    }
   ],
   "source": [
    "!head -30 {eval_bundle_dir}/core.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc59466-b43f-4536-b743-f3eb17bffe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\": [\"Sunlight is the source of energy for nearly all ecosystems.\", \"Most ecosystems are found on land instead of in water.\", \"Carbon dioxide is more available than other gases.\", \"The producers in all ecosystems are plants.\"], \"query\": \"Question: Which statement best explains why photosynthesis is the foundation of most food webs?\", \"gold\": 0}\n",
      "{\"choices\": [\"safety goggles\", \"breathing mask\", \"rubber gloves\", \"lead apron\"], \"query\": \"Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\", \"gold\": 1}\n",
      "{\"choices\": [\"brain cells\", \"bone cells\", \"muscle cells\", \"ovary cells\"], \"query\": \"Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\", \"gold\": 3}\n",
      "{\"choices\": [\"gray\", \"warm\", \"long\", \"soft\"], \"query\": \"Question: Which characteristic describes the texture of a kitten's fur?\", \"gold\": 3}\n",
      "{\"choices\": [\"a lightweight core surrounded by neutral particles\", \"a massive core surrounded by negatively-charged particles\", \"a network of interacting positive and negative particles\", \"overlapping layers of neutral, positive, and negative particles\"], \"query\": \"Question: Which best describes the structure of an atom?\", \"gold\": 1}\n"
     ]
    }
   ],
   "source": [
    "!head -5 {eval_bundle_dir}/eval_data/world_knowledge/arc_easy.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390945e9-581c-40e2-8d3d-4fe3c0722aa8",
   "metadata": {},
   "source": [
    "Just looking at that is bringing back nervous memories of studying for tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c37a1-1916-443c-aa29-cddcc04feca8",
   "metadata": {},
   "source": [
    "Might as well hand copy all this stuff now. I'll start with [core_eval.py](https://github.com/karpathy/nanochat/blob/master/nanochat/core_eval.py) which contains things like templates for the various tasks types and the code to have the model complete a prompt and score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8c3ff-b7aa-4733-9af2-70a6fdd06dc3",
   "metadata": {},
   "source": [
    "#### render_prompt_mc()\n",
    "\n",
    "Start with one of the 3 prompt rendering functions just to get the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c10ae5-ca06-42a1-b60f-c7e80550decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_core_eval import render_prompts_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d5d451-4ebf-475e-941f-b24d4e4a6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a lightweight core surrounded by neutral particles',\n",
       " 'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a massive core surrounded by negatively-charged particles',\n",
       " 'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a network of interacting positive and negative particles',\n",
       " 'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: overlapping layers of neutral, positive, and negative particles']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = {\n",
    "    'query': 'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?',\n",
    "    'choices': [\n",
    "        \"a lightweight core surrounded by neutral particles\",\n",
    "        \"a massive core surrounded by negatively-charged particles\",\n",
    "        \"a network of interacting positive and negative particles\",\n",
    "        \"overlapping layers of neutral, positive, and negative particles\"],\n",
    "    'gold': 1\n",
    "}\n",
    "continuation_delimiter = \"\\nAnswer: \"\n",
    "render_prompts_mc(item, continuation_delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9786b-c658-45d1-b5ad-008f2d62aa34",
   "metadata": {},
   "source": [
    "Why are those the prompts? How is the model supposed to complete to say which is the right answer? Maybe it will make more sense if there are also fewshow examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5d37dd-7a5b-4afe-a483-5c2d4f18b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "item2 = {\n",
    "    'query': 'Question: Which statement best explains why photosynthesis is the foundation of most food webs?',\n",
    "    'choices': [\n",
    "        \"Sunlight is the source of energy for nearly all ecosystems.\", \n",
    "        \"Most ecosystems are found on land instead of in water.\", \n",
    "        \"Carbon dioxide is more available than other gases.\", \n",
    "        \"The producers in all ecosystems are plants.\"],\n",
    "    'gold': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc6bc12-f2ca-494a-9306-d9250dea5a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a lightweight core surrounded by neutral particles',\n",
       " 'Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a massive core surrounded by negatively-charged particles',\n",
       " 'Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a network of interacting positive and negative particles',\n",
       " 'Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer: Sunlight is the source of energy for nearly all ecosystems.\\n\\nQuestion: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: overlapping layers of neutral, positive, and negative particles']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_prompts_mc(item, continuation_delimiter, fewshot_examples=[item2, item2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d73124-0117-4325-9137-624f965b8ccd",
   "metadata": {},
   "source": [
    "I still don't get it. What's the point of a prompt per choice, one of which gives the right answer?\n",
    "\n",
    "Maybe it will help to understand this function: `batch_sequences_mc(tokenizer, prompts)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b094b6b-3b94-4f43-97e6-dddf7fd70dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_core_eval import find_common_length, batch_sequences_mc\n",
    "from my_nanochat.my_tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8ff207-61fe-4f2b-9e44-24a9567bc2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for batch_sequences_mc\n",
    "find_common_length([[1,2,3,4],[1,2,3,5],[1,2,6,4,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af23eda-310e-4c22-9d13-4c2d80c0cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "017efa31-d985-4dca-ba8f-eac259daa8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([22, 22, 22, 22], [29, 30, 30, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = render_prompts_mc(item, continuation_delimiter, fewshot_examples=None)\n",
    "tokens, start_idxs, end_idxs = batch_sequences_mc(tokenizer, prompts)\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1212057-6c7b-4397-8652-84ff1975b313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17643, 58, 257, 17642, 4516],\n",
       " [17643, 58, 257, 5436, 4516],\n",
       " [17643, 58, 257, 2735, 281],\n",
       " [17643, 58, 21833, 6243, 281])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][20:25], tokens[1][20:25], tokens[2][20:25], tokens[3][20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccd31235-055c-45f6-95f9-412ce4e0834c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: a lightweight core surrounded by neutral particles'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde2028e-613a-4704-86c7-84c76e2b85c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09f07f89-f784-4bf1-ad83-6026c60a64ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer: overlapping layers of neutral, positive, and negative particles'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed8d0fb9-30fb-47dd-9b79-4de67c355c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ff937-df80-4536-adb2-35b396cd0195",
   "metadata": {},
   "source": [
    "Well I get what `batch_sequences_mc()` does but it doesn't answer the question above.\n",
    "\n",
    "Ah...the answer is in `evaluate_examle()`. Looks like for mutliple choice we're not looking for the model to complete the prompt with the correct choice. Instead we're calculating the loss for each prompt and consider the model's \"answer\" to be the prompt with the lowest loss.\n",
    "\n",
    "For sure that makes way more sense. The lower the loss, the more likely the sequence is. For example, even a small, barely trained model whould be able to say that \"he is\" is more likely than \"he pineapple\". Also, this gives a way of \"forcing\" an answer that is one of the choices.\n",
    "\n",
    "To think about this a little more clearly...say I feed in these two sequences (assume each word is a token):\n",
    "\n",
    "- he is a\n",
    "- he pineapple big\n",
    "\n",
    "Say in the first sequence he -> is with 10% (meaning 10% is on \"is\" in the vocab) and is -> a with 20%.\n",
    "\n",
    "Say in the second sequence, he -> pineapple with 1% and pineapple -> big with 1% \n",
    "\n",
    "(Not even trying to make realistic numbers.)\n",
    "\n",
    "The total chance of the first sequence is 10% * 20% = 2%\n",
    "\n",
    "The chance of the second sequence is 1% * 1% = .01%\n",
    "\n",
    "The first is way more likely. So if the sequences were like:\n",
    "\n",
    "- Q:What is a pineapple? A:A vehicle\n",
    "- Q:What is a pinappple? A:A fruit\n",
    "- Q:What is a pineapple? A:A dwelling\n",
    "- Q:What is a pineapple? A:A spiritual concept\n",
    "\n",
    "A good model should give the highest chance to the 2nd.\n",
    "\n",
    "We can get this by taking the sequence with the lowest total cross entropy loss.\n",
    "\n",
    "Let's do a small example using \"he is a\" vs \"he pineapple big\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9c41a5-9511-4d89-9015-dbb5a55830ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0137)\n",
      "tensor(0.3408)\n",
      "tensor(2.6804)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# vocab:   is  a  pineapple  big\n",
    "# index:   0   1  2          3 \n",
    "\n",
    "# make up logits for \"he is a\" showing we got it right and with high probability (-2 and -1)\n",
    "# but not bothering to actually spread 100% probability over the whole vocab\n",
    "logits_for_he_is_a = torch.tensor([[-2, -7, -7, -7], [-7, -1, -7, -7]], dtype=torch.float32)\n",
    "#                                      he -> is           is -> a\n",
    "\n",
    "target_for_he_is_a = torch.tensor([0, 1]) # indexes\n",
    "\n",
    "# and same for \"he pineapple big\" also showing we got it right but with lower probability (-5 and -5)\n",
    "logits_for_he_pineapple_big = torch.tensor([[-7, -7, -5, -7], [-7, -7, -7, -5]], dtype=torch.float32)\n",
    "#                                            he -> pineapple   pineapple -> big\n",
    "\n",
    "target_for_he_pineapple_big = torch.tensor([2, 3]) # indexes\n",
    "\n",
    "# one more example for \"he pineapple big\" where we get \"he -> pineapple\" wrong\n",
    "logits_for_he_pineapple_big_2 = torch.tensor([[-2, -7, -7, -7], [-7, -7, -7, -5]], dtype=torch.float32)\n",
    "#                                              he -> pineapple   pineapple -> big\n",
    "\n",
    "print(F.cross_entropy(logits_for_he_is_a, target_for_he_is_a))\n",
    "print(F.cross_entropy(logits_for_he_pineapple_big, target_for_he_pineapple_big))\n",
    "print(F.cross_entropy(logits_for_he_pineapple_big_2, target_for_he_pineapple_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2afd8-e6b4-4561-bb91-641271e88c01",
   "metadata": {},
   "source": [
    "Of these three examples, our best (lowest) loss is the first as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4917754d-a054-4752-8d0b-806d36064ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for token 1: 0.0200, loss for token 2: 0.0074, average: 0.0137\n"
     ]
    }
   ],
   "source": [
    "# compute the cross entropy loss manually for the first example to remember how higher prob\n",
    "# becomes lower loss\n",
    "import math\n",
    "loss_1 = -math.log(math.exp(-2) / (math.exp(-2) + math.exp(-7) * 3))\n",
    "loss_2 = -math.log(math.exp(-1) / (math.exp(-1) + math.exp(-7) * 3))\n",
    "average_loss = (loss_1 + loss_2) / 2.\n",
    "print(f\"loss for token 1: {loss_1:.4f}, loss for token 2: {loss_2:.4f}, average: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2857eeb6-4d00-45a0-aaec-c808cc1085aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for token 1: 0.0200, loss for token 2: 0.0004, average: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# so if for example we predicted is -> a with even higher probability our loss goes down\n",
    "loss_1 = -math.log(math.exp(-2) / (math.exp(-2) + math.exp(-7) * 3))\n",
    "loss_2 = -math.log(math.exp(2) / (math.exp(2) + math.exp(-7) * 3))\n",
    "average_loss = (loss_1 + loss_2) / 2.\n",
    "print(f\"loss for token 1: {loss_1:.4f}, loss for token 2: {loss_2:.4f}, average: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d25712-40e9-4561-a780-4520a65ef8b5",
   "metadata": {},
   "source": [
    "So now hand copy `evaluate_example()` leaving out parts we don't need yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaec8eb-8f08-44f8-a8ad-b8ffdcb6a5fe",
   "metadata": {},
   "source": [
    "Looks we need another helper function `stack_sequences()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86fc405-80b3-488b-8356-4db198d9b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_core_eval import stack_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d04afde-58ab-455e-b9f2-fa4c27e7e7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 5, 0],\n",
       "        [4, 7, 9, 2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_sequences([[4, 2, 5], [4, 7, 9, 2]], pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b661305-6101-4cf8-bb34-8595019d4f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure I understand what roll does\n",
    "torch.roll(torch.tensor([1,2,3]), shifts=-1, dims=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46baa28f-01e8-43a1-8e2f-99466a173e03",
   "metadata": {},
   "source": [
    "And another helper function `forward_model()` which is where we calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990e4a94-1e5a-4c3d-ae85-0222c846cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_core_eval import forward_model\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606484f5-dec6-460b-8e98-0d88ff742d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(autodetect_device_type())\n",
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fcacaa-4fdd-4c14-ae52-60260d3617ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11.1263, 11.1653,     nan],\n",
       "         [11.1395, 11.1280,     nan]], device='mps:0',\n",
       "        grad_fn=<AsStridedBackward0>),\n",
       " tensor([[ 668,  668,  668],\n",
       "         [ 668,  668, 3080]], device='mps:0'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[65, 66, 67], [100, 101, 102]], dtype=torch.long, device=device)\n",
    "forward_model(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a159b8-1c24-4eb9-9b5c-1802d725c91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.9780, 11.3036,     nan],\n",
       "         [11.1395, 11.1280,     nan]], device='mps:0',\n",
       "        grad_fn=<AsStridedBackward0>),\n",
       " tensor([[ 668,  668,  668],\n",
       "         [ 668,  668, 3080]], device='mps:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we nailed the prediction of the 2nd token in the first sequence, expect that\n",
    "# first loss to go down\n",
    "input_ids = torch.tensor([[65, 668, 67], [100, 101, 102]], dtype=torch.long, device=device)\n",
    "forward_model(model, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a3d55-8b7b-40a3-935c-a80f9d78c0d1",
   "metadata": {},
   "source": [
    "Finally we can try `evaluate_example()` for a multiple choice question (didn't add support for other types yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fbbed4-60b5-496f-8d59-b907f09e3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_core_eval import evaluate_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eaf7929-1824-41d0-969d-e0fd62c23b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\"choices\": [\"Sunlight is the source of energy for nearly all ecosystems.\", \n",
    "                 \"Most ecosystems are found on land instead of in water.\", \n",
    "                 \"Carbon dioxide is more available than other gases.\", \n",
    "                 \"The producers in all ecosystems are plants.\"], \n",
    "     \"query\": \"Question: Which statement best explains why photosynthesis is the foundation of most food webs?\", \n",
    "     \"gold\": 0},\n",
    "    \n",
    "    {\"choices\": [\"safety goggles\",\n",
    "                 \"breathing mask\",\n",
    "                 \"rubber gloves\",\n",
    "                 \"lead apron\"],\n",
    "     \"query\": \"Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\",\n",
    "     \"gold\": 1},\n",
    "    \n",
    "    {\"choices\": [\"brain cells\",\n",
    "                 \"bone cells\",\n",
    "                 \"muscle cells\",\n",
    "                 \"ovary cells\"],\n",
    "     \"query\": \"Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\",\n",
    "     \"gold\": 3},\n",
    "]\n",
    "task_meta = {\n",
    "  \"num_fewshot\": 0, # not really\n",
    "  \"task_type\": 'multiple_choice',\n",
    "  \"continuation_delimiter\": \"\\nAnswer: \",\n",
    "}\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b807d4e-f78d-4994-beec-127f6fa9464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_example(0, model, tokenizer, data, device, task_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e176ab-f673-48f4-b353-8c756817a0f2",
   "metadata": {},
   "source": [
    "### \"language modeling\"\n",
    "\n",
    "Go back and add support for \"language modeling\" and \"schema question.\" First see what a language modeling task is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba7787a-3d9b-4cbd-b78b-6560993af641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icl_tasks:\n",
      "-\n",
      "  label: hellaswag_zeroshot\n",
      "  dataset_uri: language_understanding/hellaswag.jsonl\n",
      "  num_fewshot: [0]\n",
      "  icl_task_type: multiple_choice\n",
      "-\n",
      "  label: jeopardy\n",
      "  dataset_uri: world_knowledge/jeopardy_all.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: language_modeling\n",
      "  continuation_delimiter: \"\\nAnswer: \"\n",
      "  has_categories: true\n",
      "-\n",
      "  label: bigbench_qa_wikidata\n"
     ]
    }
   ],
   "source": [
    "!head -15 {eval_bundle_dir}/core.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f65aaae-e403-4542-a6cf-318a83d733ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"context\": \"WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\", \"continuation\": \"Admiral Richard Byrd\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: Accused of accepting bribes, Francis Bacon was imprisoned in this forbidding complex in 1621\", \"continuation\": \"Tower of London\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: More than 250,000 died in fighting before France granted this African nation independence July 3, 1962\", \"continuation\": \"Algeria\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: In 1784 she founded the city of Sevastopol in her new domain of the Crimea\", \"continuation\": \"Catherine the Great\", \"category\": \"world_history\"}\n",
      "{\"context\": \"WORLD HISTORY: This Portuguese Admiral of the Indian Seas discovered & named the Amirante Islands\", \"continuation\": \"Vasco da Gama\", \"category\": \"world_history\"}\n"
     ]
    }
   ],
   "source": [
    "!head -5 {eval_bundle_dir}/eval_data/world_knowledge/jeopardy_all.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd30d08-350f-47ef-a2d6-ce11d09f75e5",
   "metadata": {},
   "source": [
    "So here do we judge by the autoregressive loss on the continuation? Let's do the prompts first.\n",
    "\n",
    "Copying `render_prompts_lm()` looks like it has an option to include the continuation, so maybe sometimes we judge by loss and other times we judge by generation?\n",
    "\n",
    "And later adding lm to `evaluate_example` I see at least here we judge by an exact match on the generated output. (For some reason we input the sequence with the continuation, but then, using the example below, we only count it as right if \":\" predicts \" Admiral\" AND \" Admiral\" predicts \" Richard\" AND \" Byrd\" predict \" Byrd\". Maybe this is just a convenience to not set max tokens? For a second I thought this is cheating, but not really, because if, say, we predict the first token wrong, we already count that as a fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3951666e-d3dd-476d-9116-c857e2ea94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_core_eval import render_prompts_lm, batch_sequences_lm, evaluate_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d63ae5-3b5c-4bba-8e06-7a2b690a4a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\\nAnswer:',\n",
       " 'WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\\nAnswer: Admiral Richard Byrd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = {\n",
    "    \"context\": \"WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\",\n",
    "    \"continuation\": \"Admiral Richard Byrd\",\n",
    "    \"category\": \"world_history\",\n",
    "}\n",
    "continuation_delimiter = \"\\nAnswer: \"\n",
    "prompts = render_prompts_lm(item, continuation_delimiter)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1262303-cbcb-4ebf-bffa-7d0ab32c36d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([34], [37])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, start_idxs, end_idxs = batch_sequences_lm(tokenizer, prompts)\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38839b03-c336-4ecc-a8f5-ec79f6ce4c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Admiral Richard Byrd'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0][34:37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e104736-b8fb-40e8-b50f-0a44f3589c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [item]\n",
    "task_meta = {\n",
    "  \"num_fewshot\": 0,\n",
    "  \"task_type\": 'language_modeling',\n",
    "  \"continuation_delimiter\": \"\\nAnswer: \",\n",
    "}\n",
    "evaluate_example(0, model, tokenizer, data, device, task_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59592b3-0063-43d2-805a-49269ede31e8",
   "metadata": {},
   "source": [
    "^ If my tiny CPU model trained for a few iterations pulled \"Admiral Richard Byrd\" OOIA that would be odd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dfe41-84ce-4c4d-945f-d61e3a1da787",
   "metadata": {},
   "source": [
    "### \"schema question\"\n",
    "\n",
    "Let's see what that's about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ad7fa9-8a59-4300-9f99-1ceaf8e1f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  icl_task_type: multiple_choice\n",
      "-\n",
      "  label: winograd\n",
      "  dataset_uri: language_understanding/winograd_wsc.jsonl\n",
      "  num_fewshot: [0]\n",
      "  icl_task_type: schema\n",
      "-\n",
      "  label: winogrande\n",
      "  dataset_uri: language_understanding/winogrande.jsonl\n",
      "  num_fewshot: [0]\n",
      "  icl_task_type: schema\n",
      "-\n",
      "  label: bigbench_dyck_languages\n",
      "  dataset_uri: symbolic_problem_solving/bigbench_dyck_languages.jsonl\n",
      "  num_fewshot: [10]\n",
      "  icl_task_type: language_modeling\n"
     ]
    }
   ],
   "source": [
    "!grep -C 5 schema {eval_bundle_dir}/core.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9269a1-1ac3-4545-9557-386b88be9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"context_options\": [\"The city councilmen refused the demonstrators a permit because the city councilmen\", \"The city councilmen refused the demonstrators a permit because the demonstrators\"], \"continuation\": \"feared violence.\", \"gold\": 0}\n",
      "{\"context_options\": [\"The city councilmen refused the demonstrators a permit because the city councilmen\", \"The city councilmen refused the demonstrators a permit because the demonstrators\"], \"continuation\": \"advocated violence.\", \"gold\": 1}\n",
      "{\"context_options\": [\"The trophy doesn't fit into the brown suitcase because the trophy\", \"The trophy doesn't fit into the brown suitcase because the suitcase\"], \"continuation\": \"is too large.\", \"gold\": 0}\n",
      "{\"context_options\": [\"The trophy doesn't fit into the brown suitcase because the trophy\", \"The trophy doesn't fit into the brown suitcase because the suitcase\"], \"continuation\": \"is too small.\", \"gold\": 1}\n",
      "{\"context_options\": [\"Joan made sure to thank Susan for all the help Joan\", \"Joan made sure to thank Susan for all the help Susan\"], \"continuation\": \"had recieved.\", \"gold\": 0}\n"
     ]
    }
   ],
   "source": [
    "!head -5 {eval_bundle_dir}/eval_data/language_understanding/winograd_wsc.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74a5b8-3944-46bc-881c-d6454dfbd24b",
   "metadata": {},
   "source": [
    "ok, I see the idea, we'll give the model, for example:\n",
    "\n",
    "The trophy doesn't fit into the brown suitcase because the trophy is too large.\n",
    "\n",
    "and\n",
    "\n",
    "The trophy doesn't fit into the brown suitcase because the suitcase is too large.\n",
    "\n",
    "and see which has higher probability. The correct \"gold\" answer is the first.\n",
    "\n",
    "Going to cheat and copy and paste his code because this seems so similar to mutliple choice and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd05caa-71ed-4c36-9dcb-603c88d0bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_core_eval import render_prompts_schema, batch_sequences_schema, evaluate_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a95cf5-5e60-493a-9c9d-86d4bc540db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The trophy doesn't fit into the brown suitcase because the trophy is too large.\",\n",
       " \"The trophy doesn't fit into the brown suitcase because the suitcase is too large.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = {\n",
    "    \"context_options\": [\n",
    "        \"The trophy doesn't fit into the brown suitcase because the trophy\", \n",
    "        \"The trophy doesn't fit into the brown suitcase because the suitcase\"], \n",
    "    \"continuation\": \"is too large.\", \n",
    "    \"gold\": 0}\n",
    "continuation_delimiter = \" \" # not sure about this\n",
    "prompts = render_prompts_schema(item, continuation_delimiter)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f27759-e77d-47cb-8428-aaceed8e19a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([13, 13], [17, 17])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, start_idxs, end_idxs = batch_sequences_schema(tokenizer, prompts)\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a561cdb6-bf17-4971-a6ab-044de4aa32dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is too large.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[1][13:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58487c56-ce09-479f-9c2c-dae684611517",
   "metadata": {},
   "source": [
    "^ I guess the reason we look at the loss of the continuation to decide the correct answer is because in the context of the parts of the token stream before, the autoregressive loss of the correct completion should be lower. We can't just look at the average loss of the whole stream because say \"...the trophy\" is say less likely than \"...the suitcase\" in general, we'll be judging that too. (However, for multiple choice we probably could just look at the whole stream. But maybe besides being slightly extra work, we reduce our \"signal\"? Especially if there are a bunch of fewshot examples in there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b420280-4fc7-4343-8301-ce28c1724799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [item]\n",
    "task_meta = {\n",
    "  \"num_fewshot\": 0,\n",
    "  \"task_type\": 'schema',\n",
    "  \"continuation_delimiter\": \" \",\n",
    "}\n",
    "evaluate_example(0, model, tokenizer, data, device, task_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c8600-0a9c-4c24-9c94-d0887b327ff0",
   "metadata": {},
   "source": [
    "### `evaluate_task()`\n",
    "\n",
    "Copy `evaluate_task()` which evaluates many examples and supports doing so across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d341fa2-ea29-4d7d-b268-f91c0f14c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_core_eval import evaluate_task\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(autodetect_device_type())\n",
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693a91b6-4dad-4bd1-a8b6-44f5ac2af1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\"choices\": [\"Sunlight is the source of energy for nearly all ecosystems.\", \n",
    "                 \"Most ecosystems are found on land instead of in water.\", \n",
    "                 \"Carbon dioxide is more available than other gases.\", \n",
    "                 \"The producers in all ecosystems are plants.\"], \n",
    "     \"query\": \"Question: Which statement best explains why photosynthesis is the foundation of most food webs?\", \n",
    "     \"gold\": 0},\n",
    "    \n",
    "    {\"choices\": [\"safety goggles\",\n",
    "                 \"breathing mask\",\n",
    "                 \"rubber gloves\",\n",
    "                 \"lead apron\"],\n",
    "     \"query\": \"Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\",\n",
    "     \"gold\": 1},\n",
    "    \n",
    "    {\"choices\": [\"brain cells\",\n",
    "                 \"bone cells\",\n",
    "                 \"muscle cells\",\n",
    "                 \"ovary cells\"],\n",
    "     \"query\": \"Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\",\n",
    "     \"gold\": 3},\n",
    "]\n",
    "task_meta = {\n",
    "  \"num_fewshot\": 0, # not really\n",
    "  \"task_type\": 'multiple_choice',\n",
    "  \"continuation_delimiter\": \"\\nAnswer: \",\n",
    "}\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa21407b-cc37-4c51-81ba-1ec430599b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333432674408"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_task(model, tokenizer, data, device, task_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f95a9c-2671-4483-bb71-758a138ef801",
   "metadata": {},
   "source": [
    "### back to `scripts/my_base_eval.py`\n",
    "\n",
    "Copy `evaluate_model()` which downloads the zip if needed and goes through all the tasks with a few options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04d254a-fc04-446b-8f40-104c60dfad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Task,Task Category,Task Type,#shots,#datapoints,Random baseline,Centered Metric?,Description\n",
      "mmlu_zeroshot,world knowledge,multiple choice,0,14042,25,,\"MMLU consists of 14,042 four-choice multiple choice questions distributed across 57 categories. The questions are in the style of academic standardized tests and the model is provided the question and the choices and is expected to choose between A, B, C, and D as its outputs. The subjects range from jurisprudence, to math, to morality.\"\n",
      "hellaswag_zeroshot,language understanding,multiple choice,0,10042,25,,\"HellaSwag consists of 10,042 multiple choice scenarios in which the model is prompted with a scenario and choose the most likely conclusion to the scenario from four possible options.\"\n"
     ]
    }
   ],
   "source": [
    "!head -3 {eval_bundle_dir}/eval_meta_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5976c27c-7a47-48bb-aa3f-c51a0bae55ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# centering example for multiple choice with 4 questions where we get half right\n",
    "random_baseline = 25\n",
    "accuracy = .5\n",
    "centered = (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)\n",
    "centered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef9f17-adb1-49c0-aadb-96d1c29afadd",
   "metadata": {},
   "source": [
    "Here goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d67386-5c43-4b4b-b4de-f0324971a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(autodetect_device_type())\n",
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=device, phase=\"eval\")\n",
    "from scripts.my_base_eval import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec46e7d-3e29-4e76-b2c3-70cc1315c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 4.38s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.68s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.07s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 7.30s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 8.19s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.5500 | centered: 0.1000 | time: 1.60s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.0000 | time: 6.62s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.4500 | centered: -0.1000 | time: 3.01s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 1.55s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.09s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.16 GiB, other allocations: 117.52 MiB, max allowed: 18.13 GiB). Tried to allocate 1.06 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_per_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-21-understand-core-metric/../my_nanochat/scripts/my_base_eval.py:74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, device, max_per_task)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_per_task \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     72\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[:max_per_task]\n\u001b[0;32m---> 74\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m results[label] \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     77\u001b[0m random_baseline \u001b[38;5;241m=\u001b[39m random_baselines[label]\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-21-understand-core-metric/../my_nanochat/my_nanochat/my_core_eval.py:200\u001b[0m, in \u001b[0;36mevaluate_task\u001b[0;34m(model, tokenizer, data, device, task_meta)\u001b[0m\n\u001b[1;32m    198\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(data), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rank, \u001b[38;5;28mlen\u001b[39m(data), world_size):\n\u001b[0;32m--> 200\u001b[0m     is_correct \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     correct[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(is_correct)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-21-understand-core-metric/../my_nanochat/my_nanochat/my_core_eval.py:175\u001b[0m, in \u001b[0;36mevaluate_example\u001b[0;34m(idx, model, tokenizer, data, device, task_meta)\u001b[0m\n\u001b[1;32m    172\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# forward the model, get autoregressive loss and argmax prediction at each token\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m losses, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# see if the losses/predictions came out correctly\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_modeling\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-21-understand-core-metric/../my_nanochat/my_nanochat/my_core_eval.py:122\u001b[0m, in \u001b[0;36mforward_model\u001b[0;34m(model, input_ids)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_model\u001b[39m(model, input_ids):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# take B x T tensor of input ids\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# return B x T tensor of losses and B x T tensor of argmax predictions\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# last column of losses is nan because we don't have a target to calculate loss\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 122\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     target_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(input_ids, shifts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m     losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m    126\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    127\u001b[0m         target_ids\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m seq_len),\n\u001b[1;32m    128\u001b[0m         reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m     )\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len)\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-21-understand-core-metric/../my_nanochat/my_nanochat/my_gpt.py:237\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, kv_cache, loss_reduction)\u001b[0m\n\u001b[1;32m    235\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m    236\u001b[0m softcap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m--> 237\u001b[0m logits \u001b[38;5;241m=\u001b[39m softcap \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.16 GiB, other allocations: 117.52 MiB, max allowed: 18.13 GiB). Tried to allocate 1.06 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "evaluate_model(model, tokenizer, device, max_per_task=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203439f6-f992-4e15-8722-e03e588845c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932f3d9a-7728-41cf-a30b-eed0dea039ff",
   "metadata": {},
   "source": [
    "Code added as part of this challenge:\n",
    "\n",
    "- Created `my_core_eval.py` with bulk of the code for evaluating CORE metric\n",
    "\n",
    "- Created `scripts/my_base_eval.py`\n",
    "\n",
    "- Added `download_file_with_lock()` to `my_common.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560b0c-f078-42c6-8ad4-6dc3fc3f5e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
