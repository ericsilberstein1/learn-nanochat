{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8d8f5a-f9ae-4fc1-abbb-e3070fefd0df",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenege. See `understand-core-metric.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161594c-f4d4-4a1d-b506-a976684e92ee",
   "metadata": {},
   "source": [
    "## CORE evaluation data examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ded23-37a2-4569-8e0e-f8bc54405e5d",
   "metadata": {},
   "source": [
    "I realized after creating `challenge-26-understand-midtrain/midtrain-data-examples.ipynb` and doing midtraining that I was forgetting / getting confused about the CORE evaluation data. This notebook with some ugly code shows a few examples for each task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93424fbe-b130-4d7c-b2f7-e51f62bb3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= hellaswag_zeroshot =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,042\n",
      "\n",
      "----------- item: 926 ------------\n",
      "Query: Cleaning windows: The man sprays windex and washes off with squeegee. The man uses\n",
      "leaf blower to dry the window. The man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man drops the reusable leaf blower in the garbage\n",
      "can.\n",
      "\n",
      "prompt 1: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man talks into a microphone.\n",
      "\n",
      "prompt 2: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man finishes and talks about the window before\n",
      "walking off.\n",
      "\n",
      "prompt 3: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man uses pocket sweepers to spray leaves off the\n",
      "windows.\n",
      "\n",
      "----------- item: 1500 ------------\n",
      "Query: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He then wears an abdominal belt and lifts the weight bar\n",
      "again and drops it.\n",
      "\n",
      "prompt 1: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He shakes his hand and moves on to the next bar with the\n",
      "burning sensation.\n",
      "\n",
      "prompt 2: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He claps and hits the bar a couple of times.\n",
      "\n",
      "prompt 3: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He then sits on a couch and goes bowling.\n",
      "\n",
      "============= jeopardy =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 2,117\n",
      "\n",
      "----------- item: 347 ------------\n",
      "Expected continuation: Philip of Macedon/Philip II\n",
      "\n",
      "prompt 0: WORLD HISTORY: He had a wife named Cleopatra, a daughter named Cleopatra, and a\n",
      "famous son, Alexander the Great Answer:\n",
      "\n",
      "----------- item: 1478 ------------\n",
      "Expected continuation: Ostrich\n",
      "\n",
      "prompt 0: SCIENCE: The largest cell is the 3-inch diameter yolk of this birds egg Answer:\n",
      "\n",
      "============= bigbench_qa_wikidata =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 20,321\n",
      "\n",
      "----------- item: 5540 ------------\n",
      "Expected continuation: English\n",
      "\n",
      "prompt 0: The language of El País is\n",
      "\n",
      "----------- item: 10097 ------------\n",
      "Expected continuation: Spain\n",
      "\n",
      "prompt 0: The country of citizenship of Carmen Amaya Amaya is\n",
      "\n",
      "============= arc_easy =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 2,376\n",
      "\n",
      "----------- item: 1030 ------------\n",
      "Query: Question: Which human body system forms a barrier between internal body structures\n",
      "and the external environment?\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: nervous\n",
      "\n",
      "prompt 1: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: digestive\n",
      "\n",
      "prompt 2: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: circulatory\n",
      "\n",
      "prompt 3: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: integumentary\n",
      "\n",
      "----------- item: 869 ------------\n",
      "Query: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: decreased variations in day length\n",
      "\n",
      "prompt 1: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: increased average air temperature\n",
      "\n",
      "prompt 2: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: increased burn rate for incoming meteoroids\n",
      "\n",
      "prompt 3: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: decreased rate of ozone layer destruction\n",
      "\n",
      "============= arc_challenge =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,172\n",
      "\n",
      "----------- item: 73 ------------\n",
      "Query: Question: Over a long period of time, running water in a river erodes the riverbed.\n",
      "This erosion causes the river to\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: stop flowing.\n",
      "\n",
      "prompt 1: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: create waves.\n",
      "\n",
      "prompt 2: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: move faster and cleaner.\n",
      "\n",
      "prompt 3: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: become deeper and wider.\n",
      "\n",
      "----------- item: 324 ------------\n",
      "Query: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: coal\n",
      "\n",
      "prompt 1: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: trees\n",
      "\n",
      "prompt 2: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: water\n",
      "\n",
      "prompt 3: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: wind\n",
      "\n",
      "============= copa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 100\n",
      "\n",
      "----------- item: 55 ------------\n",
      "Query: The boy skipped dinner, because\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: The boy skipped dinner, because his mother cooked his favorite meal.\n",
      "\n",
      "prompt 1: The boy skipped dinner, because he ate a big lunch.\n",
      "\n",
      "----------- item: 81 ------------\n",
      "Query: The horse bucked, because\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: The horse bucked, because a fly bit the horse.\n",
      "\n",
      "prompt 1: The horse bucked, because the rider stroked the horse.\n",
      "\n",
      "============= commonsense_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,221\n",
      "\n",
      "----------- item: 805 ------------\n",
      "Query: Question: Where do you store a large container? Choices: A. supermarket B. juice C.\n",
      "hostel D. cabinet Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: A\n",
      "\n",
      "prompt 1: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: B\n",
      "\n",
      "prompt 2: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: C\n",
      "\n",
      "prompt 3: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: D\n",
      "\n",
      "----------- item: 1042 ------------\n",
      "Query: Question: What is a tactic used to interfere with learning about science? Choices:\n",
      "A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer:\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: A\n",
      "\n",
      "prompt 1: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: B\n",
      "\n",
      "prompt 2: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: C\n",
      "\n",
      "prompt 3: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: D\n",
      "\n",
      "============= piqa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,838\n",
      "\n",
      "----------- item: 761 ------------\n",
      "Query: Question: To get rid of a mouse infestation in a home,\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: To get rid of a mouse infestation in a home,  Answer: look for any\n",
      "mouse you see and then put them back outside afterwards.\n",
      "\n",
      "prompt 1: Question: To get rid of a mouse infestation in a home,  Answer: place mouse\n",
      "traps in tucked away spaces and use rat poisoned food.\n",
      "\n",
      "----------- item: 1114 ------------\n",
      "Query: Question: What is the best way to cut narrow metal bars to size?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: What is the best way to cut narrow metal bars to size?  Answer: Use a\n",
      "hacksaw.\n",
      "\n",
      "prompt 1: Question: What is the best way to cut narrow metal bars to size?  Answer: Use a\n",
      "metal file.\n",
      "\n",
      "============= openbook_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 500\n",
      "\n",
      "----------- item: 479 ------------\n",
      "Query: An example of water being an electrical conductor would be what?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: An example of water being an electrical conductor would be what? lightening\n",
      "hitting water and organisms inside dying\n",
      "\n",
      "prompt 1: An example of water being an electrical conductor would be what? standing in a\n",
      "puddle and avoiding being struck by lightening\n",
      "\n",
      "prompt 2: An example of water being an electrical conductor would be what? standing in a\n",
      "field and getting struck by lightening\n",
      "\n",
      "prompt 3: An example of water being an electrical conductor would be what? grabbing a\n",
      "fence and being shocked\n",
      "\n",
      "----------- item: 227 ------------\n",
      "Query: How is electricity produced from the ocean?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: How is electricity produced from the ocean? decaying organic material from\n",
      "sealife\n",
      "\n",
      "prompt 1: How is electricity produced from the ocean? energy is accessed underwater from\n",
      "tides\n",
      "\n",
      "prompt 2: How is electricity produced from the ocean? drills to access oil supplies\n",
      "\n",
      "prompt 3: How is electricity produced from the ocean? chemical reactions produced from the\n",
      "salt in the water\n",
      "\n",
      "============= lambada_openai =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 5,153\n",
      "\n",
      "----------- item: 4112 ------------\n",
      "Expected continuation: driving\n",
      "\n",
      "prompt 0: I want to go to him, hug him and make it better, but my feet won't move.  \"Mom\n",
      "was exhausted and I was laughing with you. She almost fell asleep and went off the road.\n",
      "They could have died or been hurt and I was making love to you. I should have been there.\n",
      "If I had been there, I would have been\n",
      "\n",
      "----------- item: 2197 ------------\n",
      "Expected continuation: graduation\n",
      "\n",
      "prompt 0: \"You're moving in with me immediately. We'll be married this weekend.\"  She\n",
      "stepped back even as he moved to take her into his arms again. \"I'm graduating this\n",
      "weekend.\"  He smiled slightly, immensely proud of what she'd accomplished. \"Were you going\n",
      "to invite me to your\n",
      "\n",
      "============= hellaswag =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,042\n",
      "\n",
      "----------- item: 588 ------------\n",
      "Query: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man takes out\n",
      "a spray bottle and continues to mow the lawn.\n",
      "\n",
      "prompt 1: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man fixes his\n",
      "bra laces.\n",
      "\n",
      "prompt 2: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man stands and\n",
      "walks back across the lawn.\n",
      "\n",
      "prompt 3: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man plays a\n",
      "large keyboard, strumming on the keys and pausing intermittently.\n",
      "\n",
      "----------- item: 449 ------------\n",
      "Query: Playing violin: A man is standing on a stage playing a violin. A man\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Playing violin: A man is standing on a stage playing a violin. A man is standing\n",
      "behind him in a tuxedo.\n",
      "\n",
      "prompt 1: Playing violin: A man is standing on a stage playing a violin. A man comes and\n",
      "stands on the stage next to him.\n",
      "\n",
      "prompt 2: Playing violin: A man is standing on a stage playing a violin. A man in a\n",
      "position of power is walking on stage with the violin.\n",
      "\n",
      "prompt 3: Playing violin: A man is standing on a stage playing a violin. A man is\n",
      "performing a song on a flute.\n",
      "\n",
      "============= winograd =============\n",
      "\n",
      " This is a schema task type. Each item will be scored as correct if the \"continuation\n",
      "part\" with the highest probability is in the correct prompt. This is similar to multiple\n",
      "choice except here we have a common suffix (the continuation) and in multiple choice we\n",
      "have a common prefix (the query). The continuations are the same in each prompt so in\n",
      "isolation they would have the same probability. The key is they are judged in the context\n",
      "of the full prompt. It's also important that we look at the probabilities only of the\n",
      "continuation parts, because we're interested in which is most probable in the given\n",
      "context, not which prompt overall is more likely.\n",
      "\n",
      "Showing 2 random items of 273\n",
      "\n",
      "----------- item: 186 ------------\n",
      "Continuation part: were very much in the minority.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: When the sponsors of the bill got to the town hall, they were surprised to find\n",
      "that the room was full of opponents. The sponsors were very much in the minority.\n",
      "\n",
      "prompt 1: When the sponsors of the bill got to the town hall, they were surprised to find\n",
      "that the room was full of opponents. The opponents were very much in the minority.\n",
      "\n",
      "----------- item: 238 ------------\n",
      "Continuation part: a sympathetic look.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: When Tommy dropped his ice cream, Timmy giggled, so father gave Tommy a\n",
      "sympathetic look.\n",
      "\n",
      "prompt 1: When Tommy dropped his ice cream, Timmy giggled, so father gave Timmy a\n",
      "sympathetic look.\n",
      "\n",
      "============= winogrande =============\n",
      "\n",
      " This is a schema task type. Each item will be scored as correct if the \"continuation\n",
      "part\" with the highest probability is in the correct prompt. This is similar to multiple\n",
      "choice except here we have a common suffix (the continuation) and in multiple choice we\n",
      "have a common prefix (the query). The continuations are the same in each prompt so in\n",
      "isolation they would have the same probability. The key is they are judged in the context\n",
      "of the full prompt. It's also important that we look at the probabilities only of the\n",
      "continuation parts, because we're interested in which is most probable in the given\n",
      "context, not which prompt overall is more likely.\n",
      "\n",
      "Showing 2 random items of 1,267\n",
      "\n",
      "----------- item: 652 ------------\n",
      "Continuation part: didn't want the credit.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Natalie committed a bunch of good deeds in Samantha's name, because Natalie\n",
      "didn't want the credit.\n",
      "\n",
      "prompt 1: Natalie committed a bunch of good deeds in Samantha's name, because Samantha\n",
      "didn't want the credit.\n",
      "\n",
      "----------- item: 778 ------------\n",
      "Continuation part: was a mean person.\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: More people attended the funeral of Joel than of Derrick because Joel was a mean\n",
      "person.\n",
      "\n",
      "prompt 1: More people attended the funeral of Joel than of Derrick because Derrick was a\n",
      "mean person.\n",
      "\n",
      "============= bigbench_dyck_languages =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 1,000\n",
      "\n",
      "----------- item: 433 ------------\n",
      "Expected continuation: ) } )\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed\n",
      "properly.   Input: ( { ( Output:\n",
      "\n",
      "----------- item: 913 ------------\n",
      "Expected continuation: ) }\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed\n",
      "properly.   Input: { ( ( ) ( < [ < ( ) > ] > ) { } ( { [ ] } ) Output:\n",
      "\n",
      "============= agi_eval_lsat_ar =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 230\n",
      "\n",
      "----------- item: 226 ------------\n",
      "Query: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: A\n",
      "\n",
      "prompt 1: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: B\n",
      "\n",
      "prompt 2: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: C\n",
      "\n",
      "prompt 3: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: D\n",
      "\n",
      "----------- item: 134 ------------\n",
      "Query: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: A\n",
      "\n",
      "prompt 1: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: B\n",
      "\n",
      "prompt 2: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: C\n",
      "\n",
      "prompt 3: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: D\n",
      "\n",
      "============= bigbench_cs_algorithms =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 1,320\n",
      "\n",
      "----------- item: 336 ------------\n",
      "Expected continuation: Invalid\n",
      "\n",
      "prompt 0: Determine whether the given sequence of parentheses is properly matched.\n",
      "Sequence: ( } [ ] Valid/Invalid?\n",
      "\n",
      "----------- item: 1147 ------------\n",
      "Expected continuation: Invalid\n",
      "\n",
      "prompt 0: Determine whether the given sequence of parentheses is properly matched.\n",
      "Sequence: } ) } } ) [ ) ( ] } ) } } } ) ) ) } ) } Valid/Invalid?\n",
      "\n",
      "============= bigbench_operators =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 210\n",
      "\n",
      "----------- item: 45 ------------\n",
      "Expected continuation: 90\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result. i op j suppresses\n",
      "the j first digits of i. 125690 op 4 =\n",
      "\n",
      "----------- item: 60 ------------\n",
      "Expected continuation: 1\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result. op i is the ith odd\n",
      "positive number. op 1 =\n",
      "\n",
      "============= bigbench_repeat_copy_logic =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 32\n",
      "\n",
      "----------- item: 14 ------------\n",
      "Expected continuation: a woodchuck chucks five pounds of wood a woodchuck chucks two tons\n",
      "of wood\n",
      "\n",
      "prompt 0: repeat with logic:  Q: repeat a woodchuck chucks lots of wood two times, but\n",
      "replace lots with five pounds the first time and two tons the second time A:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: python python data python python data python python data\n",
      "\n",
      "prompt 0: repeat with logic:  Q: say python twice and data once, and then repeat all of\n",
      "this three times. A:\n",
      "\n",
      "============= squad =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 10,570\n",
      "\n",
      "----------- item: 2895 ------------\n",
      "Expected continuation: BBC HD\n",
      "\n",
      "prompt 0: Context: Virgin Media (re-branded in 2007 from NTL:Telewest) started to offer a\n",
      "high-definition television (HDTV) capable set top box, although from 30 November 2006\n",
      "until 30 July 2009 it only carried one linear HD channel, BBC HD, after the conclusion of\n",
      "the ITV HD trial. Virgin Media has claimed that other HD channels were \"locked up\" or\n",
      "otherwise withheld from their platform, although Virgin Media did in fact have an option\n",
      "to carry Channel 4 HD in the future. Nonetheless, the linear channels were not offered,\n",
      "Virgin Media instead concentrating on its Video On Demand service to carry a modest\n",
      "selection of HD content. Virgin Media has nevertheless made a number of statements over\n",
      "the years, suggesting that more linear HD channels are on the way. Question: What was the\n",
      "one linear HD channel Virgin Media carried from November 2006 to July 2009? Answer:\n",
      "\n",
      "----------- item: 5327 ------------\n",
      "Expected continuation: about three\n",
      "\n",
      "prompt 0: Context: Train operator Virgin Trains East Coast provides a half-hourly\n",
      "frequency of trains to London King's Cross, with a journey time of about three hours,\n",
      "these services call at Durham, Darlington, York, Doncaster, Newark North Gate and\n",
      "Peterborough and north to Scotland with all trains calling at Edinburgh and a small number\n",
      "of trains extended to Glasgow, Aberdeen and Inverness. CrossCountry trains serve\n",
      "destinations in Yorkshire, the Midlands and the South West. First TransPennine Express\n",
      "operates services to Manchester and Liverpool. Northern Rail provides local and regional\n",
      "services. Question: How many hours can one expect to ride the train from Newcastle to\n",
      "King's Cross? Answer:\n",
      "\n",
      "============= coqa =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 7,983\n",
      "\n",
      "----------- item: 1422 ------------\n",
      "Expected continuation: No\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the\n",
      "final question by referring to the story and the previous questions. Story: A little boy\n",
      "named Joey did not like to brush his teeth. One day, as his mother asked him to brush his\n",
      "teeth, Joey said, \"I don't want to! It's gross and a waste of time!\" In response, Joey's\n",
      "mom told him that in order for him to grow up and be big and strong like she is, he must\n",
      "brush his teeth. In disgust, with his mom watching to make sure he brushed his teeth\n",
      "right, he brushed his teeth and whined until he was finished. The next day, Joey did not\n",
      "brush his teeth and told his mom that he did. After going to school, one of Joey's friends\n",
      "said that his breath stunk and began to make fun of him. This made Joey very angry, so he\n",
      "pushed the boy over and started to cry. A teacher came over and called the principal as\n",
      "both kids were being loud and starting a fight. Joey's mom came to school and took him\n",
      "home. After asking what was wrong, Joey told his mom that he didn't brush his teeth. After\n",
      "hearing this, his mom marched him up to the bathroom and forced him to brush his teeth.\n",
      "\"You won't get teased if you brush your teeth, Joey!\" she yelled, and then left the room.\n",
      "From that day forward, Joey brushed his teeth every day so the other kids wouldn't make\n",
      "fun of him. Preceding questions: Question: What was the boys name? Answer: Joey Question:\n",
      "Joey didn't like to what? Answer: brush his teeth Question: Who asked him to one day?\n",
      "Answer: His mother  Final question: Question: Did he cooperate? Answer:\n",
      "\n",
      "----------- item: 1119 ------------\n",
      "Expected continuation: Just realized some horrible things about his father.\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the\n",
      "final question by referring to the story and the previous questions. Story: (EW.com ) --\n",
      "When last we saw Olivia Pope, she was flying off into the sunset (or maybe just flying\n",
      "off) with Jake by her sideâ€”leaving behind a broken Fitz, who'd just lost his son and\n",
      "realized some horrible things about his father. D.C. now rests in the hands of Olivia's\n",
      "own father, who once again took over B-613. So what awaits the team when season four\n",
      "starts up? How about a new face?   EW has confirmed that \"Arrested Development\" star\n",
      "Portia de Rossi will join \"Scandal's\" fourth season for a multiple-episode arc. Ellen\n",
      "DeGeneres first announced the news on Twitter, bragging about her wife's \"top secret\"\n",
      "storyline.   Could the words \"top secret\" mean de Rossi is a part of Papa Pope's B-613\n",
      "army? We'll find out Thursday, Sept. 25 at 9 p.m., when Scandal returns on ABC.   ABC fall\n",
      "premiere dates announced: 'Scandal' returns   Former 'Scandal' actor Columbus Short\n",
      "arrested in Texas   See the original story at EW.com   CLICK HERE to Try 2 RISK FREE\n",
      "issues of Entertainment Weekly   © 2011 Entertainment Weekly and Time Inc. All rights\n",
      "reserved.    Preceding questions: Question: What leading female character is flying off?\n",
      "Answer: Olivia Pope Question: Who was she with? Answer: Jake Question: Who was she\n",
      "leaving? Answer: Fitz Question: Was he okay? Answer: No Question: Why not? Answer: He'd\n",
      "just lost his son  Final question: Question: What else? Answer:\n",
      "\n",
      "============= boolq =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 3,270\n",
      "\n",
      "----------- item: 2089 ------------\n",
      "Query: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Answer: no\n",
      "\n",
      "prompt 1: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Answer: yes\n",
      "\n",
      "----------- item: 1473 ------------\n",
      "Query: Passage: 85% of MGN cases are classified as primary membranous glomerulonephritis--\n",
      "that is to say, the cause of the disease is idiopathic (of unknown origin or cause). This\n",
      "can also be referred to as idiopathic membranous nephropathy. One study has identified\n",
      "antibodies to an M-type phospholipase A receptor in 70% (26 of 37) cases evaluated. Other\n",
      "studies have implicated neutral endopeptidase and cationic bovine serum albumin as\n",
      "antigens. Question: is membranous nephropathy the same as membranous glomerulonephritis?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: 85% of MGN cases are classified as primary membranous\n",
      "glomerulonephritis--that is to say, the cause of the disease is idiopathic (of unknown\n",
      "origin or cause). This can also be referred to as idiopathic membranous nephropathy. One\n",
      "study has identified antibodies to an M-type phospholipase A receptor in 70% (26 of 37)\n",
      "cases evaluated. Other studies have implicated neutral endopeptidase and cationic bovine\n",
      "serum albumin as antigens. Question: is membranous nephropathy the same as membranous\n",
      "glomerulonephritis? Answer: no\n",
      "\n",
      "prompt 1: Passage: 85% of MGN cases are classified as primary membranous\n",
      "glomerulonephritis--that is to say, the cause of the disease is idiopathic (of unknown\n",
      "origin or cause). This can also be referred to as idiopathic membranous nephropathy. One\n",
      "study has identified antibodies to an M-type phospholipase A receptor in 70% (26 of 37)\n",
      "cases evaluated. Other studies have implicated neutral endopeptidase and cationic bovine\n",
      "serum albumin as antigens. Question: is membranous nephropathy the same as membranous\n",
      "glomerulonephritis? Answer: yes\n",
      "\n",
      "============= bigbench_language_identification =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,000\n",
      "\n",
      "----------- item: 8417 ------------\n",
      "Query: Given a sentence, select the correct language among the choices Sentence: Tiagtai\n",
      "ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  D\n",
      "\n",
      "----------- item: 9173 ------------\n",
      "Query: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_examples(items_per_task=2, random_items=True, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "814e4a49-dc57-4fec-a320-b1bfb40d1779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eval Task</th>\n",
       "      <th>Task Category</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>#shots</th>\n",
       "      <th>#datapoints</th>\n",
       "      <th>Random baseline</th>\n",
       "      <th>Centered Metric?</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mmlu_zeroshot</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>0</td>\n",
       "      <td>14042</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MMLU consists of 14,042 four-choice multiple c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hellaswag_zeroshot</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>0</td>\n",
       "      <td>10042</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HellaSwag consists of 10,042 multiple choice s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jeopardy</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>2117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeopardy consists of 2,117 Jeopardy questions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>triviaqa_sm_sub</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>question answering</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trivia QA is a question answering dataset that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gsm8k_cot</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>question answering</td>\n",
       "      <td>3</td>\n",
       "      <td>1319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GSM8K consists of 1,319 short, free-response g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>agi_eval_sat_math_cot</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>question answering</td>\n",
       "      <td>3</td>\n",
       "      <td>220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AGI Eval SAT Math consists of 220 short, free-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aqua_cot</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>question answering</td>\n",
       "      <td>3</td>\n",
       "      <td>245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AQUA-RAT (Algebra Question Answering with Rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svamp_cot</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>question answering</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVAMP consists of 300 short, free-response gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bigbench_qa_wikidata</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>20321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG-bench wikidata consists of 20,321 question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arc_easy</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>2376</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARC easy consists of 2,376 easy four-choice mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>2376</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARC easy consists of 2,376 easy four-choice mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mmlu_fewshot</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>5</td>\n",
       "      <td>14042</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MMLU consists of 14,042 four-choice multiple c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bigbench_misconceptions</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>219</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench misconceptions consists of 219 true ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>copa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COPA consists of 100 cause/effect multiple cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>siqa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>1954</td>\n",
       "      <td>33.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Social Interaction QA consists of 1954 two-cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>commonsense_qa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>1221</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Commonsense QA consists of 1,221 four-choice m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>piqa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>1838</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PIQA consists of 1,838 commonsense physical in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>openbook_qa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenBook QA consists of 500 four-choice multip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bigbench_novel_concepts</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG-bench novel concepts consists of 32 find-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bigbench_strange_stories</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>174</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG-bench strange stories consists of 174 sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bigbench_strategy_qa</td>\n",
       "      <td>commonsense reasoning</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>2289</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG-bench strategy QA consists of 2,289 very e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>0</td>\n",
       "      <td>5153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAMBADA consists of 5,153 passages take from b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hellaswag</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>10042</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HellaSwag consists of 10,042 multiple choice s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>winograd</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>schema</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Winograd Schema Challenge consists of 273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>winogrande</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>schema</td>\n",
       "      <td>0</td>\n",
       "      <td>1267</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Winogrande consists of 1,267 scenarios in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bigbench_conlang_translation</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG bench conlang translation consists of 164 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bigbench_language_identification</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG bench language identification consists of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bigbench_conceptual_combinations</td>\n",
       "      <td>language understanding</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIG bench conceptual combinations consists of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bigbench_elementary_math_qa</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>38160</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench elementary math QA consists of 38,16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bigbench_dyck_languages</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench dyck languages consists of 1000 comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>agi_eval_lsat_ar</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>3</td>\n",
       "      <td>230</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AGI Eval LSAT Analytical Reasoning consists of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bigbench_cs_algorithms</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>1320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench cs algorithms consists of 1,320 samp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bigbench_logical_deduction</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench logical deduction consists of 1500 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bigbench_operators</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench logical operators consists of 210 qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bigbench_repeat_copy_logic</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big bench repeat copy logic consists of 32 tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>simple_arithmetic_nospaces</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simple arithmetic with spaces was developed by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>simple_arithmetic_withspaces</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simple arithmetic with spaces was developed by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>math_qa</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>2983</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Math QA consists of 2,983 four-choice multiple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>logi_qa</td>\n",
       "      <td>symbolic problem solving</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>651</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogiQA consists of 651 four-choice multiple c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>pubmed_qa_labeled</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pubmed QA L consists of 1000 hand-labeled medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>squad</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>10</td>\n",
       "      <td>10570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQuAD consists of 10,570 short documents follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>agi_eval_lsat_rc</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>3</td>\n",
       "      <td>268</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LSAT Reading Comprehension consists of 268 pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>agi_eval_lsat_lr</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>3</td>\n",
       "      <td>510</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LSAT Logical Reasoning consists of 510 passage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>coqa</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>language modeling</td>\n",
       "      <td>0</td>\n",
       "      <td>7983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CoQA consists of 7,983 passage-based short fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bigbench_understanding_fables</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>189</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Understanding fables consists of 189 short st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>boolq</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>3270</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BoolQ consists of 3,270 short passages on a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>agi_eval_sat_en</td>\n",
       "      <td>reading comprehension</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAT English consists of 206 passage-based fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>winogender_mc_female</td>\n",
       "      <td>safety</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Winogender Multiple Choice (Female) datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>winogender_mc_male</td>\n",
       "      <td>safety</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Winogender Multiple Choice (Male) dataset ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>enterprise_pii_classification</td>\n",
       "      <td>safety</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>10</td>\n",
       "      <td>3395</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enterprise PII Classification was released by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bbq</td>\n",
       "      <td>safety</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>3</td>\n",
       "      <td>58492</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bias Benchmark for QA (BBQ) is a multiple choi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>gpqa_main</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPQA is a challenging dataset of 448 multiple-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>gpqa_diamond</td>\n",
       "      <td>world knowledge</td>\n",
       "      <td>multiple choice</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A subset of highest quality questions from teh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Eval Task             Task Category  \\\n",
       "0                      mmlu_zeroshot           world knowledge   \n",
       "1                 hellaswag_zeroshot    language understanding   \n",
       "2                           jeopardy           world knowledge   \n",
       "3                    triviaqa_sm_sub           world knowledge   \n",
       "4                          gsm8k_cot  symbolic problem solving   \n",
       "5              agi_eval_sat_math_cot  symbolic problem solving   \n",
       "6                           aqua_cot  symbolic problem solving   \n",
       "7                          svamp_cot  symbolic problem solving   \n",
       "8               bigbench_qa_wikidata           world knowledge   \n",
       "9                           arc_easy           world knowledge   \n",
       "10                     arc_challenge           world knowledge   \n",
       "11                      mmlu_fewshot           world knowledge   \n",
       "12           bigbench_misconceptions           world knowledge   \n",
       "13                              copa     commonsense reasoning   \n",
       "14                              siqa     commonsense reasoning   \n",
       "15                    commonsense_qa     commonsense reasoning   \n",
       "16                              piqa     commonsense reasoning   \n",
       "17                       openbook_qa     commonsense reasoning   \n",
       "18           bigbench_novel_concepts     commonsense reasoning   \n",
       "19          bigbench_strange_stories     commonsense reasoning   \n",
       "20              bigbench_strategy_qa     commonsense reasoning   \n",
       "21                    lambada_openai    language understanding   \n",
       "22                         hellaswag    language understanding   \n",
       "23                          winograd    language understanding   \n",
       "24                        winogrande    language understanding   \n",
       "25      bigbench_conlang_translation    language understanding   \n",
       "26  bigbench_language_identification    language understanding   \n",
       "27  bigbench_conceptual_combinations    language understanding   \n",
       "28       bigbench_elementary_math_qa  symbolic problem solving   \n",
       "29           bigbench_dyck_languages  symbolic problem solving   \n",
       "30                  agi_eval_lsat_ar  symbolic problem solving   \n",
       "31            bigbench_cs_algorithms  symbolic problem solving   \n",
       "32        bigbench_logical_deduction  symbolic problem solving   \n",
       "33                bigbench_operators  symbolic problem solving   \n",
       "34        bigbench_repeat_copy_logic  symbolic problem solving   \n",
       "35        simple_arithmetic_nospaces  symbolic problem solving   \n",
       "36      simple_arithmetic_withspaces  symbolic problem solving   \n",
       "37                           math_qa  symbolic problem solving   \n",
       "38                           logi_qa  symbolic problem solving   \n",
       "39                 pubmed_qa_labeled     reading comprehension   \n",
       "40                             squad     reading comprehension   \n",
       "41                  agi_eval_lsat_rc     reading comprehension   \n",
       "42                  agi_eval_lsat_lr     reading comprehension   \n",
       "43                              coqa     reading comprehension   \n",
       "44     bigbench_understanding_fables     reading comprehension   \n",
       "45                             boolq     reading comprehension   \n",
       "46                   agi_eval_sat_en     reading comprehension   \n",
       "47              winogender_mc_female                    safety   \n",
       "48                winogender_mc_male                    safety   \n",
       "49     enterprise_pii_classification                    safety   \n",
       "50                               bbq                    safety   \n",
       "51                         gpqa_main           world knowledge   \n",
       "52                      gpqa_diamond           world knowledge   \n",
       "\n",
       "             Task Type  #shots  #datapoints  Random baseline  \\\n",
       "0      multiple choice       0        14042             25.0   \n",
       "1      multiple choice       0        10042             25.0   \n",
       "2    language modeling      10         2117              0.0   \n",
       "3   question answering       3         3000              0.0   \n",
       "4   question answering       3         1319              0.0   \n",
       "5   question answering       3          220              0.0   \n",
       "6   question answering       3          245              0.0   \n",
       "7   question answering       3          300              0.0   \n",
       "8    language modeling      10        20321              0.0   \n",
       "9      multiple choice      10         2376             25.0   \n",
       "10     multiple choice      10         2376             25.0   \n",
       "11     multiple choice       5        14042             25.0   \n",
       "12     multiple choice      10          219             50.0   \n",
       "13     multiple choice       0          100             50.0   \n",
       "14     multiple choice      10         1954             33.3   \n",
       "15     multiple choice      10         1221             20.0   \n",
       "16     multiple choice      10         1838             50.0   \n",
       "17     multiple choice       0          500             25.0   \n",
       "18     multiple choice      10           32             20.0   \n",
       "19     multiple choice      10          174             50.0   \n",
       "20     multiple choice      10         2289             50.0   \n",
       "21   language modeling       0         5153              0.0   \n",
       "22     multiple choice      10        10042             25.0   \n",
       "23              schema       0          273             50.0   \n",
       "24              schema       0         1267             50.0   \n",
       "25   language modeling       0          164              0.0   \n",
       "26     multiple choice      10        10000              9.1   \n",
       "27     multiple choice      10          103             25.0   \n",
       "28     multiple choice      10        38160             20.0   \n",
       "29   language modeling      10         1000              0.0   \n",
       "30     multiple choice       3          230             20.0   \n",
       "31   language modeling      10         1320              0.0   \n",
       "32     multiple choice      10         1500             20.0   \n",
       "33   language modeling      10          210              0.0   \n",
       "34   language modeling      10           32              0.0   \n",
       "35   language modeling      10         1000              0.0   \n",
       "36   language modeling      10         1000              0.0   \n",
       "37     multiple choice      10         2983             20.0   \n",
       "38     multiple choice      10          651             27.0   \n",
       "39   language modeling      10         1000              0.0   \n",
       "40   language modeling      10        10570              0.0   \n",
       "41     multiple choice       3          268             20.0   \n",
       "42     multiple choice       3          510             20.0   \n",
       "43   language modeling       0         7983              0.0   \n",
       "44     multiple choice      10          189             20.0   \n",
       "45     multiple choice      10         3270             62.0   \n",
       "46     multiple choice       3          206             25.0   \n",
       "47     multiple choice      10           60             50.0   \n",
       "48     multiple choice      10           60             50.0   \n",
       "49     multiple choice      10         3395             50.0   \n",
       "50     multiple choice       3        58492             50.0   \n",
       "51     multiple choice       5          448             25.0   \n",
       "52     multiple choice       5          198             25.0   \n",
       "\n",
       "    Centered Metric?                                        Description  \n",
       "0                NaN  MMLU consists of 14,042 four-choice multiple c...  \n",
       "1                NaN  HellaSwag consists of 10,042 multiple choice s...  \n",
       "2                NaN  Jeopardy consists of 2,117 Jeopardy questions ...  \n",
       "3                NaN  Trivia QA is a question answering dataset that...  \n",
       "4                NaN  GSM8K consists of 1,319 short, free-response g...  \n",
       "5                NaN  AGI Eval SAT Math consists of 220 short, free-...  \n",
       "6                NaN  AQUA-RAT (Algebra Question Answering with Rati...  \n",
       "7                NaN  SVAMP consists of 300 short, free-response gra...  \n",
       "8                NaN  BIG-bench wikidata consists of 20,321 question...  \n",
       "9                NaN  ARC easy consists of 2,376 easy four-choice mu...  \n",
       "10               NaN  ARC easy consists of 2,376 easy four-choice mu...  \n",
       "11               NaN  MMLU consists of 14,042 four-choice multiple c...  \n",
       "12               NaN  Big bench misconceptions consists of 219 true ...  \n",
       "13               NaN  COPA consists of 100 cause/effect multiple cho...  \n",
       "14               NaN  Social Interaction QA consists of 1954 two-cho...  \n",
       "15               NaN  Commonsense QA consists of 1,221 four-choice m...  \n",
       "16               NaN  PIQA consists of 1,838 commonsense physical in...  \n",
       "17               NaN  OpenBook QA consists of 500 four-choice multip...  \n",
       "18               NaN  BIG-bench novel concepts consists of 32 find-t...  \n",
       "19               NaN   BIG-bench strange stories consists of 174 sho...  \n",
       "20               NaN  BIG-bench strategy QA consists of 2,289 very e...  \n",
       "21               NaN  LAMBADA consists of 5,153 passages take from b...  \n",
       "22               NaN  HellaSwag consists of 10,042 multiple choice s...  \n",
       "23               NaN   The Winograd Schema Challenge consists of 273...  \n",
       "24               NaN  The Winogrande consists of 1,267 scenarios in ...  \n",
       "25               NaN  BIG bench conlang translation consists of 164 ...  \n",
       "26               NaN  BIG bench language identification consists of ...  \n",
       "27               NaN  BIG bench conceptual combinations consists of ...  \n",
       "28               NaN  Big bench elementary math QA consists of 38,16...  \n",
       "29               NaN  Big bench dyck languages consists of 1000 comp...  \n",
       "30               NaN  AGI Eval LSAT Analytical Reasoning consists of...  \n",
       "31               NaN  Big bench cs algorithms consists of 1,320 samp...  \n",
       "32               NaN  Big bench logical deduction consists of 1500 f...  \n",
       "33               NaN  Big bench logical operators consists of 210 qu...  \n",
       "34               NaN  Big bench repeat copy logic consists of 32 tas...  \n",
       "35               NaN  Simple arithmetic with spaces was developed by...  \n",
       "36               NaN  Simple arithmetic with spaces was developed by...  \n",
       "37               NaN  Math QA consists of 2,983 four-choice multiple...  \n",
       "38               NaN   LogiQA consists of 651 four-choice multiple c...  \n",
       "39               NaN  Pubmed QA L consists of 1000 hand-labeled medi...  \n",
       "40               NaN  SQuAD consists of 10,570 short documents follo...  \n",
       "41               NaN  LSAT Reading Comprehension consists of 268 pas...  \n",
       "42               NaN  LSAT Logical Reasoning consists of 510 passage...  \n",
       "43               NaN  CoQA consists of 7,983 passage-based short fre...  \n",
       "44               NaN   Understanding fables consists of 189 short st...  \n",
       "45               NaN   BoolQ consists of 3,270 short passages on a d...  \n",
       "46               NaN   SAT English consists of 206 passage-based fou...  \n",
       "47               NaN  The Winogender Multiple Choice (Female) datase...  \n",
       "48               NaN  The Winogender Multiple Choice (Male) dataset ...  \n",
       "49               NaN  Enterprise PII Classification was released by ...  \n",
       "50               NaN  Bias Benchmark for QA (BBQ) is a multiple choi...  \n",
       "51               NaN  GPQA is a challenging dataset of 448 multiple-...  \n",
       "52               NaN  A subset of highest quality questions from teh...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(f\"{get_base_dir()}/eval_bundle/eval_meta_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9ede80-6eeb-4def-84f3-ad19aa72768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "from my_nanochat.my_core_eval import render_prompts_mc, render_prompts_lm, render_prompts_schema\n",
    "\n",
    "def print_wrap(s, remove_newlines=False):\n",
    "    if remove_newlines:\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "    print(textwrap.fill(s, 90))\n",
    "\n",
    "def print_examples(items_per_task=3, random_items=False, random_seed=None, tasks_to_print=None):\n",
    "\n",
    "    if random_seed:\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    tasks = config['icl_tasks']\n",
    "    for task in tasks:\n",
    "        if tasks_to_print is not None:\n",
    "            if task['label'] not in tasks_to_print:\n",
    "                continue\n",
    "        \n",
    "        task_type = task['icl_task_type']\n",
    "        continuation_delimiter = task.get('continuation_delimiter', ' ')\n",
    "        data_path = os.path.join(data_base_path, task['dataset_uri'])\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "        \n",
    "        print(f\"============= {task['label']} =============\\n\")\n",
    "    \n",
    "        if task_type == 'multiple_choice':\n",
    "            print_wrap(\n",
    "\"\"\"This is multiple choice so each item will be scored as correct if the choice with the highest probabiliy\n",
    "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
    "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
    "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
    "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
    "\"\"\", remove_newlines=True)\n",
    "        elif task_type == 'language_modeling':\n",
    "            print_wrap(\n",
    "\"\"\"This is a language modeling task type. Each item will be scored as correct if the model generates\n",
    "the expected continuation.\n",
    "\"\"\", remove_newlines=True)\n",
    "        elif task_type == 'schema':\n",
    "             print_wrap(\"\"\"\n",
    "This is a schema task type. Each item will be scored as correct if the \"continuation part\" with the\n",
    "highest probability is in the correct prompt. This is similar to multiple choice except here we\n",
    "have a common suffix (the continuation) and in multiple choice we have a common prefix (the query).\n",
    "The continuations are the same in each prompt so in isolation they would have the same probability.\n",
    "The key is they are judged in the context of the full prompt. It's also important that we look at\n",
    "the probabilities only of the continuation parts, because we're interested in which is most probable\n",
    "in the given context, not which prompt overall is more likely.\n",
    "\"\"\", remove_newlines=True)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        print()\n",
    "\n",
    "        if random_items:\n",
    "            print(f\"Showing {items_per_task} random items of {len(data):,d}\\n\")\n",
    "        else:\n",
    "            print(f\"Showing the first {items_per_task} items of {len(data):,d}\\n\")\n",
    "\n",
    "        \n",
    "        for i in random.sample(range(0, len(data)), items_per_task) if random_items else range(items_per_task):\n",
    "            print(f\"----------- item: {i} ------------\")\n",
    "            item = data[i]\n",
    "            if task_type == 'multiple_choice':\n",
    "                print_wrap(f\"Query: {item['query']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\") \n",
    "                prompts = render_prompts_mc(item, continuation_delimiter, [])\n",
    "            elif task_type == 'language_modeling':\n",
    "                print_wrap(f\"Expected continuation: {item['continuation']}\")\n",
    "                print()\n",
    "                prompts = render_prompts_lm(item, continuation_delimiter, [])\n",
    "                prompts = prompts[:-1] # because in CORE eval we only use the first method of scoring\n",
    "            elif task_type == 'schema':\n",
    "                print_wrap(f\"Continuation part: {item['continuation']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\")            \n",
    "                prompts = render_prompts_schema(item, continuation_delimiter, [])\n",
    "            else:\n",
    "                assert False\n",
    "    \n",
    "            for j, prompt in enumerate(prompts):\n",
    "                print_wrap(f\"prompt {j}: {prompt}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f99e3-7510-4952-ae8c-4cbc172ba7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
