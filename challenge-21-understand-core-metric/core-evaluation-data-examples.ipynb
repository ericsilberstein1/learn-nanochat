{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8d8f5a-f9ae-4fc1-abbb-e3070fefd0df",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenege. See `understand-core-metric.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161594c-f4d4-4a1d-b506-a976684e92ee",
   "metadata": {},
   "source": [
    "## CORE evaluation data examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ded23-37a2-4569-8e0e-f8bc54405e5d",
   "metadata": {},
   "source": [
    "I realized after creating `challenge-26-understand-midtrain/midtrain-data-examples.ipynb` and doing midtraining that I was forgetting / getting confused about the CORE evaluation data. This notebook with some ugly code shows a few examples for each task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "93424fbe-b130-4d7c-b2f7-e51f62bb3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= hellaswag_zeroshot =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,042\n",
      "\n",
      "----------- item: 926 ------------\n",
      "Query: Cleaning windows: The man sprays windex and washes off with squeegee. The man uses\n",
      "leaf blower to dry the window. The man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man drops the reusable leaf blower in the garbage\n",
      "can.\n",
      "\n",
      "prompt 1: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man talks into a microphone.\n",
      "\n",
      "prompt 2: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man finishes and talks about the window before\n",
      "walking off.\n",
      "\n",
      "prompt 3: Cleaning windows: The man sprays windex and washes off with squeegee. The man\n",
      "uses leaf blower to dry the window. The man uses pocket sweepers to spray leaves off the\n",
      "windows.\n",
      "\n",
      "----------- item: 1500 ------------\n",
      "Query: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He then wears an abdominal belt and lifts the weight bar\n",
      "again and drops it.\n",
      "\n",
      "prompt 1: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He shakes his hand and moves on to the next bar with the\n",
      "burning sensation.\n",
      "\n",
      "prompt 2: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He claps and hits the bar a couple of times.\n",
      "\n",
      "prompt 3: Clean and jerk: Then he goes and wipes off his sweat with a towel and lifts the\n",
      "weight bar again and drops it. He then sits on a couch and goes bowling.\n",
      "\n",
      "============= jeopardy =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 2,117\n",
      "\n",
      "----------- item: 347 ------------\n",
      "Expected continuation: Philip of Macedon/Philip II\n",
      "\n",
      "prompt 0: WORLD HISTORY: He had a wife named Cleopatra, a daughter named Cleopatra, and a\n",
      "famous son, Alexander the Great Answer:\n",
      "\n",
      "----------- item: 1478 ------------\n",
      "Expected continuation: Ostrich\n",
      "\n",
      "prompt 0: SCIENCE: The largest cell is the 3-inch diameter yolk of this birds egg Answer:\n",
      "\n",
      "============= bigbench_qa_wikidata =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 20,321\n",
      "\n",
      "----------- item: 5540 ------------\n",
      "Expected continuation: English\n",
      "\n",
      "prompt 0: The language of El País is\n",
      "\n",
      "----------- item: 10097 ------------\n",
      "Expected continuation: Spain\n",
      "\n",
      "prompt 0: The country of citizenship of Carmen Amaya Amaya is\n",
      "\n",
      "============= arc_easy =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 2,376\n",
      "\n",
      "----------- item: 1030 ------------\n",
      "Query: Question: Which human body system forms a barrier between internal body structures\n",
      "and the external environment?\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: nervous\n",
      "\n",
      "prompt 1: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: digestive\n",
      "\n",
      "prompt 2: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: circulatory\n",
      "\n",
      "prompt 3: Question: Which human body system forms a barrier between internal body\n",
      "structures and the external environment? Answer: integumentary\n",
      "\n",
      "----------- item: 869 ------------\n",
      "Query: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: decreased variations in day length\n",
      "\n",
      "prompt 1: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: increased average air temperature\n",
      "\n",
      "prompt 2: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: increased burn rate for incoming meteoroids\n",
      "\n",
      "prompt 3: Question: An increase of carbon dioxide in Earth's atmosphere is most likely to\n",
      "have which effect? Answer: decreased rate of ozone layer destruction\n",
      "\n",
      "============= arc_challenge =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,172\n",
      "\n",
      "----------- item: 73 ------------\n",
      "Query: Question: Over a long period of time, running water in a river erodes the riverbed.\n",
      "This erosion causes the river to\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: stop flowing.\n",
      "\n",
      "prompt 1: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: create waves.\n",
      "\n",
      "prompt 2: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: move faster and cleaner.\n",
      "\n",
      "prompt 3: Question: Over a long period of time, running water in a river erodes the\n",
      "riverbed. This erosion causes the river to Answer: become deeper and wider.\n",
      "\n",
      "----------- item: 324 ------------\n",
      "Query: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: coal\n",
      "\n",
      "prompt 1: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: trees\n",
      "\n",
      "prompt 2: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: water\n",
      "\n",
      "prompt 3: Question: A student is studying nonrenewable resources. Which of these resources\n",
      "could the student be studying? Answer: wind\n",
      "\n",
      "============= copa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 100\n",
      "\n",
      "----------- item: 55 ------------\n",
      "Query: The boy skipped dinner, because\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: The boy skipped dinner, because his mother cooked his favorite meal.\n",
      "\n",
      "prompt 1: The boy skipped dinner, because he ate a big lunch.\n",
      "\n",
      "----------- item: 81 ------------\n",
      "Query: The horse bucked, because\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: The horse bucked, because a fly bit the horse.\n",
      "\n",
      "prompt 1: The horse bucked, because the rider stroked the horse.\n",
      "\n",
      "============= commonsense_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,221\n",
      "\n",
      "----------- item: 805 ------------\n",
      "Query: Question: Where do you store a large container? Choices: A. supermarket B. juice C.\n",
      "hostel D. cabinet Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: A\n",
      "\n",
      "prompt 1: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: B\n",
      "\n",
      "prompt 2: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: C\n",
      "\n",
      "prompt 3: Question: Where do you store a large container? Choices: A. supermarket B. juice\n",
      "C. hostel D. cabinet Answer: D\n",
      "\n",
      "----------- item: 1042 ------------\n",
      "Query: Question: What is a tactic used to interfere with learning about science? Choices:\n",
      "A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer:\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: A\n",
      "\n",
      "prompt 1: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: B\n",
      "\n",
      "prompt 2: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: C\n",
      "\n",
      "prompt 3: Question: What is a tactic used to interfere with learning about science?\n",
      "Choices: A. sense of wonder B. accidents C. intimidation D. increased knowledge Answer: D\n",
      "\n",
      "============= piqa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 1,838\n",
      "\n",
      "----------- item: 761 ------------\n",
      "Query: Question: To get rid of a mouse infestation in a home,\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: To get rid of a mouse infestation in a home,  Answer: look for any\n",
      "mouse you see and then put them back outside afterwards.\n",
      "\n",
      "prompt 1: Question: To get rid of a mouse infestation in a home,  Answer: place mouse\n",
      "traps in tucked away spaces and use rat poisoned food.\n",
      "\n",
      "----------- item: 1114 ------------\n",
      "Query: Question: What is the best way to cut narrow metal bars to size?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: What is the best way to cut narrow metal bars to size?  Answer: Use a\n",
      "hacksaw.\n",
      "\n",
      "prompt 1: Question: What is the best way to cut narrow metal bars to size?  Answer: Use a\n",
      "metal file.\n",
      "\n",
      "============= openbook_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 500\n",
      "\n",
      "----------- item: 479 ------------\n",
      "Query: An example of water being an electrical conductor would be what?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: An example of water being an electrical conductor would be what? lightening\n",
      "hitting water and organisms inside dying\n",
      "\n",
      "prompt 1: An example of water being an electrical conductor would be what? standing in a\n",
      "puddle and avoiding being struck by lightening\n",
      "\n",
      "prompt 2: An example of water being an electrical conductor would be what? standing in a\n",
      "field and getting struck by lightening\n",
      "\n",
      "prompt 3: An example of water being an electrical conductor would be what? grabbing a\n",
      "fence and being shocked\n",
      "\n",
      "----------- item: 227 ------------\n",
      "Query: How is electricity produced from the ocean?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: How is electricity produced from the ocean? decaying organic material from\n",
      "sealife\n",
      "\n",
      "prompt 1: How is electricity produced from the ocean? energy is accessed underwater from\n",
      "tides\n",
      "\n",
      "prompt 2: How is electricity produced from the ocean? drills to access oil supplies\n",
      "\n",
      "prompt 3: How is electricity produced from the ocean? chemical reactions produced from the\n",
      "salt in the water\n",
      "\n",
      "============= lambada_openai =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 5,153\n",
      "\n",
      "----------- item: 4112 ------------\n",
      "Expected continuation: driving\n",
      "\n",
      "prompt 0: I want to go to him, hug him and make it better, but my feet won't move.  \"Mom\n",
      "was exhausted and I was laughing with you. She almost fell asleep and went off the road.\n",
      "They could have died or been hurt and I was making love to you. I should have been there.\n",
      "If I had been there, I would have been\n",
      "\n",
      "----------- item: 2197 ------------\n",
      "Expected continuation: graduation\n",
      "\n",
      "prompt 0: \"You're moving in with me immediately. We'll be married this weekend.\"  She\n",
      "stepped back even as he moved to take her into his arms again. \"I'm graduating this\n",
      "weekend.\"  He smiled slightly, immensely proud of what she'd accomplished. \"Were you going\n",
      "to invite me to your\n",
      "\n",
      "============= hellaswag =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,042\n",
      "\n",
      "----------- item: 588 ------------\n",
      "Query: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man takes out\n",
      "a spray bottle and continues to mow the lawn.\n",
      "\n",
      "prompt 1: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man fixes his\n",
      "bra laces.\n",
      "\n",
      "prompt 2: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man stands and\n",
      "walks back across the lawn.\n",
      "\n",
      "prompt 3: Cutting the grass: A man mows his lawn while being filmed from a high angle in a\n",
      "sped-up video. The man walks across the mowed lawn and sits on a bench. The man plays a\n",
      "large keyboard, strumming on the keys and pausing intermittently.\n",
      "\n",
      "----------- item: 449 ------------\n",
      "Query: Playing violin: A man is standing on a stage playing a violin. A man\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Playing violin: A man is standing on a stage playing a violin. A man is standing\n",
      "behind him in a tuxedo.\n",
      "\n",
      "prompt 1: Playing violin: A man is standing on a stage playing a violin. A man comes and\n",
      "stands on the stage next to him.\n",
      "\n",
      "prompt 2: Playing violin: A man is standing on a stage playing a violin. A man in a\n",
      "position of power is walking on stage with the violin.\n",
      "\n",
      "prompt 3: Playing violin: A man is standing on a stage playing a violin. A man is\n",
      "performing a song on a flute.\n",
      "\n",
      "============= winograd =============\n",
      "\n",
      " This is a schema task type. Each item will be scored as correct if the \"continuation\n",
      "part\" with the highest probability is in the correct prompt. This is similar to multiple\n",
      "choice except here we have a common suffix (the continuation) and in multiple choice we\n",
      "have a common prefix (the query). The continuations are the same in each prompt so in\n",
      "isolation they would have the same probability. The key is they are judged in the context\n",
      "of the full prompt. It's also important that we look at the probabilities only of the\n",
      "continuation parts, because we're interested in which is most probable in the given\n",
      "context, not which prompt overall is more likely.\n",
      "\n",
      "Showing 2 random items of 273\n",
      "\n",
      "----------- item: 186 ------------\n",
      "Continuation part: were very much in the minority.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: When the sponsors of the bill got to the town hall, they were surprised to find\n",
      "that the room was full of opponents. The sponsors were very much in the minority.\n",
      "\n",
      "prompt 1: When the sponsors of the bill got to the town hall, they were surprised to find\n",
      "that the room was full of opponents. The opponents were very much in the minority.\n",
      "\n",
      "----------- item: 238 ------------\n",
      "Continuation part: a sympathetic look.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: When Tommy dropped his ice cream, Timmy giggled, so father gave Tommy a\n",
      "sympathetic look.\n",
      "\n",
      "prompt 1: When Tommy dropped his ice cream, Timmy giggled, so father gave Timmy a\n",
      "sympathetic look.\n",
      "\n",
      "============= winogrande =============\n",
      "\n",
      " This is a schema task type. Each item will be scored as correct if the \"continuation\n",
      "part\" with the highest probability is in the correct prompt. This is similar to multiple\n",
      "choice except here we have a common suffix (the continuation) and in multiple choice we\n",
      "have a common prefix (the query). The continuations are the same in each prompt so in\n",
      "isolation they would have the same probability. The key is they are judged in the context\n",
      "of the full prompt. It's also important that we look at the probabilities only of the\n",
      "continuation parts, because we're interested in which is most probable in the given\n",
      "context, not which prompt overall is more likely.\n",
      "\n",
      "Showing 2 random items of 1,267\n",
      "\n",
      "----------- item: 652 ------------\n",
      "Continuation part: didn't want the credit.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Natalie committed a bunch of good deeds in Samantha's name, because Natalie\n",
      "didn't want the credit.\n",
      "\n",
      "prompt 1: Natalie committed a bunch of good deeds in Samantha's name, because Samantha\n",
      "didn't want the credit.\n",
      "\n",
      "----------- item: 778 ------------\n",
      "Continuation part: was a mean person.\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: More people attended the funeral of Joel than of Derrick because Joel was a mean\n",
      "person.\n",
      "\n",
      "prompt 1: More people attended the funeral of Joel than of Derrick because Derrick was a\n",
      "mean person.\n",
      "\n",
      "============= bigbench_dyck_languages =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 1,000\n",
      "\n",
      "----------- item: 433 ------------\n",
      "Expected continuation: ) } )\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed\n",
      "properly.   Input: ( { ( Output:\n",
      "\n",
      "----------- item: 913 ------------\n",
      "Expected continuation: ) }\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed\n",
      "properly.   Input: { ( ( ) ( < [ < ( ) > ] > ) { } ( { [ ] } ) Output:\n",
      "\n",
      "============= agi_eval_lsat_ar =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 230\n",
      "\n",
      "----------- item: 226 ------------\n",
      "Query: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: A\n",
      "\n",
      "prompt 1: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: B\n",
      "\n",
      "prompt 2: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: C\n",
      "\n",
      "prompt 3: Passage: Three real estate companies—RealProp, Southco, and Trustcorp—are\n",
      "considering trading buildings with one another. Each building they own is categorized as\n",
      "either class 1, class 2, or class 3, depending on its approximate value: RealProp owns the\n",
      "Garza Tower (class 1), the Yates House (class 3), and the Zimmer House (class 3). Southco\n",
      "owns the Flores Tower (class 1) and the Lynch Building (class 2). Trustcorp owns the King\n",
      "Building, the Meyer Building, and the Ortiz Building, all of which are class 2. Each trade\n",
      "must be of exactly one of the following three kinds: Trading one building for one other\n",
      "building of the same class Trading one class 1 building for two class 2 buildings Trading\n",
      "one class 2 building for two class 3 buildings Q: Which one of the following CANNOT be\n",
      "true, no matter how many trades are made? Choices: A.) The buildings owned by Trustcorp\n",
      "are the Flores Tower and the Ortiz Building. B.) The buildings owned by Southco are the\n",
      "Flores Tower and the Meyer Building. C.) The buildings owned by Southco are the Garza\n",
      "Tower and the Lynch Building. D.) The buildings owned by RealProp are the Flores Tower and\n",
      "the Garza Tower. Answer: D\n",
      "\n",
      "----------- item: 134 ------------\n",
      "Query: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: A\n",
      "\n",
      "prompt 1: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: B\n",
      "\n",
      "prompt 2: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: C\n",
      "\n",
      "prompt 3: Passage: A naturalist will give five lectures, each on a different type of bird:\n",
      "oystercatchers, petrels, rails, sandpipers, or terns. The lectures must be given in either\n",
      "Gladwyn Hall or Howard Auditorium, in an order that meets the following conditions: The\n",
      "first lecture is in Gladwyn Hall. The fourth lecture is in Howard Auditorium. Exactly\n",
      "three of the lectures are in Gladwyn Hall. The lecture on sandpipers is in Howard\n",
      "Auditorium and is given earlier than the lecture on oystercatchers. The lecture on terns\n",
      "is given earlier than the lecture on petrels, which is in Gladwyn Hall. Q: Which one of\n",
      "the following must be false? Choices: A.) The third and fourth lectures are both in Howard\n",
      "Auditorium. B.) The third and fifth lectures are both in Gladwyn Hall C.) The second and\n",
      "fifth lectures are both in Gladwyn Hall. D.) The second and third lectures are both in\n",
      "Howard Auditorium. Answer: D\n",
      "\n",
      "============= bigbench_cs_algorithms =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 1,320\n",
      "\n",
      "----------- item: 336 ------------\n",
      "Expected continuation: Invalid\n",
      "\n",
      "prompt 0: Determine whether the given sequence of parentheses is properly matched.\n",
      "Sequence: ( } [ ] Valid/Invalid?\n",
      "\n",
      "----------- item: 1147 ------------\n",
      "Expected continuation: Invalid\n",
      "\n",
      "prompt 0: Determine whether the given sequence of parentheses is properly matched.\n",
      "Sequence: } ) } } ) [ ) ( ] } ) } } } ) ) ) } ) } Valid/Invalid?\n",
      "\n",
      "============= bigbench_operators =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 210\n",
      "\n",
      "----------- item: 45 ------------\n",
      "Expected continuation: 90\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result. i op j suppresses\n",
      "the j first digits of i. 125690 op 4 =\n",
      "\n",
      "----------- item: 60 ------------\n",
      "Expected continuation: 1\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result. op i is the ith odd\n",
      "positive number. op 1 =\n",
      "\n",
      "============= bigbench_repeat_copy_logic =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 32\n",
      "\n",
      "----------- item: 14 ------------\n",
      "Expected continuation: a woodchuck chucks five pounds of wood a woodchuck chucks two tons\n",
      "of wood\n",
      "\n",
      "prompt 0: repeat with logic:  Q: repeat a woodchuck chucks lots of wood two times, but\n",
      "replace lots with five pounds the first time and two tons the second time A:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: python python data python python data python python data\n",
      "\n",
      "prompt 0: repeat with logic:  Q: say python twice and data once, and then repeat all of\n",
      "this three times. A:\n",
      "\n",
      "============= squad =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 10,570\n",
      "\n",
      "----------- item: 2895 ------------\n",
      "Expected continuation: BBC HD\n",
      "\n",
      "prompt 0: Context: Virgin Media (re-branded in 2007 from NTL:Telewest) started to offer a\n",
      "high-definition television (HDTV) capable set top box, although from 30 November 2006\n",
      "until 30 July 2009 it only carried one linear HD channel, BBC HD, after the conclusion of\n",
      "the ITV HD trial. Virgin Media has claimed that other HD channels were \"locked up\" or\n",
      "otherwise withheld from their platform, although Virgin Media did in fact have an option\n",
      "to carry Channel 4 HD in the future. Nonetheless, the linear channels were not offered,\n",
      "Virgin Media instead concentrating on its Video On Demand service to carry a modest\n",
      "selection of HD content. Virgin Media has nevertheless made a number of statements over\n",
      "the years, suggesting that more linear HD channels are on the way. Question: What was the\n",
      "one linear HD channel Virgin Media carried from November 2006 to July 2009? Answer:\n",
      "\n",
      "----------- item: 5327 ------------\n",
      "Expected continuation: about three\n",
      "\n",
      "prompt 0: Context: Train operator Virgin Trains East Coast provides a half-hourly\n",
      "frequency of trains to London King's Cross, with a journey time of about three hours,\n",
      "these services call at Durham, Darlington, York, Doncaster, Newark North Gate and\n",
      "Peterborough and north to Scotland with all trains calling at Edinburgh and a small number\n",
      "of trains extended to Glasgow, Aberdeen and Inverness. CrossCountry trains serve\n",
      "destinations in Yorkshire, the Midlands and the South West. First TransPennine Express\n",
      "operates services to Manchester and Liverpool. Northern Rail provides local and regional\n",
      "services. Question: How many hours can one expect to ride the train from Newcastle to\n",
      "King's Cross? Answer:\n",
      "\n",
      "============= coqa =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored as correct if the model\n",
      "generates the expected continuation.\n",
      "\n",
      "Showing 2 random items of 7,983\n",
      "\n",
      "----------- item: 1422 ------------\n",
      "Expected continuation: No\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the\n",
      "final question by referring to the story and the previous questions. Story: A little boy\n",
      "named Joey did not like to brush his teeth. One day, as his mother asked him to brush his\n",
      "teeth, Joey said, \"I don't want to! It's gross and a waste of time!\" In response, Joey's\n",
      "mom told him that in order for him to grow up and be big and strong like she is, he must\n",
      "brush his teeth. In disgust, with his mom watching to make sure he brushed his teeth\n",
      "right, he brushed his teeth and whined until he was finished. The next day, Joey did not\n",
      "brush his teeth and told his mom that he did. After going to school, one of Joey's friends\n",
      "said that his breath stunk and began to make fun of him. This made Joey very angry, so he\n",
      "pushed the boy over and started to cry. A teacher came over and called the principal as\n",
      "both kids were being loud and starting a fight. Joey's mom came to school and took him\n",
      "home. After asking what was wrong, Joey told his mom that he didn't brush his teeth. After\n",
      "hearing this, his mom marched him up to the bathroom and forced him to brush his teeth.\n",
      "\"You won't get teased if you brush your teeth, Joey!\" she yelled, and then left the room.\n",
      "From that day forward, Joey brushed his teeth every day so the other kids wouldn't make\n",
      "fun of him. Preceding questions: Question: What was the boys name? Answer: Joey Question:\n",
      "Joey didn't like to what? Answer: brush his teeth Question: Who asked him to one day?\n",
      "Answer: His mother  Final question: Question: Did he cooperate? Answer:\n",
      "\n",
      "----------- item: 1119 ------------\n",
      "Expected continuation: Just realized some horrible things about his father.\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the\n",
      "final question by referring to the story and the previous questions. Story: (EW.com ) --\n",
      "When last we saw Olivia Pope, she was flying off into the sunset (or maybe just flying\n",
      "off) with Jake by her sideâ€”leaving behind a broken Fitz, who'd just lost his son and\n",
      "realized some horrible things about his father. D.C. now rests in the hands of Olivia's\n",
      "own father, who once again took over B-613. So what awaits the team when season four\n",
      "starts up? How about a new face?   EW has confirmed that \"Arrested Development\" star\n",
      "Portia de Rossi will join \"Scandal's\" fourth season for a multiple-episode arc. Ellen\n",
      "DeGeneres first announced the news on Twitter, bragging about her wife's \"top secret\"\n",
      "storyline.   Could the words \"top secret\" mean de Rossi is a part of Papa Pope's B-613\n",
      "army? We'll find out Thursday, Sept. 25 at 9 p.m., when Scandal returns on ABC.   ABC fall\n",
      "premiere dates announced: 'Scandal' returns   Former 'Scandal' actor Columbus Short\n",
      "arrested in Texas   See the original story at EW.com   CLICK HERE to Try 2 RISK FREE\n",
      "issues of Entertainment Weekly   © 2011 Entertainment Weekly and Time Inc. All rights\n",
      "reserved.    Preceding questions: Question: What leading female character is flying off?\n",
      "Answer: Olivia Pope Question: Who was she with? Answer: Jake Question: Who was she\n",
      "leaving? Answer: Fitz Question: Was he okay? Answer: No Question: Why not? Answer: He'd\n",
      "just lost his son  Final question: Question: What else? Answer:\n",
      "\n",
      "============= boolq =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 3,270\n",
      "\n",
      "----------- item: 2089 ------------\n",
      "Query: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Answer: no\n",
      "\n",
      "prompt 1: Passage: The Tower of Terror buildings are among the tallest structures found at\n",
      "their respective Disney resorts. At 199 feet (60.7 m), the Florida version is the second\n",
      "tallest attraction at the Walt Disney World Resort, with only Expedition Everest 199.5\n",
      "feet (60.8 m) being taller. At the Disneyland Resort, the 183-foot (55.8 m) structure\n",
      "(which now houses Guardians of the Galaxy -- Mission: Breakout!) is the tallest building\n",
      "at the resort, as well as one of the tallest buildings in Anaheim. At Disneyland Paris, it\n",
      "is the second tallest attraction. Question: does disney world still have tower of terror?\n",
      "Answer: yes\n",
      "\n",
      "----------- item: 1473 ------------\n",
      "Query: Passage: 85% of MGN cases are classified as primary membranous glomerulonephritis--\n",
      "that is to say, the cause of the disease is idiopathic (of unknown origin or cause). This\n",
      "can also be referred to as idiopathic membranous nephropathy. One study has identified\n",
      "antibodies to an M-type phospholipase A receptor in 70% (26 of 37) cases evaluated. Other\n",
      "studies have implicated neutral endopeptidase and cationic bovine serum albumin as\n",
      "antigens. Question: is membranous nephropathy the same as membranous glomerulonephritis?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: 85% of MGN cases are classified as primary membranous\n",
      "glomerulonephritis--that is to say, the cause of the disease is idiopathic (of unknown\n",
      "origin or cause). This can also be referred to as idiopathic membranous nephropathy. One\n",
      "study has identified antibodies to an M-type phospholipase A receptor in 70% (26 of 37)\n",
      "cases evaluated. Other studies have implicated neutral endopeptidase and cationic bovine\n",
      "serum albumin as antigens. Question: is membranous nephropathy the same as membranous\n",
      "glomerulonephritis? Answer: no\n",
      "\n",
      "prompt 1: Passage: 85% of MGN cases are classified as primary membranous\n",
      "glomerulonephritis--that is to say, the cause of the disease is idiopathic (of unknown\n",
      "origin or cause). This can also be referred to as idiopathic membranous nephropathy. One\n",
      "study has identified antibodies to an M-type phospholipase A receptor in 70% (26 of 37)\n",
      "cases evaluated. Other studies have implicated neutral endopeptidase and cationic bovine\n",
      "serum albumin as antigens. Question: is membranous nephropathy the same as membranous\n",
      "glomerulonephritis? Answer: yes\n",
      "\n",
      "============= bigbench_language_identification =============\n",
      "\n",
      "This is multiple choice so each item will be scored as correct if the choice with the\n",
      "highest probabiliy matches the correct choice. To get into the mechanics a bit more, it's\n",
      "really only the probabilities  of the \"choice part\" that are looked at for each of the n\n",
      "\"prompts\". The \"choice part\" is what comes after the query. The query is repeated in each\n",
      "prompt, forming a common prefix. Think about it as which choice has the highest\n",
      "probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "Showing 2 random items of 10,000\n",
      "\n",
      "----------- item: 8417 ------------\n",
      "Query: Given a sentence, select the correct language among the choices Sentence: Tiagtai\n",
      "ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices Sentence:\n",
      "Tiagtai ni chichajak: A. Aguaruna B. Faiwol C. Kotava D. Konkomba Answer:  D\n",
      "\n",
      "----------- item: 9173 ------------\n",
      "Query: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices Sentence: Hidi\n",
      "Belet be Wain ime u nayaneeŋ tuwot, Wapum’walaŋ kumuŋŋiŋ diniŋ Mede Momooŋ u yohauta\n",
      "hatilune udaneeŋ baak. A. Sichuan Yi B. Fulfulde Macrolanguage C. Hindi D. Numangan\n",
      "Answer:  D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_examples(items_per_task=2, random_items=True, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814e4a49-dc57-4fec-a320-b1bfb40d1779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f9ede80-6eeb-4def-84f3-ad19aa72768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "from my_nanochat.my_core_eval import render_prompts_mc, render_prompts_lm, render_prompts_schema\n",
    "\n",
    "def print_wrap(s, remove_newlines=False):\n",
    "    if remove_newlines:\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "    print(textwrap.fill(s, 90))\n",
    "\n",
    "def print_examples(items_per_task=3, random_items=False, random_seed=None):\n",
    "\n",
    "    if random_seed:\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    tasks = config['icl_tasks']\n",
    "    for task in tasks:\n",
    "        task_type = task['icl_task_type']\n",
    "        continuation_delimiter = task.get('continuation_delimiter', ' ')\n",
    "        data_path = os.path.join(data_base_path, task['dataset_uri'])\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "        \n",
    "        print(f\"============= {task['label']} =============\\n\")\n",
    "    \n",
    "        if task_type == 'multiple_choice':\n",
    "            print_wrap(\n",
    "\"\"\"This is multiple choice so each item will be scored as correct if the choice with the highest probabiliy\n",
    "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
    "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
    "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
    "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
    "\"\"\", remove_newlines=True)\n",
    "        elif task_type == 'language_modeling':\n",
    "            print_wrap(\n",
    "\"\"\"This is a language modeling task type. Each item will be scored as correct if the model generates\n",
    "the expected continuation.\n",
    "\"\"\", remove_newlines=True)\n",
    "        elif task_type == 'schema':\n",
    "             print_wrap(\"\"\"\n",
    "This is a schema task type. Each item will be scored as correct if the \"continuation part\" with the\n",
    "highest probability is in the correct prompt. This is similar to multiple choice except here we\n",
    "have a common suffix (the continuation) and in multiple choice we have a common prefix (the query).\n",
    "The continuations are the same in each prompt so in isolation they would have the same probability.\n",
    "The key is they are judged in the context of the full prompt. It's also important that we look at\n",
    "the probabilities only of the continuation parts, because we're interested in which is most probable\n",
    "in the given context, not which prompt overall is more likely.\n",
    "\"\"\", remove_newlines=True)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        print()\n",
    "\n",
    "        if random_items:\n",
    "            print(f\"Showing {items_per_task} random items of {len(data):,d}\\n\")\n",
    "        else:\n",
    "            print(f\"Showing the first {items_per_task} items of {len(data):,d}\\n\")\n",
    "\n",
    "        \n",
    "        for i in random.sample(range(0, len(data)), items_per_task) if random_items else range(items_per_task):\n",
    "            print(f\"----------- item: {i} ------------\")\n",
    "            item = data[i]\n",
    "            if task_type == 'multiple_choice':\n",
    "                print_wrap(f\"Query: {item['query']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\") \n",
    "                prompts = render_prompts_mc(item, continuation_delimiter, [])\n",
    "            elif task_type == 'language_modeling':\n",
    "                print_wrap(f\"Expected continuation: {item['continuation']}\")\n",
    "                print()\n",
    "                prompts = render_prompts_lm(item, continuation_delimiter, [])\n",
    "                prompts = prompts[:-1] # because in CORE eval we only use the first method of scoring\n",
    "            elif task_type == 'schema':\n",
    "                print_wrap(f\"Continuation part: {item['continuation']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\")            \n",
    "                prompts = render_prompts_schema(item, continuation_delimiter, [])\n",
    "            else:\n",
    "                assert False\n",
    "    \n",
    "            for j, prompt in enumerate(prompts):\n",
    "                print_wrap(f\"prompt {j}: {prompt}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f99e3-7510-4952-ae8c-4cbc172ba7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
